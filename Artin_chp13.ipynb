{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ex 13.1.3**   \n",
    "let $R$ be ring which is an integral domain that contains $\\mathbb F$ as a sub-ring and may be represented as an $n$ dimensional vector space over $\\mathbb F$.  (Note: this is an actual vector space over a field not an arbitrary module.)  Prove that $R$ is a field.  \n",
    "\n",
    "*remark*  \n",
    "This should remind us of ex 10.6.2 which was to prove that an integral domain with finitely many elements is a field.  Here we have an integral domain that may have infinitely many elements, but it is still finite in some 'nice' sense -- in particular it can be interpreted as a finite dimensional vector space.  \n",
    "\n",
    "*proof:*   \n",
    "The idea is to prove that every $r\\in R-\\big\\{0\\big\\}$ has an inverse  \n",
    "\n",
    "fix $r\\in R-\\big\\{0\\big\\}$ and selecting an arbitrary element in $v\\in R$, we have  \n",
    "\n",
    "$r\\cdot v = v'$   \n",
    "$r\\cdot\\big( v + v'\\big) = r\\cdot v + r\\cdot v'$  and  \n",
    "$r\\cdot\\big( \\alpha v\\big) =\\alpha \\cdot \\big(r\\cdot v\\big)$   \n",
    "\n",
    "so $r$ is a linear operator on this vector space $V$.  Since the domain is integral, we have  \n",
    "\n",
    "$\\dim\\ker\\big(r\\big) = 0$  \n",
    "and by rank nullity,  \n",
    "$\\dim\\text{im}\\big(r\\big) = n$  \n",
    "\n",
    "hence $r$ is both injective and surjective and thus invertible  \n",
    "\n",
    "*alternatively*  \n",
    "for a more granular approach, define a basis for this finite dim vector space    \n",
    "$\\mathbf B:=\\bigg[\\begin{array}{c|c|c|c} v_1 & v_2 &\\cdots & v_{d} \\end{array}\\bigg]$  \n",
    "we then model this forward and run it backward to solve for the inverse of arbitrary $r\\in R-\\big\\{0\\big\\}$  \n",
    "\n",
    "first:  \n",
    "For arbitrary $v\\in \\mathbb R$ which is a vector space $V$ over $\\mathbb F$  we have   \n",
    "$v = \\mathbf {Bx}$   and  \n",
    "$\\mathbf B\\mathbf y = v' = r\\cdot v = r\\cdot \\big(\\mathbf B\\mathbf x\\big)= \\big(r\\cdot \\mathbf B\\big) \\mathbf x = \\big(\\mathbf B A\\big) \\mathbf x =\\mathbf B  \\big(A \\mathbf x\\big)$   \n",
    "since the basis uniquely characterizes all elements in the vector space, the problem reduces to the familiar  \n",
    "$A\\mathbf x = \\mathbf y$   \n",
    "\n",
    "second:  \n",
    "running this backwards,  \n",
    "we specialize to have $v':=1 \\in R$  (though any unit in R will do) thus  \n",
    "$1 = \\mathbf B\\mathbf y$ -- so $\\mathbf y$ is the coordinate representation of this unit with respect to our basis.  The question becomes can we solve  \n",
    "$A\\mathbf x = \\mathbf y$   \n",
    "and of course the answer is yes, because $A$ has a trivial kernel (by integrality of our ring and uniqueness of basis representation, we know $r\\cdot v = 0$ *iff* $v=0$ which is equivalent to $A\\mathbf x = \\mathbf 0$ *iff* $\\mathbf x =0$ ) \n",
    "\n",
    "so $A$ has non-zero determinant (which takes on a value in $\\mathbb F$ and hence is a unit)  and   \n",
    "$\\mathbf x = A^{-1}\\mathbf y$   \n",
    "which is unique.   \n",
    "\n",
    "Working backwards, we have   \n",
    "$r \\cdot v = r\\cdot \\mathbf B\\mathbf x = \\big(r\\cdot \\mathbf B\\big)\\big(A^{-1}\\mathbf y\\big)= \\big(\\mathbf BA\\big)A^{-1}\\mathbf y =\\mathbf B\\mathbf y = 1$   \n",
    "hence we have found the multiplicative inverse of $r$ and $r$ is invertible.  But the choice of $r\\in R-\\big\\{0\\big\\}$ was arbitrary so all non-zero elements in $R$ are invertible and $R$ is a field.   \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ex 13.2.2**  \n",
    "where $\\mathbb F[\\alpha]$ is the result of applying the substitution homomorphism $\\phi$ given by $x\\mapsto \\alpha$ to $F[x]$ where $\\alpha\\notin \\mathbb F$ (but exists in some field extension of $\\mathbb F$) and $\\alpha$ is a root of a degree $d$ monic polynomial $g\\in F[x]$  but $\\alpha$ is not a root of a lower degree polynomial in $\\mathbb F[x]$. \n",
    "\n",
    "prove:  \n",
    "$\\mathbf B := \n",
    "\\bigg[\\begin{array}{c|c|c|c|c} \n",
    "1  & \\alpha & \\alpha^2&\\cdots &\\ \\alpha^{d-1} \n",
    "\\end{array}\\bigg]$  \n",
    "is a basis for $\\mathbb F[\\alpha]$  \n",
    "\n",
    "*proof:*  \n",
    "*i.)*  linear independence with coefficients in $\\mathbb F$     \n",
    "$\\mathbf B\\mathbf v=\\mathbf 0\\implies \\mathbf v = \\mathbf 0$   \n",
    "writing this out:  \n",
    "$v_0\\cdot 1  + v_1\\cdot \\alpha + v_2\\cdot \\alpha^2 +...+ v_{d-1}\\cdot \\alpha^{d-1}  = 0$   \n",
    "which is a polyonomial of degree $\\leq d-1$ in $\\mathbb F[x]$ under the image of $\\phi$.   \n",
    "$\\implies 0=v_0=v_1=...=v_{d-1}$ because there is no (non zero) polynomial with degree $\\leq d$ in $\\mathbb F[x]$ that has $\\alpha$ as a root.  \n",
    "\n",
    "*ii.)* span  \n",
    "for reasons of linearity it is enough to prove this for a monomial $\\beta \\cdot \\alpha^k$.  If $k\\leq d-1$ the result immediately follows.  If $k=d$ then the fact that $f(\\alpha)=0$ gives us an explicit recurrence, to write $\\alpha^d$ as a linear combination of strictly lower order terms (just examine  both sides of  $\\alpha^d -f(\\alpha)=0+-\\alpha^d$) and each term is a scalar multiple of some vector in $\\mathbf B$.  If $k\\gt d$ then by applying the above recurrence, $\\alpha^k$ may be written as a linear combination of strictly lower order terms and by strong inductive hypothesis each of them is a linear combination of vectors in $\\mathbf B$.  Thus $\\beta\\cdot \\alpha^k \\in \\text{span}\\big(\\mathbf B\\big)$ for arbitrary $k$.  \n",
    "\n",
    "Thus $\\mathbf B$ forms a basis.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**corollary**  \n",
    "\n",
    "*lemma:*  \n",
    "consider $\\mathbb F\\subset \\mathbb K$ and the polynomial ring $\\mathbb F[x]$.  If $\\alpha \\in \\mathbb K$ but $\\alpha \\notin \\mathbb F$ is not a root of any polynomial $g$ of degree $\\leq d-1$ in $\\mathbb F[x]$, then for $p_1,p_2 \\in \\mathbb F[x]$, each with degree $\\leq d-1$, then  \n",
    "$\\phi\\big(p_1p_2\\big)\\neq 0$  \n",
    "where $\\phi$ is the substitution homomorphism $x\\mapsto \\alpha$  \n",
    "\n",
    "*proof:*  \n",
    "$\\phi\\big(p_1p_2\\big)=\\phi\\big(p_1\\big)\\phi\\big(p_2\\big)= \\beta_1\\cdot \\beta_2 \\neq 0$  \n",
    "where the first equality follows because this is a homomorphism and we know $\\beta_1, \\beta_2 \\in \\mathbb K-\\{0\\}$    because $\\beta_1, \\beta_2 \\in \\mathbb K$  by construction and $\\phi\\big(p_1\\big)\\neq 0$, $\\phi\\big(p_2\\big) \\neq 0$ since $\\alpha$ is not a root of either $p_1$ or $p_2$  \n",
    "\n",
    "*main argument:*   \n",
    "$\\mathbb F[\\alpha]$ is a field.   \n",
    "(This is is proven by very different means in prop 2.6a in the text\\.)   \n",
    "combining *ex 13.1.3* and *ex 13.2.2* tells us that $\\mathbb F[\\alpha]$ is a field so long as we confirm that the ring $R:=\\mathbb F[\\alpha]$ is an integral domain. To confirm integrality it suffices to prove that $\\mathbb F[\\alpha]$ has no zero divisors.  \n",
    "\n",
    "While not strictly needed, we can streamline the proof with an argument by contradiction.  \n",
    "i.e. suppose $r_1, r_2 \\in \\mathbb F[\\alpha]-\\{0\\}$ and $r_1\\cdot r_2=0$.  \n",
    "\n",
    "Then by the span argument in ex 13.2.2, each of $r_1$ and $r_2$ may be written as a linear combination of $\\alpha^k$ for $k\\in\\{0,1,2,...,d-1\\}$ which implies each or $r_1,r_2$ have degree $\\leq d-1$. Each of $r_1=\\phi\\big(p_1\\big)$ and $r_2=\\phi\\big(p_2\\big)$ for some degree at most $d-1$ polynomials $p_1,p_2\\in \\mathbb F[x]$ (just match the coefficients in $\\mathbb F$ to find them).  But this contradicts the lemma.  Hence $\\mathbb F[\\alpha]$ is an integral domain and thus a field.  \n",
    "\n",
    "*extension*  \n",
    "By the fundamental homomorphism theorem, $\\mathbb F[x]/(g) \\cong \\mathbb F[\\alpha]$ and since the RHS is a field, so is the LHS. By applying 10.7.3a (page 371) $\\mathbb F[x]/(g) \\text{ is a field }\\implies (g)\\text{ is a maximal ideal }$.  With some care, this should imply that every ideal in $\\mathbb F[x]$ is principal which is a chapter 11 (and perhaps chapter 10) result proven via rather different means.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ex 13.2.4**  \n",
    "\n",
    "consider irreducible polynomial (over $\\mathbb Q$)   \n",
    "$x^3-3x +4$   \n",
    "with complex root $\\alpha$, which means $\\alpha^3=3\\alpha -4$  \n",
    " \n",
    "$\\mathbf B := \n",
    "\\bigg[\\begin{array}{c|c|c|c|c} \n",
    "\\mathbf 1  & \\alpha & \\alpha^2 \n",
    "\\end{array}\\bigg]$   \n",
    "$r:= (\\alpha^2+\\alpha+1)$   \n",
    "thus   \n",
    "$r\\cdot \\mathbf B = \\mathbf BA = \\mathbf B \\displaystyle \\left[\\begin{matrix}1 & -4 & -4\\\\1 & 4 & -1\\\\1 & 1 & 4\\end{matrix}\\right]$   \n",
    "\n",
    "$r^{-1}\\mathbf B = \\mathbf B A^{-1}=\\mathbf B\\displaystyle \\left[\\begin{matrix}\\frac{17}{49} & \\frac{12}{49} & \\frac{20}{49}\\\\- \\frac{5}{49} & \\frac{8}{49} & - \\frac{3}{49}\\\\- \\frac{3}{49} & - \\frac{5}{49} & \\frac{8}{49}\\end{matrix}\\right] $   \n",
    "\n",
    "see comments in code below.  We can read off $r^{-1}$ by looking at the first column of $A^{-1}$  \n",
    "**note**  \n",
    "$(\\alpha^2+\\alpha+1)\\cdot \\alpha^2 = \\alpha^4+\\alpha^3 +\\alpha^2$ but this needs reduced to terms with degree $\\leq 2$.  We can do this by hand.  However, we may also consider modeling the irreducible polynomial with a Companion Matrix $C$ and with suitable modelling, multiply by $C^{-1}$ twice, i.e. multiplying by $C^{-2}$.  Also note that up to a sign, the determinant of the Companion matrix is given by the constant term of our irreducible polynomial, which is non-zero since the polynomial is irreducible (and 0 exists in any ring so we cannot factor it out).  Thus $C^{-1}$ must exist.     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}\\frac{17}{49} & \\frac{12}{49} & \\frac{20}{49}\\\\- \\frac{5}{49} & \\frac{8}{49} & - \\frac{3}{49}\\\\- \\frac{3}{49} & - \\frac{5}{49} & \\frac{8}{49}\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "Matrix([\n",
       "[17/49, 12/49, 20/49],\n",
       "[-5/49,  8/49, -3/49],\n",
       "[-3/49, -5/49,  8/49]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sympy as sp  \n",
    "a = sp.Symbol('a')\n",
    "A = sp.Matrix([[1,-4,-4],[1,4,-1],[1,1,4]])\n",
    "B = sp.Matrix([[1,a, a**2]])\n",
    "A.inv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}1 & -4 & -4\\\\1 & 4 & -1\\\\1 & 1 & 4\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "Matrix([\n",
       "[1, -4, -4],\n",
       "[1,  4, -1],\n",
       "[1,  1,  4]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}a^{2} + a + 1 & a^{2} + 4 a - 4 & 4 a^{2} - a - 4\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "Matrix([[a**2 + a + 1, a**2 + 4*a - 4, 4*a**2 - a - 4]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B @ A\n",
    "# to recover linear operator r, read off the first column--where we see the 'one vector' under the image of tau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}- \\frac{3 a^{2}}{49} - \\frac{5 a}{49} + \\frac{17}{49} & - \\frac{5 a^{2}}{49} + \\frac{8 a}{49} + \\frac{12}{49} & \\frac{8 a^{2}}{49} - \\frac{3 a}{49} + \\frac{20}{49}\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "Matrix([[-3*a**2/49 - 5*a/49 + 17/49, -5*a**2/49 + 8*a/49 + 12/49, 8*a**2/49 - 3*a/49 + 20/49]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# so for avoidance of doubt,  \n",
    "B @ A.inv()  \n",
    "# or just read off the first column of A^{-1} which is shown 2 cells above  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ex 13.2.5**   \n",
    "as noted in Harvard solution, we are following a standard approach of attacking the minimal or maximal degree element -- in this case we attack the minimal degree element -- the constant term.  And as noted above the constant term is non-zero.  So write constant in terms of other terms-- which we can do because $\\alpha $ is a root. But RHS has an extra $\\alpha$ so divide each side by $\\alpha$ and by constant term, and we recover the inverse.  \n",
    "\n",
    "a longer approach that uses the same linear algebra techniques from the prior problem is as follows  \n",
    "\n",
    "$\\mathbf B := \n",
    "\\bigg[\\begin{array}{c|c|c|c|c} \n",
    "\\mathbf 1  & \\alpha & \\alpha^2&\\cdots &\\ \\alpha^{n-1} \n",
    "\\end{array}\\bigg]$   \n",
    "$r:= \\alpha$   \n",
    "thus   \n",
    "$r\\cdot \\mathbf B = \\mathbf BA = \\mathbf B \\displaystyle \\left[\\begin{matrix}0 & -c_0\\\\ I_{n-1} & -\\mathbf c \\end{matrix}\\right]$   \n",
    "\n",
    "where $\\mathbf c$ has the other terms of the polynomial from degree $1$ to $n-1$.  \n",
    "$A$ *is* a Companion matrix so $A^{-1}$ has a well known form.  To finish, we merely need to read off the first column, as before.  \n",
    "\n",
    "note: the easiest way to compute the inverse of the Companion Matrix, is, for the first column, apply Cayely Hamilton  \n",
    "\n",
    "\n",
    "$ A^n+c_{n-1}A^{n-1}+...+c_1A+c_0I_n = \\mathbf 0$  \n",
    "$\\implies I_n=\\frac{-1}{c_0}\\Big(A^n+c_{n-1}A^{n-1}+...+c_1A\\Big)$  \n",
    "$\\implies A^{-1}=\\frac{-1}{c_0}\\Big(A^{n-1}+c_{n-1}A^{n-2}+...+c_1I_n\\Big)$  \n",
    "$\\implies A^{-1}\\mathbf e_1=\\frac{-1}{c_0}\\Big(A^{n-1}+c_{n-1}A^{n-2}+...+c_1I_n\\Big)\\mathbf e_1$  \n",
    "and as noted in \"Artin_chp12.ipynb\" under \"The Companion Matrix's minimal polynomial is the same as its characteristic polynomial\", for an $n\\times n$ Companion Matrix, the powers up to degree $n-1$ are linearly independent, which we know simply by examining the first column of the matrix, i.e. \n",
    "$I\\mathbf e_1 = \\mathbf e_1$  \n",
    "and for $k \\in \\{1,2,...,n-1\\}$   \n",
    "$C^k\\mathbf e_1 = \\mathbf e_{k+1}$  \n",
    "$\\implies A^{-1}\\mathbf e_1=\\frac{-1}{c_0}\\Big(A^{n-1}+c_{n-1}A^{n-2}+...+c_1I_n\\Big)\\mathbf e_1=\\frac{-1}{c_0}\\Big(\\mathbf e_n+c_{n-1}\\mathbf e_{n-1}+...+c_1 \\mathbf e_1\\Big)$  \n",
    "which gives the first column of the Companion Matrix's Inverse, as desired.  \n",
    "(with some more thought we see this explicit work-through with Cayley-Hamilton recovers the standard approach (e.g. in Harvard Solution) thought explicitly working with the Companion may be of independent interest.  \n",
    "\n",
    "As an aside we get columns $\\big\\{2,3,...n\\}$ of the Companion Matrix inverse in effect for free by representing the problem slightly differently  \n",
    "\n",
    "$r\\cdot \\mathbf B = \\bigg[\\begin{array}{c|c|c|c|c} \n",
    " \\alpha & \\alpha^2&\\cdots &\\ \\alpha^{n-1} &\\ \\alpha^{n} \n",
    "\\end{array}\\bigg]\\implies \\bigg[\\begin{array}{c|c|c|c|c} \n",
    " \\alpha & \\alpha^2&\\cdots &\\ \\alpha^{n-1} &\\ \\alpha^{n} \n",
    "\\end{array}\\bigg]A^{-1} = \\bigg[\\begin{array}{c|c|c|c|c} \n",
    "\\mathbf 1  & \\alpha & \\alpha^2&\\cdots &\\ \\alpha^{n-1} \n",
    "\\end{array}\\bigg]=\\mathbf B$   \n",
    "so for any column $k\\geq 2$ we merely need to select $A^{-1}\\mathbf e_{k}:=\\mathbf e_{k-1}$   \n",
    "\n",
    "\n",
    "\n",
    "Some below calculations make this explicit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}0 & 0 & 0 & 0 & 0 & 0 & 0 & - c_{0}\\\\1 & 0 & 0 & 0 & 0 & 0 & 0 & - c_{1}\\\\0 & 1 & 0 & 0 & 0 & 0 & 0 & - c_{2}\\\\0 & 0 & 1 & 0 & 0 & 0 & 0 & - c_{3}\\\\0 & 0 & 0 & 1 & 0 & 0 & 0 & - c_{4}\\\\0 & 0 & 0 & 0 & 1 & 0 & 0 & - c_{5}\\\\0 & 0 & 0 & 0 & 0 & 1 & 0 & - c_{6}\\\\0 & 0 & 0 & 0 & 0 & 0 & 1 & - c_{7}\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "Matrix([\n",
       "[0, 0, 0, 0, 0, 0, 0, -c_0],\n",
       "[1, 0, 0, 0, 0, 0, 0, -c_1],\n",
       "[0, 1, 0, 0, 0, 0, 0, -c_2],\n",
       "[0, 0, 1, 0, 0, 0, 0, -c_3],\n",
       "[0, 0, 0, 1, 0, 0, 0, -c_4],\n",
       "[0, 0, 0, 0, 1, 0, 0, -c_5],\n",
       "[0, 0, 0, 0, 0, 1, 0, -c_6],\n",
       "[0, 0, 0, 0, 0, 0, 1, -c_7]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_0 = sp.Symbol('c_0')\n",
    "c_1 = sp.Symbol('c_1')\n",
    "c_2 = sp.Symbol('c_2')\n",
    "c_3 = sp.Symbol('c_3')\n",
    "c_4 = sp.Symbol('c_4')\n",
    "c_5 = sp.Symbol('c_5')\n",
    "c_6 = sp.Symbol('c_6')\n",
    "c_7 = sp.Symbol('c_7')\n",
    "c_8 = sp.Symbol('c_8')\n",
    "c_9 = sp.Symbol('c_9')\n",
    "\n",
    "the_list = [c_0, c_1, c_2, c_3, c_4, c_5, c_6, c_7, c_8, c_9]\n",
    "\n",
    "d = 8\n",
    "# natural number <= 10 \n",
    "\n",
    "assert d<= 10 and type(d)==int\n",
    "\n",
    "C = sp.zeros(d)\n",
    "for j in range(d-1):\n",
    "    C[j+1,j] = 1\n",
    "for i in range(d):\n",
    "    C[i,-1] = -the_list[i]  \n",
    "C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}- \\frac{c_{1}}{c_{0}} & 1 & 0 & 0 & 0 & 0 & 0 & 0\\\\- \\frac{c_{2}}{c_{0}} & 0 & 1 & 0 & 0 & 0 & 0 & 0\\\\- \\frac{c_{3}}{c_{0}} & 0 & 0 & 1 & 0 & 0 & 0 & 0\\\\- \\frac{c_{4}}{c_{0}} & 0 & 0 & 0 & 1 & 0 & 0 & 0\\\\- \\frac{c_{5}}{c_{0}} & 0 & 0 & 0 & 0 & 1 & 0 & 0\\\\- \\frac{c_{6}}{c_{0}} & 0 & 0 & 0 & 0 & 0 & 1 & 0\\\\- \\frac{c_{7}}{c_{0}} & 0 & 0 & 0 & 0 & 0 & 0 & 1\\\\- \\frac{1}{c_{0}} & 0 & 0 & 0 & 0 & 0 & 0 & 0\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "Matrix([\n",
       "[-c_1/c_0, 1, 0, 0, 0, 0, 0, 0],\n",
       "[-c_2/c_0, 0, 1, 0, 0, 0, 0, 0],\n",
       "[-c_3/c_0, 0, 0, 1, 0, 0, 0, 0],\n",
       "[-c_4/c_0, 0, 0, 0, 1, 0, 0, 0],\n",
       "[-c_5/c_0, 0, 0, 0, 0, 1, 0, 0],\n",
       "[-c_6/c_0, 0, 0, 0, 0, 0, 1, 0],\n",
       "[-c_7/c_0, 0, 0, 0, 0, 0, 0, 1],\n",
       "[  -1/c_0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C.inv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "suppose $\\text{char}\\big(\\mathbb K\\big) \\neq 2$, then $q-1$ is even.  So either every (exc $\\lambda_i\\neq 1$) $k_i\\% 2 =0$ or none do.  If we have a mix where some are divisible by 2 and some not, run below argument.  If none are divisible by 2, then $C^{\\frac{q-1}{2}}-I=\\mathbf 0$ which contradicts the fact that the minimal polynomial is the minimal degree monic annihilating polynomial.  If every (exc $\\lambda_i\\neq 1$) is a multiple of 2, then $\\lambda_i^{\\frac{q-1}{2}}\\in \\{-1,1\\}$  (ref ex 13.1.1).   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Linear Algebra proof of Theorem 6.4(c):**  \n",
    "Let $\\mathbb K$ be a field where $\\big \\vert \\mathbb K\\big \\vert = q = p^r =\\text{prime}^r$. Then the multiplicative group $\\mathbb K-\\big\\{0\\big\\}$ is a cyclic group of order $q-1$.  \n",
    "\n",
    "*remark:*  \n",
    "This is proven over the bottom of page 512, through page 513, ultimately using the Structure Theorem for abelian groups, from p.472.  Theorem 6.4(d) has already been proven, earlier on page 512.  Theorem 6.4(d) tells us that the non-zero elements of $\\mathbb K$ are the $q-1$ distinct elements that satisfy $x^{q-1}-1=0$, i.e. they comprise all of the $q-1$ roots of unity that exist in $\\mathbb K$.  We take Theorem 6.4(d) as our starting point.  \n",
    "\n",
    "*proof:*  \n",
    "for **notational** cleanup we define  \n",
    "$r':=q-1=p^r -1$\n",
    "\n",
    "consider $C \\in \\mathbb K^{r'\\times r'}$, defined to be the Companion Matrix associated with the polynomial $x^{r'}-1$.  (For a review of the Companion Matrix, visit the section 'Enter the Companion Matrix' in \"Vandermonde_Matrices_Permutations_and_Discrete_Fourier_Transform.ipynb\".)    \n",
    "\n",
    "now we know  \n",
    "$g\\big(C\\big) = C^{r'}-I = \\mathbf 0$  \n",
    "per Cayley Hamilton, with $g$ being the characteristic polynomial of $C$. Referencing e.g. \"The Companion Matrix's minimal polynomial is the same as its characteristic polynomial\" in \"Artin_chp12.ipynb\" we know that $g$ **is also the minimal polynomial of** $C$.  \n",
    "\n",
    "**Note:** $C$ **is diagonalizable over** $\\mathbb K$ since it is $r'\\times r'$ and has $r'$ distinct eigenvalues (r'th roots of unity) and in fact is diagonalizable by a Vandermonde matrix.   \n",
    "\n",
    "- - - - -   \n",
    "*begin interlude on underlying idea:*  \n",
    "The underlying idea is quite simple.  The Companion matrix here only has 0s and 1s in it, and given that $C^TC = I$ over $\\mathbb Q$, this implies a single 1 in each row (or column) so $C^j$ (for natural number $j$) looks the same over any field-- in particular, it looks the same over $\\mathbb K$ and over $\\mathbb C$. Now the Companion matrix is $r'\\times r'$ and if we look at any field that has a complete set of $r'$ roots of unity, and collect them in a Vandermonde matrix $V$ (with some arbitrary ordering of the roots), then we have  \n",
    "$CV=VD$ and \n",
    "$C^jV =VD^j$   \n",
    "(nit: depending on the convention one uses for defining the Companion matrix vs its transpose, it may instead be $VC =DV$, but this is an immaterial difference that we ignore.)  \n",
    "\n",
    "The idea, then is, aside from arbitrary ordering of the columns of $V$ (equivalently: arbitrariness of labeling the rth roots on unity), the LHS $C^jV$ looks the same over any field that has a complete set of $r'$th roots of unity, thus the RHS must look the same (again ignoring the arbitrariness of the labeling).  Thus if $D^j$ has at least one row without a 1 in it for all $j\\in \\big\\{1,2,...,r-1\\big\\}$ in nice field we geometrically understand -- $\\mathbb C$-- then this still must be true when we switch the field to $\\mathbb K$-- because the $C^jV$   is the same in each case, so $VD^j$ must be the same (again up to reordering).  \n",
    "\n",
    "In and of itself this is awfully close to a proof, though rigorizing it seems to be elusive.  \n",
    "\n",
    "**One possible approach to rigorize this**, would be via a counting argument (and some chapter 5 notions).  In particular if we look at the moment curve / coordinate vector 'slices' of our Vandermonde matrix times the diagonal matrix, i.e. $VD^j$, we can see a collection of $r'$ coordinate vectors glued together.  And with arbitrary labeling, except we ensure that 1 is mapped to one, we may assign an injective (and hence surjective since the set is finite) mapping from each $r'th$ root of unity to one in $\\mathbb K$.  Label the former $\\Omega$ and the latter $\\Omega'$ each containing $r'$ coordinate vectors from the Vandermonde matrix.  Now $\\big\\{I,C, C^2,...,C^{r-1}\\big\\}$ forms a group $G$ with the binary operation of multiplication.  Using chapter 5 notions for abstract symmetry, we *should* be able to consider $g_j\\in G$ (given by $C^j$) operating on $\\Omega$ and $\\Omega'$.  And we *should*, given our invertible mapping and that one is carried to one, be able to say that there is (at least) one element in $\\Omega$ that has stabilizer order of 1, i.e. $\\big \\vert G_{\\omega_1}\\big \\vert = 1$, which should imply (at least) one element $\\Omega'$ with stabilizer order of 1, i.e. $\\big \\vert G_{\\omega_1'}\\big \\vert = 1$.  Since the identity matrix is always a stabilizer in $G$ and stabilizer order is only 1 for these elements, this means the identity matrix *is the only stabilizer* for these elements.  Thus $\\Omega'$ has at least one element $\\omega_1'$ where $(\\omega_1')^{j}\\neq 1$ for all $j\\in\\big\\{1,2,...,r'-1\\big\\}$.  \n",
    "\n",
    "this **still doesn't quite work** as it in effect assumes the existence of something akin to an isomorphism here.  \n",
    "\n",
    "*end interlude*  \n",
    "- - - - -    \n",
    "**main argument**  \n",
    "\n",
    "for each eigenvalue $\\lambda_i$, we know $C^{r'}\\mathbf v_i = \\lambda_i^{r'}\\mathbf v_i= \\mathbf v_i$   \n",
    "Theorem 6.4(d) claims there must be (at least) one $\\lambda_i$ that such that $\\lambda_i^{k_i}\\neq 1$ for all ${k_i}\\in\\{1,2,..,r'-1\\}$.  To prove is true we assume the opposite and argue in an algorithmic manner to yield a **contradiction.**    \n",
    "\n",
    "Since each $\\lambda_i^{r'}=1$ but we have assumed (for contradiction) that there is some ${k_i}\\in\\{1,2,..,r'-1\\}$ such that $\\lambda_i^{k_i}=1$ for each $i\\in\\{1,2,...,r'\\}$, then there is a minimum positive integer $k_i'$ such that $\\lambda_i^{k_i'}=1$ for each eigenvalue.  We list them out in the set $S$. Small algorithmic note: $\\big \\vert S\\big \\vert = z \\lt \\infty$, for some positive integer $z$ since there are finitely many eigenvalues.  \n",
    "\n",
    "Now set $m:=\\max\\big(S\\big)$  \n",
    "Note that by our assumption it must be the case $m\\lt r'$.  The idea will be to create an annihilating polynomial $(x^m-1)\\cdot (x^n-1)$ but neither $(x^m-1)$ nor $(x^n-1)$ annihilates $C$, and then we'll show this results in a contradiction.  \n",
    "\n",
    "Next step: delete every smaller element $s_j\\in S$ such that $m\\%s_j=0$ (i.e. where $m$ is a multiple of $s_j$).  We do this because $\\lambda_i^{s_j}=1\\implies \\lambda_i^m =1$, so these smaller $s_j$ are made redundant by the information that they annihilated by  $(x^m-1)$. Call the set that results after deletion: $S'$.    \n",
    "\n",
    "Now we know $\\big \\vert S'\\big \\vert \\geq 2$. That is, $S'$ has $m$ in it and it must have *at least* one other element-- if it did not, then we'd have  $C^m-I=\\mathbf 0$ which is even lower degree than our minimal polynomial ($C^r-I$), which contradicts the fact that the minimal polynomial is the minimal degree (unique monic) polynomial that annihilates $C$. (Again ref 'Artin_chp12.ipynb'.)       \n",
    "\n",
    "Finally set  \n",
    "$n:= \\prod_{s \\in S'\\neq m}s$, i.e. the product of all elements in $S'$ that are not $n$.   \n",
    "\n",
    "- - - - -  \n",
    "**Base Case:**  if $C^n \\neq I$, then proceed with the main argument (skip recursive case).  \n",
    "\n",
    "- - - - \n",
    "**Recursive Case:** if $C^n =I$, then we recurse, and set our new \n",
    "$S:= S'-\\{m\\}$.  Now assign $m:= \\max(S)$, and create $S'$ by deleting all elements $s_j \\in S$ such that $m\\%s_j = 0$.   And as before $\\big \\vert S'\\big\\vert \\geq 2$ because otherwise we'd have $C^m-I=\\mathbf 0$ with $m\\lt r'$, contradicting the minimal degree of the minimal polynomial. And as before define $n:= \\prod_{s \\in S'\\neq m}s$, i.e. the product of all elements in $S$ that are not $n$.  \n",
    "\n",
    "And we again check, if $C^n-I\\neq \\mathbf 0$, then call on the Base Case.  \n",
    "If $C^n-I= \\mathbf 0$, we recurse once again, once more defining $S:= S'-\\{m\\}$ and so on.  \n",
    "\n",
    "Note that our original level 1 $S$ had finite cardinality of $z$, and each time we recurse to a new level, we have $0\\lt \\big \\vert S_{\\text{new}}\\big \\vert\\leq \\big \\vert S_{\\text{old}}\\big \\vert +1$ since we are deleting at least one element (m) at each recursive call. This means that that the total number of recursions is bounded above by $z\\lt \\infty$. I.e. the algorithm must terminate after finitely many steps.  At each recursive call, we begin by assigning $S:= S'-\\{m\\}$, before this deletion $S'$ had at least 2 elements in it (otherwise we'd contradict minimal degree of minimal polynomial).  And when $S'$ has exactly 2 elements in it, it cannot be the case that $C^n-I =\\mathbf 0$ -- i.e. we cannot recurse any more-- because $n\\lt r'$ (since $n$ is the product of just a single $k_i'\\lt r'$) which again would contradict minimality of the minimal polynomial.  \n",
    "\n",
    "In short we have a recursive (/iterative) algorithm which must, after finitely many recursions(/iterations) terminate to the Base Case or raise a contradiction.  \n",
    "\n",
    "- - - - -   \n",
    "Proceeding with the Base Case:  \n",
    "\n",
    "putting all this together, we have   \n",
    "$\\big(C^n-I\\big)\\big(C^m-I\\big)=\\mathbf 0$  \n",
    "because each and every eigenvector is in the kernel of at least one of those two polynomials (and the eigenvectors of $C$ form a basis).   \n",
    "\n",
    "*one approach to finish would be to now show that $(x^n-1)(x^m-1)$ is not a multiple of $(x^{r'}-1)$ (this is implied by ex 12.7.19 where the minimal polynomial must divide any annihilating polynomial).*  One may be able to show this by directly working out the division algorithm.  However we proceed to show the result more directly via Linear Algebra methods.  \n",
    "\n",
    "First we explicitly note the following:   \n",
    "(i) $C^n\\neq I$      \n",
    "(ii) $C^m\\neq I$     \n",
    "(iii) $C^n\\neq C^m$    \n",
    "\n",
    "(i) holds as it is the defining criterion of our Base Case  \n",
    "(ii) holds because $m\\lt r'$ so if $C^m-I=\\mathbf 0\\implies $ contradiction with minimality of the minimal polynomial  \n",
    "(iii) if this were not true, then we'd have   \n",
    "$\\big(C^m-I\\big)^2 = \\big(C^n-I\\big)\\big(C^m-I\\big)=\\mathbf 0$, yet by (ii) $\\big(C^m-I\\big)\\neq \\mathbf 0$, hence the matrix $\\big(C^m-I\\big)$ would be a non-zero nilpotent matrix which is impossible since  \n",
    "$C\\text{ is diagonalizable }\\implies C^m\\text{ is diagonalizable }\\implies (C^m-I)\\text{ is diagonalizable }$  \n",
    "and non-zero nilpotent matrices are not diagonalizable (either because their minimal polynomial is $(x-0)^k$ for $k\\geq 2$ which breaks the minimal polynomial criterion for diagonalizability, ref ex 12.7.19 in Artin_chp12.ipynb or more simply by 'Artin_chp4.ipynb' we know an $r'\\times r'$ nilpotent matrix over any field has $r'$ eigenvalues of zero, so if the matrix were diagonalizable, then it would be similar to the zero matrix and hence it would be the zero matrix itself.)   \n",
    "\n",
    "\n",
    "*to finish:*   \n",
    "Multiplying out our annihilating polynomial we have  \n",
    "$C^{n+m} - C^n -C^m + I =\\big(C^n-I\\big)\\big(C^m-I\\big)=\\mathbf 0$   \n",
    "taking the trace we have  \n",
    "$\\text{trace}\\Big(C^{n+m}\\Big) - (0) -(0) + (q-1)=\\text{trace}\\Big(C^{n+m}\\Big) - \\text{trace}\\Big(C^n\\Big) -\\text{trace}\\Big(C^m\\Big) + \\text{trace}\\Big(I\\Big) =0$   \n",
    "\n",
    "**a)** referencing 'Permutation Matrices and Periodic Behavior (introducing DFT)' in the Vandermonde notebook, or by inspection, we can see that for positive integer $j$  \n",
    "$\\text{trace}\\Big(C^j\\Big) = 0$ if $j\\% r'\\neq 0$ and $C^{r'} = I_{q-1}$ so if $j\\% r'= 0$, then $\\text{trace}\\Big(C^j\\Big)=\\text{trace}\\Big(C^{r'\\cdot d}\\Big)=\\text{trace}\\Big((C^{r'})^d\\Big)=\\text{trace}\\Big((I_{q-1})^d\\Big)=\\text{trace}\\Big(I_{q-1}\\Big) = q-1$   \n",
    "**b)** in $\\mathbb K$ and in particular its subfield $\\mathbb F_p$, we can see  \n",
    "$r'\\% p=(q-1)\\% p= (p^r)\\%p - (1)\\% p = 0 - 1\\neq 0$ (ref chapter 3 for 'distributivity of the modulo'), hence  \n",
    "$\\text{trace}\\Big(I\\Big) = \\text{trace}\\Big(I_{q-1}\\Big)=q-1=-1\\neq 0$  \n",
    "**c)** putting (a) and (b) together we see, even in $\\mathbb K$:   \n",
    "$\\text{trace}\\Big(C^j\\Big)\\neq 0 \\text{  iff  }    C^j =I$  \n",
    "\n",
    "*so finally*  \n",
    "$\\text{trace}\\Big(C^{n+m}\\Big) + (q-1)=0$  \n",
    "since $\\text{trace}\\Big(C^{n+m}\\Big)$ may only take on two values (per (a)), either:  \n",
    "1.') $\\text{trace}\\Big(C^{n+m}\\Big)=0 \\implies q-1=0$ which contradicts (b)  \n",
    "2.') $\\text{trace}\\Big(C^{n+m}\\Big)=q-1 \\implies 2\\cdot (q-1)=0$.  \n",
    "But per (b), $q-1\\neq 0$ and since a field is an integral domain, this implies $2=0$.  Put differently, we have generated a contradiction for all finite fields $\\mathbb K$ where $\\text{char}\\big(\\mathbb K\\big) \\neq 2$. (It may be helpful to recall that a finite field cannot have characteristic zero because the cyclic subgroup generated by repeatedly adding $1$ has infinite cardinality.)   \n",
    "\n",
    "It remains to deal with the case $\\text{char}\\big(\\mathbb K\\big) = 2$. Since $(1.')$ still yields a contradiction in characteristic 2, it must be the case that $(2.')$ is true, so  \n",
    "$\\text{trace}\\Big(C^{n+m}\\Big)=q-1\\neq 0\\implies C^{n+m} = I$   \n",
    "per (c).  Thus we have     \n",
    "$C^n + C^m =- C^n -C^m + 2I =\\big(I\\big)- C^n -C^m + I = \\big(C^{n+m}\\big) - C^n -C^m + I =\\big(C^n-I\\big)\\big(C^m-I\\big)=\\mathbf 0$   \n",
    "(since $2=0$ or equivalently since $-1=1$ in $\\text{char}\\big(\\mathbb K\\big) = 2$)  \n",
    "'subtracting' $C^m$ from the left and right hand side tells us   \n",
    "$C^n =- C^m = C^m$   \n",
    "which contradicts (iii)  \n",
    "\n",
    "Thus our assumption that for *every* $\\lambda_i$ there is some $k_i\\in \\big\\{1,2,...,r'-1\\}$ such that $\\lambda_i^{k_i}=1$ inevitably results in a contradiction.    \n",
    "- - - -   \n",
    "**alternative finish**  \n",
    "for an alternative finish that works in almost all cases, i.e. for all fields where $\\text{char}\\big(\\mathbb K\\big) \\geq 3$  (this approach is silent on characteristic 2), we can revisit the lines    \n",
    "\n",
    "*Proceeding with the Base Case:*   \n",
    "*putting all this together, we have*      \n",
    "$\\big(C^n-I\\big)\\big(C^m-I\\big)=\\mathbf 0$  \n",
    "\n",
    "since by our Base Case $C^n \\neq I$ and also $C^m \\neq I$ by minimal polynomial argument, we can look at the matrix  \n",
    "\n",
    "$W=\\big(C^n-I\\big)\\big(C^m-I\\big)$ which is claimed to be $=\\mathbf 0$   \n",
    "$\\big(C^n-I\\big)$ is a matrix with *exactly* one +1 and one -1 on each row.  \n",
    "And $\\big(C^m-I\\big)$ is a matrix with *exactly* one +1 and one -1 in each column.  Hence  \n",
    "$w_{i,j} \\in \\big\\{-2,1,0,1,2\\big\\}$   \n",
    "now in a nod to chapter 12, we can call on an idea spiritually similar to Principle of Permanence of Identities.  Since $\\big(C^n-I\\big)$ and $\\big(C^m-I\\big)$ only have -1,0 and +1 in them. \n",
    "\n",
    "So begin by working over $\\mathbb Z$ Now $W=\\mathbf 0$ *iff* $w_{i,j}=0$ for every $i,j \\in \\big\\{1,2,...,r'\\big\\}$.  \n",
    "Since each component is integer valued, between -2, and +2, then whether or not $w_{i,j}= 0$ is preserved when looked at modulo odd prime $p$-- i.e. it also true over $\\mathbb F_p$ for prime $p\\geq 3$. Now if $\\text{char}\\big(\\mathbb K\\big)=p \\geq 3$, then each component of $W$ not only lives in $\\mathbb K$, but in the subfield $\\mathbb F_p$ since the matrices $C$ and $I$ live there.  Thus we can in fact simply evaluate the statement over $\\mathbb Z$ to see whether it holds over $\\mathbb K$.  And of course being if true over $\\mathbb Z$ it is true over $\\mathbb Q$ (the field of fractions that results from the integral domain $\\mathbb Z$).  And of course it holds over $\\mathbb Q$ *iff* it holds over the extension field $\\mathbb C$.  But when working over $\\mathbb C$ we know exactly what the $r'th$ roots of unity look like, and we choose the first one (with smallest non-zero polar angle), and select the associated eigenvector $\\mathbf v_1$. And we get  \n",
    "\n",
    "$W\\mathbf v_1=\\big(C^n-I\\big)\\Big(\\big(C^m-I\\big)\\mathbf v_1\\Big) =(\\lambda^m-1)\\big(C^n-I\\big)\\mathbf v_1= (\\lambda^m-1)(\\lambda^n-1)\\mathbf v_1\\neq 0$   \n",
    "since $\\lambda^n \\neq 1$ and $\\lambda^m\\neq 1$.  Thus by simply testing the supposed identity over $\\mathbb C$ we can verify that this supposed identity is not true over $\\mathbb K$ for odd characteristic.  \n",
    "\n",
    "note:  \n",
    "this sort of test does not have an easy fix for characteristic 2.  And it also does not obviate the need for the Base Case vs Recursive Case setup.  The issue is that $r'=q-1=p^r-1$ can be quite large so if we did not run through the Recursive Case to pare down the number of terms in the annihilating polynomial to two, or something small, we could conceivably just look at an annihilating polynomial by using all components in $S'$ in the original call (recursion level zero), but then we'd potentially look at supposed annihilating polynomial \n",
    "$\\big(C^{k_1}-I\\big)...\\big(C^{k_{r'}}-I\\big)$  \n",
    "which by the same argument of testing over $\\mathbb C$ we observe is false.  However this polynomial has $r$ matrices, each with a +1 and -1 in a given row/column.  And so our claim then is silent on $\\mathbb F_p$ for potentially very large prime p (since the resulting component values would be in $\\big\\{-2r',...,2r'\\big\\}$ we would potentially have to carve out any $p \\leq 2r'$.  It is possible there would be a workaround using some chapter 12 materials on Z modules here-- which would also deal with the characteristic 2 case -- though nothing comes to mind as of the time of writing this.  \n",
    "\n",
    "**corollary**  \n",
    "Any subgroup $H$ of $G:=(\\mathbb K-\\{0\\})^\\times$ is cyclic.  \n",
    "take arbitrary element $h_i\\in H$ and we know this generates a cyclic subgroup of order $k_i\\leq \\big \\vert H\\big \\vert = r^{''}$ (since there are finitely many elements in $H$ and it is closed under products, eventually $h_i^j =h_i^{j'}$, i.e. there is a repeat, which implies $h_i^{j''}=1$ by invertibility).  And in fact by Lagrange's theorem, we know that $k_i\\cdot d_i = r^{''}$.  This holds for arbitrary element in $H$ hence each $h_i^{r^{''}}=1$ and we want to prove that there must be some element in our (sub)group that is not one for all $j$ in $\\big\\{1,2,...,r^{''}\\big\\}$.  But since a subgroup is a group, this is an exact replica of our original problem statement, hence such an $h_i$ must exist and $H$ is therefore cyclic.   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ex 13.6.10**  \n",
    "Let $\\mathbb K$ be a finite field. Prove the the product of the nonzero elements of $\\mathbb K$ is $-1$.  \n",
    "\n",
    "This is Wilson's Theorem and is proven the same way we did in chapter 3 -- look at $\\mathbb K-\\{0\\}$ and pair each non-involutive element with its inverse and multiply them to get 1 and multiplying over all of these pairs.  Then multiply this product by the two involutive elements $\\{-1,1\\}$ to get $-1$.  Over characteristic 2, we only have one involutive element (recall ex 13.1.1), and the product is 1, which is equal to -1 in characteristic 2.  \n",
    "\n",
    "Alternatively, an amusing way to do this, using results from this section, is to note with $\\big \\vert \\mathbb F\\big \\vert =q=p^k$, and observe that with $d:= p^k-1$ we are looking at $d$th roots of unity, i.e. the product of the eigenvalues (i.e. the determinant) of the $d\\times d$ Companion Matrix, $C\\in \\mathbb K^{d\\times d}$ associated with $x^d-1$.  Said matrix is a permutation matrix (hence orthogonal) and thus $\\det\\big(C\\big)\\in\\{-1,1\\}$.   Therefore we already have the result when $\\text{char}\\big(\\mathbb K\\big)=2$.  \n",
    "\n",
    "Now consider $\\text{char}\\big(\\mathbb K\\big)\\geq 3$, i.e. for odd $p$.  Then $d$ is an odd minus one, so $d$ is even.  This implies $\\det\\big(C\\big) =-1$ which is easily confirmable by checking the expansion by minors shown in the Vandermonde matrix notebook or by inspection.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ex 13.6.16**  \n",
    "\n",
    "**(a)** Prove Lemma 6.21 for the case $\\mathbb F:= \\mathbb C$ by looking at the roots of the two polynomials  \n",
    "Lemma 6.21 on p.5.15 states that for natural numbers r,k,s, we have $r=k\\cdot s$ and with prime $p$ we have $q=p^r = p^{ks}$ and $q'=p^k$.   Then $x^{q'}-x =\\big(x-0\\big)\\cdot\\big(x^{p^k-1}-1\\big)=\\big(x-0\\big)\\cdot g_k(x)$ divides $x^q-x =\\big(x-0\\big)\\cdot\\big(x^{p^{ks}-1}-1\\big)=\\big(x-0\\big)\\cdot g_s(x)$    \n",
    "since zero exists in any ring and we merely need to show that $g_k$ divides $g_s$ -- equivalently that $g_s$ lives in the ideal generated by $g_k$ in particular in the polynomial ring $\\mathbb C[x]$  \n",
    "\n",
    "now $d:=\\text{degree}(g_k)= p^k-1$, so $g_k$ is chacterized by the dth roots of unity, of which there are $d$ distinct ones.  \n",
    "\n",
    "taking a hint from the geometric/telescoping finite series in on page 515, we can write \n",
    "\n",
    "\n",
    "$(p^k-1)(p^{ks-k}+p^{ks-k-1}+p^{ks-k-2}+....+ p + 1) = p^{ks}-1$  \n",
    "that is  \n",
    "$(p^k-1)\\cdot h = p^{ks}-1$  \n",
    "\n",
    "so for any dth root of unity $\\omega_i$ we have  \n",
    "$\\omega_i^d =1\\implies (\\omega_i^d)^h =1^h = 1 = \\omega_i^{d\\cdot h}=\\omega_i^{p^{ks}-1}$   \n",
    "Thus these $d$ distinct $\\omega_i$ are also roots of $g_s(x)$.  After factoring out these $d$ roots we see \n",
    "\n",
    "$g_s(x)=g_k\\cdot u(x)$ with remainder zero  \n",
    "\n",
    "\n",
    "**(b)** use the Principle of Permanence of Identities (ref Artin_chp12.ipynb) to derive the conclusion when $\\mathbb F$ is an arbitrary ring (or for this chapter's purposes, when it is an arbitrary field, though we prove the even stronger result here)  \n",
    "\n",
    "Since $g_s$ and $g_k$ have integer coefficients, we initially select the ring of $\\mathbb Z$ and do the factorization over $\\mathbb Z[x]$ which is a unique factorization domain, or more basically: we apply prop 3.19 from chapter 10, on page 358, with $R:=\\mathbb Z$ and since $g_k$ is monic, we may write  \n",
    "\n",
    "$g_s(x) = g_k(x)\\cdot m(x) + r(x)$  \n",
    "where $\\text{degree}\\big(r(x)\\big)\\lt d$  \n",
    "\n",
    "now see apply $d$ distinct substitution homomorphisms, for $j \\in \\big\\{1,2,...,d-1,d\\big\\}$   \n",
    "$\\phi_j: \\mathbb Z[x] \\longrightarrow \\mathbb C$ given by  \n",
    "$\\phi_j: x\\mapsto \\omega_j \\in \\mathbb C$ and \n",
    "maps any element of $z\\in \\mathbb Z$ to $z\\in \\mathbb C$ (i.e. it is the identity / inclusion map with respect to integers)   \n",
    "\n",
    "giving us   \n",
    "$0=\\phi_j\\Big(g_s(x)\\Big) = \\phi_j\\Big(g_k(x)\\Big)\\cdot \\phi_j\\Big(m(x)\\Big) + \\phi_j\\Big(r(x)\\Big)=0\\cdot \\phi_j\\Big(m(x)\\Big) + \\phi_j\\Big(r(x)\\Big) = \\phi_j\\Big(r(x)\\Big)$   \n",
    "\n",
    "\n",
    "These substitutions tells us that when we view $r(x)$ as a polynomial in $\\mathbb C[x]$ with degree $\\lt d$ we observe that it has (at least) $d$ distinct roots which implies is the zero polynomial (and thus degree $-\\infty$ by some conventions).  This is a chapter 11 result, though we know that a polynomial in $\\mathbb C[x]$ has at a number of distinct roots equal to its degree from the Vandermonde matrix notebook (which is also the Base Case for Schwartz-Zippel) and this result is then implicit from a misc. exercise in chapter 1.    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
