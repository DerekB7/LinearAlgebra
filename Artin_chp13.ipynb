{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ex 13.1.3**   \n",
    "let $R$ be ring which is an integral domain that contains $\\mathbb F$ as a sub-ring and may be represented as an $n$ dimensional vector space over $\\mathbb F$.  (Note: this is an actual vector space over a field not an arbitrary module.)  Prove that $R$ is a field.  \n",
    "\n",
    "*remark*  \n",
    "This should remind us of ex 10.6.2 which was to prove that an integral domain with finitely many elements is a field.  Here we have an integral domain that may have infinitely many elements, but it is still finite in some 'nice' sense -- in particular it can be interpreted as a finite dimensional vector space.  \n",
    "\n",
    "*proof:*   \n",
    "The idea is to prove that every $r\\in R-\\big\\{0\\big\\}$ has an inverse  \n",
    "\n",
    "fix $r\\in R-\\big\\{0\\big\\}$ and selecting an arbitrary element in $v\\in R$, we have  \n",
    "\n",
    "$r\\cdot v = v'$   \n",
    "$r\\cdot\\big( v + v'\\big) = r\\cdot v + r\\cdot v'$  and  \n",
    "$r\\cdot\\big( \\alpha v\\big) =\\alpha \\cdot \\big(r\\cdot v\\big)$   \n",
    "\n",
    "so $r$ is a linear operator on this vector space $V$.  Since the domain is integral, we have  \n",
    "\n",
    "$\\dim\\ker\\big(r\\big) = 0$  \n",
    "and by rank nullity,  \n",
    "$\\dim\\text{im}\\big(r\\big) = n$  \n",
    "\n",
    "hence $r$ is both injective and surjective and thus invertible  \n",
    "\n",
    "*alternatively*  \n",
    "for a more granular approach, define a basis for this finite dim vector space    \n",
    "$\\mathbf B:=\\bigg[\\begin{array}{c|c|c|c} v_1 & v_2 &\\cdots & v_{d} \\end{array}\\bigg]$  \n",
    "we then model this forward and run it backward to solve for the inverse of arbitrary $r\\in R-\\big\\{0\\big\\}$  \n",
    "\n",
    "first:  \n",
    "For arbitrary $v\\in \\mathbb R$ which is a vector space $V$ over $\\mathbb F$  we have   \n",
    "$v = \\mathbf {Bx}$   and  \n",
    "$\\mathbf B\\mathbf y = v' = r\\cdot v = r\\cdot \\big(\\mathbf B\\mathbf x\\big)= \\big(r\\cdot \\mathbf B\\big) \\mathbf x = \\big(\\mathbf B A\\big) \\mathbf x =\\mathbf B  \\big(A \\mathbf x\\big)$   \n",
    "since the basis uniquely characterizes all elements in the vector space, the problem reduces to the familiar  \n",
    "$A\\mathbf x = \\mathbf y$   \n",
    "\n",
    "second:  \n",
    "running this backwards,  \n",
    "we specialize to have $v':=1 \\in R$  (though any unit in R will do) thus  \n",
    "$1 = \\mathbf B\\mathbf y$ -- so $\\mathbf y$ is the coordinate representation of this unit with respect to our basis.  The question becomes can we solve  \n",
    "$A\\mathbf x = \\mathbf y$   \n",
    "and of course the answer is yes, because $A$ has a trivial kernel (by integrality of our ring and uniqueness of basis representation, we know $r\\cdot v = 0$ *iff* $v=0$ which is equivalent to $A\\mathbf x = \\mathbf 0$ *iff* $\\mathbf x =0$ ) \n",
    "\n",
    "so $A$ has non-zero determinant (which takes on a value in $\\mathbb F$ and hence is a unit)  and   \n",
    "$\\mathbf x = A^{-1}\\mathbf y$   \n",
    "which is unique.   \n",
    "\n",
    "Working backwards, we have   \n",
    "$r \\cdot v = r\\cdot \\mathbf B\\mathbf x = \\big(r\\cdot \\mathbf B\\big)\\big(A^{-1}\\mathbf y\\big)= \\big(\\mathbf BA\\big)A^{-1}\\mathbf y =\\mathbf B\\mathbf y = 1$   \n",
    "hence we have found the multiplicative inverse of $r$ and $r$ is invertible.  But the choice of $r\\in R-\\big\\{0\\big\\}$ was arbitrary so all non-zero elements in $R$ are invertible and $R$ is a field.   \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ex 13.2.2**  \n",
    "where $\\mathbb F[\\alpha]$ is the result of applying the substitution homomorphism $\\phi$ given by $x\\mapsto \\alpha$ to $F[x]$ where $\\alpha\\notin \\mathbb F$ (but exists in some field extension of $\\mathbb F$) and $\\alpha$ is a root of a degree $d$ monic polynomial $g\\in F[x]$  but $\\alpha$ is not a root of a lower degree polynomial in $\\mathbb F[x]$. \n",
    "\n",
    "prove:  \n",
    "$\\mathbf B := \n",
    "\\bigg[\\begin{array}{c|c|c|c|c} \n",
    "1  & \\alpha & \\alpha^2&\\cdots &\\ \\alpha^{d-1} \n",
    "\\end{array}\\bigg]$  \n",
    "is a basis for $\\mathbb F[\\alpha]$  \n",
    "\n",
    "*proof:*  \n",
    "*i.)*  linear independence with coefficients in $\\mathbb F$     \n",
    "$\\mathbf B\\mathbf v=\\mathbf 0\\implies \\mathbf v = \\mathbf 0$   \n",
    "writing this out:  \n",
    "$v_0\\cdot 1  + v_1\\cdot \\alpha + v_2\\cdot \\alpha^2 +...+ v_{d-1}\\cdot \\alpha^{d-1}  = 0$   \n",
    "which is a polyonomial of degree $\\leq d-1$ in $\\mathbb F[x]$ under the image of $\\phi$.   \n",
    "$\\implies 0=v_0=v_1=...=v_{d-1}$ because there is no (non zero) polynomial with degree $\\leq d$ in $\\mathbb F[x]$ that has $\\alpha$ as a root.  \n",
    "\n",
    "*ii.)* span  \n",
    "for reasons of linearity it is enough to prove this for a monomial $\\beta \\cdot \\alpha^k$.  If $k\\leq d-1$ the result immediately follows.  If $k=d$ then the fact that $f(\\alpha)=0$ gives us an explicit recurrence, to write $\\alpha^d$ as a linear combination of strictly lower order terms (just examine  both sides of  $\\alpha^d -f(\\alpha)=0+-\\alpha^d$) and each term is a scalar multiple of some vector in $\\mathbf B$.  If $k\\gt d$ then by applying the above recurrence, $\\alpha^k$ may be written as a linear combination of strictly lower order terms and by strong inductive hypothesis each of them is a linear combination of vectors in $\\mathbf B$.  Thus $\\beta\\cdot \\alpha^k \\in \\text{span}\\big(\\mathbf B\\big)$ for arbitrary $k$.  \n",
    "\n",
    "Thus $\\mathbf B$ forms a basis.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**corollary**  \n",
    "\n",
    "*lemma:*  \n",
    "consider $\\mathbb F\\subset \\mathbb K$ and the polynomial ring $\\mathbb F[x]$.  If $\\alpha \\in \\mathbb K$ but $\\alpha \\notin \\mathbb F$ is not a root of any polynomial $g$ of degree $\\leq d-1$ in $\\mathbb F[x]$, then for $p_1,p_2 \\in \\mathbb F[x]$, each with degree $\\leq d-1$, then  \n",
    "$\\phi\\big(p_1p_2\\big)\\neq 0$  \n",
    "where $\\phi$ is the substitution homomorphism $x\\mapsto \\alpha$  \n",
    "\n",
    "*proof:*  \n",
    "$\\phi\\big(p_1p_2\\big)=\\phi\\big(p_1\\big)\\phi\\big(p_2\\big)= \\beta_1\\cdot \\beta_2 \\neq 0$  \n",
    "where the first equality follows because this is a homomorphism and we know $\\beta_1, \\beta_2 \\in \\mathbb K-\\{0\\}$    because $\\beta_1, \\beta_2 \\in \\mathbb K$  by construction and $\\phi\\big(p_1\\big)\\neq 0$, $\\phi\\big(p_2\\big) \\neq 0$ since $\\alpha$ is not a root of either $p_1$ or $p_2$  \n",
    "\n",
    "*main argument:*   \n",
    "$\\mathbb F[\\alpha]$ is a field.   \n",
    "(This is is proven by very different means in prop 2.6a in the text\\.)   \n",
    "combining *ex 13.1.3* and *ex 13.2.2* tells us that $\\mathbb F[\\alpha]$ is a field so long as we confirm that the ring $R:=\\mathbb F[\\alpha]$ is an integral domain. To confirm integrality it suffices to prove that $\\mathbb F[\\alpha]$ has no zero divisors.  \n",
    "\n",
    "While not strictly needed, we can streamline the proof with an argument by contradiction.  \n",
    "i.e. suppose $r_1, r_2 \\in \\mathbb F[\\alpha]-\\{0\\}$ and $r_1\\cdot r_2=0$.  \n",
    "\n",
    "Then by the span argument in ex 13.2.2, each of $r_1$ and $r_2$ may be written as a linear combination of $\\alpha^k$ for $k\\in\\{0,1,2,...,d-1\\}$ which implies each or $r_1,r_2$ have degree $\\leq d-1$. Each of $r_1=\\phi\\big(p_1\\big)$ and $r_2=\\phi\\big(p_2\\big)$ for some degree at most $d-1$ polynomials $p_1,p_2\\in \\mathbb F[x]$ (just match the coefficients in $\\mathbb F$ to find them).  But this contradicts the lemma.  Hence $\\mathbb F[\\alpha]$ is an integral domain and thus a field.  \n",
    "\n",
    "*extension*  \n",
    "By the fundamental homomorphism theorem, $\\mathbb F[x]/(g) \\cong \\mathbb F[\\alpha]$ and since the RHS is a field, so is the LHS. By applying 10.7.3a (page 371) $\\mathbb F[x]/(g) \\text{ is a field }\\implies (g)\\text{ is a maximal ideal }$.  With some care, this should imply that every ideal in $\\mathbb F[x]$ is principal which is a chapter 11 (and perhaps chapter 10) result proven via rather different means.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ex 13.2.4**  \n",
    "\n",
    "consider irreducible polynomial (over $\\mathbb Q$)   \n",
    "$x^3-3x +4$   \n",
    "with complex root $\\alpha$, which means $\\alpha^3=3\\alpha -4$  \n",
    " \n",
    "$\\mathbf B := \n",
    "\\bigg[\\begin{array}{c|c|c|c|c} \n",
    "\\mathbf 1  & \\alpha & \\alpha^2 \n",
    "\\end{array}\\bigg]$   \n",
    "$r:= (\\alpha^2+\\alpha+1)$   \n",
    "thus   \n",
    "$r\\cdot \\mathbf B = \\mathbf BA = \\mathbf B \\displaystyle \\left[\\begin{matrix}1 & -4 & -4\\\\1 & 4 & -1\\\\1 & 1 & 4\\end{matrix}\\right]$   \n",
    "\n",
    "$r^{-1}\\mathbf B = \\mathbf B A^{-1}=\\mathbf B\\displaystyle \\left[\\begin{matrix}\\frac{17}{49} & \\frac{12}{49} & \\frac{20}{49}\\\\- \\frac{5}{49} & \\frac{8}{49} & - \\frac{3}{49}\\\\- \\frac{3}{49} & - \\frac{5}{49} & \\frac{8}{49}\\end{matrix}\\right] $   \n",
    "\n",
    "see comments in code below.  We can read off $r^{-1}$ by looking at the first column of $A^{-1}$  \n",
    "**note**  \n",
    "$(\\alpha^2+\\alpha+1)\\cdot \\alpha^2 = \\alpha^4+\\alpha^3 +\\alpha^2$ but this needs reduced to terms with degree $\\leq 2$.  We can do this by hand.  However, we may also consider modeling the irreducible polynomial with a Companion Matrix $C$ and with suitable modelling, multiply by $C^{-1}$ twice, i.e. multiplying by $C^{-2}$.  Also note that up to a sign, the determinant of the Companion matrix is given by the constant term of our irreducible polynomial, which is non-zero since the polynomial is irreducible (and 0 exists in any ring so we cannot factor it out).  Thus $C^{-1}$ must exist.     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}\\frac{17}{49} & \\frac{12}{49} & \\frac{20}{49}\\\\- \\frac{5}{49} & \\frac{8}{49} & - \\frac{3}{49}\\\\- \\frac{3}{49} & - \\frac{5}{49} & \\frac{8}{49}\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "Matrix([\n",
       "[17/49, 12/49, 20/49],\n",
       "[-5/49,  8/49, -3/49],\n",
       "[-3/49, -5/49,  8/49]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sympy as sp  \n",
    "a = sp.Symbol('a')\n",
    "A = sp.Matrix([[1,-4,-4],[1,4,-1],[1,1,4]])\n",
    "B = sp.Matrix([[1,a, a**2]])\n",
    "A.inv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}1 & -4 & -4\\\\1 & 4 & -1\\\\1 & 1 & 4\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "Matrix([\n",
       "[1, -4, -4],\n",
       "[1,  4, -1],\n",
       "[1,  1,  4]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}a^{2} + a + 1 & a^{2} + 4 a - 4 & 4 a^{2} - a - 4\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "Matrix([[a**2 + a + 1, a**2 + 4*a - 4, 4*a**2 - a - 4]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B @ A\n",
    "# to recover linear operator r, read off the first column--where we see the 'one vector' under the image of tau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}- \\frac{3 a^{2}}{49} - \\frac{5 a}{49} + \\frac{17}{49} & - \\frac{5 a^{2}}{49} + \\frac{8 a}{49} + \\frac{12}{49} & \\frac{8 a^{2}}{49} - \\frac{3 a}{49} + \\frac{20}{49}\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "Matrix([[-3*a**2/49 - 5*a/49 + 17/49, -5*a**2/49 + 8*a/49 + 12/49, 8*a**2/49 - 3*a/49 + 20/49]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# so for avoidance of doubt,  \n",
    "B @ A.inv()  \n",
    "# or just read off the first column of A^{-1} which is shown 2 cells above  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ex 13.2.5**   \n",
    "as noted in Harvard solution, we are following a standard approach of attacking the minimal or maximal degree element -- in this case we attack the minimal degree element -- the constant term.  And as noted above the constant term is non-zero.  So write constant in terms of other terms-- which we can do because $\\alpha $ is a root. But RHS has an extra $\\alpha$ so divide each side by $\\alpha$ and by constant term, and we recover the inverse.  \n",
    "\n",
    "a longer approach that uses the same linear algebra techniques from the prior problem is as follows  \n",
    "\n",
    "$\\mathbf B := \n",
    "\\bigg[\\begin{array}{c|c|c|c|c} \n",
    "\\mathbf 1  & \\alpha & \\alpha^2&\\cdots &\\ \\alpha^{n-1} \n",
    "\\end{array}\\bigg]$   \n",
    "$r:= \\alpha$   \n",
    "thus   \n",
    "$r\\cdot \\mathbf B = \\mathbf BA = \\mathbf B \\displaystyle \\left[\\begin{matrix}0 & -c_0\\\\ I_{n-1} & -\\mathbf c \\end{matrix}\\right]$   \n",
    "\n",
    "where $\\mathbf c$ has the other terms of the polynomial from degree $1$ to $n-1$.  \n",
    "$A$ *is* a Companion matrix so $A^{-1}$ has a well known form.  To finish, we merely need to read off the first column, as before.  \n",
    "\n",
    "note: the easiest way to compute the inverse of the Companion Matrix, is, for the first column, apply Cayely Hamilton  \n",
    "\n",
    "\n",
    "$ A^n+c_{n-1}A^{n-1}+...+c_1A+c_0I_n = \\mathbf 0$  \n",
    "$\\implies I_n=\\frac{-1}{c_0}\\Big(A^n+c_{n-1}A^{n-1}+...+c_1A\\Big)$  \n",
    "$\\implies A^{-1}=\\frac{-1}{c_0}\\Big(A^{n-1}+c_{n-1}A^{n-2}+...+c_1I_n\\Big)$  \n",
    "$\\implies A^{-1}\\mathbf e_1=\\frac{-1}{c_0}\\Big(A^{n-1}+c_{n-1}A^{n-2}+...+c_1I_n\\Big)\\mathbf e_1$  \n",
    "and as noted in \"Artin_chp12.ipynb\" under \"The Companion Matrix's minimal polynomial is the same as its characteristic polynomial\", for an $n\\times n$ Companion Matrix, the powers up to degree $n-1$ are linearly independent, which we know simply by examining the first column of the matrix, i.e. \n",
    "$I\\mathbf e_1 = \\mathbf e_1$  \n",
    "and for $k \\in \\{1,2,...,n-1\\}$   \n",
    "$C^k\\mathbf e_1 = \\mathbf e_{k+1}$  \n",
    "$\\implies A^{-1}\\mathbf e_1=\\frac{-1}{c_0}\\Big(A^{n-1}+c_{n-1}A^{n-2}+...+c_1I_n\\Big)\\mathbf e_1=\\frac{-1}{c_0}\\Big(\\mathbf e_n+c_{n-1}\\mathbf e_{n-1}+...+c_1 \\mathbf e_1\\Big)$  \n",
    "which gives the first column of the Companion Matrix's Inverse, as desired.  \n",
    "(with some more thought we see this explicit work-through with Cayley-Hamilton recovers the standard approach (e.g. in Harvard Solution) thought explicitly working with the Companion may be of independent interest.  \n",
    "\n",
    "As an aside we get columns $\\big\\{2,3,...n\\}$ of the Companion Matrix inverse in effect for free by representing the problem slightly differently  \n",
    "\n",
    "$r\\cdot \\mathbf B = \\bigg[\\begin{array}{c|c|c|c|c} \n",
    " \\alpha & \\alpha^2&\\cdots &\\ \\alpha^{n-1} &\\ \\alpha^{n} \n",
    "\\end{array}\\bigg]\\implies \\bigg[\\begin{array}{c|c|c|c|c} \n",
    " \\alpha & \\alpha^2&\\cdots &\\ \\alpha^{n-1} &\\ \\alpha^{n} \n",
    "\\end{array}\\bigg]A^{-1} = \\bigg[\\begin{array}{c|c|c|c|c} \n",
    "\\mathbf 1  & \\alpha & \\alpha^2&\\cdots &\\ \\alpha^{n-1} \n",
    "\\end{array}\\bigg]=\\mathbf B$   \n",
    "so for any column $k\\geq 2$ we merely need to select $A^{-1}\\mathbf e_{k}:=\\mathbf e_{k-1}$   \n",
    "\n",
    "\n",
    "\n",
    "Some below calculations make this explicit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}0 & 0 & 0 & 0 & 0 & 0 & 0 & - c_{0}\\\\1 & 0 & 0 & 0 & 0 & 0 & 0 & - c_{1}\\\\0 & 1 & 0 & 0 & 0 & 0 & 0 & - c_{2}\\\\0 & 0 & 1 & 0 & 0 & 0 & 0 & - c_{3}\\\\0 & 0 & 0 & 1 & 0 & 0 & 0 & - c_{4}\\\\0 & 0 & 0 & 0 & 1 & 0 & 0 & - c_{5}\\\\0 & 0 & 0 & 0 & 0 & 1 & 0 & - c_{6}\\\\0 & 0 & 0 & 0 & 0 & 0 & 1 & - c_{7}\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "Matrix([\n",
       "[0, 0, 0, 0, 0, 0, 0, -c_0],\n",
       "[1, 0, 0, 0, 0, 0, 0, -c_1],\n",
       "[0, 1, 0, 0, 0, 0, 0, -c_2],\n",
       "[0, 0, 1, 0, 0, 0, 0, -c_3],\n",
       "[0, 0, 0, 1, 0, 0, 0, -c_4],\n",
       "[0, 0, 0, 0, 1, 0, 0, -c_5],\n",
       "[0, 0, 0, 0, 0, 1, 0, -c_6],\n",
       "[0, 0, 0, 0, 0, 0, 1, -c_7]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_0 = sp.Symbol('c_0')\n",
    "c_1 = sp.Symbol('c_1')\n",
    "c_2 = sp.Symbol('c_2')\n",
    "c_3 = sp.Symbol('c_3')\n",
    "c_4 = sp.Symbol('c_4')\n",
    "c_5 = sp.Symbol('c_5')\n",
    "c_6 = sp.Symbol('c_6')\n",
    "c_7 = sp.Symbol('c_7')\n",
    "c_8 = sp.Symbol('c_8')\n",
    "c_9 = sp.Symbol('c_9')\n",
    "\n",
    "the_list = [c_0, c_1, c_2, c_3, c_4, c_5, c_6, c_7, c_8, c_9]\n",
    "\n",
    "d = 8\n",
    "# natural number <= 10 \n",
    "\n",
    "assert d<= 10 and type(d)==int\n",
    "\n",
    "C = sp.zeros(d)\n",
    "for j in range(d-1):\n",
    "    C[j+1,j] = 1\n",
    "for i in range(d):\n",
    "    C[i,-1] = -the_list[i]  \n",
    "C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}- \\frac{c_{1}}{c_{0}} & 1 & 0 & 0 & 0 & 0 & 0 & 0\\\\- \\frac{c_{2}}{c_{0}} & 0 & 1 & 0 & 0 & 0 & 0 & 0\\\\- \\frac{c_{3}}{c_{0}} & 0 & 0 & 1 & 0 & 0 & 0 & 0\\\\- \\frac{c_{4}}{c_{0}} & 0 & 0 & 0 & 1 & 0 & 0 & 0\\\\- \\frac{c_{5}}{c_{0}} & 0 & 0 & 0 & 0 & 1 & 0 & 0\\\\- \\frac{c_{6}}{c_{0}} & 0 & 0 & 0 & 0 & 0 & 1 & 0\\\\- \\frac{c_{7}}{c_{0}} & 0 & 0 & 0 & 0 & 0 & 0 & 1\\\\- \\frac{1}{c_{0}} & 0 & 0 & 0 & 0 & 0 & 0 & 0\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "Matrix([\n",
       "[-c_1/c_0, 1, 0, 0, 0, 0, 0, 0],\n",
       "[-c_2/c_0, 0, 1, 0, 0, 0, 0, 0],\n",
       "[-c_3/c_0, 0, 0, 1, 0, 0, 0, 0],\n",
       "[-c_4/c_0, 0, 0, 0, 1, 0, 0, 0],\n",
       "[-c_5/c_0, 0, 0, 0, 0, 1, 0, 0],\n",
       "[-c_6/c_0, 0, 0, 0, 0, 0, 1, 0],\n",
       "[-c_7/c_0, 0, 0, 0, 0, 0, 0, 1],\n",
       "[  -1/c_0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C.inv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "suppose $\\text{char}\\big(\\mathbb K\\big) \\neq 2$, then $q-1$ is even.  So either every (exc $\\lambda_i\\neq 1$) $k_i\\% 2 =0$ or none do.  If we have a mix where some are divisible by 2 and some not, run below argument.  If none are divisible by 2, then $C^{\\frac{q-1}{2}}-I=\\mathbf 0$ which contradicts the fact that the minimal polynomial is the minimal degree monic annihilating polynomial.  If every (exc $\\lambda_i\\neq 1$) is a multiple of 2, then $\\lambda_i^{\\frac{q-1}{2}}\\in \\{-1,1\\}$  (ref ex 13.1.1).   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Linear Algebra proof of Theorem 6.4(c):**  \n",
    "Let $\\mathbb K$ be a field where $\\big \\vert \\mathbb K\\big \\vert = q = p^r =\\text{prime}^r$. Then the multiplicative group $\\mathbb K-\\big\\{0\\big\\}$ is a cyclic group of order $q-1$.  \n",
    "\n",
    "*remark:*  \n",
    "This is proven over the bottom of page 512, through page 513, ultimately using the Structure Theorem for abelian groups, from p.472.  Theorem 6.4(d) has already been proven, earlier on page 512.  Theorem 6.4(d) tells us that the non-zero elements of $\\mathbb K$ are the $q-1$ distinct elements that satisfy $x^{q-1}-1=0$, i.e. they comprise all of the $q-1$ roots of unity that exist in $\\mathbb K$.  We take Theorem 6.4(d) as our starting point.  \n",
    "\n",
    "*proof:*  \n",
    "for **notational** cleanup we define  \n",
    "$m:=q-1=p^r -1$\n",
    "\n",
    "we are particularly interested in $C \\in \\mathbb K^{m\\times m}$, defined to be the Companion Matrix associated with the polynomial $x^{m}-1$.  (For a review of the Companion Matrix, visit the section 'Enter the Companion Matrix' in \"Vandermonde_Matrices_Permutations_and_Discrete_Fourier_Transform.ipynb\".)  We will in general consider $C \\in \\mathbb K^{d\\times d}$, defined to be the Companion Matrix associated with the polynomial $x^{d}-1$ where $d$ is an arbitrary natural number-- this is used later on for an induction.  \n",
    "\n",
    "now we know  \n",
    "$g\\big(C\\big) = C^{d}-I = \\mathbf 0$   \n",
    "per Cayley Hamilton, with $g$ being the characteristic polynomial of $C$. Referencing e.g. \"The Companion Matrix's minimal polynomial is the same as its characteristic polynomial\" in \"Artin_chp12.ipynb\" we know that $g$ **is also the minimal polynomial of** $C$.  We can also verify this by inspection.  \n",
    "\n",
    "**Note:** $C$ **is diagonalizable over** $\\mathbb K$ since it is $d\\times d$ and has $r'$ distinct eigenvalues (dth roots of unity) and in fact is diagonalizable by a Vandermonde matrix.   \n",
    "\n",
    "\n",
    "note that $C$ is *both* the Companion Matrix associated with $x^d-1$ (over any field) and a Permutation Matrix.  Being a Permutation matrix, it has components in $\\{0,1\\}$ so we may consider it over $\\mathbb Z$. As is common-- the spirit of the principle of permanence of identities-- we can tease out results over $\\mathbb Z$ by considering the behavior when we do a ring extension to $\\mathbb C$ yet still have those integer components.  If we do this carefully enough, we then work backwards to infer fundamental properties for this Companion matrix in $\\mathbb Z^{d\\times d}$ and use that to infer properties over any field.  \n",
    "\n",
    "- - - - -   \n",
    "*lemma:*  \n",
    "if $d$ is prime, then there are no nontrivial cyclic subgroups. This is immediate by Lagranges' Theorem.  \n",
    "\n",
    "Yet if we take any arbitrary $\\lambda \\in \\mathbb K-\\big\\{0\\big\\}$, then we may generate a cyclic subgroup \n",
    "$\\{\\lambda, \\lambda^2,...,\\lambda^k, 1\\}$ hence if $\\lambda \\neq 1$ then it generate the entire group.  Note that over any field, we have  $x^d-1$ has d distinct roots of unity, only one of them equal to 1.  Hence there are $d-1$ generators of the d-cyclic group.  \n",
    "\n",
    "*lemma:*  \n",
    "working over $\\mathbb C$:    \n",
    "if $d$ is not prime, then call $p_1$ the smallest prime number it is divisible by, i.e. $d=p_1\\cdot k$.  Then  \n",
    "$I = C^{d}= \\big(C^{p_1\\cdot k}\\big)^k\\big(C^{p_1}\\big)^k$ for some $k$ and \n",
    "\n",
    "for all natural numbers $j$,   \n",
    "$\\text{trace}\\Big(\\big(C^{p_1}\\big)^j\\Big)=\\text{trace}\\Big(P^j\\Big)$   \n",
    "and in particular  \n",
    "i.) if $j\\%k\\neq 0$  \n",
    "$\\text{trace}\\Big(\\big(C^{p_1}\\big)^j\\Big)=\\text{trace}\\Big(P^j\\Big)$   \n",
    "\n",
    "ii.) if $j\\%k=0$ and  \n",
    "$d=\\text{trace}\\Big(I_d\\Big)=\\text{trace}\\Big(\\big(C^{p_1}\\big)^j\\Big)=\\text{trace}\\Big(P^j\\Big)$   \n",
    "\n",
    "$P:= \\begin{bmatrix}\n",
    " C_{x^{p_1}-1} &  \\mathbf 0 &  \\mathbf 0& \\mathbf 0 & \\mathbf 0 &  \\cdots &  \\mathbf 0 \\\\ \n",
    "\\mathbf 0&  C_{x^{p_1}-1} &  \\mathbf 0& \\mathbf 0 & \\mathbf 0 &  \\cdots &   \\mathbf 0 \\\\ \n",
    " \\mathbf 0&   \\mathbf 0&    C_{x^{p_1}-1}&\\mathbf 0 & \\mathbf 0 &  \\cdots  &  \\mathbf 0 \\\\ \n",
    " \\mathbf 0&   \\mathbf 0& \\mathbf 0&   C_{x^{p_1}-1} &  \\mathbf 0 &\\cdots  &  \\mathbf 0 \\\\ \n",
    " \\mathbf 0&   \\mathbf 0&   \\mathbf 0& \\mathbf 0& C_{x^{p_1}-1} &  \\cdots &  \\mathbf 0 \\\\ \n",
    " \\vdots&   \\vdots&   \\vdots& \\vdots& \\vdots &  \\ddots &  \\vdots \\\\ \n",
    " \\mathbf 0&   \\mathbf 0 &  \\mathbf 0& \\mathbf 0& \\mathbf 0 &   \\cdots&  C_{x^{p_1}-1}\n",
    "\\end{bmatrix}$   \n",
    "\n",
    "where $P$ has k blocks on its diagonal, and  \n",
    "$C_{x^{p_1}-1}$ is the $p_1 \\times p_1$ Companion Matrix associated with $x^{{p_1}-1}$.  Put differently $P$ is block diagonal with $k$ of this 'mini companion' matrices.  \n",
    "\n",
    "This is verifiable either via direct algebraic manipulations, or via Newton's Identities, or by examining the underlying graphs.   \n",
    "\n",
    "now $C$ is a permutation matrix so any power of it is a permutation matrix, i.e. $C^{p_1}$ is a $d\\times d$ permutation matrix.  By inspection -- the block diagonal structure makes this obvious-- $P$ is a $d\\times d$ permutation matrix.  \n",
    "\n",
    "Since $C^{p_1}$ and $P$ are both $d\\times d$ permutation matrices and there traces match for all natural numbers they are conjugate permutation matrices.  That is  \n",
    "$C^{p_1} = Q^{-1}PQ = Q^{T}PQ$  \n",
    "where $Q$ is some $d\\times d$ permutation matrix. Ref e.g.:   \n",
    "https://math.stackexchange.com/questions/3744582/two-permutation-matrices-represent-conjugate-permutations-iff-they-have-same-cha  \n",
    "\n",
    "This means $C$ and $P$ are (permutation) similar over $\\mathbb Z$.  It *also* means they are similar over *any field* because 0 and 1 exist over any field, and the multiplication of 2 permutation matrices is a permutation matrix over any field and the inverse of a permutation matrix is just its transpose over any field.  \n",
    "\n",
    "**main proof:**  \n",
    "fix some   \n",
    "$\\text{char}\\big(\\mathbb K\\big) = p$  and some   \n",
    "$m=q-1=p^r-1$   \n",
    "\n",
    "we want to show that over $\\mathbb K$ the Companion matrix associated with $x^m-1$, $C\\in \\mathbb K^{m\\times m}$ has (at least one) eigenvalue $\\lambda$ where $\\lambda^j\\neq 1$ for $j\\in \\big\\{1,2,...,r'-1\\big\\}$  (i.e. that $\\mathbb K-\\{0\\}$ is a cyclic group.)  Note: with a small amount of extra work we can determine the exact number of eigenvalues that fit this criterion.  \n",
    "\n",
    "We proceed by Induction on $d$.  (I.e. we start with $d=1,2,3,4,5,....$ and the proof is complete once $d=m$.)  \n",
    "\n",
    "# technical nit: for now assume $\\text{char}\\big(\\mathbb K\\big) \\geq 3$ -- char 2 creates nuisances that aren't material and should be addressed at the end \n",
    "\n",
    "**Base Case:**    \n",
    "We show multiple cases for intuition.  \n",
    "for $m=1$ there is nothing to prove.  \n",
    "\n",
    "for $m=2$ and $m=3$, $m$ is prime, and by the first lemma there is an element that generates the entire (cyclic) multiplicative group.  \n",
    "\n",
    "for $m=4$, we see $m$ is not prime, and $4=2\\cdot 2 =p_1\\cdot p_1$.   \n",
    "Then with $p_1:=2$ we see that by the second lemma, $C^{p_1}=C^2$ is similar to \n",
    "\n",
    "$P:= \\begin{bmatrix}\n",
    " C_{x^{2}-1} &  \\mathbf 0\\\\ \n",
    "\\mathbf 0&  C_{x^{2}-1}\\\\ \n",
    " \\end{bmatrix}$  \n",
    "\n",
    "and by the $m=2$ case we know each block has an eigenvalue ($-1$ in this case) that generates the cyclic subgroup. But this generator $\\lambda^2=\\omega$ has a preimage under the mapping of $\\lambda^2$ that is not one (because $1^2=1$ and thus $(\\lambda^2)^2=1$.  And by Lagrange's Theorem $m$ can have no other cyclic subgroups.  \n",
    "\n",
    "for $m=5$ by our first lemma, any $\\lambda\\neq 1$ generates the cyclic subgroup.  \n",
    "\n",
    "\n",
    "**Strong Inductive Case:**  \n",
    "Now for $d\\geq 6$, the desired result is immediate by the first lemma if $d$ is prime.  Suppose $d$ is not prime and let $p_1$ be the smallest prime it is divisible by.  \n",
    "\n",
    "Then by our second lemma,  \n",
    "$C^{p_1}$ is similar to $P$ and by our induction hypothesis each block (which is $p_1\\times p_1$ for $p_1\\lt d$) has at least one eigenvalue that is a cyclic generator for the $p_1$th roots of unity.  Select one of these and call it $\\omega$.  Now $\\lambda\\neq 1\\implies \\text{pre-image under mapping }x \\mapsto x^{p_1}\\neq 1$ (because $1^{p_1}=1$ but the preimage cannot be empty, e.g. via a diagonalization argument).  So call an arbitrary element \n",
    "\n",
    "# there is still a bug in here....  \n",
    "# I think a sketch of a better finish is the 'flipped' setup  \n",
    "\n",
    "*sketch to be completed*  \n",
    "\n",
    "$n:= \\frac{d}{p_1}$   \n",
    "\n",
    "\n",
    "TBD, probably not:  or if $p_1$ has multiplicity $s\\geq 2$ then use $n:= \\frac{d}{p_1^2}$   \n",
    "\n",
    "then $C^n$ is similar to $P$ which has $p_1$ blocks, each being $n\\times n$.  **Do I need an explicit number here for exact counting?  Unclear to me.**    \n",
    "\n",
    "We should be able to say by induction hypothesis each block has a cyclic generator (i.e. $\\neq 1$ for $j\\in\\big\\{1,2,...,m-1\\}\\big\\}$.  Since $p_1$ is prime this means \n",
    "$\\lambda^j \\neq 1$ for $j\\in\\big\\{p_1,2\\cdot p_1,...,(m-1)\\cdot\\}\\big\\}$ -- THIS SECOND PART NEEDS TIGHTENED UP   \n",
    "\n",
    "Now it is true that $\\lambda^{m\\cdot p_1}=\\lambda^{d}=1$  \n",
    "But it is also true that $\\lambda^j\\neq 1$ for all $j\\in\\big\\{1,2,...,d-1\\}\\big\\}$.   If this wasn't true,  then we'd violate Lagrange's Theorem-- i.e. we'd have $\\lambda^j=1$, generating a cyclic subgroup of order $j$ where $d\\%j\\neq 0$.  \n",
    "\n",
    "\n",
    "**I have a feeling the answer MAY come from computing the same thing 2 different ways... and the solution falls out of finding an omega that isn't 1 in the C^n case and isn't 1 in the $C^{p_1}$ case.... but I should give this problem a rest for a while and come back later on**  \n",
    "\n",
    "\n",
    "**corollary**  \n",
    "Any subgroup $H$ of $G:=(\\mathbb K-\\{0\\})^\\times$ is cyclic.  \n",
    "take arbitrary element $h_i\\in H$ and we know this generates a cyclic subgroup of order $k_i\\leq \\big \\vert H\\big \\vert = r^{''}$ (since there are finitely many elements in $H$ and it is closed under products, eventually $h_i^j =h_i^{j'}$, i.e. there is a repeat, which implies $h_i^{j''}=1$ by invertibility).  And in fact by Lagrange's theorem, we know that $k_i\\cdot d_i = r^{''}$.  This holds for arbitrary element in $H$ hence each $h_i^{r^{''}}=1$ and we want to prove that there must be some element in our (sub)group that is not one for all $j$ in $\\big\\{1,2,...,r^{''}\\big\\}$.  But since a subgroup is a group, this is an exact replica of our original problem statement, hence such an $h_i$ must exist and $H$ is therefore cyclic.   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# attempt at another way  \n",
    "\n",
    "One way of proving this is to observe that  \n",
    "\n",
    "$\\text{rank}\\Big(\\big(I_d - C^j\\big)^{\\in \\mathbb C}\\Big)\\geq\\text{rank}\\Big(\\big(I_d - C^j\\big)^{\\in \\mathbb F_p}\\Big) = \\text{rank}\\Big(\\big(I_d - C^j\\big)^{\\in \\mathbb K}\\Big)$  \n",
    "because the matrix in $\\mathbb F_p$ is the same as the one on the LHS except components are modded out by $p$.  Since a matrix has rank $w$ *iff* it has a $w\\times w$ minor that is non-zero and all bigger submatrices have zero determinant (ref ex 4.Misc.10), the inequality follows.  Similarly none of the minors change when we do the field extension from $\\mathbb F_p$to $\\mathbb K$ so the ranks are the same.  Applying rank nullity, (i.e. negating and adding d to each item) we get  \n",
    "\n",
    "$\\text{dim}\\ker\\Big(\\big(I_d - C^j\\big)^{\\in \\mathbb C}\\Big)\\leq \\text{dim}\\ker\\Big(\\big(I_d - C^j\\big)^{\\in \\mathbb F_p}\\Big) = \\text{dim}\\ker\\Big(\\big(I_d - C^j\\big)^{\\in \\mathbb K}\\Big)$  \n",
    "\n",
    "The heart of this proof, then is to show that this is upper bound is met with equality for all \n",
    "$j\\in\\big\\{1,2,...,d-1\\big\\}$  \n",
    "\n",
    "$P:= \\begin{bmatrix}\n",
    " C_{x^{p_1}-1} &  \\mathbf 0 &  \\mathbf 0& \\mathbf 0 & \\mathbf 0 &  \\cdots &  \\mathbf 0 \\\\ \n",
    "\\mathbf 0&  C_{x^{p_1}-1} &  \\mathbf 0& \\mathbf 0 & \\mathbf 0 &  \\cdots &   \\mathbf 0 \\\\ \n",
    " \\mathbf 0&   \\mathbf 0&    C_{x^{p_1}-1}&\\mathbf 0 & \\mathbf 0 &  \\cdots  &  \\mathbf 0 \\\\ \n",
    " \\mathbf 0&   \\mathbf 0& \\mathbf 0&   C_{x^{p_1}-1} &  \\mathbf 0 &\\cdots  &  \\mathbf 0 \\\\ \n",
    " \\mathbf 0&   \\mathbf 0&   \\mathbf 0& \\mathbf 0& C_{x^{p_1}-1} &  \\cdots &  \\mathbf 0 \\\\ \n",
    " \\vdots&   \\vdots&   \\vdots& \\vdots& \\vdots &  \\ddots &  \\vdots \\\\ \n",
    " \\mathbf 0&   \\mathbf 0 &  \\mathbf 0& \\mathbf 0& \\mathbf 0 &   \\cdots&  C_{x^{p_1}-1}\n",
    "\\end{bmatrix}$   \n",
    "\n",
    "should be immediate  \n",
    "check algebraic multiplicities and how poly splits geo mult <= alg mult  and alg mutl met with equality/  \n",
    "\n",
    "# \n",
    "This is done by a double induction, with the 'outer for loop' given by for $d'=1,2,..., d$ and the 'inner for loop' given by for $j=1,2,...,d'-1$  \n",
    "\n",
    "*Base Case*   \n",
    "There is nothing to do for $d'=1$  \n",
    "The result is immediate for $d'=2$  \n",
    "\n",
    "*Inductive Case*    \n",
    "If $d'\\%j \\neq 0$ then there is nothing to do  \n",
    "Suppose $d'\\%j=0$  \n",
    "**INSERT**  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**main proof**  \n",
    "This is in effect, a counting argument, done via iteration/induction\n",
    "\n",
    "the idea is analogous to that used in building a basis for multivariate polynomials\n",
    "\n",
    "0.) If $d$ is prime.  Create a list $[1]$  \n",
    "1.) We first list all of the primes that divide $d$.  $[p_1,p_2,...,p_s]$, and append this to the list from step zero.  \n",
    "2.) now list all \"degree two\" terms i.e. for $i=1\\text{ to }s$, for $idx = 1\\text{ to }i$ append $p_i p_{idx}$, *except* if the product is $\\geq d$, then ignore.  \n",
    "3.) now append degree 3 terms in the same way. \n",
    "4.) continue appending terms for degree 4, 5, and so on, stopping once at $z-1$, when $p_i^z \\geq d$ for all $i$. (as in Python, the 'append' operation is to be interpreted as appending to the right of the list)   \n",
    "\n",
    "using the prior lemma on equivalence of kernels, we have \n",
    "\n",
    "Base Case:  \n",
    "$\\text{dim}\\ker\\Big(\\big(I_d - C^{p_i}\\big)^{\\in \\mathbb C}\\Big)= \\text{dim}\\ker\\Big(\\big(I_d - C^{p_i}\\big)^{\\in \\mathbb K}\\Big)$  \n",
    "\n",
    "for $i=1,...,s$  \n",
    "\n",
    "Strong Inductive Case:  \n",
    "we know this is true for all degree $z-1$ terms.  \n",
    "for all $i \\in \\text{degree}(z)$  \n",
    "we have the 'basis polynomial' $t=p_1^{u_1}p_2^{u_2}...p_s^{u_s}$, where $u_1+u_2+...+u_2=z$, i.e. $\\text{degree}\\big(t\\big)=z$  \n",
    "\n",
    "and by our lemma  \n",
    "$m=\\text{dim}\\ker\\Big(\\big(I_d - C^{t}\\big)^{\\in \\mathbb C}\\Big)= \\text{dim}\\ker\\Big(\\big(I_d - C^{t}\\big)^{\\in \\mathbb K}\\Big)$  \n",
    "\n",
    "and the number of 'new' linearly independent eigenvectors, i.e. those in the kernel of the LHS that are not in some kernel $\\big(I_d - C^{t'}\\big)$ where $t'\\neq t$ but $t\\%t' = 0$  \n",
    "is $n$, where $m-n$  are in the kernel for lower degrees.  \n",
    "\n",
    "For the RHS, by strong induction we have $d-n$ eigenvectors that were in the kernel for lower degree $t'$ and by the equality of kernels, this means the RHS *also* has $n$ 'new' linearly independent eigenvectors.  \n",
    "\n",
    "**IT MAY BE BETTER TO WRITE THIS a DIRECT SUM OF EIGENVECTORS IN PREVIOUSLY SEEN KERNELS (IN PARTICULAR LOWER POWERS THAT DIVIDE TAS A VECTOR SUM... cleans up the ideas**  \n",
    "\n",
    "(note by Lagrange's Theorem we need not consider $C^j$ for $j\\geq 2$ if $j$ does not divide $d$ -- there cannot be a cyclic subgroup generated by $\\lambda^{j'}$ for $j' \\in\\big\\{1,2,...,j\\big\\}$ in such a case)  \n",
    "\n",
    "In the $\\mathbb C$ case, we have introduced $d'$ linearly independent eigenvectors as we iterate over all the possible powers between one and $d$.  Equivalently, the sum of these kernels has dimension $d'$.  \n",
    "Yet we know by calculation in $\\mathbb C$, by looking at the first $d$ root of unity -- i.e. the one with smallest non-zero polar angle-- its associated eigenvector is not in any of the above kernels.  Hence $d'\\lt d$.  This is equivalent to saying that there must exist some $\\lambda$ such that $\\lambda^j\\neq 1$ for all $j\\in\\big\\{1,2,...,d-1\\big\\}$. \n",
    "\n",
    "When we switch the field to $\\mathbb K$, the sum of these kernel *still* has dimension $d'\\lt d$, hence in this case too, there must exist some $\\lambda$ such that $\\lambda^j\\neq 1$ for all $j\\in\\big\\{1,2,...,d-1\\big\\}$.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ex 13.6.10**  \n",
    "Let $\\mathbb K$ be a finite field. Prove the the product of the nonzero elements of $\\mathbb K$ is $-1$.  \n",
    "\n",
    "This is Wilson's Theorem and is proven the same way we did in chapter 3 -- look at $\\mathbb K-\\{0\\}$ and pair each non-involutive element with its inverse and multiply them to get 1 and multiplying over all of these pairs.  Then multiply this product by the two involutive elements $\\{-1,1\\}$ to get $-1$.  Over characteristic 2, we only have one involutive element (recall ex 13.1.1), and the product is 1, which is equal to -1 in characteristic 2.  \n",
    "\n",
    "Alternatively, an amusing way to do this, using results from this section, is to note with $\\big \\vert \\mathbb F\\big \\vert =q=p^k$, and observe that with $d:= p^k-1$ we are looking at $d$th roots of unity, i.e. the product of the eigenvalues (i.e. the determinant) of the $d\\times d$ Companion Matrix, $C\\in \\mathbb K^{d\\times d}$ associated with $x^d-1$.  Said matrix is a permutation matrix (hence orthogonal) and thus $\\det\\big(C\\big)\\in\\{-1,1\\}$.   Therefore we already have the result when $\\text{char}\\big(\\mathbb K\\big)=2$.  \n",
    "\n",
    "Now consider $\\text{char}\\big(\\mathbb K\\big)\\geq 3$, i.e. for odd $p$.  Then $d$ is an odd minus one, so $d$ is even.  This implies $\\det\\big(C\\big) =-1$ which is easily confirmable by checking the expansion by minors shown in the Vandermonde matrix notebook or by inspection.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ex 13.6.16**  \n",
    "\n",
    "**(a)** Prove Lemma 6.21 for the case $\\mathbb F:= \\mathbb C$ by looking at the roots of the two polynomials  \n",
    "Lemma 6.21 on p.5.15 states that for natural numbers r,k,s, we have $r=k\\cdot s$ and with prime $p$ we have $q=p^r = p^{ks}$ and $q'=p^k$.   Then $x^{q'}-x =\\big(x-0\\big)\\cdot\\big(x^{p^k-1}-1\\big)=\\big(x-0\\big)\\cdot g_k(x)$ divides $x^q-x =\\big(x-0\\big)\\cdot\\big(x^{p^{ks}-1}-1\\big)=\\big(x-0\\big)\\cdot g_s(x)$    \n",
    "since zero exists in any ring and we merely need to show that $g_k$ divides $g_s$ -- equivalently that $g_s$ lives in the ideal generated by $g_k$ in particular in the polynomial ring $\\mathbb C[x]$  \n",
    "\n",
    "now $d:=\\text{degree}(g_k)= p^k-1$, so $g_k$ is chacterized by the dth roots of unity, of which there are $d$ distinct ones.  \n",
    "\n",
    "taking a hint from the geometric/telescoping finite series in on page 515, we can write \n",
    "\n",
    "\n",
    "$(p^k-1)(p^{ks-k}+p^{ks-k-1}+p^{ks-k-2}+....+ p + 1) = p^{ks}-1$  \n",
    "that is  \n",
    "$(p^k-1)\\cdot h = p^{ks}-1$  \n",
    "\n",
    "so for any dth root of unity $\\omega_i$ we have  \n",
    "$\\omega_i^d =1\\implies (\\omega_i^d)^h =1^h = 1 = \\omega_i^{d\\cdot h}=\\omega_i^{p^{ks}-1}$   \n",
    "Thus these $d$ distinct $\\omega_i$ are also roots of $g_s(x)$.  After factoring out these $d$ roots we see \n",
    "\n",
    "$g_s(x)=g_k\\cdot u(x)$ with remainder zero  \n",
    "\n",
    "\n",
    "**(b)** use the Principle of Permanence of Identities (ref Artin_chp12.ipynb) to derive the conclusion when $\\mathbb F$ is an arbitrary ring (or for this chapter's purposes, when it is an arbitrary field, though we prove the even stronger result here)  \n",
    "\n",
    "Since $g_s$ and $g_k$ have integer coefficients, we initially select the ring of $\\mathbb Z$ and do the factorization over $\\mathbb Z[x]$ which is a unique factorization domain, or more basically: we apply prop 3.19 from chapter 10, on page 358, with $R:=\\mathbb Z$ and since $g_k$ is monic, we may write  \n",
    "\n",
    "$g_s(x) = g_k(x)\\cdot m(x) + r(x)$  \n",
    "where $\\text{degree}\\big(r(x)\\big)\\lt d$  \n",
    "\n",
    "now see apply $d$ distinct substitution homomorphisms, for $j \\in \\big\\{1,2,...,d-1,d\\big\\}$   \n",
    "$\\phi_j: \\mathbb Z[x] \\longrightarrow \\mathbb C$ given by  \n",
    "$\\phi_j: x\\mapsto \\omega_j \\in \\mathbb C$ and \n",
    "maps any element of $z\\in \\mathbb Z$ to $z\\in \\mathbb C$ (i.e. it is the identity / inclusion map with respect to integers)   \n",
    "\n",
    "giving us   \n",
    "$0=\\phi_j\\Big(g_s(x)\\Big) = \\phi_j\\Big(g_k(x)\\Big)\\cdot \\phi_j\\Big(m(x)\\Big) + \\phi_j\\Big(r(x)\\Big)=0\\cdot \\phi_j\\Big(m(x)\\Big) + \\phi_j\\Big(r(x)\\Big) = \\phi_j\\Big(r(x)\\Big)$   \n",
    "\n",
    "\n",
    "These substitutions tells us that when we view $r(x)$ as a polynomial in $\\mathbb C[x]$ with degree $\\lt d$ we observe that it has (at least) $d$ distinct roots which implies is the zero polynomial (and thus degree $-\\infty$ by some conventions).  This is a chapter 11 result, though we know that a polynomial in $\\mathbb C[x]$ has at a number of distinct roots equal to its degree from the Vandermonde matrix notebook (which is also the Base Case for Schwartz-Zippel) and this result is then implicit from a misc. exercise in chapter 1.    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ex 13.misc.2**  \n",
    "\n",
    "**(a and b, combined)** for finite field $\\mathbb K$ with $\\text{char}\\big(\\mathbb K\\big)\\geq 3$  \n",
    "*prove* that exactly half of the elements of the multiplicative group ($G:=(\\mathbb K -\\{0\\})^x$)  are squares.  Here are 2 proofs  \n",
    "\n",
    "i.)  using the linear algebra proof of 6.4(c) we see, that for $\\mathbb K =\\mathbb F_q$, we have the (q-1)th roots of unity with a block structure of  \n",
    "$P:= \\begin{bmatrix} C_{x^{\\frac{q-1}{2}}} &  \\mathbf 0 \\\\ \\mathbf 0&  C_{x^\\frac{q-1}{2}} \\\\ \\end{bmatrix}$\n",
    "which is two copies of the $\\frac{q-1}{2}$ roots of unity -- i.e. the eigenvalues are all mapped to that set  \n",
    "ii.) For a lightweight approach:  \n",
    "using 6.4(d) we know all non-zero elements in our multiplicative group $G$ observe $x^{q-1}=1$ and any element squared, i.e. $y=x^2$ observes  $y^\\frac{q-1}{2}=1$.   Since our elements commute (as scalars in a field do), any element obeying this is in the multiplicative (sub)group H.  Now since $G$ has a complete (/maximal cardinality) set of distinct $q-1$ roots of unity, this means it must have a complete set of $\\frac{q-1}{2}$ roots of unity -- since all of those are also $q-1$ roots of unity (i.e. if any of them were missing then $G$ wouldn't be maximally sized).  Since $H$ is maximally sized, it has cardinality $\\frac{q-1}{2}$. (check standard bound of roots to a polynomial). \n",
    "\n",
    "\n",
    "*prove* that if $\\alpha$ and $\\beta$ are non-squares, then $\\alpha\\cdot \\beta$ is a square.  \n",
    "using (ii) and Lagrange's Theorem  \n",
    "$\\alpha \\notin H\\implies \\big\\{H,\\alpha \\cdot H\\big\\}$ partitions $G$    \n",
    "$\\beta \\notin H\\implies \\big\\{H,\\beta \\cdot H\\big\\}$ partitions $G$  \n",
    "take the partition in the first line and multiply by $\\beta$ (which of course is invertible, so injective and must preserve partitions) to get:  \n",
    "\n",
    "$\\big\\{\\beta \\cdot \\alpha \\cdot H,\\beta H \\big\\}$ partitions our group, but so does $\\big\\{H,\\beta \\cdot H\\big\\}\\implies \\beta \\cdot \\alpha \\cdot H = G- \\beta H= H$ and in particular use the identity element in $H$, so $\\alpha\\cdot \\beta \\cdot 1\\in H$, as desired.  \n",
    "\n",
    "- - - - -  \n",
    "**extension**  \n",
    "if $\\alpha$ and $\\beta$ are non-squares, then so is $g=\\alpha+\\beta\\neq 0$  is not a square.  (If $\\beta = -\\alpha$ then $0$ is trivially a square, but it is not part of our multiplicative group $G$ so we ignore it here.)  \n",
    "The goal is to focus on using group multiplication, and Lagrange's Theorem, so we need to first move away from using Ring Operations involving \"+\".  \n",
    "\n",
    "so we have  \n",
    "$g=\\alpha+\\beta=\\alpha\\big(1+\\alpha^{-1}\\beta\\big)\\neq 0$  \n",
    "\n",
    "\n",
    "a) this implies $\\alpha^{-1}\\beta\\neq -1$  \n",
    "b.) $\\alpha \\notin H\\implies \\alpha^{-1}\\notin H$ since $H$ is a group, thus $\\alpha^{-1}$, like $\\alpha$ is not a square.  \n",
    "\n",
    "By prop 6.19(b) $\\big(1+\\alpha^{-1}\\beta\\big)\\in G$ i.e. roots to the defining polynomial are closed under addition -- i.e. they form a field.  And since the sum is non-zero, it must be in the multiplicative group.  \n",
    "# THIS APPEARS TO BE WRONG AND I AM CONFUSING GROUP OPERATIONS IT SEEMS... PROBABL\n",
    "That is, since $1\\in G \\text{ and }(\\alpha^{-1}\\cdot \\beta) \\in G \\text{ and } \\alpha^{-1}\\beta \\neq -1\\implies \\big(1+ \\alpha^{-1}\\beta\\big)\\in G$  \n",
    "\n",
    "But we may recurse on this. Observing that by the above solution we know $(\\alpha^{-1}\\cdot \\beta) \\in H $, so    \n",
    "$1\\in H \\text{ and }(\\alpha^{-1}\\cdot \\beta) \\in H \\text{ and } (\\alpha^{-1}\\beta) \\neq -1\\implies \\big(1+\\alpha\\beta\\big)\\in H$  \n",
    "Thus  \n",
    "$gH = \\alpha\\cdot \\Big(1+\\alpha^{-1}\\beta\\Big)H = \\alpha\\Big( h\\Big) \\cdot H=\\alpha\\Big( h \\cdot H\\Big)=\\alpha H$  \n",
    "and in particular looking at $1\\in H$, we see $g =g\\cdot 1 \\in \\alpha H$, i.e. $g$ is in the coset and not in $H$ itself, and we conclude $g=\\big(\\alpha + \\beta\\big)$ is not a square, as desired.  \n",
    "\n",
    "**TBC ADDITIONAL EXTENSION**  \n",
    "suppose $p$ is odd and $\\mathbb K$ is a quadratic extension of $\\mathbb F_p$  \n",
    "\n",
    "then if $\\gamma \\in \\mathbb K$ but $\\gamma \\notin \\mathbb F_p$  \n",
    "then $\\gamma$ obeys $x^{q-1}=1$  with $q=p^2$, i.e. $x^{(p+1)(p-1)}=1$ but $x^{(p-1)}\\neq 1$  \n",
    "some supporting calcs in  http://localhost:8888/notebooks/LinearAlgebra/gf2_solver.ipynb  \n",
    "for ideas related to this and the 2nd to last matrix problem  \n",
    "prop (3.3), p 497, would seem to be relevant  \n",
    "\n",
    "**Probably delete this  ADDITIONAL EXTENSION**  \n",
    "- - - - -  \n",
    "\n",
    "**(c)** prove that in a finite field of even order every element is a square.  \n",
    "If a finite field has even order, then it has characteristic 2.  The Frobenius Homomorphism: $x\\mapsto x^p = x^2$ is injective because if $x^p= (x')^p$ then $x':= x+y\\implies x^p = (x+y)^p = x^p + y^p\\implies y^p =0\\implies y=0$ since this is a field.  (In particular we have $p=2$ here.)  Thus the homomophism map is an injective map with domain and codomain of $\\mathbb K$, which has finitely many elements, hence it is surjective.  But since the squaring is surjective, and zero is mapped to zero, then all elements of $\\mathbb K-\\{0\\}$ are squares.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ex 13.Misc.4**  \n",
    "write this up and email people  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ex 13.Misc.10**  \n",
    "Prove that the homomorphism $\\phi: SL_n\\big(\\mathbb Z\\big) \\longrightarrow SL_n\\big(\\mathbb F_p\\big)$ obtained by reducing the matrix entries modulo $p$ is surjective.  \n",
    "\n",
    "referencing  ex. 2.2.18c generalized slightly to accommodate chapter 3 concepts like fields $\\mathbb F_p$ (see commentary in \"Artin_chp2_SLN_subgroup_generators.ipynb\"), we see every $A\\in SL_n\\big(\\mathbb F_p\\big)$ is the product of finitely many elementary type 1 matrices (or their transpose), i.e. where each elementary type 1 (or its transpose) is given by $E_1 = I + \\alpha\\mathbf e_i\\mathbf e_j^T$ for $j\\neq i$  for some $\\alpha \\in \\mathbb K = \\mathbb F_p$, i.e. in this case $\\alpha \\in \\big\\{0,1,2,....,p-1\\big\\}\\implies \\alpha \\in \\mathbb Z\\implies E_1 \\in SL_n\\big(\\mathbb Z\\big)$.  The 'splitting property' of homomorpisms then gives surjectivity.  That is \n",
    "\n",
    "$\\phi\\Big(E_1\\Big) =E_1$, so  \n",
    "\n",
    "$B\\in SL_n\\big(\\mathbb F_p\\big) = E_1^{(1)}E_1^{(2)}... E_1^{(m)}=\\phi\\Big(E_1^{(1)}\\Big)\\phi\\Big(E_1^{(2)}\\Big)... \\phi\\Big(E_1^{(m)}\\Big)=\\phi\\Big(E_1^{(1)}E_1^{(2)}... E_1^{(m)}\\Big)= \\phi\\Big(A\\Big)$  \n",
    "\n",
    "for $A= E_1^{(1)}E_1^{(2)}... E_1^{(m)}$, so $A$ is the product of integer matrices and  \n",
    "$\\det\\Big(A\\Big)= \\det\\Big(E_1^{(1)}E_1^{(2)}... E_1^{(m)}\\Big)= \\det\\Big(E_1^{(1)}\\Big)\\det\\Big(E_1^{(2)}\\Big)... \\Big(E_1^{(m)}\\Big)=1\\implies A\\in SL_n\\big(\\mathbb Z\\big)$  \n",
    "\n",
    "Thus for every $B\\in SL_n\\big(\\mathbb F_p\\big)$, $\\phi^{-1}\\Big(B\\big) \\neq \\{\\}$ because there is at least one $A\\in SL_n\\big(\\mathbb Z\\big)$ in the preimage of $\\phi$.  Therefore $\\phi$ is surjective.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
