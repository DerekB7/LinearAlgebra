{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Schur's Inequality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Schur's Inequality tells us that for any $n$ x $n$ matrix in a complex scalar field , i.e. $\\mathbf A \\in \\mathbb C^{n x n}$\n",
    "\n",
    "**Claim:**\n",
    "\n",
    "$\\big \\Vert \\mathbf A \\big \\Vert_F^{2} = \\text{trace}\\big(\\mathbf A^H \\mathbf A\\big) \\geq \\sum_{i = 1}^{n} \\big \\vert \\lambda_i\\big \\vert ^2 \\geq \\big \\vert \\sum_{i = 1}^{n} \\lambda_i^2\\big \\vert = \\Big \\vert \\text{trace}\\big(\\mathbf A \\mathbf A\\big) \\Big \\vert $\n",
    "\n",
    "note that  $\\sum_{i = 1}^{n} \\big \\vert \\lambda_i\\big \\vert ^2 \\geq \\big \\vert \\sum_{i = 1}^{n} \\lambda_i^2\\big \\vert$ was included at the end via the triangle inequality\n",
    "\n",
    "\n",
    "**Background:**\n",
    "\n",
    "We can collect all of the eigenvalues $\\big \\vert \\lambda_1 \\big \\vert \\geq \\big \\vert\\lambda_2 \\big \\vert\\geq \\big \\vert \n",
    "\\lambda_3 \\big \\vert \\geq ... \\geq \\big \\vert\\lambda_n \\big \\vert$ in the diagonal matrix $\\mathbf D$, and restate Schur's Inequality as:  \n",
    "\n",
    "\n",
    "$\\big \\vert \\big \\vert \\mathbf A \\big \\vert \\big \\vert_F^{2} \\geq  \\big \\vert \\big \\vert \\mathbf D \\big \\vert \\big \\vert_F^{2} = \\text{trace}\\big(\\mathbf D^H \\mathbf D\\big)$ \n",
    "\n",
    "Note that by Schur Decomposition, we can write $\\mathbf A = \\mathbf {Q R Q}^H$  where $\\mathbf Q$ is unitary, and $\\mathbf R$ is upper triangular.  As a reminder, recall that the eigenvalues of an upper triangular matrix are on its diagonal, hence $\\mathbf R_{i,i} = \\lambda_i$.\n",
    "\n",
    "\n",
    "**Proof:**\n",
    "revisiting the inequality, we write this as:\n",
    "\n",
    "\n",
    "$\\big \\vert \\big \\vert \\mathbf A \\big \\vert \\big \\vert_F^{2} = \\text{trace}\\big( \\big(\\mathbf {Q R Q}^H\\big)^H \\big( \\mathbf {Q R Q}^H \\big) \\big) = \\text{trace}\\big(\\mathbf {Q R}^H \\mathbf Q^H \\mathbf {Q) R Q}^H\\big) \\geq \\text{trace}\\big(\\mathbf D^H \\mathbf D\\big)$\n",
    "\n",
    "\n",
    "$\\text{trace}\\big(\\mathbf {Q R}^H \\mathbf{R Q}^H\\big) = \\text{trace}\\big(\\mathbf{(Q}^H \\mathbf{Q) R}^H \\mathbf R \\big) \\geq \\text{trace}\\big(\\mathbf D^H \\mathbf D\\big)$\n",
    "\n",
    "$\\text{trace}\\big(\\mathbf{R}^H \\mathbf R \\big) \\geq \\text{trace}\\big(\\mathbf D^H \\mathbf D\\big)$\n",
    "\n",
    "$\\text{trace}\\big(\\mathbf{ R}^H \\mathbf R \\big) = \\big \\vert \\big \\vert \\mathbf R \\big \\vert \\big \\vert_F^{2} =  \\Big(\\sum_{k = 1}^{n} \\sum_{j \\neq k}  \\mathbf R^H_{k,j} \\mathbf R_{j,k}\\Big) + \\text{trace}\\big(\\mathbf D^H \\mathbf D\\big) \\geq \\text{trace}\\big(\\mathbf D^H \\mathbf D\\big)$\n",
    "\n",
    "\n",
    "- - - - -\n",
    "Alternatively, we may say:\n",
    "\n",
    "$\\big \\vert \\big \\vert \\mathbf A \\big \\vert \\big \\vert_F^{2} = \\big \\vert \\big \\vert \\mathbf R \\big \\vert \\big \\vert_F^{2} = \\big \\vert \\big \\vert \\big(\\mathbf R - \\mathbf D\\big) \\big \\vert \\big \\vert_F^{2}  + \\big \\vert \\big \\vert \\mathbf D \\big \\vert \\big \\vert_F^{2} \\geq \\big \\vert \\big \\vert \\mathbf D \\big \\vert \\big \\vert_F^{2}$\n",
    "\n",
    "with equality **iff**\n",
    "$\\big \\vert \\big \\vert \\big(\\mathbf R - \\mathbf D\\big) \\big \\vert \\big \\vert_F^{2} = 0$,\n",
    "which occurs **iff** $\\mathbf R - \\mathbf D = \\mathbf 0$, aka this occurs **iff** $\\mathbf R = \\mathbf D$.  Thus in the case where the Schur Inequality is an equality, we know that $\\mathbf A$ is diagonalizable with mutually orthonormal eigenvectors $\\mathbf A = \\mathbf {Q RQ}^H = \\mathbf {Q D Q}^H$.  Note that this does *not* make any claims as to whether or not the eigenvalues are real or complex.\n",
    "\n",
    "\n",
    "**Technical Note:** if the inequality is an equality, then we say that $\\mathbf A$ **is a normal matrix.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interesting L1 style extension:**  \n",
    "\n",
    "The stated inequality which is based around summing squared entries is this:  \n",
    "$\\sum_{k=1}^n \\sigma_k^2 = \\big \\Vert \\mathbf A \\big \\Vert_F^{2} = \\text{trace}\\big(\\mathbf A^H \\mathbf A\\big) \\geq \\sum_{i = 1}^{n} \\big \\vert \\lambda_i\\big \\vert ^2 \\geq \\big \\vert \\sum_{i = 1}^{n} \\lambda_i^2\\big \\vert = \\Big \\vert \\text{trace}\\big(\\mathbf A \\mathbf A\\big) \\Big \\vert $\n",
    "\n",
    "There is something of an interesting L1 point of view of this, which relates the trace to the nuclear norm (sum of singular values)\n",
    "\n",
    "$\\sum_{k=1}^n \\sigma_k = \\text{trace}\\big(\\mathbf Y\\big) \\geq   \\sum_{k = 1}^{n} \\big \\vert \\lambda_k\\big \\vert \\geq \\big \\vert \\text{trace}\\big(\\mathbf {QY}\\big) \\big \\vert  = \\big \\vert \\sum_{i = 1}^{n} \\lambda_i \\big \\vert = \\Big \\vert \\text{trace}\\big(\\mathbf A \\big) \\Big \\vert $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The proof comes from noticing that  similarity transforms do not change the trace, so we may consider upper triangular $\\mathbf T$   \n",
    "\n",
    "$\\mathbf T = \\mathbf U^* \\mathbf A \\mathbf U$  \n",
    "\n",
    "but we can get the desirect relation by multiplying \n",
    "\n",
    "$\\mathbf {DT} = \\mathbf R$  \n",
    "Such that $\\mathbf R$ has all eigenvalues real-nonnegative.  \n",
    "i.e. $\\mathbf D$ is diagonal with components on the unit circle.  This means it is unitary as well, i.e. \n",
    "$\\mathbf D^* \\mathbf D = \\mathbf I$   \n",
    "\n",
    "but multiplication by a unitary matrix does not change the singular values, so we have  \n",
    "\n",
    "$\\big \\vert \\text{trace}\\big(\\mathbf {A} \\big)\\big \\vert$   \n",
    "$= \\big \\vert\\text{trace}\\big(\\mathbf {T} \\big)\\big \\vert$   \n",
    "$= \\big \\vert\\sum_{k=1}^n \\lambda_k \\big \\vert$   \n",
    "$\\leq \\sum_{k=1}^n \\big \\vert \\lambda_k \\big \\vert$   \n",
    "$= \\text{trace}\\big(\\mathbf {DT} \\big)$  \n",
    "$= \\big \\vert \\text{trace}\\big(\\mathbf {DT} \\big)\\big \\vert$  \n",
    "$= \\big \\vert\\text{trace}\\big(\\mathbf {DQY} \\big)\\big \\vert$  \n",
    "$\\leq \\text{trace}\\big(\\mathbf {Y} \\big)$  \n",
    "$= \\sum_{k=1}^n \\sigma_k $    \n",
    "$=\\big\\Vert\\mathbf {A}\\big\\Vert_{S_1}$  \n",
    "\n",
    "where $\\mathbf {QY} =\\mathbf T$ i.e. polar decomposition on the matrix $\\mathbf T$ (which is unitarily similar to $\\mathbf A$), which by construction is upper triangular with all real-non-negative eigenvalues.  It is thus Hermitian positive (semi)definite *iff* it is diagonal, which occurs *iff* $\\mathbf A$ is normal  -- i.e. the purpose of this entire writeup. But to be clear, as noted in the \"Fun with trace\" writeup-- equality conditions of the second inequality are clear in the case of $\\mathbf A$ being non-singular, however the exact inequality conditions are a bit muddy and less clear to your author when $\\mathbf A$ is singular.  \n",
    "\n",
    "The first inequality is the triangle inequality, and the second inequality (see \"Fun with trace writeup\") comes from the fact that $\\big(\\mathbf{DQ}\\big)$  is unitary and the magnitude of the trace of the product of a unitary matrix and a Hermitian positive semi-definite matrix is bounded above by the trace of said Hermitian positive semi-definite matrix -- which is in some ways an extension or generalization of the triangle inequality (esp polar form in $\\mathbb C$) to matrix traces.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as a reminder we know \n",
    "\n",
    "$\\big(\\sum_{k=1}^n \\sigma_k^2\\big)^\\frac{1}{2} \\leq  \\sum_{k=1}^n \\sigma_k \\leq n^\\frac{1}{2}\\big(\\sum_{k=1}^n \\sigma_k^2\\big)^\\frac{1}{2}$  \n",
    "by triangle inequality and then cauchy-schwarz (ones trick)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us a nice proof of *submulticativity* of the  Schatten 1-Norm / Nuclear Norm\n",
    "(cleanly for the case of n x n matrices -- with some care and padding by zeros, we can generalize to rectangular matrices where the product is defined)  \n",
    "\n",
    "if we consider the matrix $\\big(\\mathbf {AB}\\big)$ in polar form, we see  \n",
    "\n",
    "$\\big\\Vert\\mathbf {AB}\\big\\Vert_{S_1}$   \n",
    "$=\\sum_{i=1}^n \\sigma_{i (AB)}$  \n",
    "$= \\text{trace}\\Big(\\mathbf Q^* \\big(\\mathbf {AB}\\big)\\Big)$  \n",
    "$= \\text{trace}\\Big(\\big( \\mathbf Q^* \\mathbf A \\big) \\mathbf B\\big)\\Big)$  \n",
    "$= \\big \\vert \\text{trace}\\Big(\\big( \\mathbf Q^* \\mathbf A \\big) \\mathbf B\\big)\\Big)\\big \\vert $  \n",
    "$\\leq \\big \\Vert \\mathbf Q^* \\mathbf A\\big \\Vert_F \\big \\Vert  \\mathbf B\\big \\Vert_F $  \n",
    "$=  \\big \\Vert \\mathbf A\\big \\Vert_F \\big \\Vert  \\mathbf B\\big \\Vert_F $  \n",
    "$= \\big(\\sum_{i=1}^n \\sigma_{i (A)}^2\\big)^\\frac{1}{2}\\big(\\sum_{i=1}^n \\sigma_{i (B)}^2\\big)^\\frac{1}{2}  $  \n",
    "$\\leq \\big(\\sum_{i=1}^n \\sigma_{i (A)}\\big)\\big(\\sum_{i=1}^n \\sigma_{i (B)}^2\\big) $  \n",
    "$= \\big\\Vert\\mathbf {A}\\big\\Vert_{S_1} \\big\\Vert\\mathbf {B}\\big\\Vert_{S_1}$    \n",
    "where the first inequality follows by Cauchy Schwarz and the second inequality follows by above mentioned triangle inequality  \n",
    "\n",
    "\n",
    "as for *subadditivity* of the Schatten 1-Norm / Nuclear Norm, we can see it follows via quasi-linearization:  \n",
    "Using an inequality from 'Fun with trace' involving Hermitian positive (semi)definite matrices and unitary matrices and the resulting trace, we can define  \n",
    "\n",
    "$ \\sum_{i=1}^n \\sigma_{i (X)} = \\big\\Vert\\mathbf {X}\\big\\Vert_{S_1}  := \\big \\vert \\text{trace}\\Big(\\mathbf Q_0^* \\big(\\mathbf {X}\\big)\\Big)\\big \\vert$  \n",
    "\n",
    "where $\\mathbf Q_0^*$ is the (not necessarily unique) unitary matrix that maximizes \n",
    "$\\big \\vert \\text{trace}\\Big(\\mathbf Q_0^* \\big(\\mathbf {X}\\big)\\Big)\\big \\vert$  \n",
    "\n",
    "but if (sticking with square matrices for now)  \n",
    "\n",
    "$\\mathbf X = \\mathbf A + \\mathbf B$  \n",
    "\n",
    "then  \n",
    "$\\Big\\Vert\\mathbf {X}\\Big\\Vert_{S_1}$   \n",
    "$= \\Big \\vert \\text{trace}\\Big(\\mathbf Q_0^* \\big(\\mathbf {X}\\big)\\Big)\\Big \\vert$  \n",
    "$= \\Big \\vert \\text{trace}\\Big(\\mathbf Q_0^* \\big(\\mathbf {A} + \\mathbf B\\big)\\Big)\\Big \\vert$  \n",
    "$= \\Big \\vert \\text{trace}\\Big(\\mathbf Q_0^* \\mathbf {A}\\Big) + \\text{trace}\\Big(\\mathbf Q_0^* \\mathbf {B}\\Big)\\Big \\vert$  \n",
    "$\\leq \\Big \\vert \\text{trace}\\Big(\\mathbf Q_0^* \\mathbf {A}\\Big)\\Big \\vert  + \\Big \\vert \\text{trace}\\Big(\\mathbf Q_0^* \\mathbf {B}\\Big)\\Big \\vert$  \n",
    "$\\leq \\Big \\vert \\text{trace}\\Big(\\mathbf Q_1^* \\mathbf {A}\\Big)\\Big \\vert  + \\Big \\vert \\text{trace}\\Big(\\mathbf Q_2^* \\mathbf {B}\\Big)\\Big \\vert$  \n",
    "$= \\big\\Vert\\mathbf {A}\\big\\Vert_{S_1} +  \\big\\Vert\\mathbf {B}\\big\\Vert_{S_1}$   \n",
    "\n",
    "where the first inequality follows by triangle inequality, and the second inequality follows because 2 choices are better than one  \n",
    "\n",
    "Positive definiteness follows immediately for the Schatten 1-norm because  \n",
    "$0 \\leq \\Big\\Vert\\mathbf {X}\\Big\\Vert_{F} = \\Big\\Vert\\mathbf {X}\\Big\\Vert_{S_2} = \\big(\\sum_{k=1}^n \\sigma_k^2\\big)^\\frac{1}{2} \\leq  \\sum_{k=1}^n \\sigma_k = \\Big\\Vert\\mathbf {X}\\Big\\Vert_{S_1}$  \n",
    "and the Frobenius norm of a matrix is zero *iff* the matrix is zero.  Finally the homogeniety with respect to positive scaling is immediate from looking at SVD of a matrix or its polar decomposition and the fact that scalar multiplication commutes.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Schatten 1-norm, being intimately linked in with the trace can be a convenient metric to use when dealing with the trace and bounding it (just about any metric will work in finite dimensions though our life is much more enjoyable if we choose wisely).  \n",
    "\n",
    "\n",
    "**application:**   \n",
    "consider the mapping  \n",
    "$f_k: \\mathbb C^{n x n} \\longrightarrow \\mathbb C$   \n",
    "$f_k\\big(\\mathbf C^k\\big) = \\text{trace}\\big(\\mathbf C^k\\big)$  \n",
    "for natural number $k$  \n",
    "\n",
    "we prove this mapping is continuous with respect to the coefficients in $\\mathbf C$ because for any $\\epsilon \\gt 0$ there exists some $\\delta \\gt 0$ where \n",
    " \n",
    "$f_k\\Big( N\\big(\\mathbf C,\\delta\\big)\\Big) \\subseteq N\\Big( f_k\\big(\\mathbf C\\big),\\epsilon\\Big) $  \n",
    "and we use the 1 norm (here, meaning the sum of the magnitude of components in a matrix) to give our metric underlying the N, neighborhood, function above   \n",
    "\n",
    "that is we can restate the above explicitly using the 1 norm as   \n",
    "$\\Big \\Vert \\text{trace}\\Big(\\big(\\mathbf C + \\mathbf E\\big)^k\\Big) - \\text{trace}\\big(\\mathbf C^k\\big)\\Big \\Vert_{1}  = \\Big \\vert \\text{trace}\\Big(\\big(\\mathbf C + \\mathbf E\\big)^k\\Big) - \\text{trace}\\big(\\mathbf C^k\\big)\\Big \\vert \\lt \\epsilon  $  \n",
    "\n",
    "for any  \n",
    "$\\big\\Vert\\mathbf {E}\\big\\Vert_{1} =   \\big\\Vert \\big(\\mathbf C + \\mathbf E\\big) - \\mathbf C\\big\\Vert_{1}\\lt \\delta$  \n",
    "\n",
    "and along the way, use the nuclear norm as a nice bridge between results  \n",
    "- - - -  \n",
    "\n",
    "**proof**  \n",
    "the idea is to make use of linearity of the trace and the binomial theorem to get  \n",
    "\n",
    "$\\Big \\Vert \\text{trace}\\Big(\\big(\\mathbf C + \\mathbf E\\big)^k\\Big) - \\text{trace}\\big(\\mathbf C^k\\big)\\Big \\Vert_{1}  $   \n",
    "$= \\Big \\Vert \\text{trace}\\Big(\\big(\\mathbf C + \\mathbf E\\big)^k - \\mathbf C^k\\Big)\\Big \\Vert_{1}  $  \n",
    "$= \\Big \\Vert \\text{trace}\\Big(\\sum_{i=1}^k \\binom{k}{i}\\mathbf C^{i} \\mathbf E^{k-i}\\Big) \\Big \\Vert_{1}  $    \n",
    "\n",
    "**except**   \n",
    "while there will be $\\binom{k}{i}$ terms in the summation with *total* multiplications of $i$ by $\\mathbf C$ and total multiplications by $\\mathbf E$ of $(k-i)$ we know that matrix multiplications do not generally commute.  However submultiplicativity and subadditivity of 'nice' norms -- in particular the Schatten 1 norm-- come to the rescue, by mapping to a convenient scalar case which is an upperbound, where we do have commutativity amongst the resulting real (and non-negative) scalars.   \n",
    "\n",
    "with  \n",
    "$ 0 \\leq m = \\big \\Vert  \\mathbf C\\big \\Vert_{S_1} $   \n",
    "we can prove the desired result as follows  \n",
    "\n",
    "$\\Big \\Vert \\text{trace}\\Big(\\big(\\mathbf C + \\mathbf E\\big)^k\\Big) - \\text{trace}\\big(\\mathbf C^k\\big)\\Big \\Vert_{1}$    \n",
    "$= \\Big\\vert \\text{trace}\\Big(\\big(\\mathbf C + \\mathbf E\\big)^k- \\mathbf C^k \\Big)\\Big\\vert$  \n",
    "$\\leq  \\Big \\Vert \\big(\\mathbf C + \\mathbf E\\big)^k- \\mathbf C^k \\Big \\Vert_{S_1}$ see preceding section \"Interesting L1 style extension\" with $\\mathbf A:= \\Big(\\big(\\mathbf C + \\mathbf E\\big)^k- \\mathbf C^k\\Big)$     \n",
    "$\\leq  \\sum_{i=1}^k \\binom{k}{i} \\Big \\Vert  \\mathbf C\\Big \\Vert_{S_1}^{i} \\Big\\Vert\\mathbf E \\Big \\Vert_{S_1}^{k-i}  $     (by subadditivity, then submultiplicativity)  \n",
    "$= \\sum_{i=1}^k \\binom{k}{i} m^{i} \\Big\\Vert\\mathbf E \\Big \\Vert_{S_1}^{k-i}  $   \n",
    "$\\leq  \\sum_{i=1}^k \\binom{k}{i} m^{i} \\Big\\Vert\\mathbf E \\Big \\Vert_{S_1}  $  for sufficiently small $\\Big\\Vert\\mathbf E  \\Big \\Vert_{S_1}$ ($i.e. \\leq 1$)  \n",
    "$\\leq \\Big\\Vert\\mathbf E  \\Big \\Vert_{S_1} \\cdot \\sum_{i=0}^k \\binom{k}{i} m^{i}   $    \n",
    "$= \\Big\\Vert\\mathbf E  \\Big \\Vert_{S_1} \\cdot \\big(1 + m\\big)^k  $     \n",
    "$= \\Big\\Vert\\mathbf E  \\Big \\Vert_{S_1} \\cdot M  $  for some positive constant $M\\gt 0$   \n",
    "$\\leq \\big(n^\\frac{1}{2}\\cdot \\sum_{i}\\sum_{j}  \\vert e_{i,j}\\vert\\big)\\cdot M$  \n",
    "$= n^\\frac{1}{2} \\cdot \\big \\Vert \\mathbf E\\big \\Vert_1 \\cdot M$  \n",
    "\n",
    "hence selecting $\\delta := \\min\\big(1, \\frac{\\epsilon}{\\sqrt{n} \\cdot M}\\big)$  completes the argument  \n",
    "- - - -  \n",
    "with respect to the final inequality, consider that   \n",
    "\n",
    "$\\Big\\Vert\\mathbf E  \\Big \\Vert_{S_1} = \\sum_{k=1}^n \\sigma_k \\leq n^\\frac{1}{2}\\big(\\sum_{k=1}^n \\sigma_k^2\\big)^\\frac{1}{2} =  n^\\frac{1}{2}\\cdot \\big(\\sum_{i}\\sum_{j}  \\vert e_{i,j}\\vert^2\\big)^\\frac{1}{2} \\leq n^\\frac{1}{2}\\cdot \\sum_{i}\\sum_{j}  \\vert e_{i,j}\\vert$  \n",
    "\n",
    "where results follow from Cauchy-Schwarz, then triangle inequality  \n",
    "these inequalities make it clear that we use the Schatten 1 norm for convenience, but e.g. we could have just as easily used the Schatten 2 norm (aka Frobenius norm) because    \n",
    "\n",
    "$\\big \\vert \\text{trace}\\big(\\mathbf A\\big)  \\big \\vert $  \n",
    "$ =\\big \\vert \\sum_{k=1}^n  \\lambda_k \\big \\vert$  \n",
    "$\\leq \\sum_{k=1}^n \\big \\vert \\lambda_k \\big \\vert$  \n",
    "$\\leq n^\\frac{1}{2}\\big(\\sum_{k=1}^n \\big \\vert \\lambda_k \\big \\vert^2\\big)^\\frac{1}{2}$  \n",
    "$\\leq n^\\frac{1}{2}\\big(\\sum_{k=1}^n \\sigma_k^2\\big)^\\frac{1}{2}$  (Schur's Inequality)  \n",
    "$= n^\\frac{1}{2}\\big \\Vert A \\big \\Vert_F$  \n",
    "$\\leq n^\\frac{1}{2}\\cdot \\sum_{i}\\sum_{j}  \\vert e_{i,j}\\vert$  \n",
    "$= n^\\frac{1}{2}\\big \\Vert A \\big \\Vert_1$  \n",
    "- - - -   \n",
    "\n",
    "thus we've proven for any given $\\epsilon \\gt 0$ there exists some $\\delta \\gt 0$ such that     \n",
    "$f_k\\Big( N\\big(\\mathbf C,\\delta\\big)\\Big) \\subseteq N\\Big( f_k\\big(\\mathbf C\\big),\\epsilon\\Big)$\n",
    "\n",
    "hence for each natural number $k$, we see that $f_k$ is continuous for any n x n matrix $\\mathbf C$ and in particular varies continuously with respect to the magnitude of the change in components of $\\mathbf C$  \n",
    "\n",
    "**corollary:**  \n",
    "The coefficients of the characteristic polynomial of a matrix $\\in \\mathbb C^{n x n}$ vary continuously with its entries.  \n",
    "\n",
    "*proof:*  \n",
    "apply Newton's Identities (see 2 proofs at end of Vandermonde Matrix writeup) in sequence.  \n",
    "The above proves that $\\text{trace}\\big(\\mathbf C^k\\big)$ varies continously with the entries of $\\mathbf C$.  But the characteristic polynomial  \n",
    "$p(x) = x^n + a_{n-1}x^{n-1} + a_{n-2}x^{n-2} +... + a_{1}x^{1}+ a_0$  \n",
    "\n",
    "has a leading coefficient of one, and $a_{n-1}$ immediately and obviously varies continuously with the coefficients of the diagonal of $\\mathbf C$.  This is enough to set up an induction (which we carry out for finitely many steps since a polynomial has only finitely many terms).  \n",
    "\n",
    "in particular for $0\\lt r \\lt n$ we have  \n",
    "\n",
    "$a_{n-r}  = -\\frac{1}{r} \\sum_{k=1}^{r} a_{n-r + k}\\cdot \\text{trace}\\big(\\mathbf C^k\\big) $\n",
    "\n",
    "where by inductive hypothesis, we know that all $ a_{n-r + k}$ vary continuously with the components of $\\mathbf C$, and the preceding proofs shows that $\\text{trace}\\big(\\mathbf C^k\\big)$ varies continuously as well.  It is immediate that $a_{n-r}$ varies continuously with components of $\\mathbf C$ because it is written as a linear combination/ composition involving finitely many terms of sums and products consisting solely of items that vary continuously with components of $\\mathbf C$.  \n",
    "\n",
    "the final terms (i.e. determinant multiplied by the sign function) also varies continuously with components of $\\mathbf C$ because \n",
    "\n",
    "$(-1)^n \\det\\big(\\mathbf C\\big)= a_0 = \\frac{-1}{n} \\big(a_1 \\cdot \\text{trace}\\big(\\mathbf C^{1}\\big) +  a_2 \\cdot \\text{trace}\\big(\\mathbf C^{2}\\big)  + ... + a_n \\cdot \\text{trace}\\big(\\mathbf C^{n}\\big)\\big)$  \n",
    "\n",
    "(via Netwon's Identities, or Cayley Hamilton.  Note that if $n$ is even then $a_0$ is equal to the determinant.  If $n$ is odd we can of course re-run the above argument on $\\big(-\\mathbf C\\big)$ to see that the determinant of $\\mathbf C$ varies continuously with its coefficients-- there are more direct approaches-- in particular working with principal minors--though teasing out the conclusion via manipulation of the trace has appeal to your author.)  \n",
    "\n",
    "This again, consists of a finite number of operations involving sums and products of things that vary continuously with the components of $\\mathbf C$ and hence we conclude that the result, $a_0$ varies continuously with the coefficients with $\\mathbf C$.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in the special case where $\\mathbf A$ is a rank one matrix **in reals**, we get the following:\n",
    "\n",
    "where $\\mathbf A = \\mathbf {xy}^T = \\mathbf {xy}^H$\n",
    "\n",
    "hence \n",
    "\n",
    "$\\big \\vert \\text{trace}\\Big(\\big(\\mathbf {xy}^H\\big)^2\\Big)\\big \\vert = \\big \\vert \\lambda_1^2 + 0 + 0+ .... + 0 \\big \\vert  = \\big \\vert \\lambda_1^2 \\big \\vert = \\big \\vert \\lambda_1 \\big \\vert^2 $\n",
    "\n",
    "i.e. triangle inequality is not needed, and we can in fact look at \n",
    "\n",
    "$\\big \\Vert \\mathbf D \\big \\Vert_F^2 = \\big \\vert \\text{trace}\\big(\\big(\\mathbf {xy}^H\\big)^2\\big)\\big \\vert = \\big \\vert \\text{trace}\\big(\\mathbf {xy}^H\\big)\\big \\vert^2 = \\big \\vert \\lambda_1 \\big \\vert^2$  \n",
    "\n",
    "now we see that \n",
    "\n",
    "$\\big \\vert \\big \\vert \\mathbf {xy}^H \\big \\vert \\big \\vert_F^{2} =  \\frac{1}{2}\\big \\vert \\big \\vert \\big(\\mathbf R - \\mathbf R^H \\big) \\big \\vert \\big \\vert_F^{2}  + \\big \\vert \\big \\vert \\mathbf D \\big \\vert \\big \\vert_F^{2} = \\frac{1}{2}\\big \\vert \\big \\vert \\big(\\mathbf {xy}^H - \\mathbf {yx}^H \\big) \\big \\vert \\big \\vert_F^{2}  + \\big \\vert \\big \\vert \\mathbf D \\big \\vert \\big \\vert_F^{2} =  \\frac{1}{2}\\big \\vert \\big \\vert \\big(\\mathbf {xy}^H - \\mathbf {yx}^H \\big) \\big \\vert \\big \\vert_F^{2}  + \\big \\vert \\text{trace}\\big(\\mathbf {xy}^H\\big)\\big \\vert^2$\n",
    "\n",
    "This is the Lagrange Identity in reals.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Immediate Consequence:** \n",
    "\n",
    "For any **unitary** (or if in reals: Orthgonal) $n$ x $n$ matrix, $\\mathbf U$, which must have eigenvalues, $\\big \\vert \\lambda_k\\big \\vert = 1$, for $k = \\{1, 2, ...,n\\}$  (see middle part of \"Fun_with_Trace_and_Quadratic_Forms_CauchySchwartz_.ipynb\" under the heading *Thoughts on Unitary Matrices* for a proof of this, using quadratic forms / singular values, to upper and lower bound the eigenvalue magnitudes)\n",
    "\n",
    "Noting that $\\mathbf U^H \\mathbf U = \\mathbf I$, for any $n$ x $n$ unitary matrix and applying Schur's Inequality, we have:\n",
    "\n",
    "\n",
    "\n",
    "$\\big \\vert \\big \\vert \\mathbf U \\big \\vert \\big \\vert_F^{2} = \\text{trace}\\big(\\mathbf U^H \\mathbf U\\big) = \\text{trace}\\big(\\mathbf I \\big) = n \\geq \\big \\vert \\big \\vert \\mathbf D \\big \\vert \\big \\vert_F^{2} =  \\sum_{i = 1}^{n}  \\lambda_i^H \\lambda_i= \\sum_{i = 1}^{n} \\big \\vert \\lambda_i\\big \\vert ^2 =  \\sum_{i = 1}^{n} 1^2  =  \\sum_{i = 1}^{n} 1 = n  $\n",
    "\n",
    "hence: \n",
    "\n",
    "$n = \\big \\vert \\big \\vert \\mathbf U \\big \\vert \\big \\vert_F^{2} = \\big \\vert \\big \\vert \\mathbf D \\big \\vert \\big \\vert_F^{2}$ \n",
    "\n",
    "which tell us that, if we wanted, we could diagonalize $\\mathbf U$ with mutually orthonormal eigenvectors, $\\mathbf U = \\mathbf{QDQ}^H$.\n",
    "\n",
    "\n",
    "*Link of SVD and Eigen decompositions for Normal Matrices* \n",
    "\n",
    "If we were going to do SVD on $\\mathbf U$, notice that the left and right singular vectors would be the same, except for an issue of rotations on complex plane. Starting with the above eigendecomposition:  \n",
    "\n",
    "$\\mathbf U = \\mathbf {Q  D Q}^H $\n",
    "\n",
    "now we make the substitution $\\mathbf D =\\mathbf{\\Lambda \\Sigma} = \\mathbf{\\Lambda \\mathbf I} = \\mathbf \\Lambda $\n",
    "\n",
    "That is we factor $\\mathbf D$ into two diagonal matrices -- $\\mathbf \\Sigma$ which must be real valued and non-negative, and hence has the magnitudes of all the values in $\\mathbf D$, and the remaining complex numbers / rotations / angles (i.e. information on the unit circle) in $\\mathbf D$ gets allocated to $\\mathbf \\Lambda$.  Note that since all singular values are equal to one in a unitary matrix, then $\\mathbf \\Sigma = \\mathbf I$, which can make the decomposition a bit pedantic.    \n",
    "\n",
    "So, we have $\\mathbf U =  \\mathbf {Q  D Q}^H =  \\mathbf{ Q \\Lambda} \\mathbf {\\Sigma Q }^H= \\big( \\mathbf{ Q \\Lambda}\\big)  \\mathbf {\\Sigma Q }^H = \\big( \\mathbf{ Q \\Lambda}\\big) \\mathbf I \\mathbf {Q }^H = \\big( \\mathbf{ Q \\Lambda}\\big)  \\mathbf {Q }^H $, which is to say that the left and right singular vectors are the same -- and in fact can be chosen to be the eigenvectors, *if* we relax the constraint that $\\mathbf \\Sigma$ has only real valued, non-negative entries. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**More on Normal Matrices**\n",
    "\n",
    "Another way to define / test for a **square matrix** $\\mathbf A$ being normal, is the following:\n",
    "\n",
    "if \n",
    "\n",
    "$\\mathbf{AA}^H = \\mathbf A^H \\mathbf A$  \n",
    "\n",
    "Then: first we observe that $\\big(\\mathbf A^H \\mathbf A\\big)$ is a Hermitian matrix, and $\\big(\\mathbf{AA}^H\\big)$ is also a Hermitian matrix.  This means that each can be diagonalized with a unitary basis of eigenvectors.  \n",
    "\n",
    "Because $\\mathbf{AA}^H = \\mathbf A^H \\mathbf A$, we know that each side has the same eigenvalues. (Side note: in general $\\mathbf{AB}$ and $\\mathbf B \\mathbf A$ must have the same non-zero eigenvalues, as proven in the Vandermonde Matrix writeup.)  Because each side is equivalent, we can select eigenvectors for each side to be equivalent.  \n",
    "\n",
    "Thus \n",
    "\n",
    "$\\mathbf U \\mathbf D \\mathbf U^H = \\mathbf{AA}^H = \\mathbf A^H \\mathbf A = \\mathbf V \\mathbf D \\mathbf V^H$\n",
    "\n",
    "where $\\mathbf U = \\mathbf V$.  \n",
    "\n",
    "However, recall that $\\mathbf V$ and $\\mathbf U$ are the right and left singular vectors for $\\mathbf A$.  \n",
    "\n",
    "\n",
    "Thus we can go through our process of doing singular value decomposition on $\\mathbf A$, *except we no longer enforce the constraint / defintion of all singular values being real and non-negative* -- instead we simply ensure that $\\mathbf \\Sigma ^H \\mathbf \\Sigma = \\mathbf D$, observing that it is always the case with square matrices that $\\mathbf \\Sigma ^H \\mathbf \\Sigma =  \\mathbf \\Sigma \\mathbf \\Sigma^H$ and we get:\n",
    "\n",
    "$\\mathbf A = \\mathbf{U \\Sigma V}^H =  \\mathbf{U \\Sigma U}^H$\n",
    "\n",
    "and hence we have diagonalized $\\mathbf A$ with a unitary basis of eigenvectors.  Thus $\\mathbf A$ is normal.  \n",
    "\n",
    "*note: In case the reader is currious as to how we can be sure to select actual correct complex numbers for $\\mathbf \\Sigma$, since all we seem to know is the squared magnitude of each entry -- one simple approach is to use quadratic forms*  \n",
    "\n",
    "$\\mathbf U = \\bigg[\\begin{array}{c|c|c|c}\n",
    "\\mathbf u_1 & \\mathbf u_2 &\\cdots & \\mathbf u_n\\end{array}\\bigg] $\n",
    "\n",
    "where $\\mathbf x_k = \\mathbf u_k$, we see that the following result\n",
    "\n",
    "$\\mathbf x_k^H \\mathbf A \\mathbf x_k = \\sigma_k$\n",
    "\n",
    "which gives us the exact complex number associated with $\\sigma_k$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**another look at normal matrices:**  \n",
    "\n",
    "The claim is that matrices are normal **iff** $\\mathbf A \\mathbf A^H = \\mathbf A^H \\mathbf A$  which we indicated is the same as the matrix being unitarily diagonalizable, via Schur's Inequality.  \n",
    "\n",
    "Let's examine the Schur decompositions of each of these matrices.  \n",
    "\n",
    "I.e. if these two matrices are the same, we see:  \n",
    "\n",
    " $\\mathbf A \\mathbf A^H  = \\big(\\mathbf Q \\mathbf R \\mathbf Q^H \\big)\\big(\\mathbf Q \\mathbf R^H \\mathbf Q^H\\big) = \\mathbf Q \\mathbf R  \\mathbf R^H \\mathbf Q^H = \\mathbf Q \\mathbf R^H  \\mathbf R \\mathbf Q^H = \\big(\\mathbf Q \\mathbf R^H \\mathbf Q^H \\big) \\big(\\mathbf Q \\mathbf R \\mathbf Q^H \\big) = \\mathbf A^H \\mathbf A$  \n",
    " \n",
    "thus the statement comes down to verifying that \n",
    "\n",
    "$\\mathbf Q \\mathbf R  \\mathbf R^H \\mathbf Q^H = \\mathbf Q \\mathbf R^H  \\mathbf R \\mathbf Q^H $\n",
    "\n",
    "and since $\\mathbf Q$ is full rank (and unitary) we can mutliply on the left by $\\mathbf Q^H$ and on the right by $\\mathbf Q$, without changing the problem, which gets us:  \n",
    " \n",
    "$ \\mathbf R  \\mathbf R^H  = \\mathbf R^H  \\mathbf R  $\n",
    "\n",
    "\n",
    "because diagonal matrices commute, it is easy to verify that if $\\mathbf R = \\mathbf D$ then the statement is true, i.e. that \n",
    "\n",
    "$ \\mathbf D  \\mathbf D^H  = \\mathbf D^H  \\mathbf D  $\n",
    "\n",
    "What is more subtle is verifying the other leg of the *iff*, i.e. that if $ \\mathbf R  \\mathbf R^H  = \\mathbf R^H  \\mathbf R  $ it *must be that the case that* $\\mathbf R = \\mathbf D$  \n",
    "\n",
    "Note that if two matrices are equal, then their diagonal entries must be the same.  And, because of special structure (as will become clear) in triangular matrices, it is enough to verify the implications of the 'sameness' of the diagonal entries of the two matrices $\\big(\\mathbf R  \\mathbf R^H\\big)$  and  $\\big(\\mathbf R^H  \\mathbf R \\big)$.  \n",
    "\n",
    "now lets look at our upper triangular matrix $\\mathbf R$ which has $\\mathbf A$'s eigenvalues along its diagonal.  We can partition this two different ways. \n",
    "\n",
    "first by columns  \n",
    "\n",
    "$\\mathbf R = \n",
    "\\bigg[\\begin{array}{c|c|c|c}\n",
    "\\mathbf r_1 & \\mathbf r_2 &\\cdots & \\mathbf r_{n}\n",
    "\\end{array}\\bigg]$\n",
    "\n",
    "then by rows \n",
    "\n",
    "$\\mathbf R= \n",
    "\\begin{bmatrix}\n",
    "\\tilde{ \\mathbf r_1}^T \\\\\n",
    "\\tilde{ \\mathbf r_2}^T \\\\ \n",
    "\\vdots\\\\ \n",
    "\\tilde{ \\mathbf r}_{n-1}^T \\\\ \n",
    "\\tilde{ \\mathbf r_n}^T\n",
    "\\end{bmatrix}\n",
    "$   \n",
    "\n",
    "\n",
    "consider the $j$th diagonal entry of $\\big(\\mathbf R^H  \\mathbf R \\big)$.  It is given by $\\big(\\mathbf R^H  \\mathbf R \\big) = \\mathbf r_j^H \\mathbf r_j = \\langle\\ \\mathbf r_j, \\mathbf r_j\\rangle  = \\big \\Vert \\mathbf r_j \\big \\Vert_2^2  $\n",
    "\n",
    "it's a bit messy, but when we consider the the jth entry of $\\big(\\mathbf R  \\mathbf R^H \\big)$, it is given by  \n",
    "$\\big(\\tilde{ \\mathbf r_j}^T\\big) \\big(\\tilde{ \\mathbf r_j}^T\\big)^H = \\langle\\ \\tilde{ \\mathbf r_j}^T, \\tilde{ \\mathbf r_j}^T \\rangle = \\big \\Vert \\tilde{ \\mathbf r_j}^T \\big \\Vert_2^2 $\n",
    "\n",
    "\n",
    "Thus by examining the diagonal entries of $\\big(\\mathbf R^H  \\mathbf R \\big) $  and $\\big(\\mathbf R  \\mathbf R^H \\big)$ which must be equal since $\\mathbf {AA}^H = \\mathbf A^H \\mathbf A$ for normal matrices-- we are actually looking at the squared length (2 norms) of each column and each row of $\\mathbf R$.  \n",
    "\n",
    "In general, of course, $\\text{trace}\\big(\\mathbf R^H  \\mathbf R \\big) = \\text{trace}\\big(\\mathbf R  \\mathbf R^H \\big)$ by the cyclic property of trace, but looks at the summed and aggregated values.  Looking at diagonal entries and examining the implications if each jth diagonal entry is the same... is a lot more enlightening.  (There are analogies with special structure in markov chains -- in general irreducible positive time recurrent chains having global balance equations that must be satsified -- i.e. at the summation level for each state, however *time reversible* chains satisfy the detailed balance equations i.e. at a grangular, pre-summation level which allows us to squeeze special insights out of them.)   \n",
    "\n",
    "\n",
    "Specifically consider the following dynamic programming inspired approach.  (Note: this could just be referred to as induction, but that seems to miss some of the essence of the overlapping subproblems that exist here.)  \n",
    "\n",
    "- - - -\n",
    "*The Close*  \n",
    "\n",
    "Suppose we look at the squared length of column $\\mathbf r_1$  and compare it to the squared length of row $\\tilde{ \\mathbf r_1}^T$.  Being upper triangular, we know $\\big \\Vert \\mathbf r_1 \\big \\Vert_2^2 = \\big \\vert \\lambda_1 \\big \\vert^2 $ .  But we are insisting the diagonal entries of $\\big(\\mathbf R^H  \\mathbf R \\big)$  and $\\big(\\mathbf R  \\mathbf R^H \\big)$ are the same, and hence we have $\\big \\Vert \\mathbf r_1 \\big \\Vert_2^2 = \\big \\Vert \\tilde{ \\mathbf r_1}^T \\big \\Vert_2^2 = \\big \\vert \\lambda_1 \\big \\vert^2 $.  We know $\\tilde{ \\mathbf r_1}^T$ always contains $\\lambda_1$, but the comparison length tells us it *only* contains $\\lambda_1$ i.e. row 1 has only an element on the diagonal.  \n",
    "\n",
    "Put another way, $\\big\\Vert \\big(\\tilde{ \\mathbf r_1}^T - \\lambda_1 \\mathbf e_1^T\\big)^T \\big \\Vert_2^2 = 0$ which occurs **iff** $\\big(\\tilde{ \\mathbf r_1}^T - \\lambda_1 \\mathbf e_1^T\\big)^T  = \\mathbf 0$, where as a reminder we have the standard basis vectors given by:  $\\mathbf I = \n",
    "\\bigg[\\begin{array}{c|c|c|c}\n",
    "\\mathbf e_1 & \\mathbf e_2 &\\cdots & \\mathbf e_{n}\n",
    "\\end{array}\\bigg]$\n",
    "\n",
    "\n",
    "Now we proceed to column 2 and row 2. When evaluating $\\big \\Vert \\mathbf r_2 \\big \\Vert_2^2$, we think through the following:  being upper triangular we, again, know there are only zeros below the diagonal, and as we've just uncovered, everything above the diagonal (i.e. i.e. everything in row 1 that isn't $\\lambda_1$ ) is a zero as well, and hence we know $\\big \\Vert \\mathbf r_2 \\big \\Vert_2^2 = \\big \\vert \\lambda_2 \\big \\vert^2 $.  But $\\big \\Vert \\tilde{ \\mathbf r_2}^T \\big \\Vert_2^2  = \\big \\Vert \\mathbf r_2 \\big \\Vert_2^2 = \\big \\vert \\lambda_2 \\big \\vert^2 $ and hence we discover that everything in row 2 must be a zero except for the eigenvalue.  \n",
    "\n",
    "Now we could proceed most formally via induction, but the idea is that of overlapping subproblems -- i.e. we repeat the above process for $k = 3, 4, ..., n$ and for each $k$ we recognize that $\\big \\Vert \\mathbf r_k \\big \\Vert_2^2 = \\big \\vert \\lambda_k \\big \\vert^2 $, because the preceding subproblems tell us that there are only zeros above the diagonal entry  for column $k$ (i.e. for rows $i = \\{1,..., k-1\\})$.  But then, because $\\big \\Vert \\tilde{ \\mathbf r_k}^T \\big \\Vert_2^2  = \\big \\Vert \\mathbf r_k \\big \\Vert_2^2 = \\big \\vert \\lambda_k \\big \\vert^2 $ we discover that there cannot be anything non-zero to the right of the diagonal for row $k$ either (and of course being upper triangular, there are only zeros to the left of the diagonal).  And after repeating this process for all columns in $\\mathbf R$, we have verified that $\\mathbf R$ is in fact diagonal -- i.e. $\\mathbf A$ is unitarily diagonalizable and in line with Schur's Inequality.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**yet another look at at Normal Matrices**\n",
    "\n",
    "if two matrices are normal, we have \n",
    "\n",
    "$\\mathbf A^H \\mathbf A = \\mathbf {AA}^H$\n",
    "\n",
    "or equivalently:\n",
    "\n",
    "$\\mathbf A^H \\mathbf A - \\mathbf {AA}^H = \\mathbf 0$\n",
    "\n",
    "now looking at the squared Frobenius norm of this matrix, we can get another test for normality in general... that is we positive definiteness for norms -- in particular that the (squared) Frobenius norm of a matrix is zero **iff** the matrix is zero.\n",
    "\n",
    "\n",
    "$\\big \\Vert \\mathbf A^H \\mathbf A - \\mathbf {AA}^H \\big \\Vert_F^2 = \\text{trace}\\Big(\\big(\\mathbf A^H \\mathbf A - \\mathbf {AA}^H \\big)^H\\big(\\mathbf A^H \\mathbf A - \\mathbf {AA}^H \\big)\\big)= 0$\n",
    "\n",
    "we can expand this to\n",
    "\n",
    "$ \\text{trace}\\Big(\\big(\\mathbf A^H \\mathbf A\\big)^2 \\Big) + \\text{trace}\\Big(\\big(\\mathbf A \\mathbf A^H\\big)^2 \\Big) - \\text{trace}\\Big(\\big(\\mathbf A^H \\mathbf A\\big)\\big(\\mathbf A \\mathbf A^H\\big) \\Big) - \\text{trace}\\Big(\\big(\\mathbf A \\mathbf A^H\\big)\\big(\\mathbf A^H \\mathbf A\\big) \\Big)  =  0$  \n",
    "\n",
    "\n",
    "and using cyclic property of the trace, then re-arranging terms:\n",
    "\n",
    "$ \\text{trace}\\Big(\\big(\\mathbf A^H \\mathbf A\\big)^2 \\Big) + \\text{trace}\\Big( \\mathbf A^H \\mathbf A \\mathbf A^H \\mathbf A\\Big) - \\text{trace}\\Big(\\mathbf A^H \\mathbf A \\mathbf A \\mathbf A^H \\Big) - \\text{trace}\\Big(\\mathbf A \\mathbf A^H \\mathbf A^H \\mathbf A \\Big)  =  0$  \n",
    "\n",
    "\n",
    "\n",
    "$2\\text{trace}\\Big(\\big(\\mathbf A^H \\mathbf A\\big)^2 \\Big)  =  \\text{trace}\\Big(\\mathbf A^H \\mathbf A^H  \\mathbf A \\mathbf A \\Big) + \\text{trace}\\Big(\\mathbf A^H \\mathbf A^H   \\mathbf A \\mathbf A \\Big) = 2 \\text{trace}\\Big(\\mathbf A^H \\mathbf A^H \\mathbf A \\mathbf A  \\Big)  = 2 \\text{trace}\\Big(\\big(\\mathbf A^2\\big)^H \\big(\\mathbf A^2\\big)  \\Big)  $ \n",
    "\n",
    "which gives us another test for normality: We may say a matrix is normal **iff**\n",
    "\n",
    "$\\text{trace}\\Big(\\big(\\mathbf A^H \\mathbf A\\big)^2 \\Big) =  \\text{trace}\\Big(\\big(\\mathbf A^2\\big)^H \\big(\\mathbf A^2\\big)  \\Big)   $ \n",
    "\n",
    "or put differently, we may say \n",
    "\n",
    "$\\text{trace}\\Big(\\big(\\mathbf A^H \\mathbf A\\big)^2 \\Big) \\geq  \\text{trace}\\Big(\\big(\\mathbf A^2\\big)^H \\big(\\mathbf A^2\\big)  \\Big)   $ \n",
    "\n",
    "with equality **iff** $\\mathbf A$ is normal.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**extension:** skew Hermitian matrices are normal, and hence unitarily diagonalizable. \n",
    "\n",
    "A skew Hermitian matrix, is some $n$ x $n$ matrix, $\\mathbf A$, where $\\mathbf A = - \\mathbf A^H$ or equivalently $-\\mathbf A =  \\mathbf A^H$.  Note that the definition and implications are the same in the real case of skew symmetric, except it is worth noting that in the real case, the diagonal elements of $\\mathbf A$ must be zero (because the only real number that is equal to its negative conjugate is zero). \n",
    "\n",
    "if $\\mathbf A = - \\mathbf A^H$, then we can say that \n",
    "\n",
    "\n",
    "$\\mathbf {A A}^H = \\mathbf A \\big(\\mathbf A^H \\big)  = \\mathbf A \\big(-\\mathbf A \\big) = - \\mathbf A^2$  \n",
    "$\\mathbf A^H \\mathbf A = \\big(\\mathbf A^H \\big) \\mathbf A   = \\big(- \\mathbf A\\big) \\mathbf A  = - \\mathbf A^2$\n",
    "\n",
    "hence $\\mathbf {A A}^H = \\mathbf A^H \\mathbf A$  \n",
    "\n",
    "which means that a skew Hermitian matrix $\\mathbf A$ is normal.  \n",
    "\n",
    "It is perhaps worth noting that in either the real case or the complex case, we may do a Schur Decomposition on $\\mathbf A$, and see that $\\lambda_k = -\\lambda_k^H$ for $k = \\{1, 2, ..., n\\}$ , i.e. that each eigenvalue is equal to the negative of its own conjugate.  This means that each eigenvalue is either purely imaginary, or equal to zero.  One immediate consequence for the real case, is that for an $n$ x $n$ real, skew symmetric $\\mathbf A$, if $n$ is odd, we know that it is singular.  Why? For real matrices, complex eigenvalues come in conjugate pairs, which means that there must be (at least) one element that does not have a pair, and hence it cannot be imaginary and hence must be zero.  \n",
    "\n",
    "Note: we can also verify singularity of odd dimensional skew symmetric matrices another way. Recalling that the determinant is a multi-linear function, and how a scalar impacts it, we may say:  \n",
    "\n",
    "$\\det\\Big(\\mathbf A\\Big) = \\det\\Big(\\mathbf A^T \\Big) = \\det\\Big( \\big(-\\mathbf A\\big) \\Big) = \\det\\Big((-1) \\mathbf A\\Big)  = (-1)^n \\det\\Big(\\mathbf A\\Big)$  \n",
    "\n",
    "or equivalently \n",
    "\n",
    "$\\det\\Big(\\mathbf A\\Big) = \\det\\Big(\\mathbf A^T \\Big) = \\det\\Big( \\big(-\\mathbf A\\big) \\Big) = \\det\\Big(\\big(-\\mathbf I\\big) \\mathbf A\\Big)  = \\det\\Big(-\\mathbf I\\Big) \\det\\Big( \\mathbf A\\Big) =(-1)^n \\det\\Big(\\mathbf A\\Big)$  \n",
    "\n",
    "hence if $n$ is odd, we have $\\det\\big(\\mathbf A\\big) = -\\det\\big(\\mathbf A\\big)$ which occurs **iff** $\\det\\big(\\mathbf A\\big) = 0$, i.e. $\\mathbf A$ is singular.  \n",
    "\n",
    "Note that this is a moderately more general result.  It more directly applies to scalar fields not of characteristic zero (though there's easy workarounds with the eigenvalue case here), *and* it tells us additional information about a matrix with complex scalars (i.e. with at least one scalar with a non-zero imaginary part) that is skew symmetric (but not skew hermitian) -- while such matrices would not in general be normal, and they would not have a 'requirement' for eigenvalues coming in conjugate pairs, they would be singular if $n$ is odd.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An interesting *additional* way to look at Real Skew Symmetric matrices is that they in some sense represent bipartite graphs \n",
    "\n",
    "(note while the Zero Matrix is *technically* skew symmetric, we confine the discuss below to all non-zero real skew symmetric matrices, below, in the interest of linguistic clarity)  \n",
    "\n",
    "- - - - -  \n",
    "consider that for odd natural number $k$, we have   \n",
    "$\\text{trace}\\big(\\mathbf A^k\\big) = \\text{trace}\\big((\\mathbf A^T)^k\\big)  = \\text{trace}\\big((-\\mathbf A)^k\\big) = (-1)^k\\text{trace}\\big(\\mathbf A^k\\big) = -\\text{trace}\\big(\\mathbf A^k\\big) $  \n",
    "\n",
    "hence \n",
    "$2\\text{trace}\\big(\\mathbf A^k\\big)=0\\to \\text{trace}\\big(\\mathbf A^k\\big)=0$  \n",
    "i.e. real skew symmetric matrices are traceless for odd powers. Recalling how this insight was used in \"Blocked_Matrices_Sympy_BipartiteGraphs.ipynb\" for bipartite graphs, we can immediately recognize that the spectra of $\\mathbf A$ is given by  \n",
    "\n",
    "$\\{\\lambda_1, -\\lambda_1, \\lambda_2, -\\lambda_2, ..., \\lambda_{n/2}, -\\lambda_{n/2}\\}$  \n",
    "(technical nit: if $n$ is odd, we can instead end with $\\frac{n-1}{2}$ and insert a zero as we know such a matrix is singular, based on the above determinant argument) \n",
    "\n",
    "and we know   \n",
    "$0 \\lt \\big\\Vert \\mathbf A\\big \\Vert_F^2 = \\text{trace}\\big(\\mathbf A^T \\mathbf A\\big)=-\\text{trace}\\big(\\mathbf A^2\\big)$ \n",
    "\n",
    "(the LHS is due to positive definiteness and the fact that we've carved the zero matrix out from this)  \n",
    "\n",
    "or equivalently,  \n",
    "$\\text{trace}\\big(\\mathbf A^2\\big) \\lt 0 \\lt \\big\\Vert \\mathbf A\\big \\Vert_F^2 $ \n",
    "\n",
    "\n",
    "but revisiting Schur's Inequality, we can see for a real skew symmetric matrix that we have an equality case \n",
    "\n",
    "$\\big \\Vert \\mathbf A \\big \\Vert_F^{2} = \\text{trace}\\big(\\mathbf A^H \\mathbf A\\big) \\geq \\sum_{i = 1}^{n} \\big \\vert \\lambda_i\\big \\vert ^2 \\geq \\big \\vert \\sum_{i = 1}^{n} \\lambda_i^2\\big \\vert = \\Big \\vert \\text{trace}\\big(\\mathbf A \\mathbf A\\big) \\Big \\vert$  \n",
    "\n",
    "in particular the triangle inequality becoming an equality tells us the each $\\lambda_i^2$ must point the 'same direction'.  \n",
    "\n",
    "consider a complex 'mixed' eigenvalue given by \n",
    "$\\lambda = a + bi$  \n",
    "\n",
    "where $a \\neq 0$, $b \\neq 0$  \n",
    "\n",
    "then \n",
    "$\\lambda^2 = (a + bi)^2 = a^2 - b^2 + 2abi$  \n",
    "but since our matrix is real, the eigenvalues come in conjugate pairs, so the conjugate is  \n",
    "$\\bar{\\lambda}^2 = (a - bi)^2 = a^2 - b^2 - 2abi$  \n",
    "\n",
    "which cannot 'point' in the same direction since $2abi \\neq 0$  \n",
    "\n",
    "hence $\\mathbf A$ cannot have mixed complex eigenvalues.  This leaves us with the option of purely real eigenvalues and/or purely imaginary (where $\\lambda =0$ may be interpretted as either one).  \n",
    "\n",
    "Again the triangle inequality comes into play:  \n",
    "if $\\lambda_k = a \\neq 0$ and $\\lambda_{k+1} = -a$ (recall bipartite style traces), then  \n",
    "$\\lambda_k^2 = a^2 = \\lambda_{k+1}^2$  \n",
    "\n",
    "but if \n",
    "$\\lambda_j = bi \\neq 0$ and (for bipartite and conjugate pair reasons) $\\lambda_{j+1} = -bi$, then  \n",
    "$\\lambda_{j}^2 = (bi)^2 = -b^2 = \\lambda_{j+1}^2$  \n",
    "\n",
    "However $\\lambda_j$ and $\\lambda_k$ cannot both exist -- if they did, then the triangle inequality would be violated   \n",
    "i.e. $\\lambda_j^2 $ is negative and $\\lambda_k^2$ is positive, so they point in 'opposite directions'. \n",
    "\n",
    "Hence the eigenvalues must be either purely real or purely imaginary.  But since all purely real numbers, squared, are non-negative, the associated trace of $\\mathbf A^2$ would have to be non-negative, yet we know  \n",
    "$\\text{trace}\\big(\\mathbf A^2\\big) \\lt 0 $  \n",
    "\n",
    "hence $\\mathbf A$ must have purely imaginary eigenvalues.   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is problem 49, from \n",
    "\n",
    "\"Matrices : Theory & Applications\n",
    "Additional exercises\"\n",
    "\n",
    "found here: \n",
    "\n",
    "http://perso.ens-lyon.fr/serre/DPF/exobis.pdf\n",
    "\n",
    "\n",
    "For $\\mathbf M \\in \\mathbb R^{n x n}$\n",
    "\n",
    "**claim:**  \n",
    "\n",
    "$\\Big( \\text{trace}\\big( \\mathbf M \\big) \\Big)^2  \\leq rank\\big(\\mathbf M\\big)\\text{trace}\\big(\\mathbf M^T \\mathbf M\\big) = rank\\big(\\mathbf M\\big)\\big \\Vert \\mathbf M\\big\\Vert_F^2$\n",
    "\n",
    "for the proof, suppose that we have well ordered eigenvalues, $k$ of which are not zero, i.e. where $0 \\leq k \\leq n$, given below:\n",
    "\n",
    "$\\big \\vert \\lambda_1 \\big \\vert \\geq \\big \\vert \\lambda_2 \\big \\vert \\geq ... \\geq \\big \\vert \\lambda_k \\big \\vert \\geq 0 = \\big \\vert \\lambda_{k+1} \\big \\vert = .... = \\big \\vert \\lambda_{n}\\big \\vert$\n",
    "\n",
    "\n",
    "(Note that while the field is Reals, we observe the typical relaxation that allows Complex numbers during intermediate steps involving eigenvalues.) \n",
    "\n",
    "**proof:**    \n",
    "we may collect all $n$ eigenvalues in a diagonal matrix $\\mathbf \\Lambda$, and the $k$ non-zero eigenvalues in a $k$ x $k$ diagonal matrix $\\mathbf D$.  The ones vector $\\mathbf 1$ has $k$ entries, each equal to one. \n",
    "\n",
    "$\\Big( \\text{trace}\\big( \\mathbf M \\big) \\Big)^2  = \\langle \\mathbf 1 \\,, \\big( \\mathbf {D1}\\big)\\rangle^2 \\leq \\langle \\mathbf 1 \\,, \\mathbf 1\\rangle\\cdot \\langle \\big(\\mathbf {D1}\\big) \\,, \\big( \\mathbf {D1}\\big)\\rangle    = k \\cdot \\text{trace}\\big(\\mathbf D^H \\mathbf D\\big) = k\\cdot \\text{trace}\\big(\\mathbf \\Lambda^H \\mathbf \\Lambda \\big) = k \\cdot \\sum_{i=1}^n \\big\\vert \\lambda_i\\big\\vert ^2 $\n",
    "\n",
    "where the above inequality is given by Cauchy Schwartz\n",
    "\n",
    "$\\Big( \\text{trace}\\big( \\mathbf M \\big) \\Big)^2 \\leq k \\cdot\\sum_{i=1}^n \\big\\vert \\lambda_i\\big\\vert ^2  \\leq rank\\big(\\mathbf M\\big) \\sum_{i=1}^n \\big\\vert \\lambda_i\\big\\vert ^2 $\n",
    "\n",
    "from here we observe that the $k$ non-zero eigenvalues of a matrix are a *lower bound* on its rank.  \n",
    "\n",
    "Justification: any real $n$ x $n$ matrix is unitarily similar to an upper triangular one.  Interpretted in terms of Gaussian Elimination, such an upper triangular matrix has at least $k$ pivots.  Put differently, once said matrix is put in reduced row echelon form, its number of pivots (i.e. its rank) cannot be less than $k$.   \n",
    "\n",
    "$\\Big( \\text{trace}\\big( \\mathbf M \\big) \\Big)^2 \\leq rank\\big(\\mathbf M\\big) \\sum_{i=1}^n \\big\\vert \\lambda_i\\big\\vert ^2 \\leq rank\\big(\\mathbf M\\big)\\big \\Vert \\mathbf M\\big\\Vert_F^2$ \n",
    "\n",
    "via Schur's Inequality.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Misc. extension:**  \n",
    "\n",
    "**claim:** Suppose $\\mathbf A$ and $\\mathbf B$ are normal. (Note: this implies they are square) Further suppose that $\\mathbf {AB}$ is normal.  Then $\\mathbf {BA}$ is normal as well.  \n",
    "\n",
    "**proof:**  \n",
    "By Schur Inequality, since $\\mathbf {AB}$ is normal, we know \n",
    "$\\big\\Vert \\mathbf {AB} \\big\\Vert_F^2 = \\sum_{i=1}^n \\Big(\\big\\vert \\lambda\\big(\\mathbf {AB}\\big)_i \\big\\vert^2\\Big) $\n",
    "\n",
    "Now we make use of the fact that $\\big(\\mathbf{AB}\\big)$ and $\\big(\\mathbf {BA}\\big)$ have the same eigenvalues.  \n",
    "\n",
    "*the main argument:*  \n",
    "$\\sum_{i=1}^n \\Big(\\big\\vert \\lambda\\big(\\mathbf {BA}\\big)_i \\big\\vert^2\\Big)$   \n",
    "$=\\sum_{i=1}^n \\Big(\\big\\vert \\lambda\\big(\\mathbf {AB}\\big)_i \\big\\vert^2\\Big)$   \n",
    "$=\\big\\Vert \\mathbf {AB} \\big\\Vert_F^2 $   \n",
    "$= \\text{trace}\\big(\\mathbf B^H \\mathbf A^H \\mathbf A \\mathbf B\\big)$   \n",
    "$= \\text{trace}\\big(\\mathbf A^H \\mathbf A \\mathbf B \\mathbf B^H \\big) $  \n",
    "$= \\text{trace}\\big(\\mathbf A \\mathbf A^H \\mathbf B^H \\mathbf B \\big) $  \n",
    "$= \\text{trace}\\big(\\mathbf A^H \\mathbf B^H \\mathbf B \\mathbf A \\big) $  \n",
    "$=  \\big\\Vert \\mathbf {BA} \\big\\Vert_F^2$   \n",
    "\n",
    "where we make use of the cyclic property of the trace, and $\\mathbf A \\mathbf A^H =\\mathbf A^H \\mathbf A $ due to the normality of $\\mathbf A$, and  $\\mathbf B^H \\mathbf B = \\mathbf B \\mathbf B^H $ due to the normality of $\\mathbf B$\n",
    "\n",
    "Hence we've seen \n",
    "$\\big\\Vert \\mathbf {BA} \\big\\Vert_F^2 = \\sum_{i=1}^n \\Big(\\big\\vert \\lambda\\big(\\mathbf {BA}\\big)_i \\big\\vert^2\\Big)$, i.e. that it satisfies the Schur Inequality with equality, and hence $\\big(\\mathbf{BA}\\big)$ is normal. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
