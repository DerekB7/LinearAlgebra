{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Schur's Inequality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Schur's Inequality tells us that for any $n$ x $n$ matrix in a complex field (where reals are a special subset), i.e. $\\mathbf A \\in \\mathbb C^{n x n}$\n",
    "\n",
    "**Claim:**\n",
    "\n",
    "$\\big \\Vert \\mathbf A \\big \\Vert_F^{2} = trace\\big(\\mathbf A^H \\mathbf A\\big) \\geq \\sum_{i = 1}^{n} \\big \\vert \\lambda_i\\big \\vert ^2 \\geq \\big \\vert \\sum_{i = 1}^{n} \\lambda_i^2\\big \\vert = \\Big \\vert trace\\big(\\mathbf A \\mathbf A\\big) \\Big \\vert $\n",
    "\n",
    "note that  $\\sum_{i = 1}^{n} \\big \\vert \\lambda_i\\big \\vert ^2 \\geq \\big \\vert \\sum_{i = 1}^{n} \\lambda_i^2\\big \\vert$ was included at the end via the triangle inequality\n",
    "\n",
    "\n",
    "**Background:**\n",
    "\n",
    "We can collect all of the eigenvalues $\\big \\vert \\lambda_1 \\big \\vert \\geq \\big \\vert\\lambda_2 \\big \\vert\\geq \\big \\vert \n",
    "\\lambda_3 \\big \\vert \\geq ... \\geq \\big \\vert\\lambda_n \\big \\vert$ in the diagonal matrix $\\mathbf D$, and restate Schur's Inequality as:  \n",
    "\n",
    "\n",
    "$\\big \\vert \\big \\vert \\mathbf A \\big \\vert \\big \\vert_F^{2} \\geq  \\big \\vert \\big \\vert \\mathbf D \\big \\vert \\big \\vert_F^{2} = trace\\big(\\mathbf D^H \\mathbf D\\big)$ \n",
    "\n",
    "Note that by Schur Decomposition, we can write $\\mathbf A = \\mathbf {Q R Q}^H$  where $\\mathbf Q$ is unitary, and $\\mathbf R$ is upper triangular.  As a reminder, recall that the eigenvalues of an upper triangular matrix are on its diagonal, hence $\\mathbf R_{i,i} = \\lambda_i$.\n",
    "\n",
    "\n",
    "**Proof:**\n",
    "revisiting the inequality, we write this as:\n",
    "\n",
    "\n",
    "$\\big \\vert \\big \\vert \\mathbf A \\big \\vert \\big \\vert_F^{2} = trace\\big( \\big(\\mathbf {Q R Q}^H\\big)^H \\big( \\mathbf {Q R Q}^H \\big) \\big) = trace\\big(\\mathbf {Q R}^H \\mathbf Q^H \\mathbf {Q) R Q}^H\\big) \\geq trace\\big(\\mathbf D^H \\mathbf D\\big)$\n",
    "\n",
    "\n",
    "$trace\\big(\\mathbf {Q R}^H \\mathbf{R Q}^H\\big) = trace\\big(\\mathbf{(Q}^H \\mathbf{Q) R}^H \\mathbf R \\big) \\geq trace\\big(\\mathbf D^H \\mathbf D\\big)$\n",
    "\n",
    "$trace\\big(\\mathbf{R}^H \\mathbf R \\big) \\geq trace\\big(\\mathbf D^H \\mathbf D\\big)$\n",
    "\n",
    "$trace\\big(\\mathbf{ R}^H \\mathbf R \\big) = \\big \\vert \\big \\vert \\mathbf R \\big \\vert \\big \\vert_F^{2} =  \\Big(\\sum_{k = 1}^{n} \\sum_{j \\neq k}  \\mathbf R^H_{k,j} \\mathbf R_{j,k}\\Big) + trace\\big(\\mathbf D^H \\mathbf D\\big) \\geq trace\\big(\\mathbf D^H \\mathbf D\\big)$\n",
    "\n",
    "\n",
    "- - - - -\n",
    "Alternatively, we may say:\n",
    "\n",
    "$\\big \\vert \\big \\vert \\mathbf A \\big \\vert \\big \\vert_F^{2} = \\big \\vert \\big \\vert \\mathbf R \\big \\vert \\big \\vert_F^{2} = \\big \\vert \\big \\vert \\big(\\mathbf R - \\mathbf D\\big) \\big \\vert \\big \\vert_F^{2}  + \\big \\vert \\big \\vert \\mathbf D \\big \\vert \\big \\vert_F^{2} \\geq \\big \\vert \\big \\vert \\mathbf D \\big \\vert \\big \\vert_F^{2}$\n",
    "\n",
    "with equality **iff**\n",
    "$\\big \\vert \\big \\vert \\big(\\mathbf R - \\mathbf D\\big) \\big \\vert \\big \\vert_F^{2} = 0$,\n",
    "which occurs **iff** $\\mathbf R - \\mathbf D = \\mathbf 0$, aka this occurs **iff** $\\mathbf R = \\mathbf D$.  Thus in the case where the Schur Inequality is an equality, we know that $\\mathbf A$ is diagonalizable with mutually orthonormal eigenvectors $\\mathbf A = \\mathbf {Q RQ}^H = \\mathbf {Q D Q}^H$.  Note that this does *not* make any claims as to whether or not the eigenvalues are real or complex.\n",
    "\n",
    "\n",
    "**Technical Note:** if the inequality is an equality, then we say that $\\mathbf A$ **is a normal matrix.**\n",
    "\n",
    "\n",
    "- - - - \n",
    "** ugly extensions:**  \n",
    "\n",
    "**in the special case where all eigenvalues are known to be real:  ** \n",
    "\n",
    "$\\big \\vert \\big \\vert \\mathbf A \\big \\vert \\big \\vert_F^{2} = \\big \\vert \\big \\vert \\mathbf R \\big \\vert \\big \\vert_F^{2} =    \\big \\vert \\big \\vert \\big(\\mathbf R - \\mathbf D\\big) \\big \\vert \\big \\vert_F^{2}  + \\big \\vert \\big \\vert \\mathbf D \\big \\vert \\big \\vert_F^{2}    = \\frac{1}{2}\\big \\vert \\big \\vert \\big(\\mathbf R - \\mathbf R^H \\big) \\big \\vert \\big \\vert_F^{2}  + \\big \\vert \\big \\vert \\mathbf D \\big \\vert \\big \\vert_F^{2}  \\geq \\big \\Vert \\mathbf D \\big \\Vert_F^2 $\n",
    "\n",
    "it is worth mentioning that \n",
    "\n",
    "$\\frac{1}{2}\\big \\vert \\big \\vert \\big(\\mathbf R - \\mathbf R^H \\big) \\big \\vert \\big \\vert_F^{2} = \\frac{1}{2}\\big \\vert \\big \\vert \\mathbf Q\\big(\\mathbf R - \\mathbf R^H \\big) \\big \\vert \\big \\vert_F^{2} = \\frac{1}{2}\\big \\vert \\big \\vert \\mathbf Q \\big (\\mathbf R - \\mathbf R^H \\big)\\mathbf Q^H \\big \\vert \\big \\vert_F^{2}= \\frac{1}{2}\\big \\vert \\big \\vert \\big(\\mathbf{QRQ}^H - \\mathbf{QR}^H\\mathbf Q^H \\big) \\big \\vert \\big \\vert_F^{2} = \\frac{1}{2} \\big \\Vert \\mathbf A - \\mathbf A^H \\big \\Vert_F^2 $\n",
    "\n",
    "hence, if we take our equation that   \n",
    "\n",
    "$\\big \\vert \\big \\vert \\mathbf A \\big \\vert \\big \\vert_F^{2} = \\frac{1}{2}\\big \\vert \\big \\vert \\big(\\mathbf R - \\mathbf R^H \\big) \\big \\vert \\big \\vert_F^{2}  + \\big \\vert \\big \\vert \\mathbf D \\big \\vert \\big \\vert_F^{2} $\n",
    "\n",
    "and re-arrange terms:  \n",
    "$\\big \\vert \\big \\vert \\mathbf D \\big \\vert \\big \\vert_F^{2} = \\big \\vert \\big \\vert \\mathbf A \\big \\vert \\big \\vert_F^{2} - \\frac{1}{2}\\big \\vert \\big \\vert \\big(\\mathbf R - \\mathbf R^H \\big) \\big \\vert \\big \\vert_F^{2}  $\n",
    "\n",
    "\n",
    "\n",
    "and make the substitution outlined above:  \n",
    "\n",
    "$\\sum_{k=1}^n \\big \\vert \\lambda_i\\big \\vert ^2 = \\big \\vert \\big \\vert \\mathbf D \\big \\vert \\big \\vert_F^{2} = \\big \\vert \\big \\vert \\mathbf A \\big \\vert \\big \\vert_F^{2} - \\frac{1}{2} \\big \\Vert \\mathbf A - \\mathbf A^H \\big \\Vert_F^2 $\n",
    "\n",
    "** In the special case where we know all eigenvalues are imaginary:  **\n",
    "\n",
    "(or more technically, when we know all real parts are zero, e.g. skew hermitian matrices)\n",
    "\n",
    "we could do \n",
    "\n",
    "$\\sum_{k=1}^n \\big \\vert \\lambda_i\\big \\vert ^2 = \\big \\vert \\big \\vert \\mathbf D \\big \\vert \\big \\vert_F^{2} =  \\frac{1}{2}\\big \\vert \\big \\vert \\big(\\mathbf R + \\mathbf R^H \\big) \\big \\vert \\big \\vert_F^{2}   = \\big \\vert \\big \\vert \\mathbf A \\big \\vert \\big \\vert_F^{2} - \\frac{1}{2} \\big \\Vert \\mathbf A + \\mathbf A^H \\big \\Vert_F^2 $\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in the special case where $\\mathbf A$ is a rank one matrix **in reals**, we get the following:\n",
    "\n",
    "where $\\mathbf A = \\mathbf {xy}^T = \\mathbf {xy}^H$\n",
    "\n",
    "hence \n",
    "\n",
    "$\\big \\vert trace\\Big(\\big(\\mathbf {xy}^H\\big)^2\\Big)\\big \\vert = \\big \\vert \\lambda_1^2 + 0 + 0+ .... + 0 \\big \\vert  = \\big \\vert \\lambda_1^2 \\big \\vert = \\big \\vert \\lambda_1 \\big \\vert^2 $\n",
    "\n",
    "i.e. triangle inequality is not needed, and we can in fact look at \n",
    "\n",
    "$\\big \\Vert \\mathbf D \\big \\Vert_F^2 = \\big \\vert trace\\big(\\big(\\mathbf {xy}^H\\big)^2\\big)\\big \\vert = \\big \\vert trace\\big(\\mathbf {xy}^H\\big)\\big \\vert^2 = \\big \\vert \\lambda_1 \\big \\vert^2$  \n",
    "\n",
    "now we see that \n",
    "\n",
    "$\\big \\vert \\big \\vert \\mathbf {xy}^H \\big \\vert \\big \\vert_F^{2} =  \\frac{1}{2}\\big \\vert \\big \\vert \\big(\\mathbf R - \\mathbf R^H \\big) \\big \\vert \\big \\vert_F^{2}  + \\big \\vert \\big \\vert \\mathbf D \\big \\vert \\big \\vert_F^{2} = \\frac{1}{2}\\big \\vert \\big \\vert \\big(\\mathbf {xy}^H - \\mathbf {yx}^H \\big) \\big \\vert \\big \\vert_F^{2}  + \\big \\vert \\big \\vert \\mathbf D \\big \\vert \\big \\vert_F^{2} =  \\frac{1}{2}\\big \\vert \\big \\vert \\big(\\mathbf {xy}^H - \\mathbf {yx}^H \\big) \\big \\vert \\big \\vert_F^{2}  + \\big \\vert trace\\big(\\mathbf {xy}^H\\big)\\big \\vert^2$\n",
    "\n",
    "This is the Lagrange Identity in reals.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Immediate Consequence:** \n",
    "\n",
    "For any unitary (or if in reals: Orthgonal) $n$ x $n$ matrix, $\\mathbf U$, which must have eigenvalues, $\\big \\vert \\lambda_k\\big \\vert = 1$, for $k = \\{1, 2, ...,n\\}$  (see middle part of \"Fun_with_Trace_and_Quadratic_Forms_CauchySchwartz_.ipynb\" under the heading *Thoughts on Unitary Matrices* for a proof of this, using quadratic forms / singular values, to upper and lower bound the eigenvalue magnitudes)\n",
    "\n",
    "Noting that $\\mathbf U^H \\mathbf U = \\mathbf I$, for any $n$ x $n$ unitary matrix and applying Schur's Inequality, we have:\n",
    "\n",
    "\n",
    "\n",
    "$\\big \\vert \\big \\vert \\mathbf U \\big \\vert \\big \\vert_F^{2} = trace\\big(\\mathbf U^H \\mathbf U\\big) = trace\\big(\\mathbf I \\big) = n \\geq \\big \\vert \\big \\vert \\mathbf D \\big \\vert \\big \\vert_F^{2} =  \\sum_{i = 1}^{n}  \\lambda_i^H \\lambda_i= \\sum_{i = 1}^{n} \\big \\vert \\lambda_i\\big \\vert ^2 =  \\sum_{i = 1}^{n} 1^2  =  \\sum_{i = 1}^{n} 1 = n  $\n",
    "\n",
    "hence: \n",
    "\n",
    "$n = \\big \\vert \\big \\vert \\mathbf U \\big \\vert \\big \\vert_F^{2} = \\big \\vert \\big \\vert \\mathbf D \\big \\vert \\big \\vert_F^{2}$ \n",
    "\n",
    "which tell us that, if we wanted, we could diagonalize $\\mathbf U$ with mutually orthonormal eigenvectors, $\\mathbf U = \\mathbf{QDQ}^H$.\n",
    "\n",
    "\n",
    "*Link of SVD and Eigen decompositions for Normal Matrices* \n",
    "\n",
    "If we were going to do SVD on $\\mathbf U$, notice that the left and right singular vectors would be the same, except for an issue of rotations on complex plane. Starting with the above eigendecomposition:  \n",
    "\n",
    "$\\mathbf U = \\mathbf {Q  D Q}^H $\n",
    "\n",
    "now we make the substitution $\\mathbf D =\\mathbf{\\Lambda \\Sigma} = \\mathbf{\\Lambda \\mathbf I} = \\mathbf \\Lambda $\n",
    "\n",
    "That is we factor $\\mathbf D$ into two diagonal matrices -- $\\mathbf \\Sigma$ which must be real valued and non-negative, and hence has the magnitudes of all the values in $\\mathbf D$, and the remaining complex numbers / rotations / angles (i.e. information on the unit circle) in $\\mathbf D$ gets allocated to $\\mathbf \\Lambda$.  Note that since all singular values are equal to one in a unitary matrix, then $\\mathbf \\Sigma = \\mathbf I$, which can make the decomposition a bit pedantic.    \n",
    "\n",
    "So, we have $\\mathbf U =  \\mathbf {Q  D Q}^H =  \\mathbf{ Q \\Lambda} \\mathbf {\\Sigma Q }^H= \\big( \\mathbf{ Q \\Lambda}\\big)  \\mathbf {\\Sigma Q }^H = \\big( \\mathbf{ Q \\Lambda}\\big) \\mathbf I \\mathbf {Q }^H = \\big( \\mathbf{ Q \\Lambda}\\big)  \\mathbf {Q }^H $, which is to say that the left and right singular vectors are the same -- and in fact can be chosen to be the eigenvectors, if we relax the constraint that $\\mathbf \\Sigma$ has only real valued, non-negative entries.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**More on Normal Matrices**\n",
    "\n",
    "Another way to define / test for a **square matrix** $\\mathbf A$ being normal, is the following:\n",
    "\n",
    "if \n",
    "\n",
    "$\\mathbf{AA}^H = \\mathbf A^H \\mathbf A$  \n",
    "\n",
    "Then: first we observe that $\\big(\\mathbf A^H \\mathbf A\\big)$ is a Hermitian matrix, and $\\big(\\mathbf{AA}^H\\big)$ is also a Hermitian matrix.  This means that each can be diagonalized with a unitary basis of eigenvectors.  \n",
    "\n",
    "Because $\\mathbf{AA}^H = \\mathbf A^H \\mathbf A$, we know that each side has the same eigenvalues. (Side note: in general $\\mathbf{AB}$ and $\\mathbf B \\mathbf A$ must have the same non-zero eigenvalues, as proven in the Vandermonde Matrix writeup.)  Because each side is equivalent, we can select eigenvectors for each side to be equivalent.  \n",
    "\n",
    "Thus \n",
    "\n",
    "$\\mathbf U \\mathbf D \\mathbf U^H = \\mathbf{AA}^H = \\mathbf A^H \\mathbf A = \\mathbf V \\mathbf D \\mathbf V^H$\n",
    "\n",
    "where $\\mathbf U = \\mathbf V$.  \n",
    "\n",
    "However, recall that $\\mathbf V$ and $\\mathbf U$ are the right and left singular vectors for $\\mathbf A$.  \n",
    "\n",
    "\n",
    "Thus we can go through our process of doing singular value decomposition on $\\mathbf A$, *except we no longer enforce the constraint / defintion of all singular values being real and non-negative* -- instead we simply ensure that $\\mathbf \\Sigma ^H \\mathbf \\Sigma = \\mathbf D$, observing that it is always the case with square matrices that $\\mathbf \\Sigma ^H \\mathbf \\Sigma =  \\mathbf \\Sigma \\mathbf \\Sigma^H$ and we get:\n",
    "\n",
    "$\\mathbf A = \\mathbf{U \\Sigma V}^H =  \\mathbf{U \\Sigma U}^H$\n",
    "\n",
    "and hence we have diagonalized $\\mathbf A$ with a unitary basis of eigenvectors.  Thus $\\mathbf A$ is normal.  \n",
    "\n",
    "*note: In case the reader is currious as to how we can be sure to select actual correct complex numbers for $\\mathbf \\Sigma$, since all we seem to know is the squared magnitude of each entry -- one simple approach is to use quadratic forms*  \n",
    "\n",
    "$\\mathbf U = \\bigg[\\begin{array}{c|c|c|c}\n",
    "\\mathbf u_1 & \\mathbf u_2 &\\cdots & \\mathbf u_n\\end{array}\\bigg] $\n",
    "\n",
    "where $\\mathbf x_k = \\mathbf u_k$, we see that the following result\n",
    "\n",
    "$\\mathbf x_k^H \\mathbf A \\mathbf x_k = \\sigma_k$\n",
    "\n",
    "which gives us the exact complex number associated with $\\sigma_k$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**another look at normal matrices:  **  \n",
    "\n",
    "The claim is that matrices are normal **iff** $\\mathbf A \\mathbf A^H = \\mathbf A^H \\mathbf A$  which we indicated is the same as the matrix being unitarily diagonalizable, via Schur's Inequality.  \n",
    "\n",
    "Let's examine the Schur decompositions of each of these matrices.  \n",
    "\n",
    "I.e. if these two matrices are the same, we see:  \n",
    "\n",
    " $\\mathbf A \\mathbf A^H  = \\big(\\mathbf Q \\mathbf R \\mathbf Q^H \\big)\\big(\\mathbf Q \\mathbf R^H \\mathbf Q^H\\big) = \\mathbf Q \\mathbf R  \\mathbf R^H \\mathbf Q^H = \\mathbf Q \\mathbf R^H  \\mathbf R \\mathbf Q^H = \\big(\\mathbf Q \\mathbf R^H \\mathbf Q^H \\big) \\big(\\mathbf Q \\mathbf R \\mathbf Q^H \\big) = \\mathbf A^H \\mathbf A$  \n",
    " \n",
    "thus the statement comes down to verifying that \n",
    "\n",
    "$\\mathbf Q \\mathbf R  \\mathbf R^H \\mathbf Q^H = \\mathbf Q \\mathbf R^H  \\mathbf R \\mathbf Q^H $\n",
    "\n",
    "and since $\\mathbf Q$ is full rank (and unitary) we can mutliply on the left by $\\mathbf Q^H$ and on the right by $\\mathbf Q$, without changing the problem, which gets us:  \n",
    " \n",
    "$ \\mathbf R  \\mathbf R^H  = \\mathbf R^H  \\mathbf R  $\n",
    "\n",
    "\n",
    "because diagonal matrices commute, it is easy to verify that if $\\mathbf R = \\mathbf D$ then the statement is true, i.e. that \n",
    "\n",
    "$ \\mathbf D  \\mathbf D^H  = \\mathbf D^H  \\mathbf D  $\n",
    "\n",
    "What is more subtle is verifying the other leg of the *iff*, i.e. that if $ \\mathbf R  \\mathbf R^H  = \\mathbf R^H  \\mathbf R  $ it *must be that the case that* $\\mathbf R = \\mathbf D$  \n",
    "\n",
    "Note that if two matrices are equal, then their diagonal entries must be the same.  And, because of special structure (as will become clear) in triangular matrices, it is enough to verify the implications of the 'sameness' of the diagonal entries of the two matrices $\\big(\\mathbf R  \\mathbf R^H\\big)$  and  $\\big(\\mathbf R^H  \\mathbf R \\big)$.  \n",
    "\n",
    "now lets look at our upper triangular matrix $\\mathbf R$ which has $\\mathbf A$'s eigenvalues along its diagonal.  We can partition this two different ways. \n",
    "\n",
    "first by columns  \n",
    "\n",
    "$\\mathbf R = \n",
    "\\bigg[\\begin{array}{c|c|c|c}\n",
    "\\mathbf r_1 & \\mathbf r_2 &\\cdots & \\mathbf r_{n}\n",
    "\\end{array}\\bigg]$\n",
    "\n",
    "then by rows \n",
    "\n",
    "$\\mathbf R= \n",
    "\\begin{bmatrix}\n",
    "\\tilde{ \\mathbf r_1}^T \\\\\n",
    "\\tilde{ \\mathbf r_2}^T \\\\ \n",
    "\\vdots\\\\ \n",
    "\\tilde{ \\mathbf r}_{n-1}^T \\\\ \n",
    "\\tilde{ \\mathbf r_n}^T\n",
    "\\end{bmatrix}\n",
    "$   \n",
    "\n",
    "\n",
    "consider the $j$th diagonal entry of $\\big(\\mathbf R^H  \\mathbf R \\big)$.  It is given by $\\big(\\mathbf R^H  \\mathbf R \\big) = \\mathbf r_j^H \\mathbf r_j = \\langle\\ \\mathbf r_j, \\mathbf r_j\\rangle  = \\big \\Vert \\mathbf r_j \\big \\Vert_2^2  $\n",
    "\n",
    "it's a bit messy, but when we consider the the jth entry of $\\big(\\mathbf R  \\mathbf R^H \\big)$, it is given by  \n",
    "$\\big(\\tilde{ \\mathbf r_j}^T\\big) \\big(\\tilde{ \\mathbf r_j}^T\\big)^H = \\langle\\ \\tilde{ \\mathbf r_j}^T, \\tilde{ \\mathbf r_j}^T \\rangle = \\big \\Vert \\tilde{ \\mathbf r_j}^T \\big \\Vert_2^2 $\n",
    "\n",
    "\n",
    "Thus by examining the diagonal entries of $\\big(\\mathbf R^H  \\mathbf R \\big) $  and $\\big(\\mathbf R  \\mathbf R^H \\big)$ which must be equal since $\\mathbf {AA}^H = \\mathbf A^H \\mathbf A$ for normal matrices-- we are actually looking at the squared length (2 norms) of each column and each row of $\\mathbf R$.  \n",
    "\n",
    "In general, of course, $trace\\big(\\mathbf R^H  \\mathbf R \\big) = trace\\big(\\mathbf R  \\mathbf R^H \\big)$ by the cyclic property of trace, but that is a look at summed and aggregated values.  Looking at diagonal entries and examining the implications if each jth diagonal entry is the same... is a lot more enlightening.  \n",
    "\n",
    "Specifically consider the following dynamic programming inspired approach.  (Note: this could just be referred to as induction, but that seems to miss some of the essence of the overlapping subproblems that exist here.)  \n",
    "\n",
    "- - - -\n",
    "*The Close*  \n",
    "\n",
    "Suppose we look at the squared length of column $\\mathbf r_1$  and compare it to the squared length of row $\\tilde{ \\mathbf r_1}^T$.  Being upper triangular, we know $\\big \\Vert \\mathbf r_1 \\big \\Vert_2^2 = \\big \\vert \\lambda_1 \\big \\vert^2 $ .  But we are insisting the diagonal entries of $\\big(\\mathbf R^H  \\mathbf R \\big)$  and $\\big(\\mathbf R  \\mathbf R^H \\big)$ are the same, and hence we have $\\big \\Vert \\mathbf r_1 \\big \\Vert_2^2 = \\big \\Vert \\tilde{ \\mathbf r_1}^T \\big \\Vert_2^2 = \\big \\vert \\lambda_1 \\big \\vert^2 $.  We know $\\tilde{ \\mathbf r_1}^T$ always contains $\\lambda_1$, but the comparison length tells us it *only* contains $\\lambda_1$ i.e. row 1 has only an element on the diagonal.  \n",
    "\n",
    "Put another way, $\\big\\Vert \\big(\\tilde{ \\mathbf r_1}^T - \\lambda_1 \\mathbf e_1^T\\big)^T \\big \\Vert_2^2 = 0$ which occurs **iff** $\\big(\\tilde{ \\mathbf r_1}^T - \\lambda_1 \\mathbf e_1^T\\big)^T  = \\mathbf 0$, where as a reminder we have the standard basis vectors given by:  $\\mathbf I = \n",
    "\\bigg[\\begin{array}{c|c|c|c}\n",
    "\\mathbf e_1 & \\mathbf e_2 &\\cdots & \\mathbf e_{n}\n",
    "\\end{array}\\bigg]$\n",
    "\n",
    "\n",
    "Now we proceed to column 2 and row 2. When evaluating $\\big \\Vert \\mathbf r_2 \\big \\Vert_2^2$, we think through the following:  being upper triangular we, again, know there are only zeros below the diagonal, and as we've just uncovered, everything above the diagonal (i.e. i.e. everything in row 1 that isn't $\\lambda_1$ ) is a zero as well, and hence we know $\\big \\Vert \\mathbf r_2 \\big \\Vert_2^2 = \\big \\vert \\lambda_2 \\big \\vert^2 $.  But $\\big \\Vert \\tilde{ \\mathbf r_2}^T \\big \\Vert_2^2  = \\big \\Vert \\mathbf r_2 \\big \\Vert_2^2 = \\big \\vert \\lambda_2 \\big \\vert^2 $ and hence we discover that everything in row 2 must be a zero except for the eigenvalue.  \n",
    "\n",
    "Now we could proceed most formally via induction, but the idea is that of overlapping subproblems -- i.e. we repeat the above process for $k = 3, 4, ..., n$ and for each $k$ we recognize that $\\big \\Vert \\mathbf r_k \\big \\Vert_2^2 = \\big \\vert \\lambda_k \\big \\vert^2 $, because the preceding subproblems tell us that there are only zeros above the diagonal entry  for column $k$ (i.e. for rows $i = \\{1,..., k-1\\})$.  But then, because $\\big \\Vert \\tilde{ \\mathbf r_k}^T \\big \\Vert_2^2  = \\big \\Vert \\mathbf r_k \\big \\Vert_2^2 = \\big \\vert \\lambda_k \\big \\vert^2 $ we discover that there cannot be anything non-zero to the right of the diagonal for row $k$ either (and of course being upper triangular, there are only zeros to the left of the diagonal).  And after repeating this process for all columns in $\\mathbf R$, we have verified that $\\mathbf R$ is in fact diagonal -- i.e. $\\mathbf A$ is unitarily diagonalizable and in line with Schur's Inequality.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** yet another look at at Normal Matrices**\n",
    "\n",
    "if two matrices are normal, we have \n",
    "\n",
    "$\\mathbf A^H \\mathbf A = \\mathbf {AA}^H$\n",
    "\n",
    "or equivalently:\n",
    "\n",
    "$\\mathbf A^H \\mathbf A - \\mathbf {AA}^H = \\mathbf 0$\n",
    "\n",
    "now looking at the squared Frobenius norm of this matrix, we can get another test for normality in general... that is we recall that the (squared) Frobenius norm of a matrix is zero **iff** the matrix is zero.\n",
    "\n",
    "\n",
    "$\\big \\Vert \\mathbf A^H \\mathbf A - \\mathbf {AA}^H \\big \\Vert_F^2 = trace\\Big(\\big(\\mathbf A^H \\mathbf A - \\mathbf {AA}^H \\big)^H\\big(\\mathbf A^H \\mathbf A - \\mathbf {AA}^H \\big)\\big)= 0$\n",
    "\n",
    "we can expand this to\n",
    "\n",
    "$ trace\\Big(\\big(\\mathbf A^H \\mathbf A\\big)^2 \\Big) + trace\\Big(\\big(\\mathbf A \\mathbf A^H\\big)^2 \\Big) - trace\\Big(\\big(\\mathbf A^H \\mathbf A\\big)\\big(\\mathbf A \\mathbf A^H\\big) \\Big) - trace\\Big(\\big(\\mathbf A \\mathbf A^H\\big)\\big(\\mathbf A^H \\mathbf A\\big) \\Big)  =  0$  \n",
    "\n",
    "\n",
    "and using cyclic property of the trace, then re-arranging terms:\n",
    "\n",
    "$ trace\\Big(\\big(\\mathbf A^H \\mathbf A\\big)^2 \\Big) + trace\\Big( \\mathbf A^H \\mathbf A \\mathbf A^H \\mathbf A\\Big) - trace\\Big(\\mathbf A^H \\mathbf A \\mathbf A \\mathbf A^H \\Big) - trace\\Big(\\mathbf A \\mathbf A^H \\mathbf A^H \\mathbf A \\Big)  =  0$  \n",
    "\n",
    "\n",
    "\n",
    "$2trace\\Big(\\big(\\mathbf A^H \\mathbf A\\big)^2 \\Big)  =  trace\\Big(\\mathbf A^H \\mathbf A^H  \\mathbf A \\mathbf A \\Big) + trace\\Big(\\mathbf A^H \\mathbf A^H   \\mathbf A \\mathbf A \\Big) = 2 trace\\Big(\\mathbf A^H \\mathbf A^H \\mathbf A \\mathbf A  \\Big)  = 2 trace\\Big(\\big(\\mathbf A^2\\big)^H \\big(\\mathbf A^2\\big)  \\Big)  $ \n",
    "\n",
    "which gives us another test for normality: We may say a matrix is normal **iff**\n",
    "\n",
    "$trace\\Big(\\big(\\mathbf A^H \\mathbf A\\big)^2 \\Big) =  trace\\Big(\\big(\\mathbf A^2\\big)^H \\big(\\mathbf A^2\\big)  \\Big)   $ \n",
    "\n",
    "or put differently, we may say \n",
    "\n",
    "$trace\\Big(\\big(\\mathbf A^H \\mathbf A\\big)^2 \\Big) \\geq  trace\\Big(\\big(\\mathbf A^2\\big)^H \\big(\\mathbf A^2\\big)  \\Big)   $ \n",
    "\n",
    "with equality **iff** $\\mathbf A$ is normal.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**extension: ** skew Hermitian matrices are normal, and hence unitarily diagonalizable. \n",
    "\n",
    "A skew Hermitian matrix, is some $n$ x $n$ matrix, $\\mathbf A$, where $\\mathbf A = - \\mathbf A^H$ or equivalently $-\\mathbf A =  \\mathbf A^H$.  Note that the definition and implications are the same in the real case of skew symmetric, except it is worth noting that in the real case, the diagonal elements of $\\mathbf A$ must be zero (because the only real number that is equal to its negative conjugate is zero). \n",
    "\n",
    "if $\\mathbf A = - \\mathbf A^H$, then we can say that \n",
    "\n",
    "\n",
    "$\\mathbf {A A}^H = \\mathbf A \\big(\\mathbf A^H \\big)  = \\mathbf A \\big(-\\mathbf A \\big) = - \\mathbf A^2$  \n",
    "$\\mathbf A^H \\mathbf A = \\big(\\mathbf A^H \\big) \\mathbf A   = \\big(- \\mathbf A\\big) \\mathbf A  = - \\mathbf A^2$\n",
    "\n",
    "hence $\\mathbf {A A}^H = \\mathbf A^H \\mathbf A$  \n",
    "\n",
    "which means that a skew Hermitian matrix $\\mathbf A$ is normal.  \n",
    "\n",
    "It is perhaps worth noting that in either the real case or the complex case, we may do a Schur Decomposition on $\\mathbf A$, and see that $\\lambda_k = -\\lambda_k^H$ for $k = \\{1, 2, ..., n\\}$ , i.e. that each eigenvalue is equal to the negative of its own conjugate.  This means that each eigenvalue is either purely imaginary, or equal to zero.  One immediate consequence for the real case, is that for an $n$ x $n$ real, skew symmetric $\\mathbf A$, if $n$ is odd, we know that it is singular.  Why? For real matrices, complex eigenvalues come in conjugate pairs, which means that there must be (at least) one element that does not have a pair, and hence it cannot be imaginary and hence must be zero.  \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is problem 49, from \n",
    "\n",
    "\"Matrices : Theory & Applications\n",
    "Additional exercises\"\n",
    "\n",
    "found here: \n",
    "\n",
    "http://perso.ens-lyon.fr/serre/DPF/exobis.pdf\n",
    "\n",
    "\n",
    "For $\\mathbf M \\in \\mathbb R^{n x n}$\n",
    "\n",
    "**claim: **  \n",
    "\n",
    "$\\Big( trace \\big( \\mathbf M \\big) \\Big)^2  \\leq rank\\big(\\mathbf M\\big)trace\\big(\\mathbf M^T \\mathbf M\\big) = rank\\big(\\mathbf M\\big)\\big \\Vert \\mathbf M\\big\\Vert_F^2$\n",
    "\n",
    "for the proof, suppose that we have well ordered eigenvalues, $k$ of which are not zero, i.e. where $0 \\leq k \\leq n$, given below:\n",
    "\n",
    "$\\big \\vert \\lambda_1 \\big \\vert \\geq \\big \\vert \\lambda_2 \\big \\vert \\geq ... \\geq \\big \\vert \\lambda_k \\big \\vert \\geq 0 = \\big \\vert \\lambda_{k+1} \\big \\vert = .... = \\big \\vert \\lambda_{n}\\big \\vert$\n",
    "\n",
    "\n",
    "(Note that while the field is Reals, we observe the typical relaxation that allows Complex numbers during intermediate steps involving eigenvalues.) \n",
    "\n",
    "**proof:  **    \n",
    "we may collect all $n$ eigenvalues in a diagonal matrix $\\mathbf \\Lambda$, and the $k$ non-zero eigenvalues in a $k$ x $k$ diagonal matrix $\\mathbf D$.  The ones vector $\\mathbf 1$ has $k$ entries, each equal to one. \n",
    "\n",
    "$\\Big( trace \\big( \\mathbf M \\big) \\Big)^2  = \\langle \\mathbf 1 \\,, \\big( \\mathbf {D1}\\big)\\rangle^2 \\leq \\langle \\mathbf 1 \\,, \\mathbf 1\\rangle* \\langle \\big(\\mathbf {D1}\\big) \\,, \\big( \\mathbf {D1}\\big)\\rangle    = k *trace\\big(\\mathbf D^H \\mathbf D\\big) = k* trace\\big(\\mathbf \\Lambda^H \\mathbf \\Lambda \\big) = k * \\sum_{i=1}^n \\big\\vert \\lambda_i\\big\\vert ^2 $\n",
    "\n",
    "where the above inequality is given by Cauchy Schwartz\n",
    "\n",
    "$\\Big( trace \\big( \\mathbf M \\big) \\Big)^2 \\leq k *\\sum_{i=1}^n \\big\\vert \\lambda_i\\big\\vert ^2  \\leq rank\\big(\\mathbf M\\big) \\sum_{i=1}^n \\big\\vert \\lambda_i\\big\\vert ^2 $\n",
    "\n",
    "where we note that the $k$ non-zero eigenvalues of a matrix are a *lower bound* on its rank.  Justification: any real $n$ x $n$ matrix is unitarily similar to an upper triangular one.  Interpretted in terms of Gaussian Elimination, such an upper triangular matrix has at least $k$ pivots.  Put differently, once said matrix is put in reduced row echelon form, its number of pivots (i.e. its rank) cannot be less than $k$.   \n",
    "\n",
    "$\\Big( trace \\big( \\mathbf M \\big) \\Big)^2 \\leq rank\\big(\\mathbf M\\big) \\sum_{i=1}^n \\big\\vert \\lambda_i\\big\\vert ^2 \\leq rank\\big(\\mathbf M\\big)\\big \\Vert \\mathbf M\\big\\Vert_F^2$ \n",
    "\n",
    "via Schur's Inequality.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Misc. extension:**  \n",
    "\n",
    "**claim:** Suppose $\\mathbf A$ and $\\mathbf B$ are normal.  Further suppose that $\\mathbf {AB}$ is normal.  Then $\\mathbf {BA}$ is normal as well.  \n",
    "\n",
    "**proof:**\n",
    "\n",
    "By Schur Inequality, since $\\mathbf {AB}$ is normal, we know \n",
    "\n",
    "$\\big\\Vert \\mathbf {AB} \\big\\Vert_F^2 = \\sum_{i=1}^n \\Big(\\big\\vert \\lambda\\big(\\mathbf {AB}\\big)_i \\big\\vert^2\\Big) =  \\sum_{i=1}^n \\Big(\\big\\vert \\lambda\\big(\\mathbf {BA}\\big)_i \\big\\vert^2\\Big)$\n",
    "\n",
    "note that $\\mathbf A$ and $\\mathbf B$ both must be $n$ x $n$.  On the right hand side we make use of the fact that $\\big(\\mathbf{AB}\\big)$ and $\\big(\\mathbf {BA}\\big)$ have the same eigenvalues.  (The key idea is that the $trace\\Big(\\big(\\mathbf{AB}\\big)^k\\Big) = trace\\Big(\\big(\\mathbf{BA}\\big)^k\\Big)$ for any natural number $k$ -- see discussion about $\\frac{3}{4}$ the way down in the Vandermonde matrix writeup for more information.) \n",
    "\n",
    "Thus if we can prove $\\big\\Vert \\mathbf {AB} \\big\\Vert_F^2 = \\big\\Vert \\mathbf {BA} \\big\\Vert_F^2$, then we have proved \n",
    "\n",
    "$\\big\\Vert \\mathbf {BA} \\big\\Vert_F^2 = \\sum_{i=1}^n \\Big(\\big\\vert \\lambda\\big(\\mathbf {BA}\\big)_i \\big\\vert^2\\Big)$, i.e. that it satisfies the Schur Inequality with equality, and hence $\\big(\\mathbf{BA}\\big)$ is normal. \n",
    "\n",
    "*the main argument:*  \n",
    "\n",
    "$\\big\\Vert \\mathbf {AB} \\big\\Vert_F^2 = trace\\big(\\mathbf B^H \\mathbf A^H \\mathbf A \\mathbf B\\big)$\n",
    "\n",
    "$\\big\\Vert \\mathbf {AB} \\big\\Vert_F^2 = trace\\big(\\mathbf A^H \\mathbf A \\mathbf B \\mathbf B^H \\big) = trace\\big(\\mathbf A \\mathbf A^H \\mathbf B^H \\mathbf B \\big) = trace\\big(\\mathbf A^H \\mathbf B^H \\mathbf B \\mathbf A \\big) =  \\big\\Vert \\mathbf {BA} \\big\\Vert_F^2$\n",
    "\n",
    "where we make use of the cyclic property of the trace, and $\\mathbf A \\mathbf A^H =\\mathbf A^H \\mathbf A $ due to the normality of $\\mathbf A$, and  $\\mathbf B^H \\mathbf B = \\mathbf B \\mathbf B^H $ due to the normality of $\\mathbf B$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [py35]",
   "language": "python",
   "name": "Python [py35]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
