{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[1/2,1/2,0,0],\n",
    "              [1/2,1/2,0,0],\n",
    "              [0,0,1,0],\n",
    "               [0,0,0,1]\n",
    "             ])\n",
    "ones_v = np.ones(3)\n",
    "\n",
    "ones_mat = np.outer(ones_v, one_v.T) \n",
    "ones_mat = ones_mat/np.trace(ones_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 1.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0.]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P = np.zeros((4,4))\n",
    "P[0,-1] = 1\n",
    "for i in range(1,4):\n",
    "    P[i,i-1] = 1\n",
    "P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sympy as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}0 & 0 & 0 & 0 & 1\\\\1 & 0 & 0 & 0 & 0\\\\0 & 1 & 0 & 0 & 0\\\\0 & 0 & 1 & 0 & 0\\\\0 & 0 & 0 & 1 & 0\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "Matrix([\n",
       "[0, 0, 0, 0, 1],\n",
       "[1, 0, 0, 0, 0],\n",
       "[0, 1, 0, 0, 0],\n",
       "[0, 0, 1, 0, 0],\n",
       "[0, 0, 0, 1, 0]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = 5\n",
    "P = sp.zeros(m,m)\n",
    "P[0,-1] = 1\n",
    "for i in range(1,m):\n",
    "    P[i,i-1] = 1\n",
    "P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = 6\n",
    "B = sp.eye(5) - P**m\n",
    "B.rank()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}1 & 0 & 0 & 0 & 0\\\\0 & 1 & 0 & 0 & 0\\\\0 & 0 & 1 & 0 & 0\\\\0 & 0 & 0 & 1 & 0\\\\0 & 0 & 0 & 0 & 1\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "Matrix([\n",
       "[1, 0, 0, 0, 0],\n",
       "[0, 1, 0, 0, 0],\n",
       "[0, 0, 1, 0, 0],\n",
       "[0, 0, 0, 1, 0],\n",
       "[0, 0, 0, 0, 1]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P**5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\left[\\begin{matrix}0 & 0 & 0 & 0 & 1\\\\1 & 0 & 0 & 0 & 0\\\\0 & 1 & 0 & 0 & 0\\\\0 & 0 & 1 & 0 & 0\\\\0 & 0 & 0 & 1 & 0\\end{matrix}\\right]\n"
     ]
    }
   ],
   "source": [
    "print(sp.latex(P))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5  , 0.25 , 0.125, 0.125],\n",
       "       [0.5  , 0.25 , 0.125, 0.125],\n",
       "       [0.   , 0.5  , 0.25 , 0.25 ],\n",
       "       [0.   , 0.   , 0.5  , 0.5  ]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A@B@C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.375, 0.25 , 0.25 , 0.125],\n",
       "       [0.375, 0.25 , 0.25 , 0.125],\n",
       "       [0.25 , 0.25 , 0.25 , 0.25 ],\n",
       "       [0.   , 0.25 , 0.25 , 0.5  ]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A@B@C@A@A@C@B@B@B@B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1. , 0. , 0. , 0. ],\n",
       "       [0. , 0.5, 0.5, 0. ],\n",
       "       [0. , 0.5, 0.5, 0. ],\n",
       "       [0. , 0. , 0. , 1. ]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B = P@A@P.T\n",
    "B\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1. , 0. , 0. , 0. ],\n",
       "       [0. , 1. , 0. , 0. ],\n",
       "       [0. , 0. , 0.5, 0.5],\n",
       "       [0. , 0. , 0.5, 0.5]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C = (P@P)@A@((P@P).T)\n",
    "C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5, 0.5, 0. , 0. ],\n",
       "       [0.5, 0.5, 0. , 0. ],\n",
       "       [0. , 0. , 1. , 0. ],\n",
       "       [0. , 0. , 0. , 1. ]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = ones_mat - A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3333333333333335"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.trace(B.T@B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.33333333, 0.33333333, 0.33333333],\n",
       "       [0.33333333, 0.33333333, 0.33333333],\n",
       "       [0.33333333, 0.33333333, 0.33333333]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ones_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in general for scalars in $\\mathbb C$  \n",
    "\n",
    "the fact that eigenvalues, as roots to the characteristic polynomial vary continuously with their coefficients (see discussion below) and that coefficients to the characteristic polynomial vary continuously with entries in a matrix (standard proof uses principle minors -- for an alternative proof using trace and various matrix powers see \"Schurs_Inequality.ipynd\"), means that eigenvalues vary continously with components of a matrix.  \n",
    "\n",
    "**insert skeptch of proof that roots to monic polynomial vary continuously with coefficients using rudiments of planar topology, in particular winding numbers count roots, a useful inequality from Beardon, and a basic linear homotopy.**  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fact that eigenvalues vary continuously with matrix components allows some rather nice analytical proofs of results involving eigenvalues. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In particular it allows for some very nice proofs of results with Hermitian (inclusive of real symmetric) matrices since the eigenvalues vary contiuously with components of the matrix, but so long as we ensure Hermiticity the entire time, we know the eigenvalues must be real, and we may make use of Intermediate Value Theorem, in particular with respect to sign changes in eigenvalues implies a zero in between, which has rank implications, and we may use rank as a useful invariant along the way.  (There is some machinery buried, however, related to a theorem from Kato which ensures we can actually parameterize the eigenvalues as continuous functions... see \"Eigenvalue Continuity and Gerschgorin's Theorem\" by Li and Zhang  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sylvester's Law of Inertia**  \n",
    "\n",
    "*note:* this closely follows the approach in Meyer's Matrix Analysis, an accordingly usese QR factorization, though we could just as easily use SVD or other spectral theorem related results  \n",
    "\n",
    "The argument is for Hermitian $\\mathbf A \\in \\mathbb C^{\\text{n  x  n}}$, the matrix has a certain number of positive, negative and zero eigenvalues $\\big(r, k, m\\big)$ (where $n=r+k+m$) which is called the signature.  This is invariant under congruence transforms.  I.e. for any invertible $\\mathbf B \\in \\mathbb C^{\\text{n  x  n}}$  the signature of \n",
    "\n",
    "$\\mathbf B^* \\mathbf{AB}$ is the same as that for $\\mathbf A$.  \n",
    "\n",
    "\n",
    "*proof:*  \n",
    "\n",
    "run QR factorization on $\\mathbf B$ to get $\\mathbf B = \\mathbf Q\\mathbf R$. Since $\\mathbf B$ is non-singular we know the diagonal components of $\\mathbf R$ are non-zero, and infact we insist they are positive (if they are not positive make use of diagonal matrix $\\mathbf D$ which has all components on the unit circle such that $\\big(\\mathbf {DR}\\big)$ has all positive components on the diagonal and see that \n",
    "\n",
    "$\\mathbf B = \\mathbf Q \\mathbf R = \\mathbf Q \\mathbf I\\mathbf R = \\mathbf Q \\mathbf D^* \\mathbf D \\mathbf R = \\big(\\mathbf Q \\mathbf D^*\\big) \\big(\\mathbf D \\mathbf R\\big) $\n",
    "\n",
    "where $\\big(\\mathbf Q \\mathbf D^*\\big)$ is unitary and $\\big(\\mathbf D \\mathbf R\\big)$ is upper triangular with positive components on the diagonal.  Accordingly we assume WLOG that $\\mathbf B = \\mathbf Q\\mathbf R$ with $\\mathbf R$ having strictly positive entries on the diagonal.  \n",
    "\n",
    "now consider \n",
    "\n",
    "$\\mathbf X(\\tau) := \\tau \\mathbf Q + (1-\\tau)\\mathbf Q \\mathbf R = \\mathbf Q\\big(\\tau \\mathbf I + (1-\\tau) \\mathbf R\\big)$  \n",
    "for $\\tau \\in [0,1]$  \n",
    "\n",
    "and notice that for any $\\tau$ in the domain $\\det\\big(\\mathbf X(\\tau)\\big) \\neq 0$  \n",
    "\n",
    "finally considering \n",
    "\n",
    "$\\mathbf Y(\\tau) := \\mathbf X^*(\\tau)\\mathbf A\\mathbf X(\\tau)$  \n",
    "or  \n",
    "$\\mathbf Y(\\tau) = \\big(\\tau \\mathbf I + (1-\\tau) \\mathbf R^*\\big)\\mathbf Q^* \\mathbf A\\mathbf Q\\big(\\tau \\mathbf I + (1-\\tau) \\mathbf R\\big)$  \n",
    "\n",
    "where $\\mathbf Y(\\tau)$ is Hermitian for any $\\tau \\in [0,1]$  \n",
    "\n",
    "in particular   \n",
    "$\\mathbf Y(0)= \\mathbf Q^*\\mathbf R^* \\mathbf A\\mathbf Q\\mathbf R = \\mathbf B^* \\mathbf A\\mathbf B$  \n",
    "$\\mathbf Y(1) = \\mathbf Q^* \\mathbf A\\mathbf Q$  \n",
    "which is similar to $\\mathbf A$ and hence has the same eigenvalues as $\\mathbf A$  \n",
    "\n",
    "and for any $\\tau \\in [0,1]$  \n",
    "\n",
    "$\\text{rank}\\big(\\mathbf Y(\\tau)\\big) = \\text{rank}\\big(\\mathbf A\\big)$  \n",
    "(because $\\mathbf X(\\tau)$ is always invertible)  \n",
    "hence rank is a key invariant.    \n",
    "\n",
    "\n",
    "The proof follows in two stages:   \n",
    "*stage 1:*  consider the case where $\\text{rank}\\big(\\mathbf A\\big) = n$, i.e. our matrix of interest is invertible.  It is immediate that \n",
    "$\\mathbf Y(0)$ has the same signature $\\big(r, k, m\\big)$ as $\\mathbf Y(1)$ because if it did not then (at least) one of the eigenvalues changed sign and by intermediate value theorem there is some $\\tau^* $   where   \n",
    "$\\text{rank}\\big(\\mathbf Y(\\tau^*)\\big) \\lt n$ which contradicts the constancy of rank.  \n",
    "\n",
    "*stage 2:* consider the case where $\\text{rank}\\big(\\mathbf A\\big) = n-m \\lt n$    \n",
    "There are two ways to finish.  One is again to observe that rank is an invariant for $\\mathbf Y(\\tau)$ and if there is a change in signature then at least one sign change occurred and some eigenvalue went from positive to negative (or vice versa) crossing zero in between.  Topologically we observe that the preimage of each and every eigenvalue taking value zero is a closed set, since the image point $[0]$ is closed.  Inevitably this means that if a 'crossing' occurred then there must be too many eigenvalues equal to zero (rank deficiency) or too few eigenvalues equal to zero (rank excess) for some $\\tau^*$ when (one of) the crossing(s) ocurred.  This is conceptually simple but not so easy to write out.  \n",
    "\n",
    "An alternative approach is the one suggested, in effect by Meyer.  Supposing $\\mathbf A$ is singular, put all of its $n$ eigenvalues in a set.  There are $l$ distinct eigenvalues and call the minimum pairwise distance between these $l$ eigenvalues $d_1$.  Now re-run the argument \n",
    "\n",
    "$\\mathbf Y(\\tau, \\alpha) = \\mathbf X^*(\\tau)\\big(\\mathbf A + \\alpha \\mathbf I\\big)\\mathbf X(\\tau)$  \n",
    "\n",
    "where $\\alpha$ is a real valued slack parameter, initially set equal to $0$.  Supposing for a contradiction that the signature changes, we know it must have occurred by a change in positives vs negative (since zeros constant as rank is invariant) and assume WLOG that the number of positives decreases and the number of negatives decreased.  Now look at the eigenvalues of \n",
    "$\\mathbf Y(0,\\alpha =0)$  \n",
    "\n",
    "and again place them in a set, computing the minimum pairwise distance between them, calling it $d_2$.  \n",
    "\n",
    "Now set $\\alpha = \\alpha^* = \\frac{1}{2}\\min{d_1,d_2}$  \n",
    "\n",
    "and we have a contradiciton because $\\mathbf Y(0,\\alpha =\\alpha^*)$ has the same signature as $\\mathbf Y(0,\\alpha =0)$, and in particular has extra negative signs (the affine shift of eigenvalues was too small to have a sign crossing), but $\\big(\\mathbf A + \\alpha^* \\mathbf I\\big)$ is non-singular so we know it cannot have had a sign change for $\\tau \\in [0,1]$ by the stage one argument, and in fact it has the same number of negative eigenvalues as $\\mathbf A$ which is which is equal to the number of negative eigenvalues in $\\mathbf Y(0,\\alpha =\\alpha^*)$ strictly less than the number of negative eigenvalues of $\\mathbf Y(0,\\alpha =\\alpha^*)$, a contradiction.  \n",
    "\n",
    "*this second stage argument could be cleaned up / streamlined a bit*  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "https://cklixx.people.wm.edu/ELA-LiZhangMS4123.pdf  \n",
    "\n",
    "the interpretation for the non-singular of $\\mathbf A$  \n",
    "with \n",
    "\n",
    "$p_(x,\\tau) = \\det\\big(x\\mathbf I - \\mathbf Y(\\tau)\\big)$    \n",
    "(or equivalently considering this as a linear combination of the minors of $\\mathbf Y(\\tau)$  or using newton's identity on $\\mathbf Y(\\tau)$)  \n",
    "\n",
    "compute a winding number around  \n",
    "the circle centered at m, with radius $m$  and $m$ chosen such that it is bigger than any possible eigenvalue of $\\mathbf Y(t)$  -- e.g. using Schur's Test and operator 2 norm submultiplicativity (aka Schatten $\\infty$ norm) we have  \n",
    "\n",
    "$\\Big \\Vert \\mathbf Y(\\tau)\\Big \\Vert_2 $  \n",
    "$= \\Big \\Vert \\big(\\tau \\mathbf I + (1-\\tau) \\mathbf R^*\\big)\\mathbf Q^* \\mathbf A\\mathbf Q\\big(\\tau \\mathbf I + (1-\\tau) \\mathbf R\\big)\\Big \\Vert_2 $  \n",
    "$\\leq \\Big \\Vert \\big(\\tau \\mathbf I + (1-\\tau) \\mathbf R\\big)\\Big \\Vert_2^2 \\Big \\Vert\\mathbf Q^* \\mathbf A\\mathbf Q\\Big \\Vert_2  $   \n",
    "$\\leq \\Big\\{\\tau^2 \\Big \\Vert \\mathbf I\\big \\Vert_2^2 + 2\\tau(1-\\tau) \\big \\Vert \\mathbf I \\big \\Vert_2 \\cdot \\big \\Vert \\mathbf R\\Big \\Vert_2 + (1-\\tau) \\big \\Vert \\mathbf R \\big \\Vert_2^2\\Big\\} \\big \\Vert \\mathbf A\\big \\Vert_2  $   \n",
    "$\\lt \\Big\\{1 + \\big \\Vert \\mathbf R\\Big \\Vert_2 + \\big \\Vert \\mathbf R \\big \\Vert_2^2\\Big\\} \\big \\Vert \\mathbf A\\big \\Vert_2  $   (noting that $2\\tau(1-\\tau)\\leq \\frac{1}{2} \\lt 1$ by $\\text{GM}\\leq \\text{AM}$ and $\\big\\Vert \\mathbf I \\big \\Vert_2 = 1$)  \n",
    "$=\\Big\\{1 + \\big \\Vert \\mathbf B\\Big \\Vert_2 + \\big \\Vert \\mathbf B \\big \\Vert_2^2\\Big\\} \\big \\Vert \\mathbf A\\big \\Vert_2  $  \n",
    "and we can bound the maximal singular value of $\\mathbf B$ by applying the Schur Test (i.e. maximal row magnitude sum (L1 norm of each row and take the max) times maximal column magnitude sum, then take square root -- see \"Fun with trace\" notebook in linear algebra folder) so $\\big \\Vert \\mathbf B\\big \\Vert_2 = \\sigma \\leq \\gamma_1$   -- and we can apply the same test/inequality to $\\mathbf A$ to get $\\big \\Vert \\mathbf A\\big \\Vert_2 = \\sigma \\leq \\gamma_2$  \n",
    "\n",
    "and selecting \n",
    "\n",
    "$2m:=\\Big(1 + \\gamma_1 + \\gamma_1^2 \\Big) \\gamma_2$  \n",
    "(Gerschgorin discs may also be used instead of the Schur test)  \n",
    "\n",
    "gives a satisfactory result.  \n",
    "\n",
    "note:  \n",
    "an even easier approach is to use the fact that  \n",
    "$\\sigma \\leq \\big(\\sum_{k=1}^n \\sigma_k\\big)^\\frac{1}{2} = \\big \\Vert \\mathbf M \\big \\Vert_F$  \n",
    "and then use subadditivity and submultiplicativity of the Frobenius norm to get a very simple and crude upper bound on maximal singular value (or maximal magnitude eigenvalue)  \n",
    "\n",
    "\n",
    "now we may compute the winding number around (about zero) over this closed circular path radius $m$ centered at $m$ and observe that we have a homotopy from $\\tau \\in [0,1]$  between  \n",
    "\n",
    "$p_(x,0) \\to p(x,1)$  \n",
    "\n",
    "The value zero can never be in this path because \n",
    "$p_(x,\\tau)$ only has real roots (Hermitian matrices generate the polynomial), and our closed path in our domain only intersects the real axis in two locations -- $2m$ and $0$ neither of which can be roots to  $p_(x,\\tau)$ because the maximal eigenvalue of $p(x,\\tau)$ is $\\lt 2m$  and $p(0,\\tau) \\neq 0$  because $\\text{rank}\\big(\\mathbf Y(\\tau)\\big) = n$ for any $\\tau \\in [0,1]$.  \n",
    "\n",
    "This means \n",
    "$p_(x,0)$ and $p(x,1)$  \n",
    "have the same winding number and hence same number of roots in this region -- and indeed on the entire positive real line (since neither may have eigenvalues $\\gt 2m$).  Neither $p_(x,0)$ nor $p(x,1)$ has eigenvalues of $x=0$ and they have the same degree, so they must have the same number of negative eigenvalues as well.  \n",
    "- - - -  \n",
    "As in the Meyer setup, this implies the result even when $\\text{rank}\\big(\\mathbf A\\big) \\lt n$.  In this case we call the smallest magnitude negative eigenvalue of $\\mathbf A:= d$ (where $d \\lt 0$)  \n",
    "\n",
    "(if $\\mathbf A$ has no negative eigenvalues then the desired result is immediate because $\\mathbf A$ is positive semidefinite and $\\mathbf B^* \\mathbf{AB} = \\mathbf B^* \\mathbf C^* \\mathbf {CB} $  so the product is positive semi-definite, and we note that rank doesn't change when multiplying by an invertible matrix, so we proceed by assuming as least one negative eigenvalue -- and there are in any case finitely many of them)  \n",
    "\n",
    "now consider re-running the above argument on  \n",
    "$\\mathbf A'_k := \\mathbf A + \\frac{\\vert d\\vert}{1 + k}\\mathbf I$  \n",
    "\n",
    "This gives a density argument in the simplest form because  \n",
    "$\\lambda_i\\big(\\mathbf A'_k\\big) = \\lambda_i\\big(\\mathbf A\\big) + \\frac{\\vert d\\vert}{1 + k}$  \n",
    "or  \n",
    "$\\lambda_i\\big(\\mathbf A\\big) = \\lambda_i\\big(\\mathbf A'_k\\big)-\\frac{\\vert d\\vert}{1 + k}  $  \n",
    "\n",
    "\n",
    "so this tells us that we have $\\text{rank}\\big(\\mathbf A'_k\\big) = n$ for all natural numbers $k$ and that the number of negative eigenvalues of $\\mathbf A'_k$ is the same as the number of negative eigenvalues of $\\mathbf A$   \n",
    "\n",
    "Application of the prior result tells us that for all negative eigenvalues of $\\mathbf A'$ we have that they are still negative after the congruence transform (i.e. $\\mathbf B^* \\mathbf A'_k \\mathbf B$) \n",
    "so for any negative eigenvalue (recalling that the multiplicities are the same for $\\mathbf A'$ and $\\mathbf A$) we have  \n",
    "\n",
    "$\\lambda_i\\big(\\mathbf A'_k\\big) \\lt 0 \\longrightarrow \\lambda_i\\big(\\mathbf B^* \\mathbf A'_k \\mathbf B\\big) \\lt 0 $   \n",
    "\n",
    "This combined with the topological continuity of eigenvalues gives the desired result -- i.e. suppose for a contradiction that the number of positive eigenvalues in $\\mathbf B^*\\mathbf A\\mathbf B$ is strictly greater than in $\\mathbf A$, since $\\mathbf B^*\\mathbf A\\mathbf B$ and $\\mathbf A$ have the same rank (which implies same number of zero eigenvalues) this means some negative eigenvalue of $\\mathbf A$ must have 'crossed over' into a positive one.  We can   lower bound this distance by considering \n",
    "\n",
    "$\\lambda_{\\text{min positive}}\\big(\\mathbf B^*\\mathbf A\\mathbf B\\big) -  \\lambda_{\\text{smallest magnitude negative}}\\big(\\mathbf A\\big) \\geq \\lambda_{\\text{min positive}}\\big(\\mathbf B^*\\mathbf A\\mathbf B\\big) - 0 = \\lambda_{\\text{min positive}}\\big(\\mathbf B^*\\mathbf A\\mathbf B\\big) = c \\gt 0 $\n",
    "\n",
    "with  \n",
    "$\\lambda_{\\text{min positive}}\\big(\\mathbf B^*\\mathbf A\\mathbf B\\big) = c $    \n",
    "\n",
    "since the eigenvalues vary (topologically) continuously with the components of the matrix, we select $\\epsilon := \\frac{c}{3} \\gt 0$ which implies a $\\delta \\gt 0$ neighborhood where \n",
    "\n",
    "if $\\big \\Vert \\mathbf B^*\\mathbf A\\mathbf B - \\mathbf C \\big \\Vert_F \\lt 0 \\longrightarrow \\text{eigenvalue difference}\\big(\\mathbf B^*\\mathbf A\\mathbf B - \\mathbf C\\big) \\lt \\epsilon$  \n",
    "\n",
    "setting $\\mathbf C:= \\mathbf B^*\\mathbf A'_k\\mathbf B$  \n",
    "\n",
    "$\\big \\Vert \\mathbf B^*\\mathbf A\\mathbf B - \\mathbf C \\big \\Vert_F = \\frac{\\vert d\\vert}{1 + k}\\big \\Vert \\mathbf B^*\\mathbf B \\big \\Vert_F \\lt \\delta$   \n",
    "for large enough k gives  \n",
    "\n",
    "$c \\leq \\text{magnitude of eigenvalue difference}\\big(\\mathbf B^*\\mathbf A\\mathbf B - \\mathbf C\\big) \\lt \\epsilon =\\frac{c}{3}$   \n",
    "which is a contradiction \n",
    "\n",
    "(where eigenvalue difference is taken to be the minimum magnitude over all permutations, e.g. as on the 2nd page in the above cited paper from Li and Zhang-- equivalently, the metric used when proving topological eigenvalue continuity in Horn and Johnson 2nd edition of *Matrix Analysis*)  \n",
    "\n",
    "This implies we cannot have an increase in the multiplicities of the positive eigenvalues after the congruence transform, i.e. with $\\mathbf B^*\\mathbf A\\mathbf B$, and re-running the argument on $-\\mathbf A$ implies that we cannot have an increase in the number of negative eigenvalues of $\\mathbf A$ after the congruence transform. Since the number of zero eigenvalues (i.e. dimension of nullspace for hermitian matrices or equivalently: rank) is preserved under congruence transform, we have  \n",
    "\n",
    "$\\lambda_{\\text{number positive}}\\big(\\mathbf B^*\\mathbf A\\mathbf B\\big) \\leq \\lambda_{\\text{number positive}}\\big(\\mathbf A\\big)$  \n",
    "$\\lambda_{\\text{number negative}}\\big(\\mathbf B^*\\mathbf A\\mathbf B\\big) \\leq \\lambda_{\\text{number negative}}\\big(\\mathbf A\\big)$  \n",
    "$\\lambda_{\\text{number zero}}\\big(\\mathbf B^*\\mathbf A\\mathbf B\\big) = \\lambda_{\\text{number zero}}\\big(\\mathbf A\\big)$  \n",
    "\n",
    "\n",
    "\n",
    "and summing over the bound we have  \n",
    "\n",
    "$n $  \n",
    "$= \\lambda_{\\text{number positive}}\\big(\\mathbf B^*\\mathbf A\\mathbf B\\big) + \\lambda_{\\text{number negative}}\\big(\\mathbf B^*\\mathbf A\\mathbf B\\big) + \\lambda_{\\text{number zero}}\\big(\\mathbf B^*\\mathbf A\\mathbf B\\big)$  \n",
    "$\\leq \\lambda_{\\text{number positive}}\\big(\\mathbf A\\big) + \\lambda_{\\text{number negative}}\\big(\\mathbf A\\big) + \\lambda_{\\text{number zero}}\\big(\\mathbf A\\big)$  \n",
    "$ = n$  \n",
    "\n",
    "where equality is reach **iff** each inequality is an equality -- and since equality *is* reached we know the multiplicities must be the same.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With continuity of eigenvalues in hand we can easily prove some other interesting results, in particular   \n",
    " **Cauchy's Theorem of Interlacing of Eigenvalues**      \n",
    "\n",
    "Consider in particular an \n",
    "\n",
    "$\\text{n  x   r}$ matrix $\\mathbf Q_t$ with mutually orthnormal columns but $r\\lt n$  \n",
    "i.e. $\\mathbf Q_t$ stands for Q \"tall\". \n",
    "so \n",
    "\n",
    "$\\mathbf Q_t^* \\mathbf Q_t = \\mathbf I_r$  but   \n",
    "$\\mathbf Q_t \\mathbf Q_t^* = \\mathbf P$  which is a projector (hermitian and idempotent).  \n",
    "\n",
    "the result holds in general however **we focus on the the most important special case of** $r := n-1$  \n",
    "\n",
    "what Cauchy's Interlacing Theorem says is that the $n$ eigenvalues of $\\mathbf A$ interlace with those of $(n-1)$ eigenvalues of $\\big(\\mathbf Q_t^* \\mathbf A\\mathbf Q_t\\big)$, i.e.  \n",
    "\n",
    "$\\lambda_1 \\geq \\sigma_1 \\geq \\lambda_2 \\geq \\sigma_2 \\geq .... \\geq\\lambda_{n-2} \\geq \\sigma_{n-2}\\geq \\lambda_{n-1}\\geq \\sigma_{n-1} \\geq \\lambda_n$  \n",
    "\n",
    "The change in dimension between $\\mathbf A$ and  $\\big(\\mathbf Q_t^* \\mathbf A\\mathbf Q_t\\big)$ is awkward, though.  A better approach is to consider  \n",
    "\n",
    "$\\mathbf Q_z = \\bigg[\\begin{array}{c|c}\\mathbf Q_t & \\mathbf {0}\\end{array}\\bigg]$  \n",
    "where the 'z' denotes zero padded  \n",
    "\n",
    "so now we compare  \n",
    "\n",
    "$\\mathbf A$ and  $\\big(\\mathbf Q_z^* \\mathbf A\\mathbf Q_z\\big)$  \n",
    "\n",
    "now $\\big(\\mathbf Q_z^* \\mathbf A\\mathbf Q_z\\big)$ has same non-zero spectra as $\\big(\\mathbf Q_z\\mathbf Q_z^* \\mathbf A\\big)$ and   \n",
    "$\\big(\\mathbf Q_z\\mathbf Q_z^* \\mathbf A\\big) = \\big(\\mathbf Q_t\\mathbf Q_t^* \\mathbf A\\big)$  \n",
    "(check via outer product interpretation of matrix multiplication)   \n",
    "and  \n",
    "\n",
    "$\\big(\\mathbf Q_t\\mathbf Q_t^* \\mathbf A\\big)$ has same spectra as $\\big(\\mathbf Q_t^* \\mathbf A\\mathbf Q_t\\big)$ except the latter is $\\text{(n-1) x (n-1)}$ and necessarily has one less zero eigenvalue for dimension reasons   \n",
    "\n",
    "now further consider  \n",
    "$\\mathbf Q_z =\\mathbf Q\\begin{bmatrix}\n",
    "\\mathbf I_r & \\mathbf 0\\mathbf 0^T \\\\ \n",
    " \\mathbf 0\\mathbf 0^T& \\big(\\mathbf 0\\mathbf 0^T\\big)_{n-r}\n",
    "\\end{bmatrix} = \\mathbf Q \\mathbf D$    \n",
    "\n",
    "again focusing on  \n",
    "$\\mathbf Q_z =\\mathbf Q\\begin{bmatrix}\n",
    "\\mathbf I_{n-1} & \\mathbf 0\\mathbf 0^T \\\\ \n",
    " \\mathbf 0\\mathbf 0^T& \\big(\\mathbf 0\\mathbf 0^T\\big)_{1}\n",
    "\\end{bmatrix} = \\mathbf Q \\mathbf D$    \n",
    "\n",
    "where $\\mathbf Q$ is the square unitary matrix and $\\mathbf D$ is a diagonal matrix that is idempotent  \n",
    "\n",
    "so we ultimately compare the spectra of  \n",
    "\n",
    "$\\mathbf A$ and  $\\Big(\\mathbf Q_z^* \\mathbf A\\mathbf Q_z\\Big)  = \\Big(\\mathbf D \\mathbf Q^* \\mathbf A\\mathbf Q \\mathbf D\\Big)= \\Big(\\mathbf D^* \\mathbf {B D}\\Big)$   \n",
    "\n",
    "where  \n",
    "$\\mathbf B = \\mathbf Q^* \\mathbf A\\mathbf Q$  \n",
    "i.e. it is unitarily similar to $\\mathbf A$ and so still hermitian and with the same spectrum   \n",
    "\n",
    "*main argument*  \n",
    "We start here with the **simple eigenvalue case** and build out the general proof.    \n",
    "\n",
    "consider \n",
    "\n",
    "$\\mathbf X(\\tau) = \\tau\\mathbf D + (1-\\tau)\\mathbf I_n$  \n",
    "for $\\tau \\in [0,1]$  \n",
    "\n",
    "note that for $\\tau \\in [0,1)$ we know $\\mathbf X(\\tau) = \\tau\\mathbf D + (1-\\tau)\\mathbf I_n$  is non-singular (and Hermitian positive definite in particular)  \n",
    "\n",
    "and  \n",
    "$\\mathbf Y(\\tau) = \\mathbf X(\\tau)\\mathbf B \\mathbf X(\\tau) = \\mathbf X(\\tau)^*\\mathbf B \\mathbf X(\\tau)$  \n",
    "so by Sylvester's Law of Interia, we know that for $\\tau \\in [0,1)$  \n",
    "$\\text{signature}\\big(\\mathbf Y(\\tau)\\big) = \\text{signature}\\big(\\mathbf B\\big) = \\text{signature}\\big(\\mathbf A\\big)$\n",
    "\n",
    "we can then make an inference about $\\mathbf D^*\\mathbf B \\mathbf D=\\mathbf X(\\tau)^*\\mathbf B \\mathbf X(\\tau)$  at $\\tau =1$ by the (topological continuity of eigenvalue.  \n",
    "\n",
    "The argument proceeds, in effect by a for loop through each eigenvalue of $\\mathbf A$ and 'pinning' them with Sylvesters Law of Inertia, so  where the $\\gamma_i \\in \\mathbb R$ are chosen such that \n",
    "\n",
    "$i=1$   \n",
    "$\\big(\\mathbf B + \\gamma_i \\mathbf I_n\\big)$  \n",
    "is positive-semi definite with a single eigenvalue equal to zero \n",
    "i.e. with signature $(n-1,0,1)$ \n",
    "\n",
    "$i=2$  \n",
    "$\\big(\\mathbf B + \\gamma_i \\mathbf I_n\\big)$  \n",
    "with signature $(n-2,1,1)$  \n",
    "$\\vdots$  \n",
    "$i=n$  \n",
    "$\\big(\\mathbf B + \\gamma_i \\mathbf I_n\\big)$  \n",
    "with signature $(0,1,n-1)$  \n",
    "\n",
    "\n",
    "and in general for $i \\in \\{1,2,...,n\\}$  \n",
    "$\\big(\\mathbf B + \\gamma_i \\mathbf I_n\\big)$  \n",
    "with signature $(n-i,i-1,1)$   \n",
    "\n",
    "so again recalling \n",
    "$\\mathbf Y_i(\\tau) = \\mathbf X(\\tau)^*\\big(\\mathbf B + \\gamma_i \\mathbf I\\big) \\mathbf X(\\tau)$\n",
    "- - - -  \n",
    "for $i = 1, ..., n$  \n",
    "\n",
    "in case of i=1 \n",
    "we have \n",
    "$\\mathbf Y_1(\\tau)$   \n",
    "has signature $(n-1,0,1)$  \n",
    "for $\\tau \\in [0,1)$   \n",
    "and for $\\tau = 1$  we have \n",
    "has signature  $(\\leq n-1,0, \\geq 1)$ \n",
    "i.e. there cannot be an increase in the number of negative eigenvalues nor can there be an increase in positive eigenvalues (both for continuity reasons and more basically that $\\mathbf X(\\tau=1)$ is singular.)  \n",
    "\n",
    "This tells us that after shiting by $\\gamma_1$, we know $\\sigma_r$ i.e. the smallest eigenvalue for $\\mathbf Q_z^*\\big(\\mathbf A +\\gamma_1\\mathbf I\\big) \\mathbf Q_z $ is at least zero --i.e.   \n",
    "$0 = \\lambda_n +\\gamma_1 \\leq \\sigma_r +\\gamma_1 \\longrightarrow  \\lambda_n \\leq \\sigma_r$  \n",
    "\n",
    "in case of i=2  \n",
    "we have \n",
    "$\\mathbf Y_2(\\tau)$   \n",
    "has signature $(n-2,1,1)$  \n",
    "for $\\tau \\in [0,1)$   \n",
    "and for $\\tau = 1$  we have \n",
    "has signature $(\\leq n-2, \\leq 1, \\geq 1)$  \n",
    "But this tells us that after shifting by $\\gamma_2$ we know $\\sigma_{r-1}$ i.e. the 2nd smallest eigenvalue for $\\mathbf Q_z^*\\big(\\mathbf A +\\gamma_2\\mathbf I\\big) \\mathbf Q_z $ is at least zero and in particular  \n",
    "$0 = \\lambda_{n-1} +\\gamma_2 \\leq \\sigma_{r-1} +\\gamma_2\\longrightarrow \\lambda_{n-1}\\leq \\sigma_{r-1} $   \n",
    "it *also* tells us that  \n",
    "$\\sigma_{r} +\\gamma_2 \\leq 0 = \\lambda_{n-1} +\\gamma_2 \\longrightarrow \\sigma_{r} \\leq \\lambda_{n-1} $   \n",
    "*this is the interlacing property*  \n",
    "\n",
    "*remark:*  \n",
    "The reader should be able to see a pattern forming at this point.  Additionally, even though $\\mathbf Q_z^*\\big(\\mathbf A +\\gamma_2\\mathbf I\\big) \\mathbf Q_z$ has an extra zero in it vs\n",
    "$\\mathbf Q_t^*\\big(\\mathbf A +\\gamma_2\\mathbf I\\big) \\mathbf Q_t $  \n",
    "we can see this is innocuous -- because if we ignore the superflous zero, we can *still* see \n",
    "$0\\leq \\sigma_{r-1} +\\gamma_2$ so the desired relation follows  \n",
    "**insert more explanation here re:**   \n",
    "$\\sigma_{r} +\\gamma_2 \\leq 0 = \\lambda_{n-1} +\\gamma_2 \\longrightarrow \\sigma_{r} \\leq \\lambda_{n-1} $   \n",
    "\n",
    "\n",
    "*now for the general case of* $i= k$ we have  \n",
    "i=k  \n",
    "we have \n",
    "$\\mathbf Y_k(\\tau)$   \n",
    "has signature $(n-k,k-1,1)$  \n",
    "for $\\tau \\in [0,1)$   \n",
    "and for $\\tau = 1$  we have \n",
    "has signature $(\\leq n-k, \\leq k-1, \\geq 1)$  \n",
    "But this tells us that after shifting by $\\gamma_k$ we know $\\sigma_{r-k-1}$ is at least zero and in particular  \n",
    "\n",
    "$0 = \\lambda_{n-k-1} +\\gamma_k \\leq \\sigma_{r-k-1} +\\gamma_2\\longrightarrow \\lambda_{n-k-1}\\leq \\sigma_{r-k-1} $   \n",
    "*and* we know  \n",
    "$ \\sigma_{r-k-2} +\\gamma_2 \\leq 0 = \\lambda_{n-k-1} +\\gamma_k \\longrightarrow \\sigma_{r-k-2} \\leq \\lambda_{n-k-1} $   \n",
    "\n",
    "*note:* \n",
    "some  obvious special handling is needed to address the largest and smallest eigenvalues since they are corner cases  \n",
    "\n",
    "This *is* Cauchy Eigenvalue Interlacing and the argument in the case where eigenvalues are not simple (i.e. repeated roots) proceeds in an identical manner -- though it is perhaps less enjoyable.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**corollary of Cauchy Interlacing**  \n",
    "\n",
    "for Hermitian $\\mathbf A$  with eigenvalues $\\lambda_i$ \n",
    "(all matrices are assumed n x n unless otherwise noted)  \n",
    "\n",
    "$\\mathbf A + \\mathbf {yy}^*$  has eigenvalues $\\sigma_i$ that interlace those of $\\mathbf A$, i.e.  \n",
    "$\\sigma_1\\geq \\lambda_1\\geq \\sigma_2\\geq \\lambda_2\\geq....\\geq \\sigma_n\\geq \\lambda_n$  \n",
    "\n",
    "**proof**  \n",
    "we assume WLOG that $\\mathbf A \\succ \\mathbf 0$ \n",
    "(if not, instead run the argument on $\\mathbf A+\\delta \\mathbf I$ for large enough $\\delta\\gt 0$ )   \n",
    "\n",
    "via positive definiteness we have  \n",
    "$\\mathbf A= \\mathbf B\\mathbf B^*$  \n",
    "for some  \n",
    "$\\mathbf B = \\bigg[\\begin{array}{c|c|c|c} \n",
    "\\mathbf b_1 & \\mathbf b_2 &\\cdots & \\mathbf b_{n} \n",
    "\\end{array}\\bigg]$  \n",
    "augment B to  \n",
    "$\\mathbf C :=\\bigg[\\begin{array}{c|c|c|c|c} \n",
    "\\mathbf b_1 & \\mathbf b_2 &\\cdots & \\mathbf b_{n} & \\mathbf y\n",
    "\\end{array}\\bigg]$  \n",
    "\n",
    "then    \n",
    "$\\mathbf C\\mathbf C^*=\\mathbf A + \\mathbf {yy}^*$   \n",
    "\n",
    "and consider  \n",
    "$\\mathbf P:=\\begin{bmatrix}\\mathbf I_n\\\\ \\mathbf {0}\\end{bmatrix}$  \n",
    "observing that  \n",
    "$\\mathbf P^*\\mathbf P = \\mathbf I_n$   \n",
    "\n",
    "by Cauchy Eigenvalue Interlacing we have  \n",
    "$\\mathbf C^*\\mathbf C \\text{ interlaces } \\mathbf P^*\\mathbf C^*\\mathbf C \\mathbf P = \\mathbf B^*\\mathbf B$  \n",
    "$\\sigma_1\\geq \\lambda_1\\geq \\sigma_2\\geq \\lambda_2\\geq....\\geq \\sigma_n\\geq \\lambda_n\\geq \\sigma_{n+1}=0$  \n",
    "\n",
    "where the result follows because $\\mathbf B\\mathbf B^*=\\mathbf A$ has the same eigenvalues as $\\mathbf B^*\\mathbf B$  and $\\mathbf C^*\\mathbf C$ has the same non-zero eigenvalues as $\\mathbf C\\mathbf C^*=\\mathbf A + \\mathbf {yy}^*$, which for dimension reasons means their eigenvalues are the same except for the 'dummy eigenvalue' $\\sigma_{n+1}=0$ that was necessarily appended to the end.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*note: \"Cauchyâ€™s Interlace Theorem forEigenvalues of Hermitian Matrices\"*  \n",
    "https://pdfs.semanticscholar.org/68b2/f12f71940481380503651adc8306d2d70a7e.pdf\n",
    "\n",
    "mentions that the Cauchy Interlace Theorem has been proven via use of Sylvester's Law of Inertia...  \n",
    "and cites  \n",
    "\n",
    "page 186 of  \n",
    "B. N. Parlett,The Symmetric Eigenvalue Problems, Prentice-Hall, Englewood Cliffs, NJ, 1980\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
