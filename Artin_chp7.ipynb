{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ex 7.3.8**  \n",
    "for some unit vector $\\mathbf w \\in \\mathbb R^n$  \n",
    "**(a)** prove that the matrix $\\mathbf P = \\mathbf I - 2\\mathbf {ww}^T$ is orthgonal  \n",
    "*by inspection or diagonalization, or:*  since $\\mathbf P$ is real symmetric, it suffices to chec k $\\mathbf P^2= \\mathbf I$ (i.e. an involution), so \n",
    "$\\mathbf P^2 = \\mathbf I -2\\mathbf {ww}^T-2\\mathbf {ww}^T + 4\\mathbf w \\big(\\mathbf w^T\\mathbf w\\big)\\mathbf {w}^T = \\mathbf I - 4 \\mathbf {ww}^T +4 \\mathbf {ww}^T =\\mathbf I$   \n",
    "note: this confirms that $\\mathbf P$ is real orthogonal, and making use of this we know all eigenvalues of $\\mathbf P$ are on the unit circle -- but in fact +1 or -1 since $\\mathbf P$ is involutive-- then taking the trace we see  \n",
    "$\\text{trace}\\big(\\mathbf P\\big)= \\text{trace}\\big(\\mathbf I\\big) + - 2\\cdot\\text{trace}\\big(\\mathbf {ww}^T\\big)= n -2$  which means that $\\mathbf P$ has $(n-1)$ eigenvalues of $(+1)$ and $1$ eigenvalue of $(-1)$-- there are of course many ways at this result. But this also means that $\\det\\big(\\mathbf P\\big) = -1$ which is something we will exploit as a corollary to ex 7.3.9  \n",
    "\n",
    "**(b)**  Prove that multiplication by $\\mathbf P$ is a reflection through the space orthogonal to $\\mathbf w$.  I.e. if $\\mathbf v = c\\mathbf w + \\mathbf w'$ for $\\mathbf w'\\perp \\mathbf w$, then \n",
    "$\\mathbf P\\mathbf v = c\\mathbf P\\mathbf w + \\mathbf P\\mathbf w' = c\\big(\\mathbf I - 2\\mathbf {ww}^T\\big)\\mathbf w +\\big(\\mathbf I - 2\\mathbf {ww}^T\\big)\\mathbf w'= c\\big(\\mathbf w - 2\\mathbf w\\big) + \\mathbf I\\mathbf w'+ \\mathbf 0 = -c\\mathbf w + \\mathbf w' $     \n",
    "\n",
    "**(c)** let $\\mathbf x$ and $\\mathbf y$ be arbitrary vectors in $\\mathbb R^n$ with the same length (using standard metric).  \n",
    "Select a vector $\\mathbf w$ such that $\\mathbf P = \\mathbf I - 2\\mathbf {ww}^T$ is orthogonal and $\\mathbf P\\mathbf x = \\mathbf y$.  (Note this argues for existence of $\\mathbf P$ *not* uniqueness.)  \n",
    "\n",
    "in the case that $\\mathbf x = \\mathbf 0$ then $\\mathbf y$ is necessarily zero as well (e.g. because it has same length as $\\mathbf x$). \n",
    "\n",
    "*remark:* geometrically this is rather interesting to think about in the case 2 dimensional case  \n",
    "\n",
    "(i) if $\\mathbf x= \\mathbf y $ then selecting $\\mathbf w=\\mathbf 0$ gives the result and we are done    \n",
    "(ii) if $\\mathbf x =-\\mathbf y$ then selecting $\\mathbf w = \\mathbf x$ gives the result and we're done  \n",
    "(iii) since $\\mathbf x$ and $\\mathbf y$ have the same lengths and we are working in reals, in all other cases we must have $\\mathbf y$ and $\\mathbf x$ linearly independent.  \n",
    "\n",
    "so consider using part b of this exercise, consider     \n",
    "$\\mathbf x = c\\mathbf w + \\mathbf w'$  and $\\mathbf y = \\mathbf P \\mathbf x = -c\\mathbf w + \\mathbf w'$  \n",
    "\n",
    "then  we have  \n",
    "\n",
    "$\\mathbf x - \\mathbf y = 2c\\mathbf w$  or  \n",
    "$\\mathbf w = \\frac{1}{2c}\\big(\\mathbf x - \\mathbf y\\big)$  \n",
    "and since $\\mathbf w$ has length one this implies  \n",
    "$\\big \\vert c \\big \\vert = \\frac{1}{2}\\big \\Vert \\mathbf x - \\mathbf y\\big \\Vert_2$  \n",
    "the choice of sign for $c$ is arbitrary, so we may select the case of positive $c$ i.e.   \n",
    "$\\mathbf w = \\big(\\mathbf x - \\mathbf y\\big)\\cdot \\big \\Vert \\mathbf x - \\mathbf y \\big \\Vert_2^{-1}$  \n",
    "\n",
    "a quick check then shows that \n",
    "$\\mathbf w' = \\mathbf x + \\mathbf y$ meets the orthogonality requirements and that \n",
    "$\\mathbf x = c\\mathbf w + \\mathbf w'$ and  \n",
    "$\\mathbf y = -c\\mathbf w + \\mathbf w'$  \n",
    "which completes the requirements for this problem  \n",
    "\n",
    "*remark*  \n",
    "There is an interesting and near obvious parallel that comes up when dealing with e.g. iid standard normal r.v.'s $X$ and $Y$.  If we consider  \n",
    "\n",
    "$A=\\frac{1}{\\sqrt{2}}(X+Y)$ and $B=\\frac{1}{\\sqrt{2}}(X-Y)$, these too are iid standard normals  \n",
    "(e.g. recall sums of independent normals give a normal r.v., then check the covariance between $A$ and $B$)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*the below works through this in some detail.  It may be skipped.*  \n",
    "The below works through the above in blocked detail, starting with the 'hyper vector' (page 96) given by $\\mathbf B$, though in this case we can be sure that $\\mathbf B$ is real invertible matrix (e.g. created via the basis creation algorithm).  So consider  \n",
    "\n",
    "$\\mathbf B = \n",
    "\\bigg[\\begin{array}{c|c|c|c|c} \n",
    "\\mathbf x  & \\mathbf y & \\mathbf b_{3} & \\cdots & \\mathbf b_n \n",
    "\\end{array}\\bigg]$\n",
    "\n",
    "where $\\det\\big(\\mathbf B\\big) \\neq 0$  \n",
    "(technically invertibility of $\\mathbf B$ is not used in what follows, however we selected it to have a spanning set of linearly independent vectors so as to fit with other common operations on a 'hypervector')  \n",
    "\n",
    "now consider the linear map  \n",
    "\n",
    "$ A :=\\begin{bmatrix}1 & 1 &\\mathbf 0^T \\\\ -1 & 1 & \\mathbf 0^T \\\\ \\mathbf 0 & \\mathbf 0 & \\mathbf I_{n-2}\\end{bmatrix}$   \n",
    "\n",
    "multiplying on the right of $\\mathbf B$ gives  \n",
    "$\\mathbf B  A = \\bigg[\\begin{array}{c|c|c|c|c} \n",
    "\\mathbf x -\\mathbf y  &\\mathbf x + \\mathbf y & \\mathbf b_{3} & \\cdots & \\mathbf b_n \n",
    "\\end{array}\\bigg] = \\bigg[\\begin{array}{c|c|c|c|c} \n",
    "\\alpha\\mathbf w  & \\mathbf w_2 & \\mathbf b_{3} & \\cdots & \\mathbf b_n \n",
    "\\end{array}\\bigg] $  \n",
    "\n",
    "where $\\alpha \\mathbf w^T\\mathbf w_2=\\big(\\mathbf x-\\mathbf y\\big)^T\\big(\\mathbf x + \\mathbf y\\big) = \\mathbf x^T\\mathbf x + \\mathbf x^T \\mathbf y -\\mathbf y^T \\mathbf x -\\mathbf y^T\\mathbf y = 0$  \n",
    "by symmetry of the real (inner) dot product and the fact that $\\mathbf x$ and $\\mathbf y$ have the same lengths. Hence we know that $\\big(\\mathbf x+\\mathbf y\\big)$ and $\\big(\\mathbf x - \\mathbf y\\big)$ are orthogonal, and $\\mathbf w$ has norm one, hence has some scalar $\\alpha$ in front of it-- and of course $\\vert \\alpha \\vert =\\big\\Vert \\mathbf x - \\mathbf y\\big \\Vert_2^{-1}$ which agrees with what we had above.  \n",
    "\n",
    "inverting $A$ gives  \n",
    "$\\bigg[\\begin{array}{c|c|c} \n",
    "\\mathbf x  & \\mathbf y & *\n",
    "\\end{array}\\bigg] $  \n",
    "$ = \\bigg[\\begin{array}{c|c|c} \n",
    "\\mathbf x -\\mathbf y  &\\mathbf x + \\mathbf y & * \n",
    "\\end{array}\\bigg] \\mathbf A^{-1}$  \n",
    "$=\\bigg[\\begin{array}{c|c|c} \n",
    "\\mathbf x -\\mathbf y  &\\mathbf x + \\mathbf y & * \n",
    "\\end{array}\\bigg]\\begin{bmatrix}\\frac{1}{2} & \\frac{-1}{2} &\\mathbf 0^T \\\\ \\frac{1}{2} & \\frac{1}{2} & \\mathbf 0^T \\\\ \\mathbf 0 & \\mathbf 0 & \\mathbf I_{n-2}\\end{bmatrix}$  \n",
    "$=\\bigg[\\begin{array}{c|c|c} \n",
    "\\alpha \\mathbf w  &\\mathbf w_2& *  \n",
    "\\end{array}\\bigg]\\begin{bmatrix}\\frac{1}{2} & \\frac{-1}{2} &\\mathbf 0^T \\\\ \\frac{1}{2} & \\frac{1}{2} & \\mathbf 0^T \\\\ \\mathbf 0 & \\mathbf 0 & \\mathbf I_{n-2}\\end{bmatrix}    \n",
    "=\\bigg[\\begin{array}{c|c|c} \n",
    "\\frac{1}{2}\\alpha\\mathbf w +\\frac{1}{2}\\beta \\mathbf w_2  &-\\frac{1}{2}\\alpha\\mathbf w +\\frac{1}{2}\\beta \\mathbf w_2 & *\n",
    "\\end{array}\\bigg] $  \n",
    "\n",
    "using a change of variables since the length of $\\mathbf w'$ is not specified, consider  \n",
    "$\\mathbf w' := \\frac{1}{2}\\beta \\mathbf w_2$   \n",
    "$\\mathbf x = c \\mathbf w +  \\mathbf w'$  and  \n",
    "$\\mathbf y = -c \\mathbf w +  \\mathbf w'$  \n",
    "as required in this problem  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*remark:* if we were to work in $\\mathbb C$ with an eye to applying this to unitary matrices, the above wouldn't literally work as \n",
    "\n",
    "to simplify the calculations, assume WLOG that  \n",
    "$\\big \\Vert \\mathbf x \\big \\Vert_2 = \\big \\Vert \\mathbf y \\big \\Vert_2=1$   \n",
    "\n",
    "$\\big(\\mathbf x- \\mathbf y\\big)^* \\big(\\mathbf x+ \\mathbf y\\big) = \\mathbf x^*\\mathbf x+\\mathbf y^*\\mathbf y-\\mathbf y^*\\mathbf x+\\mathbf x^*\\mathbf y = 2\\cdot\\text{im}\\big(\\mathbf x^*\\mathbf y\\big)$   \n",
    "the result can be saved via the use of polarization, i.e. instead considering  \n",
    "$\\big(\\mathbf x- \\alpha\\mathbf y\\big)^* \\big(\\mathbf x+ \\alpha\\mathbf y\\big)$  \n",
    "if $\\mathbf x^*\\mathbf y=0$ (or in fact any real number), then select $\\alpha :=1$, otherwise selection \n",
    "$\\alpha = e^{i\\theta}$ where $\\theta$  is the polar angle of $\\big(\\mathbf x^*\\mathbf y\\big)^*=\\mathbf y^*\\mathbf x$ -- in words, this is the value of $\\mathbf x^*\\mathbf y$, with the length shrunk to one, and then taking conjugating the value so $\\alpha \\cdot \\mathbf x^*\\mathbf y = \\big \\vert \\mathbf x^*\\mathbf y\\big \\vert \\in \\mathbb R$ (positivity is an added bonus though not really needed)  \n",
    "\n",
    "so  \n",
    "$\\big(\\mathbf x- \\alpha\\mathbf y\\big)^* \\big(\\mathbf x+ \\alpha\\mathbf y\\big) = \\mathbf x^*\\mathbf x+ \\big\\vert \\alpha\\big\\vert^2 \\cdot \\mathbf y^*\\mathbf y-(\\bar{\\alpha}\\cdot\\mathbf y)^*\\mathbf x+\\mathbf x^*(\\alpha\\cdot\\mathbf y)  = -\\big(\\mathbf x^* \\alpha\\cdot\\mathbf y\\big)^*+\\mathbf x^*\\alpha\\cdot \\mathbf y = -\\big(\\mathbf x^* \\alpha\\cdot\\mathbf y\\big)+\\mathbf x^*\\alpha\\cdot \\mathbf y =0 $  \n",
    "because taking the conjugate transpose of a real scalar gives that same real scalar   \n",
    "\n",
    "so select $\\mathbf w\\propto \\mathbf x-\\alpha\\mathbf y $  and revisit  \n",
    "$\\mathbf x = c\\mathbf w + \\mathbf w'$  and $\\alpha\\mathbf y = \\mathbf P \\mathbf x = -c\\mathbf w + \\mathbf w'$  \n",
    "to see  \n",
    "$\\mathbf x - \\alpha\\mathbf y = 2 c \\mathbf w$ as desired  \n",
    "and $\\mathbf w'$ is orthogonal to $\\mathbf w$ and its length isn't constrained to one, so we may rescale as needed, e.g. with  \n",
    "$c\\mathbf w =\\gamma(\\mathbf x-\\alpha\\mathbf y) $   \n",
    "$\\mathbf w' = \\gamma\\big(\\mathbf x+ \\alpha\\mathbf y\\big)$    \n",
    "$\\mathbf x = 2\\gamma \\mathbf x$ \n",
    "(implying $\\gamma =\\frac{1}{2}$)  \n",
    "\n",
    "$\\alpha \\mathbf y = -c\\mathbf w + \\mathbf w'= -\\gamma(\\mathbf x-\\alpha\\mathbf y) + \\gamma\\big(\\mathbf x+ \\alpha\\mathbf y\\big)=2\\gamma\\cdot\\alpha \\mathbf y$, again implying $\\gamma = \\frac{1}{2}$ so we are consistent.  \n",
    "\n",
    "*extension:*  \n",
    "if we follow the below ex 7.3.9 and we *could*, mostly, adapt it to unitary matrices, what it tell us is  \n",
    "with unitary matrices of the form $\\mathbf P^{(i)} = \\mathbf I -2\\mathbf w_i \\mathbf w_i^*$  \n",
    "we can attack the first column vector $\\mathbf q_1$ so $\\mathbf P^{(1)}\\mathbf q_1 = \\alpha_1\\mathbf e_1$ (with standard basis vector $\\mathbf e_1$)  \n",
    "\n",
    "\n",
    "$\\mathbf P^{(1)}\\mathbf Q= \\begin{bmatrix} \\alpha_1 & \\mathbf 0^T \\\\ \\mathbf 0 & \\mathbf Q^{(2)} \\end{bmatrix}$   and the ending would be  \n",
    "$\\mathbf P^{(k)}...\\mathbf P^{(2)}\\mathbf P^{(1)}\\mathbf Q = \\mathbf D$ where $\\mathbf D$ is diagonal and unitary, i.e. $d_{i,i} =\\alpha_i$  so  \n",
    "$ \\mathbf D^*\\mathbf P^{(k)}...\\mathbf P^{(2)}\\mathbf P^{(1)}\\mathbf Q = \\mathbf I$  \n",
    "\n",
    "path connectedness then follows for dimension $n\\geq$ with even $k$ by the same argument as below (and $\\mathbf D^*$ is easily connected to the identity by 'rotating' each diagonal component over the unit circle).  \n",
    "\n",
    "However the above portion isn't really needed when working in the complex plane.  We could prove directly that a single unitary Householder matrix is path connected to the identity by using a one-size fits all argument to show that *any* unitary matrix is path connected to the identity.  Making use of the normality of unitary matrices, we have    \n",
    "$\\mathbf Q = \\mathbf U\\mathbf \\Lambda \\mathbf U^*$  \n",
    "where $\\mathbf U$ is unitary and all eigenvalues are on the unit circle, so $\\Lambda$ is unitary and the product is unitary for any arbitrary $\\lambda_i$.  Then we merely need to 'rotate' the values of $\\lambda_i$ along the unit circle to be one at $\\tau =1$ implying $\\mathbf Q(1) = \\mathbf U\\mathbf I  \\mathbf U^* = I$  \n",
    "This is a much easier result than in the real orthogonal matrix case-- in such a case while the matrices are diagonalizable over $\\mathbb C$ we couldn't guarantee that any path to the identity stays real (and in fact if the determinant of a real orthogonal matrix is $-1$ this is impossible by e.g. intermediate value theorem, though the below shows that if the determinant is +1, then the result *is* possible)     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ex 7.3.9**  \n",
    "considering any orthogonal matrix $\\mathbf Q \\in \\mathbb R^\\text{n x n}$  (where n is some natural number $\\geq 2$)  \n",
    "\n",
    "using results from ex 7.3.8, with orthgonal matrices of the form $\\mathbf P^{(i)} = \\mathbf I -2\\mathbf w_i \\mathbf w_i^T$  \n",
    "we can attack the first column vector $\\mathbf q_1$ so $\\mathbf P^{(1)}\\mathbf q_1 = \\mathbf e_1$ (with standard basis vector $\\mathbf e_1$)  \n",
    "\n",
    "\n",
    "$\\mathbf P^{(1)}\\mathbf Q= \\begin{bmatrix} 1 & \\mathbf 0^T \\\\ \\mathbf 0 & \\mathbf Q^{(2)} \\end{bmatrix}$    \n",
    "where we use the fact that the matrices on the LHS are in the Orthogonal group, so their product must be as well.  The first column of the RHS is $\\mathbf e_1$ by design.  But e.g. from ex 7.2.5, we know that the first row of an orthogonal matrix has norm 1 as well, and since the top left component of the RHS is 1, this implies all other components on the first row are zero and hence the first row is given by $\\mathbf e_1^T$.  Examining the blocked structure (and the fact that the RHS must be orthogonal) we see  $\\big(\\mathbf Q^{(2)}\\big)^T \\mathbf Q^{(2)} = \\mathbf I_{n-1}$, i.e. $\\mathbf Q^{(2)}$ is necessarily orthogonal as well.    \n",
    "\n",
    "To finish this off we may formalize with induction, but it seems preferable to work through this by recursing on the smaller subproblem of making the first column of $\\mathbf Q^{(2)}$ become the first standard basis vector for an n-1 dimensional space, i.e. by applying  \n",
    "$\\begin{bmatrix} 1 & \\mathbf 0^T \\\\ \\mathbf 0 & \\mathbf P^{(2)} \\end{bmatrix}\\mathbf P^{(1)}\\mathbf Q=\\begin{bmatrix} 1 & \\mathbf 0^T \\\\ \\mathbf 0 & \\mathbf P^{(2)} \\end{bmatrix}\\begin{bmatrix} 1 & \\mathbf 0^T \\\\ \\mathbf 0 & \\mathbf Q^{(2)} \\end{bmatrix}=\\begin{bmatrix} 1 & \\mathbf 0^T \\\\ \\mathbf 0 & \\mathbf P^{(2)}\\mathbf Q^{(2)} \\end{bmatrix}$  \n",
    "where this new blocked matrix is necessarily orthogonal as well.  The $\\text{n-1 x n-1}$ submatrix in the bottom corner is given by  \n",
    "$\\mathbf P^{(2)}\\mathbf Q^{(2)} =   \\begin{bmatrix} 1 & \\mathbf 0^T \\\\ \\mathbf 0 & \\mathbf Q^{(3)} \\end{bmatrix}$  \n",
    "\n",
    "for avoidance of doubt notice:  \n",
    "$\\begin{bmatrix} 1 & \\mathbf 0^T \\\\ \\mathbf 0 & \\mathbf P^{(2)} \\end{bmatrix}= \\mathbf I - 2\\mathbf {xx}^T$  where $\\mathbf x = \\sum_{k=2}^n \\alpha_k\\mathbf e_k$ so the above blocked matrix is still conforming in structure (i.e. it is still Householder, as we will shortly observe).  \n",
    "\n",
    "Then we keep repeating by attacking the first column of $\\mathbf Q^{(r)}$ at each the $r$th iteration and map it to $\\mathbf e_1 \\in \\mathbb R^{n-r+1}$.  At each pass, we make incremental progress and can assert that the columns $j\\in\\{1,2,...,r\\}$ of the RHS are $\\mathbf e_j \\in \\mathbb R^n$  and hence after at most $n$ applications we have recovered the identity matrix. That is, we've proven, for any orthogonal $\\mathbf Q \\in \\mathbf R^\\text{n x n}$,  \n",
    "\n",
    "$\\mathbf P^{(k)}...\\mathbf P^{(2)}\\mathbf P^{(1)}\\mathbf Q = \\mathbf I$ or  \n",
    "$\\mathbf Q=\\big(\\mathbf P^{(k)}...\\mathbf P^{(2)}\\mathbf P^{(1)}\\big)^T$  \n",
    "for some $1\\leq k \\leq n$ \n",
    "\n",
    "Matrices of the form $\\mathbf P^{(i)}$ are known as Householder Matrix / Householder Reflections, which in addition to being geometrically interesting, have uses in numerical linear algebra   \n",
    "\n",
    "**corollary**  \n",
    "These Householder matrices, i.e. matrices of the form $\\mathbf P^{(i)}$ are the *generators* of the orthogonal group.  In particular this tells us that if $\\det\\big(\\mathbf Q\\big )=1$ it is in $\\text{SO}(n)$ and it may be decomposed into in an even number of products of $\\mathbf P^{(i)}\\neq \\mathbf I$, so $\\det\\big(\\mathbf P^{(i)}\\big) = -1$ .  It we examine two of these at at time, we have  \n",
    "\n",
    "$\\mathbf P^{(i)}\\mathbf P^{(j)} = \\big(\\mathbf I -2\\mathbf w_1\\mathbf w_1^T\\big)\\big(\\mathbf I -2\\mathbf w_2\\mathbf w_2^T\\big) $  \n",
    "\n",
    "in the case of dimension $n\\geq 3$  since $\\mathbf P^{(i)}$  and $\\mathbf P^{(j)}$  each have some $\\mathbf w_i$ and $\\mathbf w_j$, but there are at least 3 linearly independent vectors in this space, with some 3rd linear independent vector $\\mathbf y$ (use e.g. the basis creation algorithm from chapter 3 to find it), and after running Gram Schmidt we have $\\mathbf y\\mapsto \\mathbf z$ such that $\\mathbf z$ is orthonormal to $\\mathbf w_i$ and $\\mathbf w_j$,  then consider \n",
    "\n",
    "$\\mathbf P^{(i)}(\\tau) = \\mathbf I - 2\\mathbf v_i(\\tau)\\mathbf v_i(\\tau)^T$  \n",
    "where for $\\tau \\in [0,1]$  \n",
    "$\\mathbf v_i(\\tau) = \\sqrt{1-\\tau}\\cdot\\mathbf w_i + \\sqrt{\\tau}\\cdot \\mathbf z$  \n",
    "and  \n",
    "$\\big\\Vert \\mathbf v_i(\\tau)\\big\\Vert_2^2 = (1-\\tau)\\cdot\\big\\Vert \\mathbf w_i \\big\\Vert_2^2 +\\tau\\cdot\\big\\Vert \\mathbf z \\big\\Vert_2^2+ 2\\sqrt{\\tau(1-\\tau)}\\cdot\\mathbf w_i^T\\mathbf z = (1-\\tau)\\cdot 1 + \\tau\\cdot 1 + 0 = 1$  \n",
    "so $\\mathbf v_i(\\tau)$ is unit length and thus $\\mathbf P^{(i)}(\\tau)$ is a Householder matrix for all $\\tau$ in our domain.  \n",
    "\n",
    "This then implies at $\\tau =0$  \n",
    "$\\mathbf P^{(i)}(0)\\mathbf P^{(j)}(0) = \\mathbf P^{(i)}\\mathbf P^{(j)}$  \n",
    "and at $\\tau = 1$  \n",
    "$\\mathbf P^{(i)}(1)\\mathbf P^{(j)}(1) = \\big(\\mathbf I - 2\\mathbf {zz}^T\\big)^2 = \\mathbf I$  \n",
    "\n",
    "hence any arbitrary pair of Householder matrices in dimension $n\\geq 3$ is path connected to the identity.  Using results from ex 2.misc.8 we can then infer that any matrix generated by even number $r$ Householder matrices is path connected to the identity. *And* we can infer that $r$ is even *iff* the determinant of an orthogonal matrix is 1-- which is to say we've proven that $\\text{SO}(n)$ is path connected (since every element is path connected to the identity and this is an equivalence relation).    \n",
    "\n",
    "Note: the above does not consider the $n=2$ case (and the $n=1$ case trivially holds true).  For 2 dimensions there is a simple and direct argument for path connectedness of $\\text{SO(n)}$ for the $\\text{2 x 2}$  case, in particular write out any matrix (e.g. revisiting Artin chapter 5 if needed) as   \n",
    "\n",
    "$\\mathbf Q_\\theta = \\begin{bmatrix} \\cos(\\theta) & -\\sin(\\theta) \\\\ \\sin(\\theta) & \\cos(\\theta) \\end{bmatrix} $   \n",
    "for $\\theta \\in [0,2\\pi)$  \n",
    "and directly consider the path given by any $\\mathbf Q_\\theta \\to \\mathbf I$ as $\\theta \\to 0$  \n",
    "\n",
    "**closing remark on uniqueness of real rotation matrix representation in 2 x 2 case**  \n",
    "consider that all matrices in SO(2) come in the form  \n",
    "$\\mathbf Q_\\theta = \\begin{bmatrix} \\cos(\\theta) & -\\sin(\\theta) \\\\ \\sin(\\theta) & \\cos(\\theta) \\end{bmatrix} $   \n",
    "\n",
    "by first supposing we fix the first row as \n",
    "\n",
    "$\\begin{bmatrix} \\mathbf v_1^T \\end{bmatrix} $   \n",
    "where $\\big\\Vert \\mathbf v_1\\big \\Vert_2 =1$  \n",
    "then there is exactly one linearly independent vector $\\mathbf v_2$ in the nullspace by rank-nullity.  \n",
    "\n",
    "In fact we have 2 unique homongenous solutions to $\\mathbf v_1^T\\mathbf x = 0$     \n",
    "when we insist on $\\big \\Vert \\mathbf x\\big \\Vert_2 =1$ \n",
    "i.e. our choice $\\mathbf v_2$ is unique except it may be rescaled by +1 or -1.  The selection of which scalar is then fixed when we require \n",
    "$\\det\\left(\\begin{bmatrix} \\mathbf v_1^T\\\\ \\mathbf v_2^T \\end{bmatrix}\\right) =1$   \n",
    "\n",
    "working backwards, we can be sure that \n",
    "$\\mathbf v_1^T =\\begin{bmatrix} \\cos(\\theta) & -\\sin(\\theta)\\end{bmatrix} $   \n",
    "since the sum of squares is one and the sign of each component enforces a quadrant in the unit circle and the modulus of sine (and cosine) are surjective and injective maps to $[0,1]$, once we've specified a quadrant.  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "section 7.4:   \n",
    "1) Sylvester's Law of Inertia for Complex case -- hit existence via spectral theorem, then uniqueness  \n",
    "do spectral theorem for existence, then rank, then uniqueness, with the form / matrix notation as needed  \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ex 7.4.4**  \n",
    "prove that if   \n",
    "$\\mathbf x^* \\mathbf A \\mathbf x \\in \\mathbb R$  for all $\\mathbf x \\in \\mathbb C^n\\longrightarrow \\mathbf A = \\mathbf A^*$  \n",
    "\n",
    "1.) write  \n",
    " $\\mathbf A = \\frac{1}{2}\\big(\\mathbf A + \\mathbf A^*\\big)+\\frac{1}{2}\\big(\\mathbf A - \\mathbf A^*\\big)= \\mathbf H + \\mathbf S$   \n",
    "i.e. decompose $\\mathbf A$ into a Hermitian and skew-Hermitian part  \n",
    "\n",
    "now we want to estimate the skew-Hermitian part.  \n",
    "\n",
    "so consider   \n",
    "$\\mathbf x^* \\mathbf A \\mathbf x \\in \\mathbb R \\longrightarrow \\mathbf x^* \\mathbf A \\mathbf x = \\big(\\mathbf x^* \\mathbf A \\mathbf x\\big)^* = \\mathbf x^* \\mathbf A^* \\mathbf x \\longrightarrow \\mathbf x^* \\mathbf A^* \\mathbf x -\\mathbf x^* \\mathbf A^* \\mathbf x = \\mathbf x^*\\big(\\mathbf A -\\mathbf A^*\\big) \\mathbf x = \\mathbf x^* \\big(2\\mathbf S\\big)\\mathbf x = 0$  \n",
    "\n",
    "This implies that any eigenvalue of the matrix $\\mathbf S$ must be zero.  We can finish by observing that skew-Hermitian matrices are diagonalizable since they are normal (normal matrices are developed in section 7 of this chapter), and hence $\\mathbf S$ is the zero matrix, or by considering the positive-definiteness of the (squared) Frobenius norm  \n",
    "$\\big \\Vert \\mathbf S\\big \\Vert_F^2 = \\text{trace}\\big(\\mathbf S^*\\mathbf S\\big) =  -1\\cdot\\text{trace}\\big(\\mathbf S^2\\big)= -1 \\cdot \\sum_{k=1}^n \\lambda_k^2 = -1 \\cdot \\sum_{k=1}^n 0 = 0\\longrightarrow \\mathbf S = \\mathbf 0$  \n",
    "\n",
    "since the skew Hermitian matrix is zero, we have  \n",
    "$\\mathbf A = \\frac{1}{2}\\big(\\mathbf A + \\mathbf A^*\\big)+0 = \\frac{1}{2}\\big(\\mathbf A + \\mathbf A^*\\big)$   \n",
    "so $\\mathbf A$ is Hermitian  \n",
    "\n",
    "\n",
    "*remark*  \n",
    "This also proves that for a matrix to be positive (semi)definite over a domain of $\\mathbf C^n $x $\\mathbf C^n$, it must be Hermitian (since having a real-nonegative quadratic form is a subset of having a real quadratic form)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7.4.16**  \n",
    "\n",
    "let $P$ be the (complex) vector space of polynomials with degree $\\leq n$  \n",
    "\n",
    "**(a)** show that  \n",
    "$\\langle f, g\\rangle = \\int_0^{2\\pi}\\bar{f(e^{i\\theta}})g(e^{i\\theta})d\\theta= \\int_0^{2\\pi}\\text{conj}\\big({f(e^{i\\theta}}\\big)g(e^{i\\theta})d\\theta$   \n",
    "is a positive definite hermitian form on $P$  \n",
    "(note the conjugation bar should be over all of $f(e^{i\\theta})$ though it doesn't seems to be rendering that way)  \n",
    "a positive definite hermitian form is equivalent to saying that we have an inner product in a complex space  \n",
    "\n",
    "so we confirm  \n",
    "(i) linearity in the second variable -- immediate  \n",
    "\n",
    "(ii) conjugate linearity under second variable --immediate  \n",
    "*note: this is the way Artin defines the inner product... the more common/standard definition is to linear in first argument and conjugate linear in the second argument*   \n",
    "\n",
    "(iii)  conjugate symmetry  \n",
    "${\\langle g, f\\rangle}=\\text{conj}\\big(\\langle f, g\\rangle\\big)$  \n",
    "$\\text{conj}\\big(\\langle f, g\\rangle\\big) = \\text{conj}\\big(\\int_0^{2\\pi}\\text{conj}\\big({f(e^{i\\theta}}\\big)g(e^{i\\theta})d\\theta\\big)= \\int_0^{2\\pi}\\text{conj}\\big({g(e^{i\\theta}}\\big)g(e^{i\\theta})d\\theta={\\langle g, f\\rangle}$  \n",
    "(writing this out as a limit of Riemann sums over real and imaginary parts can be helpful if needed)  \n",
    "\n",
    "(iv) we finally need to confirm this is positive definte / an actual inner product, i.e.  \n",
    "$\\langle g, g\\rangle \\geq 0$ with equality *iff* $g$ is the zero polynomial  \n",
    "\n",
    "first observe  \n",
    "$\\langle g, g\\rangle = \\int_0^{2\\pi}\\bar{g(e^{i\\theta}})g(e^{i\\theta})d\\theta= \\int_0^{2\\pi}\\big \\vert g(e^{i\\theta})\\big \\vert^2 d\\theta$  \n",
    "\n",
    "if $g$ is identically zero then the integral evaluates to zero.  Suppose $g$ is not identically zero, then it has at most $n$ distinct roots.  Using linearity, we have  \n",
    "\n",
    "$\\int_0^{2\\pi}\\big \\vert g(e^{i\\theta})\\big \\vert^2 d\\theta = \\sum_{k=0}^{n}\\int_{\\frac{k}{n+1}\\cdot 2\\pi}^{\\frac{k+1}{n+1}2\\pi}\\big \\vert g(e^{i\\theta})\\big \\vert^2 d\\theta$    \n",
    "\n",
    "where each of the n+1 intervals $[\\frac{k}{n+1}2\\pi,\\frac{k+1}{n+1}2\\pi]$  and being continuous (and real valued) $\\big \\vert g(e^{i\\theta})\\big \\vert^2$ has a minimum over each -- in all cases each the minimum is at least zero, and in at least one of them the minimum is strictly positive, thus the integral is strictly positive, as required.  \n",
    "**(b)**  \n",
    "find an orthonormal basis for this form.  We do this abstractly with the 'hyper-vector'  \n",
    "\n",
    "$\\mathbf B = \n",
    "\\bigg[\\begin{array}{c|c|c|c|c} \n",
    "\\mathbf b_0  & \\mathbf b_1 & \\mathbf b_2 & \\cdots & \\mathbf b_n \n",
    "\\end{array}\\bigg]=\\bigg[\\begin{array}{c|c|c|c|c} \n",
    "\\mathbf 1  & \\mathbf x & \\mathbf x^2 & \\cdots & \\mathbf x^n \n",
    "\\end{array}\\bigg]$  \n",
    "\n",
    "via applying Gram Schmidt we have  \n",
    "$\\mathbf q_0' = \\mathbf b_0\\longrightarrow \\mathbf q_0 = \\frac{1}{\\sqrt{2\\pi}}$  \n",
    "and for $k\\in\\{1,2,...,n\\}$  \n",
    "$\\mathbf q_k' = \\mathbf b_k - \\sum_{j=1}^{k-1}\\langle \\mathbf q_j, \\mathbf b_k \\rangle\\mathbf q_j$  \n",
    "(orthogonalization)  \n",
    "and  \n",
    "$\\mathbf q_k = \\alpha_k \\mathbf q_k' = \\frac{\\mathbf q_k'}{\\langle \\mathbf q_k', \\mathbf q_k' \\rangle^\\frac{1}{2}}=\\frac{\\mathbf q_k'}{\\big \\Vert \\mathbf q_k'\\big \\Vert_2}$  \n",
    "(normalization)  \n",
    "\n",
    "\n",
    "(i) this is an orthonormal sequence because if we select any $\\mathbf q_k$, then  \n",
    "direct checking of $\\mathbf q_0$ and $\\mathbf q_1$ tells us  \n",
    "$\\alpha_1 \\langle \\mathbf q_0,\\mathbf q_1\\rangle= \\langle \\mathbf q_0,\\mathbf q_1'\\rangle= \\langle \\mathbf q_0,\\mathbf b_1\\rangle- \\langle \\mathbf q_0, \\mathbf b_1 \\rangle\\cdot \\langle \\mathbf q_0,\\mathbf q_0\\rangle=\\langle \\mathbf q_0,\\mathbf b_1\\rangle- \\langle \\mathbf q_0, \\mathbf b_1\\rangle =0$  \n",
    "\n",
    "\n",
    "for $i \\neq k$  \n",
    "since conjugation doesn't change whether the result is zero or not we may confirm / assume WLOG that $i\\lt k$ due to conjugate symmetry of ${\\langle g, f\\rangle}=\\text{conj}\\big(\\langle f, g\\rangle\\big)$ \n",
    "we may also rescale the inner product by non-zero values -- this too doesn't change whether the inner product is zero   \n",
    "\n",
    "for $i=0, 1, 2,...., n-1$  \n",
    "   for $k = i+1,1,2,...,n$  \n",
    "$\\alpha_k \\langle \\mathbf q_i,\\mathbf q_k\\rangle= \\langle \\mathbf q_i,\\mathbf q_k'\\rangle= \\langle \\mathbf q_i,\\mathbf b_k\\rangle- \\sum_{j=1}^{k-1}\\langle \\mathbf q_j, \\mathbf b_k \\rangle\\cdot \\langle \\mathbf q_i,\\mathbf q_j\\rangle= \\langle \\mathbf q_i,\\mathbf b_k\\rangle- \\langle \\mathbf q_i, \\mathbf b_k \\rangle= 0$    \n",
    "noting that  \n",
    "$\\sum_{j=1}^{k-1}\\langle \\mathbf q_j, \\mathbf b_k \\rangle\\cdot \\langle \\mathbf q_i,\\mathbf q_j\\rangle = \\langle \\mathbf q_i, \\mathbf b_k \\rangle$  \n",
    "because $\\langle \\mathbf q_i,\\mathbf q_j\\rangle=1$ if $i=j$ and \n",
    "$\\langle \\mathbf q_i,\\mathbf q_j\\rangle=0$ if $i\\neq j$ by (in particular $i\\lt j$) induction hypothesis  \n",
    "\n",
    "if we revisit the equation  \n",
    "$\\mathbf q_k' = \\mathbf b_k - \\sum_{j=1}^{k-1}\\langle \\mathbf q_j, \\mathbf b_k \\rangle\\mathbf q_j $  \n",
    "$\\longrightarrow  \\mathbf b_k = \\mathbf q_k' + \\sum_{j=1}^{k-1}\\langle \\mathbf q_j, \\mathbf b_k \\rangle\\mathbf q_j$  \n",
    "or  \n",
    "$\\mathbf b_k = \\gamma_k \\cdot \\mathbf q_k + \\sum_{j=1}^{k-1}\\langle \\mathbf q_j, \\mathbf b_k \\rangle\\mathbf q_j$  \n",
    "and considering $\\langle \\mathbf q_k, \\mathbf b_k\\rangle $  we see   \n",
    "$\\langle \\mathbf q_k, \\mathbf b_k\\rangle  = \\gamma_k \\cdot 1 + \\sum_{j=1}^{k-1}\\langle \\mathbf q_j, \\mathbf b_k \\rangle\\cdot 0 \\longrightarrow \\gamma_k =\\langle \\mathbf q_k, \\mathbf b_k\\rangle$  \n",
    "thus  \n",
    "$\\mathbf b_k = \\langle \\mathbf q_k, \\mathbf b_k\\rangle \\cdot \\mathbf q_k + \\sum_{j=1}^{k-1}\\langle \\mathbf q_j, \\mathbf b_k \\rangle\\mathbf q_j =   r_{k,k}\\mathbf q_k + \\sum_{j=1}^{k-1} r_{j,k}\\mathbf q_j $     \n",
    "\n",
    "i.e.  \n",
    "$\\mathbf B = \n",
    "\\bigg[\\begin{array}{c|c|c|c|c} \n",
    "\\mathbf 1  & \\mathbf x & \\mathbf x^2 & \\cdots & \\mathbf x^n \n",
    "\\end{array}\\bigg] = \\bigg[\\begin{array}{c|c|c|c|c} \n",
    "\\mathbf q_0  & \\mathbf q_1 & \\mathbf q_2 & \\cdots & \\mathbf q_n \n",
    "\\end{array}\\bigg] \\begin{bmatrix}\n",
    "r_{0,0} & r_{0,1}& r_{0,2}&\\dots & r_{0,n} \\\\ \n",
    "0 & r_{1,1}& r_{1,2}&\\dots & r_{1,n}\\\\  \n",
    "0 & 0 & r_{2,2}&\\dots & r_{2,n} \\\\\n",
    "\\vdots & \\vdots & \\dots & \\ddots& \\vdots \\\\ \n",
    "0 & 0 & 0&\\dots & r_{n,n} \\\\ \n",
    "\\end{bmatrix}= \\mathbf {Q}R$  \n",
    "\n",
    "This *is* **QR factorization** and agrees with the procedure e.g. on page 242  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7.5.13**  \n",
    "Suppose $\\mathbf A$ is Hermitian. Prove there is some unitary $\\mathbf Q$ with determinant 1, such that $\\mathbf {QAQ}^* = \\mathbf D$  for diagonal matrix $\\mathbf D$   \n",
    "\n",
    "*proof:*  \n",
    "by spectral theorem we have  \n",
    "$\\mathbf {UAU}^* = \\mathbf D$  \n",
    "with unitary $\\mathbf U$  \n",
    "\n",
    "and we know $\\det\\big(\\mathbf U\\big) = \\gamma$  for some $\\gamma$ on the unit circle   \n",
    "\n",
    "now let $\\alpha^n = \\gamma$  \n",
    "(i.e. $\\alpha$ isn't unique per se -- there are $n$ distinct values on the unit circle that will suffice)   \n",
    "\n",
    "then consider  \n",
    "$\\mathbf Q :=\\bar{\\alpha}\\mathbf U$  \n",
    "\n",
    "so  \n",
    "$\\mathbf {QAQ}^* =\\big(\\bar{\\alpha}\\mathbf U\\big) \\mathbf A \\big(\\bar{\\alpha}\\mathbf U\\big)^* =\\bar{\\alpha} \\mathbf {UAU}^*\\alpha=\\bar{\\alpha}\\alpha\\cdot \\mathbf {UAU}^*= 1\\cdot\\mathbf {UAU}^* = \\mathbf D$  \n",
    "and  \n",
    "$\\det\\big(\\mathbf Q\\big) =\\det\\big(\\bar{\\alpha}\\mathbf I\\mathbf U\\big)=\\det\\big(\\bar{\\alpha}\\mathbf I\\big)\\det\\big(\\mathbf U\\big)=(\\bar{\\alpha})^n \\cdot \\gamma = \\bar{\\gamma}\\cdot \\gamma = 1$  \n",
    "as desired  \n",
    "\n",
    "alternatively, instead of taking nth roots, this may be proven by using elementary matrices of the 3rd type, with the top left element being equal to $\\gamma$ and then $\\mathbf Q:= \\mathbf U \\mathbf E$, so   \n",
    "$\\mathbf A = \\mathbf U\\mathbf D \\mathbf U^*=\\mathbf U\\mathbf I\\mathbf D \\mathbf U^*= \\mathbf U\\mathbf E\\mathbf E^*\\mathbf D \\mathbf U^*=\\mathbf U\\mathbf E\\mathbf D \\mathbf E^*\\mathbf U^*= \\big(\\mathbf U\\mathbf E\\big)\\mathbf D \\big(\\mathbf U\\mathbf E\\big)^* = \\mathbf Q \\mathbf D \\mathbf Q^*$   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7.8.4**  \n",
    "*claim:* The eigenvalues of a real skew matrix $\\mathbf A$  are purely imaginary   \n",
    "\n",
    "observe    \n",
    "$-\\mathbf A^2 =\\mathbf A^T \\mathbf A \\succeq 0$  \n",
    "and $\\mathbf A^T \\mathbf A$ only has real non-negative eigenvalues, which means $\\mathbf A^2$ only has real non-positive eigenvalues.  This means that $\\mathbf A$ necessarily has purely imaginary eigenvalues.  (For 2 other, different proofs, see the \"Schur's Inequality\" notebook.)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7.7.10**  \n",
    "Let $V$ be finite dim complex vector space with positive definite Hermitian form (i.e. inner product) $\\langle, \\rangle$  \n",
    "and let $T\\longrightarrow V$ be a linear operator on $V$.  Let $A$ be the matrix of $T$ with respect to the orthonormal basis $\\mathbf B$.  The *adjoint operator* $T^*$ is *defined* as the operator whose matrix with respect to the same basis is $A^*$.  \n",
    "\n",
    "**a.)** Prove that $T$ and $T^*$ are related by the equations $\\langle T\\mathbf v, \\mathbf w\\rangle=\\langle \\mathbf v,T^* \\mathbf w\\rangle$ and $\\langle \\mathbf v, T\\mathbf w\\rangle=\\langle T^*\\mathbf v, \\mathbf w\\rangle$ for all $\\mathbf v, \\mathbf w\\in V$.  Prove that the first of these equations characterizes $T^*$  \n",
    "\n",
    "**b.)**   \n",
    "prove that the definition of $T^*$ doesn't depend on choice of orthonormal basis  \n",
    "*remark:*  this is similar to the way the determinant was defined, by fixing a basis and then showing it is invariant to a change of basis. In this argument we fix an arbitrary orthonormal basis and show that the definition of $T^*$ is invariant to change in orthonormal basis.   \n",
    "\n",
    "\n",
    "*proof for a and b*  \n",
    "for orthonormal $\\mathbf B$  \n",
    "$\\mathbf v = \\mathbf B \\mathbf x$  \n",
    "$\\mathbf w = \\mathbf B \\mathbf y$  \n",
    "\n",
    "$\\langle T\\mathbf v, \\mathbf w\\rangle$  \n",
    "$=\\langle T\\mathbf B\\mathbf x, \\mathbf B\\mathbf y\\rangle$  \n",
    "$=\\langle \\mathbf B (A\\mathbf x), \\mathbf B\\mathbf y\\rangle$  \n",
    "$=\\langle \\mathbf B \\mathbf z, \\mathbf B\\mathbf y\\rangle$  \n",
    "$=\\langle \\sum_{k=1}^n  \\mathbf b_k z_k , \\sum_{i=1}^n  \\mathbf b_i y_i\\rangle$  \n",
    "$=\\sum_{k=1}^n \\bar{z_k}\\langle  \\mathbf b_k  , \\sum_{i=1}^n  \\mathbf b_i y_i\\rangle$  \n",
    "$=\\sum_{k=1}^n \\bar{z_k}\\langle  \\mathbf b_k  ,  \\mathbf b_k y_k\\rangle$  \n",
    "$=\\sum_{k=1}^n \\bar{z_k}y_k\\langle  \\mathbf b_k  ,  \\mathbf b_k \\rangle$  \n",
    "$=\\sum_{k=1}^n \\bar{z_k}y_k$  \n",
    "$=\\mathbf z^*\\mathbf y$  \n",
    "$=\\mathbf x^* \\mathbf A^*\\mathbf y$  \n",
    "$=\\mathbf x^* \\big(\\mathbf A^*\\mathbf y\\big)$  \n",
    "$=\\langle \\mathbf B\\mathbf x, \\mathbf B\\mathbf A^*\\mathbf y\\rangle$  \n",
    "$=\\langle \\mathbf B\\mathbf x, T^*\\mathbf B\\mathbf y\\rangle$  \n",
    "$=\\langle \\mathbf v, T^*\\mathbf w\\rangle$  \n",
    "\n",
    "to prove *b.)* note that we may select a different orthonormal basis \n",
    "$\\mathbf v = \\mathbf Q \\mathbf x'$  \n",
    "$\\mathbf w = \\mathbf Q \\mathbf y'$  \n",
    "and rerun the above argument verbatim to get the same result.\n",
    "\n",
    "As a **not fullyy developed alternative approach**  since B and Q are both (orthonormal) bases there is some linear mapping between $\\mathbf Q$ and $\\mathbf B$.  In particular  \n",
    "$\\mathbf B = \\mathbf Q\\mathbf U$  \n",
    "and since the LHS is orthonormal, $\\mathbf U$ is as well this implies $\\mathbf U$ is unitary  \n",
    "\n",
    "$\\mathbf v = \\mathbf Q \\mathbf x'= \\mathbf Q\\mathbf U\\mathbf U^{-1} \\mathbf x' =\\mathbf B \\mathbf U^{-1}\\mathbf x'=\\mathbf B \\mathbf x$  \n",
    "\n",
    "note that for any $\\mathbf v \\in V$  \n",
    "\n",
    "so $\\mathbf U^{-1}\\mathbf x' =\\mathbf x\\longrightarrow \\mathbf x' =\\mathbf U\\mathbf x$  \n",
    "and the same applies for relating $\\mathbf y'$ and $\\mathbf y$  \n",
    "$\\langle \\mathbf v, \\mathbf v\\rangle = \\big\\Vert \\mathbf x\\big\\Vert_2^2 = \\langle \\mathbf B\\mathbf x, \\mathbf B\\mathbf x\\rangle =\\langle \\mathbf Q\\mathbf x', \\mathbf Q\\mathbf x'\\rangle=\\langle \\mathbf Q \\mathbf U\\mathbf x, \\mathbf Q\\mathbf U\\mathbf x\\rangle = \\big \\Vert \\mathbf U \\mathbf x\\big\\Vert_2^2$  \n",
    "but the choice of $\\mathbf x$ was arbitrary so $\\mathbf U$ must be unitary, e.g. via application of Cauchy- Schwarz, we have  \n",
    "\n",
    "$\\Big \\Vert \\mathbf x\\big \\Vert_2^2 = \\langle \\mathbf Q \\mathbf U\\mathbf x, \\mathbf Q\\mathbf U\\mathbf x\\rangle = \\langle  \\mathbf U\\mathbf x, \\mathbf U\\mathbf x\\rangle = \\langle  \\mathbf x, \\mathbf U^*\\mathbf U\\mathbf x\\rangle\\leq \\Big \\Vert \\mathbf x\\big \\Vert_2\\Big \\Vert \\mathbf U^*\\mathbf U \\mathbf x\\big \\Vert_2 \\leq \\Big \\Vert \\mathbf x\\big \\Vert_2^2$    \n",
    "and this is met with equality, so $\\mathbf U^*\\mathbf U = \\alpha\\mathbf I$  for some $\\alpha$ on the unit circle, and $\\mathbf U^*\\mathbf U$ necessarily has non-negative numbers on its diagonal so $\\alpha = 1$.  \n",
    "\n",
    "note: the 2nd inequality needs justification -- this too follow from Cauchy Schwarz  \n",
    "$\\Big \\Vert \\mathbf U^*\\mathbf x\\Big \\Vert_2^2 = \\langle \\mathbf U^* \\mathbf x,\\mathbf U^*\\mathbf x\\rangle= \\langle\\mathbf x,\\mathbf U\\mathbf U^*\\mathbf x\\rangle \\leq \\Big \\Vert \\mathbf x\\Big\\Vert_2\\cdot \\Big \\Vert \\mathbf U\\mathbf U^*\\mathbf x\\Big\\Vert_2=\\Big \\Vert \\mathbf x\\Big\\Vert_2\\cdot \\Big \\Vert \\mathbf U^*\\mathbf x\\Big\\Vert_2\\longrightarrow \\Big \\Vert \\mathbf U^*\\mathbf x\\Big \\Vert_2 \\leq \\Big \\Vert \\mathbf x\\Big\\Vert_2$   \n",
    "\n",
    "then  \n",
    "$\\langle T\\mathbf v, \\mathbf w\\rangle$  \n",
    "$=\\langle T\\mathbf Q\\mathbf x', \\mathbf Q\\mathbf y'\\rangle$  \n",
    "$=\\big(\\mathbf x'\\big)^* \\mathbf A'\\big(\\mathbf y'\\big)$  \n",
    "$=\\big(\\mathbf U\\mathbf x\\big)^* \\mathbf A'\\big(\\mathbf U\\mathbf y\\big)$  \n",
    "$=\\mathbf x^* \\mathbf U^*\\mathbf A'\\mathbf U\\mathbf y$  \n",
    "$=\\mathbf x^*\\big( \\mathbf U^{-1}\\mathbf A'\\mathbf U\\big)\\mathbf y$  \n",
    "$=\\mathbf x^* \\mathbf A\\mathbf y$  \n",
    "$=\\langle \\mathbf v, T^*\\mathbf w\\rangle$  \n",
    "\n",
    "so an orthonormal change of of basis preserves the positive definite Hermitian product and thus the orthonormal selection of basis used in coming up with a matrix whose conjugate transpose defines $T^*$ does not matter.  \n",
    "\n",
    "\n",
    "\n",
    "**c.)**  \n",
    "let $\\mathbf v$ be an eigenvector for $T$ with eigenvalue $\\lambda$ and $W$ be the subspace orthogonal to $\\mathbf v$.  prove that $W$ is $T^*$ invariant.  For $\\mathbf w\\in W$  \n",
    "\n",
    "$\\lambda\\cdot \\langle \\mathbf v, \\mathbf w\\rangle = \\langle T\\mathbf v, \\mathbf w\\rangle  = \\langle \\mathbf v, T^*\\mathbf w\\rangle $  \n",
    "\n",
    "and the definition of $T^*$ invariant, adapted from pages 116-117 would be $T^*W \\subset W$ i.e. $T(\\mathbf w)\\in W$ for all $\\mathbf w \\in W$  \n",
    "\n",
    "and $W$ is being the subspace consisting of all $\\mathbf w$ such that\n",
    "$\\langle \\mathbf v, \\mathbf w\\rangle = 0$  \n",
    "\n",
    "and \n",
    "$0 =\\lambda \\cdot 0 = \\lambda \\cdot \\langle \\mathbf v, \\mathbf w\\rangle = \\langle T\\mathbf v, \\mathbf w\\rangle =\\langle \\mathbf v, T^*\\mathbf w\\rangle$  \n",
    "so $T^*\\mathbf w\\subset W$ \n",
    "i.e. $T^*$ does not alter the fact that any $\\mathbf w \\in W$ is orthogonal to $\\mathbf v$  \n",
    "\n",
    "\n",
    "**11**  \n",
    "Prove that for any linear operator $T$, $TT^*$ is Hermitian  \n",
    "**tbc**   \n",
    "the definition of Hermitian linear operator is given on page 253 as \n",
    "$ \\langle T'\\mathbf v, \\mathbf w\\rangle = \\langle \\mathbf v, T'\\mathbf w\\rangle $  for all $\\mathbf v, \\mathbf w\\in V$  \n",
    "\n",
    "selecting $T' = TT^*$ and using our identity from the prior exercise, we have  \n",
    " $\\langle TT*\\mathbf v, \\mathbf w\\rangle = \\langle \\mathbf v, (TT*)^*\\mathbf w\\rangle = \\langle \\mathbf v, TT*\\mathbf w\\rangle$  \n",
    " so the operator is Hermitian  \n",
    "\n",
    "**12**   \n",
    "**pending**  \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7.7.12**  \n",
    "\n",
    "a normal operator $T:V\\longrightarrow V$ is defined as normal *iff* $TT^* = T^*T$  \n",
    "\n",
    "(a)  \n",
    "(b) assume $T$ is normal with eigenvector $\\mathbf v$.  Prove $\\mathbf v$ is an eigenvector for $T^*$ and compute the eigenvalue  \n",
    "\n",
    "\n",
    "$\\big\\Vert T^*\\mathbf v\\big\\Vert_2^2 = \\langle T^*\\mathbf v, T^*\\mathbf v\\rangle=  \\langle \\mathbf v, TT^*\\mathbf v\\rangle\\leq \\big\\Vert \\mathbf v\\big\\Vert_2 \\big\\Vert TT^*\\mathbf v\\big\\Vert_2=\\big\\Vert \\mathbf v\\big\\Vert_2 \\big\\Vert T^*T\\mathbf v\\big\\Vert_2= \\vert\\lambda\\vert\\big\\Vert \\mathbf v\\big\\Vert_2 \\big\\Vert T^*\\mathbf v\\big\\Vert_2 $  \n",
    "\n",
    "By Cauchy-Schwarz, with equality *iff* $ T^*\\mathbf v \\propto  \\mathbf v$  \n",
    "but the upper bound is met with equality because   \n",
    "$\\big\\Vert T^*\\mathbf v\\big\\Vert_2 = \\langle T^*\\mathbf v, T^*\\mathbf v\\rangle^\\frac{1}{2}= \\langle T\\mathbf v, T\\mathbf v\\rangle^\\frac{1}{2}=\\big\\Vert T\\mathbf v\\big\\Vert_2 = \\vert \\lambda\\vert \\big\\Vert \\mathbf v\\big\\Vert_2  $  \n",
    "hence $\\mathbf v$ is an eigenvector for $T^*$ with eigenvalue $\\gamma$  \n",
    "pluggin this into our original expression we have  \n",
    "\n",
    "$\\vert \\lambda\\vert^2 \\big\\Vert \\mathbf v\\big\\Vert_2^2 =  \\langle T\\mathbf v, T\\mathbf v\\rangle=  \\lambda\\cdot\\langle T\\mathbf v, \\mathbf v\\rangle=  \\lambda\\cdot\\langle \\mathbf v, T^*\\mathbf v\\rangle=\\lambda\\cdot\\gamma \\cdot \\langle \\mathbf v, \\mathbf v\\rangle$  \n",
    "thus  \n",
    "$\\gamma = \\bar{\\lambda}$  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7.8.5**\n",
    "*Cayley Transform*   \n",
    "Let $\\mathbf S$ be real skew symmetric  \n",
    "*i.)* prove $\\big(\\mathbf I + \\mathbf S\\big)$ is invertible  \n",
    "$\\big(\\mathbf I + \\mathbf S\\big)^T\\big(\\mathbf I + \\mathbf S\\big) = \\big(\\mathbf I + \\mathbf S^T\\big)\\big(\\mathbf I + \\mathbf S\\big)= \\mathbf I + \\mathbf S + \\mathbf S^T +\\mathbf S^T\\mathbf S = \\mathbf I + \\mathbf S - \\mathbf S +\\mathbf S^T\\mathbf S = \\mathbf I +\\mathbf S^T\\mathbf S$  \n",
    "and the addition of a positive definite matrix (\\mathbf I) plus a positive semi-definite matrix is positive definite and hence non-singular.  Thus $\\big(\\mathbf I + \\mathbf S\\big)$ is nonsingular.  \n",
    "\n",
    "*note:*    \n",
    "$\\big(\\mathbf I - \\mathbf S\\big)= \\big(\\mathbf I  +\\mathbf S^T\\big) =\\big(\\mathbf I + \\mathbf S\\big)^T  $   \n",
    "so the above argument *also* proves that $\\big(\\mathbf I - \\mathbf S\\big)$ is non-singular  \n",
    "\n",
    "\n",
    "*ii.)* prove $\\mathbf Q:=\\big(\\mathbf I - \\mathbf S\\big)\\big(\\mathbf I + \\mathbf S\\big)$ is orthogonal  \n",
    "$\\Big(\\big(\\mathbf I - \\mathbf S\\big)\\big(\\mathbf I + \\mathbf S\\big)^{-1}\\Big)^T\\big(\\mathbf I - \\mathbf S\\big)\\big(\\mathbf I + \\mathbf S\\big)^{-1}$  \n",
    "$=\\big(\\mathbf I + \\mathbf S\\big)^{-T}\\big(\\mathbf I - \\mathbf S^T\\big)\\big(\\mathbf I - \\mathbf S\\big)\\big(\\mathbf I + \\mathbf S\\big)^{-1}$   \n",
    "$=\\big(\\mathbf I + \\mathbf S\\big)^{-T}\\big(\\mathbf I + \\mathbf S\\big)\\big(\\mathbf I - \\mathbf S\\big)\\big(\\mathbf I + \\mathbf S\\big)^{-1}$   \n",
    "$=\\big(\\mathbf I + \\mathbf S\\big)^{-T}\\big(\\mathbf I - \\mathbf S\\big)\\big(\\mathbf I + \\mathbf S\\big)\\big(\\mathbf I + \\mathbf S\\big)^{-1}$   \n",
    "$=\\big(\\mathbf I + \\mathbf S\\big)^{-T}\\big(\\mathbf I - \\mathbf S\\big)$   \n",
    "$=\\mathbf I$  \n",
    "\n",
    "to confirm the last step, it suffices to check the inverse:  \n",
    "\n",
    "$\\Big(\\big(\\mathbf I + \\mathbf S\\big)^{-T}\\big(\\mathbf I - \\mathbf S\\big)\\Big)^{-1}$   \n",
    "$=\\big(\\mathbf I - \\mathbf S\\big)^{-1}\\big(\\mathbf I + \\mathbf S\\big)^{T}$   \n",
    "$=\\big(\\mathbf I - \\mathbf S\\big)^{-1}\\big(\\mathbf I + \\mathbf S^T\\big)$   \n",
    "$=\\big(\\mathbf I - \\mathbf S\\big)^{-1}\\big(\\mathbf I - \\mathbf S\\big)$   \n",
    "$=\\mathbf I$  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7.8.6**\n",
    "Let $\\mathbf A$ be a real skew symmetric matrix  \n",
    "(a) prove $\\det\\big(\\mathbf A\\big)\\geq 0$ \n",
    "*a common approach invokes the fact that $\\mathbf A$ has purely imaginary eigenvalues which come in conjugate pairs if non-zero*  \n",
    "an alternative approach uses the fact that (8.5 on page 261 -- the proof of which was left as an exercise... see ex 7.8.7 below to derive this congruence)  \n",
    "\n",
    "if $\\mathbf A$ is singular then $\\det\\big(\\mathbf A\\big) =0$.  Supposing $\\mathbf A$ is nonsingular, we have  \n",
    "\n",
    "via congruence transforms we have  \n",
    "$\\mathbf M^T\\mathbf A\\mathbf M = \\begin{bmatrix}\\mathbf 0 & \\mathbf I_r \\\\ -\\mathbf I_r & \\mathbf 0\\end{bmatrix}$  \n",
    "and  \n",
    "$\\det\\Big(\\begin{bmatrix}\\mathbf 0 & \\mathbf I_\\frac{r}{2} \\\\ -\\mathbf I_\\frac{r}{2} & \\mathbf 0\\end{bmatrix}\\Big)  =\\det\\Big(\\begin{bmatrix}\\mathbf I_\\frac{r}{2} & \\mathbf 0 \\\\ \\mathbf 0 & -\\mathbf I_\\frac{r}{2}\\end{bmatrix}\\begin{bmatrix}\\mathbf 0 & \\mathbf I_\\frac{r}{2} \\\\ \\mathbf I_\\frac{r}{2} & \\mathbf 0\\end{bmatrix}\\Big)=\\det\\Big(\\begin{bmatrix}\\mathbf I_\\frac{r}{2} & \\mathbf 0 \\\\ \\mathbf 0 & -\\mathbf I_\\frac{r}{2}\\end{bmatrix}\\big)\\det\\big(\\begin{bmatrix}\\mathbf 0 & \\mathbf I_\\frac{r}{2} \\\\ \\mathbf I_\\frac{r}{2} & \\mathbf 0\\end{bmatrix}\\Big) =(-1)^\\frac{r}{2} \\cdot (-1)^\\frac{r}{2} = (-1)^{r}=  1$  \n",
    "\n",
    "where $r$ is even  \n",
    "\n",
    "The diagonal matrix determinant is immediate and second matrix is permutation and symmetric... referencing the discussion of Householder matrices we can observe it has the same number of +1 and -1 eigenvalues -- i.e. $\\frac{r}{2}$ of them so the determinant of that too is $(-1)^\\frac{r}{2}$. An alternative approach is to recognize that up to graph isomorphism, i.e. permutation matrix similarity, we have the standard sympletic basis being given by a block diagonal matrix with each block consisting of     \n",
    "$\\begin{bmatrix} 0 & 1 \\\\ -1 & 0\\end{bmatrix}$ and *this* has a determinant of 1.  Earlier work (chapter 2) on block triangular matrices then implies the determinant is 1.    \n",
    "\n",
    "so  \n",
    "$1 =\\det\\big(\\mathbf M^T\\mathbf A\\mathbf M \\big)=\\det\\big(\\mathbf M^T\\big)\\det\\big(\\mathbf A\\big)\\det\\big(\\mathbf M \\big)=\\det\\big(\\mathbf A\\big)\\det\\big(\\mathbf M \\big)^2$ and being real \n",
    "$\\det\\big(\\mathbf M \\big)^2\\gt 0\\longrightarrow \\det\\big(\\mathbf A\\big) \\gt 0$ in the nonsingular case \n",
    "\n",
    "(b) This is a corollary to the above work. Suppose $\\mathbf A$ has integer entries.  Prove its determinant is an integer, squared.  If $\\mathbf A$ is singular, the result is immediate.  Suppose it is non-singular, then our prior work tells us:  \n",
    "$\\det\\big(\\mathbf A\\big) = \\det\\big(\\mathbf M^{-1} \\big)^2\\cdot 1= \\det\\big(\\mathbf M^{-1} \\big)^2$  \n",
    "The LHS is an integer because $\\mathbf A$ has integer entries  \n",
    "This implies  $\\det\\big(\\mathbf M^{-1} \\big) \\in \\mathbb N$  \n",
    "i.e. we may change our the field here to be $\\mathbb Q$.  The process follows identically and we still have this decomposition.  Now  \n",
    "if \n",
    "$\\det\\big(\\mathbf M^{-1} \\big) = \\alpha$ and $\\alpha \\not \\in \\mathbb N$, then $\\alpha$ is a fraction with some prime in the denominator that is not in the numerator (Fundamental Theorem of Arithmetic -- see appendix). Then this implies $\\det\\big(\\mathbf A\\big) =\\alpha^2 \\not \\in \\mathbb N$ which is a contradiction.  \n",
    "   \n",
    "The positive integer given by $\\det\\big(A\\big)^\\frac{1}{2}$ is called the Pfaffian.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7.8.7**  \n",
    "for a skew-symmetric form defined as  \n",
    "$V \\text{ x } V\\longrightarrow \\mathbb F$  \n",
    "$\\langle \\mathbf v, \\mathbf v \\rangle = 0$   \n",
    "for all $\\mathbf v \\in V$  \n",
    "\n",
    "we define *orthogonal* with respect to skew symmetric bilinear forms to mean $\\langle \\mathbf v, \\mathbf x \\rangle = 0$.  This implies all vectors are self orthogonal in a skew symmetric form (in some ways the 'opposite' of when we have a pos definite symmetric form -- standard inner products).  \n",
    "\n",
    "we define null space to be the set of $\\mathbf v' \\in V$ such that \n",
    "$\\langle \\mathbf v, \\mathbf v' \\rangle = 0$  for all $v \\in V$.  \n",
    "i.e. $\\mathbf v'$ are *orthogonal to everything*  \n",
    "once we introduce bases and have a skew symmetric matrix $\\mathbf A$ to effect our bilinear form, these will be vectors in the nullspace of $\\mathbf A$   \n",
    "\n",
    "a vector space with a skew symmetric bilinear form may be bipartitioned into a portion that is degenerate and a portion that is non-degenerate. Degenerate subspaces include the zero vector and all all $\\mathbf v'$ in the nullspace.  The non-degenerate form / associated subspace includes the zero vector and for all  $\\mathbf 0 \\neq \\mathbf v' \\in V$ there exists some $v \\in V$ such that $\\langle \\mathbf v, \\mathbf v' \\rangle \\neq 0$.  Note that  $\\mathbf v' \\neq \\alpha \\mathbf v$ (for some scalar $\\alpha \\in \\mathbb F$) because if it did, \n",
    "$\\langle \\mathbf v, \\mathbf v' \\rangle=\\langle \\alpha \\mathbf v', \\mathbf v' \\rangle=\\alpha \\langle \\mathbf v', \\mathbf v' \\rangle = 0$ since all vectors are self-orthogonal in a skew symmetric form.   \n",
    "\n",
    "**a)**  \n",
    "a form is nondegenerate *iff* the matrix with respect to any basis is invertible.  It's for n x n $\\mathbf A$ the result is immediate-- if $\\text{rank}\\big(A\\big) = n$, this is preserved under change of basis (congruence transforms), so for non-zero $\\mathbf x$ , in particular with coordinate k being nonzero, we have  \n",
    "$\\mathbf x^T \\mathbf A \\mathbf y\\neq 0$ for some $\\mathbf y$, in particular with $\\mathbf y:= \\mathbf A^{-1}\\mathbf e_k$ the bilinear form evaluates to $x_k \\neq 0$.  On the other handd, if $\\text{rank}\\big(A\\big) \\leq n-1$, this too is preserved under congruence transforms and implies there is some $\\mathbf y \\neq \\mathbf 0$ such that $\\mathbf {Ay} =\\mathbf 0$ and hence $\\mathbf x^T \\mathbf 0 = \\mathbf x^T\\mathbf {Ay} = 0$ for all $\\mathbf x$ hence the form is degenerate.  \n",
    "\n",
    "**b)**  \n",
    "if $W$ is the subspace containing all $v\\in V$ such that the form is non-degenerate, then $V = W \\otimes W^\\perp$ i) its immediate that $W\\cap W^\\perp = \\{0\\}$ since if $\\mathbf w \\neq \\mathbf 0$ and it is an element of $W\\cap W^\\perp$ then because $\\mathbf w \\in W$ there is some $\\mathbf v$ such that $\\langle \\mathbf w, \\mathbf v\\rangle \\neq 0$ but because $\\mathbf \\in W^\\perp$, then $\\langle \\mathbf w, \\mathbf v\\rangle \\neq 0$ for all $\\mathbf v \\in V$ (this is our definition of degenerate).  Thus it can only be the case that the zero vector is in both.  To show that this direct sum spans all of $V$ requires a little more care than was given in the book on top of page 244... the span really comes from the construction of $W$ and $W^\\perp$.  We place the zero vector in each.  Now for any non-zero $\\mathbf v \\in V$ if it is degenerate with respect to the skew symmetric form then we assign it to $\\mathbf W^\\perp$ and if not we assign it to $W$.  \n",
    "\n",
    "**c,d)**  \n",
    "if the form is identically zero then all $\\mathbf v\\in V$ are in the nullspace.  If the form is not identically zero, then not all $\\mathbf v \\in V$ are in the nullspace.  So there is some $\\mathbf w_1$ and some $\\mathbf w_2$ such that  $\\langle \\mathbf w_1, \\mathbf w_2 \\rangle = \\gamma \\neq 0$.  I.e. this implies the existence of 2 linearly independent vectors (reference the above $\\mathbf v' \\neq \\alpha \\mathbf v$).  \n",
    "\n",
    "we may now consider $\\mathbf w_1' = \\mathbf w_1$ and $\\mathbf w_2' = \\gamma^{-1}\\mathbf w_2$ so we have  \n",
    "$\\langle \\mathbf w_1', \\mathbf w_2' \\rangle=1$ and of course, being skew symmetric, $\\langle \\mathbf w_2', \\mathbf w_1' \\rangle=-1$ \n",
    "\n",
    "this forms a non-degenerate subspace of $\\alpha_1\\mathbf w_1' + \\alpha_2\\mathbf w_2'$  \n",
    "\n",
    "let $W$ be the subspace of $V$ that has all non-degenerate vectors and $W^\\perp$ be the subspace of degenerate vectors.  $\\mathbf w_1, \\mathbf w_2\\in W$ and if $\\dim W = 2$ then we are done.  Suppose $\\dim W = r \\gt 2$.  Then this implies the existence of some non-degenerate $\\mathbf w_3$ that is linearly independent of $\\mathbf w_1', \\mathbf w_2'$.  So looking at our hyper vector we have  \n",
    "\n",
    "\n",
    "$\\mathbf B = \n",
    "\\bigg[\\begin{array}{c|c|c|c||c|c|c|c} \n",
    "\\mathbf w_1  & \\mathbf w_2 & \\mathbf w_{3} & \\cdots&\\mathbf w_r &\\mathbf w^\\perp_1 & ...&  \\mathbf w^\\perp_{m} \n",
    "\\end{array}\\bigg]$  \n",
    "where by construction these vectors are linearly independent (and hence generate $V$)  \n",
    "\n",
    "now consider   \n",
    "$\\mathbf w_{3}^* := \\mathbf w_3 - \\alpha_1\\mathbf w_1+\\alpha_2\\mathbf w_2 + \\sum_{k=1}^m \\alpha_k^\\perp\\mathbf w^\\perp_k$\n",
    "and observe it is still linearly independent i.e.  \n",
    "\n",
    "$\\bigg[\\begin{array}{c|c|c|c||c|c|c|c} \n",
    "\\mathbf w_1  & \\mathbf w_2 & \\mathbf w_{3} & \\cdots&\\mathbf w_r &\\mathbf w^\\perp_1 & ...&  \\mathbf w^\\perp_{m} \n",
    "\\end{array}\\bigg]A= \\bigg[\\begin{array}{c|c|c|c||c|c|c|c} \\mathbf w_1  & \\mathbf w_2 & \\mathbf w_{3}^* & \\cdots&\\mathbf w_r &\\mathbf w^\\perp_1 & ...&  \\mathbf w^\\perp_{m} \n",
    "\\end{array}\\bigg]$  \n",
    "with invertible $A$ (which is the identity matrix except column 3 is allowed to have off diagonal entries hence $\\det\\big(A) = 1$)  \n",
    "\n",
    "\n",
    "we now in effect run through 'QR factorization' but using 'skewed Gram Schmidt'  \n",
    "first 'normalize' our vectors   \n",
    "$\\langle \\mathbf w_3, \\mathbf w_1' \\rangle = \\alpha_1$  \n",
    "$\\langle \\mathbf w_3, \\mathbf w_2' \\rangle = \\alpha_2$  \n",
    "\n",
    "Then set  \n",
    "$\\mathbf w_3^* := \\mathbf w_3 -\\alpha_2\\mathbf w_1' + \\alpha_1 \\mathbf w_2'$  \n",
    "which gives  \n",
    "$\\langle \\mathbf w_3^*, \\mathbf w_1' \\rangle = \\langle \\mathbf w_3, \\mathbf w_1' \\rangle - \\alpha_2\\langle \\mathbf w_1', \\mathbf w_1' \\rangle - \\alpha_1\\langle \\mathbf w_2', \\mathbf w_1' \\rangle = \\alpha_1 + 0 + \\alpha_1\\cdot (-1) = 0$  \n",
    "and \n",
    "$\\langle \\mathbf w_3^*, \\mathbf w_2' \\rangle = \\langle \\mathbf w_3, \\mathbf w_2' \\rangle - \\alpha_2\\langle \\mathbf w_1', \\mathbf w_2' \\rangle - \\alpha_1\\langle \\mathbf w_2', \\mathbf w_2' \\rangle = \\alpha_2 - \\alpha_2 \\cdot (+1)  + 0 = 0$  \n",
    "\n",
    "however $W$ is a subspace of non-degenerate vectors (and $\\mathbf w_3^*$ is linearly independent of $\\mathbf w_1$ and $\\mathbf w_2$ so it is non-zero), thus this implies the existence of some 4th linearly independent vector \n",
    "$\\mathbf w_4 \\in W$ such that $\\langle \\mathbf w_3^*, \\mathbf w_4 \\rangle =\\gamma \\neq 0$.  We repeat the above process to create $\\mathbf w_4^*$ so $\\langle \\mathbf w_4^*, \\mathbf w_1' \\rangle = 0 = \\langle \\mathbf w_4^*, \\mathbf w_2'\\rangle$.  But using bilinearity we can observe that  $\\langle \\mathbf w_3^*, \\mathbf w_4 \\rangle= \\langle \\mathbf w_3^*, \\mathbf w_4^* \\rangle =\\gamma \\neq 0$.  Now we do the 'normalization' so  \n",
    "$\\mathbf w_3' := \\mathbf w_3^*$  and $\\mathbf w_4' := \\gamma^{-1}\\mathbf w_4^*$  \n",
    "\n",
    "if $\\dim W=r =4$ we are done.  If $d\\gt 4$ repeat the process above.  And continue repeating until we have a 'skewed orthogonal set' that spans $W$. Notice at each stage there necessarily are 2 new vectors introduced $\\big\\{\\mathbf w_j', \\mathbf w_{j+1}'\\big\\}$ for $j$ odd (and since d is finite this process stops after finitely many iterations).  This tells us $d\\text{ % } 2 = 0$ i.e. $d$ is even.  \n",
    "\n",
    "We thus have  \n",
    "\n",
    "$\\mathbf BR' = \n",
    "\\bigg[\\begin{array}{c|c|c|c||c|c|c|c} \n",
    "\\mathbf w_1  & \\mathbf w_2 & \\mathbf w_{3} & \\cdots&\\mathbf w_r &\\mathbf w^\\perp_1 & ...&  \\mathbf w^\\perp_{m} \n",
    "\\end{array}\\bigg]R' = \n",
    "\\bigg[\\begin{array}{c|c|c|c||c|c|c|c} \n",
    "\\mathbf w_1'  & \\mathbf w_2' & \\mathbf w_{3}' & \\cdots&\\mathbf w_r' &\\mathbf w^\\perp_1 & ...&  \\mathbf w^\\perp_{m} \n",
    "\\end{array}\\bigg]$  \n",
    "\n",
    "where \n",
    "$R' =\\begin{bmatrix}R_0 & \\mathbf 0 \\\\ \\mathbf 0 & \\mathbf I_m \\end{bmatrix}$   \n",
    "where $R_0$ is upper triangular with no zeros on the diagonal (and indeed we know $r_{j,j}=1$ for off j)  \n",
    "\n",
    "or with  \n",
    "$R:= (R')^{-1}$  \n",
    "\n",
    "$B = \\bigg[\\begin{array}{c|c|c|c||c|c|c|c} \n",
    "\\mathbf w_1  & \\mathbf w_2 & \\mathbf w_{3} & \\cdots&\\mathbf w_r &\\mathbf w^\\perp_1 & ...&  \\mathbf w^\\perp_{m} \n",
    "\\end{array}\\bigg] = \n",
    "\\bigg[\\begin{array}{c|c|c|c||c|c|c|c} \n",
    "\\mathbf w_1'  & \\mathbf w_2' & \\mathbf w_{3}' & \\cdots&\\mathbf w_r' \n",
    "&\\mathbf w^\\perp_1 & ...&  \\mathbf w^\\perp_{m} \n",
    "\\end{array}\\bigg]R$   \n",
    "which emphasizes this / similarity with QR factorization  \n",
    "\n",
    "where for $j\\in\\{1,3,5,..., r-1\\}$  \n",
    "(recalling again that r must be even)   \n",
    "$\\langle \\mathbf w_j', \\mathbf w_{k}' \\rangle = 0$ for $k\\neq j+1$  and  \n",
    "$\\langle \\mathbf w_j', \\mathbf w_{j+1}' \\rangle = 1$  \n",
    "(skew symmetry then implies the result for even j)  \n",
    "\n",
    "\n",
    "*to close out loose ends*  \n",
    "using the first form, if we have \n",
    "$\\mathbf BR' =\n",
    "\\bigg[\\begin{array}{c|c|c|c||c|c|c|c} \n",
    "\\mathbf w_1'  & \\mathbf w_2' & \\mathbf w_{3}' & \\cdots&\\mathbf w_r' &\\mathbf w^\\perp_1 & ...&  \\mathbf w^\\perp_{m} \n",
    "\\end{array}\\bigg]$  \n",
    "as our 'orthonormal' basis, then  (referencing page 239)  \n",
    "\n",
    "with \n",
    "$\\mathbf v = \\big(\\mathbf BR'\\big)\\mathbf x$ and  \n",
    "$\\mathbf z = \\big(\\mathbf BR'\\big)\\mathbf y$  \n",
    "\n",
    "$\\langle \\mathbf v, \\mathbf z\\rangle$  \n",
    "$= \\langle \\big(\\sum_{i=1}^r x_i\\mathbf w_i'\\big)+\\big(\\sum_{i=1}^m x_{n+i}\\mathbf w_i^\\perp\\big), \\big(\\big(\\sum_{k=1}^r y_i\\mathbf w_k'\\big)+\\big(\\sum_{k=1}^m y_{n+k}\\mathbf w_k^\\perp\\big)\\rangle$  \n",
    "$=\\langle \\sum_{i=1}^r x_i\\mathbf w_i', \\sum_{k=1}^r y_k\\mathbf w_k'\\rangle$  \n",
    "$=\\sum_{i=1}^r x_i\\langle \\mathbf w_i', \\sum_{k=1}^r y_k\\mathbf w_k'\\rangle$  \n",
    "$=\\sum_{i=1}^r\\sum_{k=1}^r y_k x_i\\langle \\mathbf w_i', \\mathbf w_k'\\rangle$  \n",
    "$=\\sum_{i=1}^r\\sum_{k=1}^r y_k x_i \\mathbf A_\\text{non-degenerate}$  \n",
    "\n",
    "with, for $i,k \\in\\{1,2,...,r\\}$\n",
    "$a_{i,k} = \\langle \\mathbf w_i', \\mathbf w_k'\\rangle$ and  \n",
    "$a_{i,k} :=0$ otherwise  \n",
    "\n",
    "Thus we have  \n",
    "$\\mathbf C := \\begin{bmatrix} 0 & 1 \\\\ -1 & 0\\end{bmatrix}$  \n",
    "and  \n",
    "\n",
    "$\\mathbf A :=\\begin{bmatrix}\\mathbf A_\\text{non-degenerate} &\\mathbf 0 \\\\ \\mathbf 0 & \\mathbf 0\\end{bmatrix}$  \n",
    "\n",
    "where $\\mathbf A \\in \\mathbb F^\\text{(r + m) x (r+m)}$ and  \n",
    "\n",
    "$\\mathbf A_\\text{non-degenerate} \\in \\mathbb F^\\text{r x r}$  \n",
    "and it is block diagonal with $\\frac{r}{2}$  blocks of $\\mathbf C$ \n",
    "\n",
    "note that up to graph isomorphism  \n",
    "$\\mathbf A_\\text{non-degenerate}$ is equivalent to \n",
    "\n",
    "$\\mathbf J_{2n} = \\begin{bmatrix}\\mathbf 0 & \\mathbf I_\\frac{r}{2} \\\\ -\\mathbf I_\\frac{r}{2} & \\mathbf 0\\end{bmatrix}$  \n",
    "\n",
    "which is the standard symplectic basis (see page 261)    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*remark:*  \n",
    "if we work through an extremely close process for symmetric forms over reals (or conjugate symmetric / Hermitian forms over $\\mathbb C$) and recover congruence to   \n",
    "$\\begin{bmatrix}\\mathbf I_p & \\mathbf 0 &\\mathbf 0 \\\\ \\mathbf 0 & -\\mathbf I_m & \\mathbf 0 \\\\ \\mathbf 0 & \\mathbf 0 & \\mathbf 0\\end{bmatrix}$  \n",
    "\n",
    "where the number of positive elements and negative elements (and for dimension reasons 0s on the diagonal as well) is an invariant, per Sylvester's Law of Inertia.  The problem with trying to replicate the above argument when our field is $\\mathbb Q$ or $\\mathbb F_p$ is that in general the normalization stage, when dealing with the non-degenerate portion of symmetric form -- selecting $\\alpha$ such that $\\langle \\alpha\\mathbf w_j, \\alpha \\mathbf w_j\\rangle = \\vert \\alpha\\vert^2 \\cdot \\langle \\mathbf w_j, \\mathbf w_j\\rangle = 1$  implies that, up to a sign, $ \\alpha = \\big(\\langle \\mathbf w_j, \\mathbf w_j\\rangle\\big)^\\frac{1}{2}$  but for other fields, that square root may not exist.  \n",
    "\n",
    "\n",
    "note in the curious case of a complex (not Hermitian) symmetric bilinear form, we recover congruence to \n",
    "\n",
    "$\\begin{bmatrix}\\mathbf D & \\mathbf 0 \\\\\\mathbf 0 & \\mathbf 0\\end{bmatrix}$  \n",
    "where $\\mathbf D$  has points on the unit circle.  The (not unique) square roots of $\\mathbf D$ exist, so this gives us the answer to *ex 7.2.12* -- every complex symmetric non-singular matrix $\\mathbf A = \\mathbf P^T\\mathbf P$  (i.e. take some branch of square root and $\\mathbf P: = \\mathbf D^\\frac{1}{2}\\mathbf M^{-1}$ where $\\mathbf M$ is a congruence transform... note Artin uses $\\mathbf Q$ instead of $\\mathbf M$ though this can be confusing since $\\mathbf Q$ typically indicates unitary/orthogonal matrices but that is *not* the case for this congruence transform.)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
