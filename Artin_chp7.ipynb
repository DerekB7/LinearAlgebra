{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a blurb on projections and polarization to adapt 7.3.8 this to the complex case may be interesting...  \n",
    "ref page 518 of Meyer's Matrix Analysis for inspiration  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ex 7.3.8**  \n",
    "for some unit vector $\\mathbf w \\in \\mathbb R^n$  \n",
    "**(a)** prove that the matrix $\\mathbf P = \\mathbf I - 2\\mathbf {ww}^T$ is orthgonal  \n",
    "*by inspection or diagonalization, or:*  since $\\mathbf P$ is real symmetric, it suffices to chec k $\\mathbf P^2= \\mathbf I$ (i.e. an involution), so \n",
    "$\\mathbf P^2 = \\mathbf I -2\\mathbf {ww}^T-2\\mathbf {ww}^T + 4\\mathbf w \\big(\\mathbf w^T\\mathbf w\\big)\\mathbf {w}^T = \\mathbf I - 4 \\mathbf {ww}^T +4 \\mathbf {ww}^T =\\mathbf I$   \n",
    "note: this confirms that $\\mathbf P$ is real orthogonal, and making use of this we know all eigenvalues of $\\mathbf P$ are on the unit circle -- but in fact +1 or -1 since $\\mathbf P$ is involutive-- then taking the trace we see  \n",
    "$\\text{trace}\\big(\\mathbf P\\big)= \\text{trace}\\big(\\mathbf I\\big) + - 2\\cdot\\text{trace}\\big(\\mathbf {ww}^T\\big)= n -2$  which means that $\\mathbf P$ has $(n-1)$ eigenvalues of $(+1)$ and $1$ eigenvalue of $(-1)$-- there are of course many ways at this result. But this also means that $\\det\\big(\\mathbf P\\big) = -1$ which is something we will exploit as a corollary to ex 7.3.9  \n",
    "\n",
    "**(b)**  Prove that multiplication by $\\mathbf P$ is a reflection through the space orthogonal to $\\mathbf w$.  I.e. if $\\mathbf v = c\\mathbf w + \\mathbf w'$ for $\\mathbf w'\\perp \\mathbf w$, then \n",
    "$\\mathbf P\\mathbf v = c\\mathbf P\\mathbf w + \\mathbf P\\mathbf w' = c\\big(\\mathbf I - 2\\mathbf {ww}^T\\big)\\mathbf w +\\big(\\mathbf I - 2\\mathbf {ww}^T\\big)\\mathbf w'= c\\big(\\mathbf w - 2\\mathbf w\\big) + \\mathbf I\\mathbf w'+ \\mathbf 0 = -c\\mathbf w + \\mathbf w' $     \n",
    "\n",
    "**(c)** let $\\mathbf x$ and $\\mathbf y$ be arbitrary vectors in $\\mathbb R^n$ with the same length (using standard metric).  \n",
    "Select a vector $\\mathbf w$ such that $\\mathbf P = \\mathbf I - 2\\mathbf {ww}^T$ is orthogonal and $\\mathbf P\\mathbf x = \\mathbf y$.  (Note this argues for existence of $\\mathbf P$ *not* uniqueness.)  \n",
    "\n",
    "in the case that $\\mathbf x = \\mathbf 0$ then $\\mathbf y$ is necessarily zero as well (e.g. because it has same length as $\\mathbf x$). \n",
    "\n",
    "*remark:* geometrically this is rather interesting to think about in the case 2 dimensional case  \n",
    "\n",
    "(i) if $\\mathbf x= \\mathbf y $ then selecting $\\mathbf w=\\mathbf 0$ gives the result and we are done    \n",
    "(ii) if $\\mathbf x =-\\mathbf y$ then selecting $\\mathbf w = \\mathbf x$ gives the result and we're done  \n",
    "(iii) since $\\mathbf x$ and $\\mathbf y$ have the same lengths and we are working in reals, in all other cases we must have $\\mathbf y$ and $\\mathbf x$ linearly independent.  \n",
    "\n",
    "so consider using part b of this exercise, consider     \n",
    "$\\mathbf x = c\\mathbf w + \\mathbf w'$  and $\\mathbf y = \\mathbf P \\mathbf x = -c\\mathbf w + \\mathbf w'$  \n",
    "\n",
    "then  we have  \n",
    "\n",
    "$\\mathbf x - \\mathbf y = 2c\\mathbf w$  or  \n",
    "$\\mathbf w = \\frac{1}{2c}\\big(\\mathbf x - \\mathbf y\\big)$  \n",
    "and since $\\mathbf w$ has length one this implies  \n",
    "$\\big \\vert c \\big \\vert = \\frac{1}{2}\\big \\Vert \\mathbf x - \\mathbf y\\big \\Vert_2$  \n",
    "the choice of sign for $c$ is arbitrary, so we may select the case of positive $c$ i.e.   \n",
    "$\\mathbf w = \\big(\\mathbf x - \\mathbf y\\big)\\cdot \\big \\Vert \\mathbf x - \\mathbf y \\big \\Vert_2^{-1}$  \n",
    "\n",
    "a quick check then shows that \n",
    "$\\mathbf w' = \\mathbf x + \\mathbf y$ meets the orthogonality requirements and that \n",
    "$\\mathbf x = c\\mathbf w + \\mathbf w'$ and  \n",
    "$\\mathbf y = -c\\mathbf w + \\mathbf w'$  \n",
    "which completes the requirements for this problem  \n",
    "\n",
    "*remark*  \n",
    "There is an interesting and near obvious parallel that comes up when dealing with e.g. iid standard normal r.v.'s $X$ and $Y$.  If we consider  \n",
    "\n",
    "$A=\\frac{1}{\\sqrt{2}}(X+Y)$ and $B=\\frac{1}{\\sqrt{2}}(X-Y)$, these too are iid standard normals  \n",
    "(e.g. recall sums of independent normals give a normal r.v., then check the covariance between $A$ and $B$)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*the below works through this in some detail.  It may be skipped.*  \n",
    "The below works through the above in blocked detail, starting with the 'hyper vector' (page 96) given by $\\mathbf B$, though in this case we can be sure that $\\mathbf B$ is real invertible matrix (e.g. created via the basis creation algorithm).  So consider  \n",
    "\n",
    "$\\mathbf B = \n",
    "\\bigg[\\begin{array}{c|c|c|c|c} \n",
    "\\mathbf x  & \\mathbf y & \\mathbf b_{3} & \\cdots & \\mathbf b_n \n",
    "\\end{array}\\bigg]$\n",
    "\n",
    "where $\\det\\big(\\mathbf B\\big) \\neq 0$  \n",
    "(technically invertibility of $\\mathbf B$ is not used in what follows, however we selected it to have a spanning set of linearly independent vectors so as to fit with other common operations on a 'hypervector')  \n",
    "\n",
    "now consider the linear map  \n",
    "\n",
    "$ A :=\\begin{bmatrix}1 & 1 &\\mathbf 0^T \\\\ -1 & 1 & \\mathbf 0^T \\\\ \\mathbf 0 & \\mathbf 0 & \\mathbf I_{n-2}\\end{bmatrix}$   \n",
    "\n",
    "multiplying on the right of $\\mathbf B$ gives  \n",
    "$\\mathbf B  A = \\bigg[\\begin{array}{c|c|c|c|c} \n",
    "\\mathbf x -\\mathbf y  &\\mathbf x + \\mathbf y & \\mathbf b_{3} & \\cdots & \\mathbf b_n \n",
    "\\end{array}\\bigg] = \\bigg[\\begin{array}{c|c|c|c|c} \n",
    "\\alpha\\mathbf w  & \\mathbf w_2 & \\mathbf b_{3} & \\cdots & \\mathbf b_n \n",
    "\\end{array}\\bigg] $  \n",
    "\n",
    "where $\\alpha \\mathbf w^T\\mathbf w_2=\\big(\\mathbf x-\\mathbf y\\big)^T\\big(\\mathbf x + \\mathbf y\\big) = \\mathbf x^T\\mathbf x + \\mathbf x^T \\mathbf y -\\mathbf y^T \\mathbf x -\\mathbf y^T\\mathbf y = 0$  \n",
    "by symmetry of the real (inner) dot product and the fact that $\\mathbf x$ and $\\mathbf y$ have the same lengths. Hence we know that $\\big(\\mathbf x+\\mathbf y\\big)$ and $\\big(\\mathbf x - \\mathbf y\\big)$ are orthogonal, and $\\mathbf w$ has norm one, hence has some scalar $\\alpha$ in front of it-- and of course $\\vert \\alpha \\vert =\\big\\Vert \\mathbf x - \\mathbf y\\big \\Vert_2^{-1}$ which agrees with what we had above.  \n",
    "\n",
    "inverting $A$ gives  \n",
    "$\\bigg[\\begin{array}{c|c|c} \n",
    "\\mathbf x  & \\mathbf y & *\n",
    "\\end{array}\\bigg] $  \n",
    "$ = \\bigg[\\begin{array}{c|c|c} \n",
    "\\mathbf x -\\mathbf y  &\\mathbf x + \\mathbf y & * \n",
    "\\end{array}\\bigg] \\mathbf A^{-1}$  \n",
    "$=\\bigg[\\begin{array}{c|c|c} \n",
    "\\mathbf x -\\mathbf y  &\\mathbf x + \\mathbf y & * \n",
    "\\end{array}\\bigg]\\begin{bmatrix}\\frac{1}{2} & \\frac{-1}{2} &\\mathbf 0^T \\\\ \\frac{1}{2} & \\frac{1}{2} & \\mathbf 0^T \\\\ \\mathbf 0 & \\mathbf 0 & \\mathbf I_{n-2}\\end{bmatrix}$  \n",
    "$=\\bigg[\\begin{array}{c|c|c} \n",
    "\\alpha \\mathbf w  &\\mathbf w_2& *  \n",
    "\\end{array}\\bigg]\\begin{bmatrix}\\frac{1}{2} & \\frac{-1}{2} &\\mathbf 0^T \\\\ \\frac{1}{2} & \\frac{1}{2} & \\mathbf 0^T \\\\ \\mathbf 0 & \\mathbf 0 & \\mathbf I_{n-2}\\end{bmatrix}    \n",
    "=\\bigg[\\begin{array}{c|c|c} \n",
    "\\frac{1}{2}\\alpha\\mathbf w +\\frac{1}{2}\\beta \\mathbf w_2  &-\\frac{1}{2}\\alpha\\mathbf w +\\frac{1}{2}\\beta \\mathbf w_2 & *\n",
    "\\end{array}\\bigg] $  \n",
    "\n",
    "using a change of variables since the length of $\\mathbf w'$ is not specified, consider  \n",
    "$\\mathbf w' := \\frac{1}{2}\\beta \\mathbf w_2$   \n",
    "$\\mathbf x = c \\mathbf w +  \\mathbf w'$  and  \n",
    "$\\mathbf y = -c \\mathbf w +  \\mathbf w'$  \n",
    "as required in this problem  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*remark:* if we were to work in $\\mathbb C$ with an eye to applying this to unitary matrices, the above wouldn't literally work as \n",
    "\n",
    "to simplify the calculations, assume WLOG that  \n",
    "$\\big \\Vert \\mathbf x \\big \\Vert_2 = \\big \\Vert \\mathbf y \\big \\Vert_2=1$   \n",
    "\n",
    "$\\big(\\mathbf x- \\mathbf y\\big)^* \\big(\\mathbf x+ \\mathbf y\\big) = \\mathbf x^*\\mathbf x+\\mathbf y^*\\mathbf y-\\mathbf y^*\\mathbf x+\\mathbf x^*\\mathbf y = 2\\cdot\\text{im}\\big(\\mathbf x^*\\mathbf y\\big)$   \n",
    "the result can be saved via the use of polarization, i.e. instead considering  \n",
    "$\\big(\\mathbf x- \\alpha\\mathbf y\\big)^* \\big(\\mathbf x+ \\alpha\\mathbf y\\big)$  \n",
    "if $\\mathbf x^*\\mathbf y=0$ (or in fact any real number), then select $\\alpha :=1$, otherwise selection \n",
    "$\\alpha = e^{i\\theta}$ where $\\theta$  is the polar angle of $\\big(\\mathbf x^*\\mathbf y\\big)^*=\\mathbf y^*\\mathbf x$ -- in words, this is the value of $\\mathbf x^*\\mathbf y$, with the length shrunk to one, and then taking conjugating the value so $\\alpha \\cdot \\mathbf x^*\\mathbf y = \\big \\vert \\mathbf x^*\\mathbf y\\big \\vert \\in \\mathbb R$ (positivity is an added bonus though not really needed)  \n",
    "\n",
    "so  \n",
    "$\\big(\\mathbf x- \\alpha\\mathbf y\\big)^* \\big(\\mathbf x+ \\alpha\\mathbf y\\big) = \\mathbf x^*\\mathbf x+ \\big\\vert \\alpha\\big\\vert^2 \\cdot \\mathbf y^*\\mathbf y-(\\bar{\\alpha}\\cdot\\mathbf y)^*\\mathbf x+\\mathbf x^*(\\alpha\\cdot\\mathbf y)  = -\\big(\\mathbf x^* \\alpha\\cdot\\mathbf y\\big)^*+\\mathbf x^*\\alpha\\cdot \\mathbf y = -\\big(\\mathbf x^* \\alpha\\cdot\\mathbf y\\big)+\\mathbf x^*\\alpha\\cdot \\mathbf y =0 $  \n",
    "because taking the conjugate transpose of a real scalar gives that same real scalar   \n",
    "\n",
    "so select $\\mathbf w\\propto \\mathbf x-\\alpha\\mathbf y $  and revisit  \n",
    "$\\mathbf x = c\\mathbf w + \\mathbf w'$  and $\\alpha\\mathbf y = \\mathbf P \\mathbf x = -c\\mathbf w + \\mathbf w'$  \n",
    "to see  \n",
    "$\\mathbf x - \\alpha\\mathbf y = 2 c \\mathbf w$ as desired  \n",
    "and $\\mathbf w'$ is orthogonal to $\\mathbf w$ and its length isn't constrained to one, so we may rescale as needed, e.g. with  \n",
    "$c\\mathbf w =\\gamma(\\mathbf x-\\alpha\\mathbf y) $   \n",
    "$\\mathbf w' = \\gamma\\big(\\mathbf x+ \\alpha\\mathbf y\\big)$    \n",
    "$\\mathbf x = 2\\gamma \\mathbf x$ \n",
    "(implying $\\gamma =\\frac{1}{2}$)  \n",
    "\n",
    "$\\alpha \\mathbf y = -c\\mathbf w + \\mathbf w'= -\\gamma(\\mathbf x-\\alpha\\mathbf y) + \\gamma\\big(\\mathbf x+ \\alpha\\mathbf y\\big)=2\\gamma\\cdot\\alpha \\mathbf y$, again implying $\\gamma = \\frac{1}{2}$ so we are consistent.  \n",
    "\n",
    "*extension:*  \n",
    "if we follow the below ex 7.3.9 and we *could*, mostly, adapt it to unitary matrices, what it tell us is  \n",
    "with unitary matrices of the form $\\mathbf P^{(i)} = \\mathbf I -2\\mathbf w_i \\mathbf w_i^*$  \n",
    "we can attack the first column vector $\\mathbf q_1$ so $\\mathbf P^{(1)}\\mathbf q_1 = \\alpha_1\\mathbf e_1$ (with standard basis vector $\\mathbf e_1$)  \n",
    "\n",
    "\n",
    "$\\mathbf P^{(1)}\\mathbf Q= \\begin{bmatrix} \\alpha_1 & \\mathbf 0^T \\\\ \\mathbf 0 & \\mathbf Q^{(2)} \\end{bmatrix}$   and the ending would be  \n",
    "$\\mathbf P^{(k)}...\\mathbf P^{(2)}\\mathbf P^{(1)}\\mathbf Q = \\mathbf D$ where $\\mathbf D$ is diagonal and unitary, i.e. $d_{i,i} =\\alpha_i$  so  \n",
    "$ \\mathbf D^*\\mathbf P^{(k)}...\\mathbf P^{(2)}\\mathbf P^{(1)}\\mathbf Q = \\mathbf I$  \n",
    "\n",
    "path connectedness then follows for dimension $n\\geq$ with even $k$ by the same argument as below (and $\\mathbf D^*$ is easily connected to the identity by 'rotating' each diagonal component over the unit circle).  \n",
    "\n",
    "However the above portion isn't really needed when working in the complex plane.  We could prove directly that a single unitary Householder matrix is path connected to the identity by using a one-size fits all argument to show that *any* unitary matrix is path connected to the identity.  Making use of the normality of unitary matrices, we have    \n",
    "$\\mathbf Q = \\mathbf U\\mathbf \\Lambda \\mathbf U^*$  \n",
    "where $\\mathbf U$ is unitary and all eigenvalues are on the unit circle, so $\\Lambda$ is unitary and the product is unitary for any arbitrary $\\lambda_i$.  Then we merely need to 'rotate' the values of $\\lambda_i$ along the unit circle to be one at $\\tau =1$ implying $\\mathbf Q(1) = \\mathbf U\\mathbf I  \\mathbf U^* = I$  \n",
    "This is a much easier result than in the real orthogonal matrix case-- in such a case while the matrices are diagonalizable over $\\mathbb C$ we couldn't guarantee that any path to the identity stays real (and in fact if the determinant of a real orthogonal matrix is $-1$ this is impossible by e.g. intermediate value theorem, though the below shows that if the determinant is +1, then the result *is* possible)     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ex 7.3.9**  \n",
    "considering any orthogonal matrix $\\mathbf Q \\in \\mathbb R^\\text{n x n}$  (where n is some natural number $\\geq 2$)  \n",
    "\n",
    "using results from ex 7.3.8, with orthgonal matrices of the form $\\mathbf P^{(i)} = \\mathbf I -2\\mathbf w_i \\mathbf w_i^T$  \n",
    "we can attack the first column vector $\\mathbf q_1$ so $\\mathbf P^{(1)}\\mathbf q_1 = \\mathbf e_1$ (with standard basis vector $\\mathbf e_1$)  \n",
    "\n",
    "\n",
    "$\\mathbf P^{(1)}\\mathbf Q= \\begin{bmatrix} 1 & \\mathbf 0^T \\\\ \\mathbf 0 & \\mathbf Q^{(2)} \\end{bmatrix}$    \n",
    "where we use the fact that the matrices on the LHS are in the Orthogonal group, so their product must be as well.  The first column of the RHS is $\\mathbf e_1$ by design.  But e.g. from ex 7.2.5, we know that the first row of an orthogonal matrix has norm 1 as well, and since the top left component of the RHS is 1, this implies all other components on the first row are zero and hence the first row is given by $\\mathbf e_1^T$.  Examining the blocked structure (and the fact that the RHS must be orthogonal) we see  $\\big(\\mathbf Q^{(2)}\\big)^T \\mathbf Q^{(2)} = \\mathbf I_{n-1}$, i.e. $\\mathbf Q^{(2)}$ is necessarily orthogonal as well.    \n",
    "\n",
    "To finish this off we may formalize with induction, but it seems preferable to work through this by recursing on the smaller subproblem of making the first column of $\\mathbf Q^{(2)}$ become the first standard basis vector for an n-1 dimensional space, i.e. by applying  \n",
    "$\\begin{bmatrix} 1 & \\mathbf 0^T \\\\ \\mathbf 0 & \\mathbf P^{(2)} \\end{bmatrix}\\mathbf P^{(1)}\\mathbf Q=\\begin{bmatrix} 1 & \\mathbf 0^T \\\\ \\mathbf 0 & \\mathbf P^{(2)} \\end{bmatrix}\\begin{bmatrix} 1 & \\mathbf 0^T \\\\ \\mathbf 0 & \\mathbf Q^{(2)} \\end{bmatrix}=\\begin{bmatrix} 1 & \\mathbf 0^T \\\\ \\mathbf 0 & \\mathbf P^{(2)}\\mathbf Q^{(2)} \\end{bmatrix}$  \n",
    "where this new blocked matrix is necessarily orthogonal as well.  The $\\text{n-1 x n-1}$ submatrix in the bottom corner is given by  \n",
    "$\\mathbf P^{(2)}\\mathbf Q^{(2)} =   \\begin{bmatrix} 1 & \\mathbf 0^T \\\\ \\mathbf 0 & \\mathbf Q^{(3)} \\end{bmatrix}$  \n",
    "\n",
    "for avoidance of doubt notice:  \n",
    "$\\begin{bmatrix} 1 & \\mathbf 0^T \\\\ \\mathbf 0 & \\mathbf P^{(2)} \\end{bmatrix}= \\mathbf I - 2\\mathbf {xx}^T$  where $\\mathbf x = \\sum_{k=2}^n \\alpha_k\\mathbf e_k$ so the above blocked matrix is still conforming in structure (i.e. it is still Householder, as we will shortly observe).  \n",
    "\n",
    "Then we keep repeating by attacking the first column of $\\mathbf Q^{(r)}$ at each the $r$th iteration and map it to $\\mathbf e_1 \\in \\mathbb R^{n-r+1}$.  At each pass, we make incremental progress and can assert that the columns $j\\in\\{1,2,...,r\\}$ of the RHS are $\\mathbf e_j \\in \\mathbb R^n$  and hence after at most $n$ applications we have recovered the identity matrix. That is, we've proven, for any orthogonal $\\mathbf Q \\in \\mathbf R^\\text{n x n}$,  \n",
    "\n",
    "$\\mathbf P^{(k)}...\\mathbf P^{(2)}\\mathbf P^{(1)}\\mathbf Q = \\mathbf I$ or  \n",
    "$\\mathbf Q=\\big(\\mathbf P^{(k)}...\\mathbf P^{(2)}\\mathbf P^{(1)}\\big)^T$  \n",
    "for some $1\\leq k \\leq n$ \n",
    "\n",
    "Matrices of the form $\\mathbf P^{(i)}$ are known as Householder Matrix / Householder Reflections, which in addition to being geometrically interesting, have uses in numerical linear algebra   \n",
    "\n",
    "**corollary**  \n",
    "These Householder matrices, i.e. matrices of the form $\\mathbf P^{(i)}$ are the *generators* of the orthogonal group.  In particular this tells us that if $\\det\\big(\\mathbf Q\\big )=1$ it is in $\\text{SO}(n)$ and it may be decomposed into in an even number of products of $\\mathbf P^{(i)}\\neq \\mathbf I$, so $\\det\\big(\\mathbf P^{(i)}\\big) = -1$ .  It we examine two of these at at time, we have  \n",
    "\n",
    "$\\mathbf P^{(i)}\\mathbf P^{(j)} = \\big(\\mathbf I -2\\mathbf w_1\\mathbf w_1^T\\big)\\big(\\mathbf I -2\\mathbf w_2\\mathbf w_2^T\\big) $  \n",
    "\n",
    "in the case of dimension $n\\geq 3$  since $\\mathbf P^{(i)}$  and $\\mathbf P^{(j)}$  each have some $\\mathbf w_i$ and $\\mathbf w_j$, but there are at least 3 linearly independent vectors in this space, with some 3rd linear independent vector $\\mathbf y$ (use e.g. the basis creation algorithm from chapter 3 to find it), and after running Gram Schmidt we have $\\mathbf y\\mapsto \\mathbf z$ such that $\\mathbf z$ is orthonormal to $\\mathbf w_i$ and $\\mathbf w_j$,  then consider \n",
    "\n",
    "$\\mathbf P^{(i)}(\\tau) = \\mathbf I - 2\\mathbf v_i(\\tau)\\mathbf v_i(\\tau)^T$  \n",
    "where for $\\tau \\in [0,1]$  \n",
    "$\\mathbf v_i(\\tau) = \\sqrt{1-\\tau}\\cdot\\mathbf w_i + \\sqrt{\\tau}\\cdot \\mathbf z$  \n",
    "and  \n",
    "$\\big\\Vert \\mathbf v_i(\\tau)\\big\\Vert_2^2 = (1-\\tau)\\cdot\\big\\Vert \\mathbf w_i \\big\\Vert_2^2 +\\tau\\cdot\\big\\Vert \\mathbf z \\big\\Vert_2^2+ 2\\sqrt{\\tau(1-\\tau)}\\cdot\\mathbf w_i^T\\mathbf z = (1-\\tau)\\cdot 1 + \\tau\\cdot 1 + 0 = 1$  \n",
    "so $\\mathbf v_i(\\tau)$ is unit length and thus $\\mathbf P^{(i)}(\\tau)$ is a Householder matrix for all $\\tau$ in our domain.  \n",
    "\n",
    "This then implies at $\\tau =0$  \n",
    "$\\mathbf P^{(i)}(0)\\mathbf P^{(j)}(0) = \\mathbf P^{(i)}\\mathbf P^{(j)}$  \n",
    "and at $\\tau = 1$  \n",
    "$\\mathbf P^{(i)}(1)\\mathbf P^{(j)}(1) = \\big(\\mathbf I - 2\\mathbf {zz}^T\\big)^2 = \\mathbf I$  \n",
    "\n",
    "hence any arbitrary pair of Householder matrices in dimension $n\\geq 3$ is path connected to the identity.  Using results from ex 2.misc.8 we can then infer that any matrix generated by even number $r$ Householder matrices is path connected to the identity. *And* we can infer that $r$ is even *iff* the determinant of an orthogonal matrix is 1-- which is to say we've proven that $\\text{SO}(n)$ is path connected (since every element is path connected to the identity and this is an equivalence relation).    \n",
    "\n",
    "Note: the above does not consider the $n=2$ case (and the $n=1$ case trivially holds true).  For 2 dimensions there is a simple and direct argument for path connectedness of $\\text{SO(n)}$ for the $\\text{2 x 2}$  case, in particular write out any matrix (e.g. revisiting Artin chapter 5 if needed) as   \n",
    "\n",
    "$\\mathbf Q_\\theta = \\begin{bmatrix} \\cos(\\theta) & -\\sin(\\theta) \\\\ \\sin(\\theta) & \\cos(\\theta) \\end{bmatrix} $   \n",
    "for $\\theta \\in [0,2\\pi)$  \n",
    "and directly consider the path given by any $\\mathbf Q_\\theta \\to \\mathbf I$ as $\\theta \\to 0$  \n",
    "\n",
    "**closing remark on uniqueness of real rotation matrix representation in 2 x 2 case**  \n",
    "consider that all matrices in SO(2) come in the form  \n",
    "$\\mathbf Q_\\theta = \\begin{bmatrix} \\cos(\\theta) & -\\sin(\\theta) \\\\ \\sin(\\theta) & \\cos(\\theta) \\end{bmatrix} $   \n",
    "\n",
    "by first supposing we fix the first row as \n",
    "\n",
    "$\\begin{bmatrix} \\mathbf v_1^T \\end{bmatrix} $   \n",
    "where $\\big\\Vert \\mathbf v_1\\big \\Vert_2 =1$  \n",
    "then there is exactly one linearly independent vector $\\mathbf v_2$ in the nullspace by rank-nullity.  \n",
    "\n",
    "In fact we have 2 unique homongenous solutions to $\\mathbf v_1^T\\mathbf x = 0$     \n",
    "when we insist on $\\big \\Vert \\mathbf x\\big \\Vert_2 =1$ \n",
    "i.e. our choice $\\mathbf v_2$ is unique except it may be rescaled by +1 or -1.  The selection of which scalar is then fixed when we require \n",
    "$\\det\\left(\\begin{bmatrix} \\mathbf v_1^T\\\\ \\mathbf v_2^T \\end{bmatrix}\\right) =1$   \n",
    "\n",
    "working backwards, we can be sure that \n",
    "$\\mathbf v_1^T =\\begin{bmatrix} \\cos(\\theta) & -\\sin(\\theta)\\end{bmatrix} $   \n",
    "since the sum of squares is one and the sign of each component enforces a quadrant in the unit circle and the modulus of sine (and cosine) are surjective and injective maps to $[0,1]$, once we've specified a quadrant.  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "section 7.4:   \n",
    "1) Sylvester's Law of Inertia for Complex case -- hit existence via spectral theorem, then uniqueness  \n",
    "do spectral theorem for existence, then rank, then uniqueness, with the form / matrix notation as needed  \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ex 7.4.4**  \n",
    "prove that if   \n",
    "$\\mathbf x^* \\mathbf A \\mathbf x \\in \\mathbb R$  for all $\\mathbf x \\in \\mathbb C^n\\longrightarrow \\mathbf A = \\mathbf A^*$  \n",
    "\n",
    "1.) write  \n",
    " $\\mathbf A = \\frac{1}{2}\\big(\\mathbf A + \\mathbf A^*\\big)+\\frac{1}{2}\\big(\\mathbf A - \\mathbf A^*\\big)= \\mathbf H + \\mathbf S$   \n",
    "i.e. decompose $\\mathbf A$ into a Hermitian and skew-Hermitian part  \n",
    "\n",
    "now we want to estimate the skew-Hermitian part.  \n",
    "\n",
    "so consider   \n",
    "$\\mathbf x^* \\mathbf A \\mathbf x \\in \\mathbb R \\longrightarrow \\mathbf x^* \\mathbf A \\mathbf x = \\big(\\mathbf x^* \\mathbf A \\mathbf x\\big)^* = \\mathbf x^* \\mathbf A^* \\mathbf x \\longrightarrow \\mathbf x^* \\mathbf A^* \\mathbf x -\\mathbf x^* \\mathbf A^* \\mathbf x = \\mathbf x^*\\big(\\mathbf A -\\mathbf A^*\\big) \\mathbf x = \\mathbf x^* \\big(2\\mathbf S\\big)\\mathbf x = 0$  \n",
    "\n",
    "This implies that any eigenvalue of the matrix $\\mathbf S$ must be zero.  We can finish by observing that skew-Hermitian matrices are diagonalizable since they are normal (normal matrices are developed in section 7 of this chapter), and hence $\\mathbf S$ is the zero matrix, or by considering the positive-definiteness of the (squared) Frobenius norm  \n",
    "$\\big \\Vert \\mathbf S\\big \\Vert_F^2 = \\text{trace}\\big(\\mathbf S^*\\mathbf S\\big) =  -1\\cdot\\text{trace}\\big(\\mathbf S^2\\big)= -1 \\cdot \\sum_{k=1}^n \\lambda_k^2 = -1 \\cdot \\sum_{k=1}^n 0 = 0\\longrightarrow \\mathbf S = \\mathbf 0$  \n",
    "\n",
    "since the skew Hermitian matrix is zero, we have  \n",
    "$\\mathbf A = \\frac{1}{2}\\big(\\mathbf A + \\mathbf A^*\\big)+0 = \\frac{1}{2}\\big(\\mathbf A + \\mathbf A^*\\big)$   \n",
    "so $\\mathbf A$ is Hermitian  \n",
    "\n",
    "\n",
    "*remark*  \n",
    "This also proves that for a matrix to be positive (semi)definite over a domain of $\\mathbf C^n $x $\\mathbf C^n$, it must be Hermitian (since having a real-nonegative quadratic form is a subset of having a real quadratic form)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7.4.16**  \n",
    "\n",
    "let $P$ be the (complex) vector space of polynomials with degree $\\leq n$  \n",
    "\n",
    "**(a)** show that  \n",
    "$\\langle f, g\\rangle = \\int_0^{2\\pi}\\bar{f(e^{i\\theta}})g(e^{i\\theta})d\\theta= \\int_0^{2\\pi}\\text{conj}\\big({f(e^{i\\theta}}\\big)g(e^{i\\theta})d\\theta$   \n",
    "is a positive definite hermitian form on $P$  \n",
    "(note the conjugation bar should be over all of $f(e^{i\\theta})$ though it doesn't seems to be rendering that way)  \n",
    "a positive definite hermitian form is equivalent to saying that we have an inner product in a complex space  \n",
    "\n",
    "so we confirm  \n",
    "(i) linearity in the second variable -- immediate  \n",
    "\n",
    "(ii) conjugate linearity under second variable --immediate  \n",
    "*note: this is the way Artin defines the inner product... the more common/standard definition is to linear in first argument and conjugate linear in the second argument*   \n",
    "\n",
    "(iii)  conjugate symmetry  \n",
    "${\\langle g, f\\rangle}=\\text{conj}\\big(\\langle f, g\\rangle\\big)$  \n",
    "$\\text{conj}\\big(\\langle f, g\\rangle\\big) = \\text{conj}\\big(\\int_0^{2\\pi}\\text{conj}\\big({f(e^{i\\theta}}\\big)g(e^{i\\theta})d\\theta\\big)= \\int_0^{2\\pi}\\text{conj}\\big({g(e^{i\\theta}}\\big)g(e^{i\\theta})d\\theta={\\langle g, f\\rangle}$  \n",
    "(writing this out as a limit of Riemann sums over real and imaginary parts can be helpful if needed)  \n",
    "\n",
    "(iv) we finally need to confirm this is positive definte / an actual inner product, i.e.  \n",
    "$\\langle g, g\\rangle \\geq 0$ with equality *iff* $g$ is the zero polynomial  \n",
    "\n",
    "first observe  \n",
    "$\\langle g, g\\rangle = \\int_0^{2\\pi}\\bar{g(e^{i\\theta}})g(e^{i\\theta})d\\theta= \\int_0^{2\\pi}\\big \\vert g(e^{i\\theta})\\big \\vert^2 d\\theta$  \n",
    "\n",
    "if $g$ is identically zero then the integral evaluates to zero.  Suppose $g$ is not identically zero, then it has at most $n$ distinct roots.  Using linearity, we have  \n",
    "\n",
    "$\\int_0^{2\\pi}\\big \\vert g(e^{i\\theta})\\big \\vert^2 d\\theta = \\sum_{k=0}^{n}\\int_{\\frac{k}{n+1}\\cdot 2\\pi}^{\\frac{k+1}{n+1}2\\pi}\\big \\vert g(e^{i\\theta})\\big \\vert^2 d\\theta$    \n",
    "\n",
    "where each of the n+1 intervals $[\\frac{k}{n+1}2\\pi,\\frac{k+1}{n+1}2\\pi]$  and being continuous (and real valued) $\\big \\vert g(e^{i\\theta})\\big \\vert^2$ has a minimum over each -- in all cases each the minimum is at least zero, and in at least one of them the minimum is strictly positive, thus the integral is strictly positive, as required.  \n",
    "**(b)**  \n",
    "find an orthonormal basis for this form.  We do this abstractly with the 'hyper-vector'  \n",
    "\n",
    "$\\mathbf B = \n",
    "\\bigg[\\begin{array}{c|c|c|c|c} \n",
    "\\mathbf b_0  & \\mathbf b_1 & \\mathbf b_2 & \\cdots & \\mathbf b_n \n",
    "\\end{array}\\bigg]=\\bigg[\\begin{array}{c|c|c|c|c} \n",
    "\\mathbf 1  & \\mathbf x & \\mathbf x^2 & \\cdots & \\mathbf x^n \n",
    "\\end{array}\\bigg]$  \n",
    "\n",
    "via applying Gram Schmidt we have  \n",
    "$\\mathbf q_0' = \\mathbf b_0\\longrightarrow \\mathbf q_0 = \\frac{1}{\\sqrt{2\\pi}}$  \n",
    "and for $k\\in\\{1,2,...,n\\}$  \n",
    "$\\mathbf q_k' = \\mathbf b_k - \\sum_{j=1}^{k-1}\\langle \\mathbf q_j, \\mathbf b_k \\rangle\\mathbf q_j$  \n",
    "(orthogonalization)  \n",
    "and  \n",
    "$\\mathbf q_k = \\alpha_k \\mathbf q_k' = \\frac{\\mathbf q_k'}{\\langle \\mathbf q_k', \\mathbf q_k' \\rangle^\\frac{1}{2}}=\\frac{\\mathbf q_k'}{\\big \\Vert \\mathbf q_k'\\big \\Vert_2}$  \n",
    "(normalization)  \n",
    "\n",
    "\n",
    "(i) this is an orthonormal sequence because if we select any $\\mathbf q_k$, then  \n",
    "direct checking of $\\mathbf q_0$ and $\\mathbf q_1$ tells us  \n",
    "$\\alpha_1 \\langle \\mathbf q_0,\\mathbf q_1\\rangle= \\langle \\mathbf q_0,\\mathbf q_1'\\rangle= \\langle \\mathbf q_0,\\mathbf b_1\\rangle- \\langle \\mathbf q_0, \\mathbf b_1 \\rangle\\cdot \\langle \\mathbf q_0,\\mathbf q_0\\rangle=\\langle \\mathbf q_0,\\mathbf b_1\\rangle- \\langle \\mathbf q_0, \\mathbf b_1\\rangle =0$  \n",
    "\n",
    "\n",
    "for $i \\neq k$  \n",
    "since conjugation doesn't change whether the result is zero or not we may confirm / assume WLOG that $i\\lt k$ due to conjugate symmetry of ${\\langle g, f\\rangle}=\\text{conj}\\big(\\langle f, g\\rangle\\big)$ \n",
    "we may also rescale the inner product by non-zero values -- this too doesn't change whether the inner product is zero   \n",
    "\n",
    "for $i=0, 1, 2,...., n-1$  \n",
    "   for $k = i+1,1,2,...,n$  \n",
    "$\\alpha_k \\langle \\mathbf q_i,\\mathbf q_k\\rangle= \\langle \\mathbf q_i,\\mathbf q_k'\\rangle= \\langle \\mathbf q_i,\\mathbf b_k\\rangle- \\sum_{j=1}^{k-1}\\langle \\mathbf q_j, \\mathbf b_k \\rangle\\cdot \\langle \\mathbf q_i,\\mathbf q_j\\rangle= \\langle \\mathbf q_i,\\mathbf b_k\\rangle- \\langle \\mathbf q_i, \\mathbf b_k \\rangle= 0$    \n",
    "noting that  \n",
    "$\\sum_{j=1}^{k-1}\\langle \\mathbf q_j, \\mathbf b_k \\rangle\\cdot \\langle \\mathbf q_i,\\mathbf q_j\\rangle = \\langle \\mathbf q_i, \\mathbf b_k \\rangle$  \n",
    "because $\\langle \\mathbf q_i,\\mathbf q_j\\rangle=1$ if $i=j$ and \n",
    "$\\langle \\mathbf q_i,\\mathbf q_j\\rangle=0$ if $i\\neq j$ by (in particular $i\\lt j$) induction hypothesis  \n",
    "\n",
    "if we revisit the equation  \n",
    "$\\mathbf q_k' = \\mathbf b_k - \\sum_{j=1}^{k-1}\\langle \\mathbf q_j, \\mathbf b_k \\rangle\\mathbf q_j $  \n",
    "$\\longrightarrow  \\mathbf b_k = \\mathbf q_k' + \\sum_{j=1}^{k-1}\\langle \\mathbf q_j, \\mathbf b_k \\rangle\\mathbf q_j$  \n",
    "or  \n",
    "$\\mathbf b_k = \\gamma_k \\cdot \\mathbf q_k + \\sum_{j=1}^{k-1}\\langle \\mathbf q_j, \\mathbf b_k \\rangle\\mathbf q_j$  \n",
    "and considering $\\langle \\mathbf q_k, \\mathbf b_k\\rangle $  we see   \n",
    "$\\langle \\mathbf q_k, \\mathbf b_k\\rangle  = \\gamma_k \\cdot 1 + \\sum_{j=1}^{k-1}\\langle \\mathbf q_j, \\mathbf b_k \\rangle\\cdot 0 \\longrightarrow \\gamma_k =\\langle \\mathbf q_k, \\mathbf b_k\\rangle$  \n",
    "thus  \n",
    "$\\mathbf b_k = \\langle \\mathbf q_k, \\mathbf b_k\\rangle \\cdot \\mathbf q_k + \\sum_{j=1}^{k-1}\\langle \\mathbf q_j, \\mathbf b_k \\rangle\\mathbf q_j =   r_{k,k}\\mathbf q_k + \\sum_{j=1}^{k-1} r_{j,k}\\mathbf q_j $     \n",
    "\n",
    "i.e.  \n",
    "$\\mathbf B = \n",
    "\\bigg[\\begin{array}{c|c|c|c|c} \n",
    "\\mathbf 1  & \\mathbf x & \\mathbf x^2 & \\cdots & \\mathbf x^n \n",
    "\\end{array}\\bigg] = \\bigg[\\begin{array}{c|c|c|c|c} \n",
    "\\mathbf q_0  & \\mathbf q_1 & \\mathbf q_2 & \\cdots & \\mathbf q_n \n",
    "\\end{array}\\bigg] \\begin{bmatrix}\n",
    "r_{0,0} & r_{0,1}& r_{0,2}&\\dots & r_{0,n} \\\\ \n",
    "0 & r_{1,1}& r_{1,2}&\\dots & r_{1,n}\\\\  \n",
    "0 & 0 & r_{2,2}&\\dots & r_{2,n} \\\\\n",
    "\\vdots & \\vdots & \\dots & \\ddots& \\vdots \\\\ \n",
    "0 & 0 & 0&\\dots & r_{n,n} \\\\ \n",
    "\\end{bmatrix}= \\mathbf {Q}R$  \n",
    "\n",
    "This *is* **QR factorization** and agrees with the procedure e.g. on page 242  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7.5.13**  \n",
    "Suppose $\\mathbf A$ is Hermitian. Prove there is some unitary $\\mathbf Q$ with determinant 1, such that $\\mathbf {QAQ}^* = \\mathbf D$  for diagonal matrix $\\mathbf D$   \n",
    "\n",
    "*proof:*  \n",
    "by spectral theorem we have  \n",
    "$\\mathbf {UAU}^* = \\mathbf D$  \n",
    "with unitary $\\mathbf U$  \n",
    "\n",
    "and we know $\\det\\big(\\mathbf U\\big) = \\gamma$  for some $\\gamma$ on the unit circle   \n",
    "\n",
    "now let $\\alpha^n = \\gamma$  \n",
    "(i.e. $\\alpha$ isn't unique per se -- there are $n$ distinct values on the unit circle that will suffice)   \n",
    "\n",
    "then consider  \n",
    "$\\mathbf Q :=\\bar{\\alpha}\\mathbf U$  \n",
    "\n",
    "so  \n",
    "$\\mathbf {QAQ}^* =\\big(\\bar{\\alpha}\\mathbf U\\big) \\mathbf A \\big(\\bar{\\alpha}\\mathbf U\\big)^* =\\bar{\\alpha} \\mathbf {UAU}^*\\alpha=\\bar{\\alpha}\\alpha\\cdot \\mathbf {UAU}^*= 1\\cdot\\mathbf {UAU}^* = \\mathbf D$  \n",
    "and  \n",
    "$\\det\\big(\\mathbf Q\\big) =\\det\\big(\\bar{\\alpha}\\mathbf I\\mathbf U\\big)=\\det\\big(\\bar{\\alpha}\\mathbf I\\big)\\det\\big(\\mathbf U\\big)=(\\bar{\\alpha})^n \\cdot \\gamma = \\bar{\\gamma}\\cdot \\gamma = 1$  \n",
    "as desired  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
