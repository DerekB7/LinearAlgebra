{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ex 7.3.8**  \n",
    "for some unit vector $\\mathbf w \\in \\mathbb R^n$  \n",
    "**(a)** prove that the matrix $\\mathbf P = \\mathbf I - 2\\mathbf {ww}^T$ is orthgonal  \n",
    "*by inspection or diagonalization, or:*  since $\\mathbf P$ is real symmetric, it suffices to chec k $\\mathbf P^2= \\mathbf I$ (i.e. an involution), so \n",
    "$\\mathbf P^2 = \\mathbf I -2\\mathbf {ww}^T-2\\mathbf {ww}^T + 4\\mathbf w \\big(\\mathbf w^T\\mathbf w\\big)\\mathbf {w}^T = \\mathbf I - 4 \\mathbf {ww}^T +4 \\mathbf {ww}^T =\\mathbf I$   \n",
    "note: this confirms that $\\mathbf P$ is real orthogonal, and making use of this we know all eigenvalues of $\\mathbf P$ are on the unit circle -- but in fact +1 or -1 since $\\mathbf P$ is involutive-- then taking the trace we see  \n",
    "$\\text{trace}\\big(\\mathbf P\\big)= \\text{trace}\\big(\\mathbf I\\big) + - 2\\text{trace}\\big(\\mathbf {ww}^T\\big)= n -2$  which means that $\\mathbf P$ has (n-1) eigenvalues of 1 and 1 eigenvalue of (-1)-- there are of course many ways at this result. But this also means that $\\det\\big(\\mathbf P\\big) = -1$ which is something we will exploit as a corollary to ex 7.3.9  \n",
    "\n",
    "**(b)**  Prove that multiplication by $\\mathbf P$ is a reflection through the space orthogonal to $\\mathbf w$.  I.e. if $\\mathbf v = c\\mathbf w + \\mathbf w'$ for $\\mathbf w'\\perp \\mathbf w$, then \n",
    "$\\mathbf P\\mathbf v = c\\mathbf P\\mathbf w + \\mathbf P\\mathbf w' = c\\big(\\mathbf I - 2\\mathbf {ww}^T\\big)\\mathbf w +\\big(\\mathbf I - 2\\mathbf {ww}^T\\big)\\mathbf w'= c\\big(\\mathbf w - 2\\mathbf w\\big) + \\mathbf I\\mathbf w'+ \\mathbf 0 = -c\\mathbf w + \\mathbf w' $     \n",
    "\n",
    "**(c)** let $\\mathbf x$ and $\\mathbf y$ be arbitrary vectors in $\\mathbb R^n$ with the same length (using standard metric).  \n",
    "Select a vector $\\mathbf w$ such that $\\mathbf P = \\mathbf I - 2\\mathbf {ww}^T$ is orthogonal and $\\mathbf P\\mathbf x = \\mathbf y$.  (Note this argues for existence of $\\mathbf P$ *not* uniqueness.)  \n",
    "\n",
    "in the case that $\\mathbf x = \\mathbf 0$ then $\\mathbf y$ is necessarily zero as well (e.g. because it has same length as $\\mathbf x$). \n",
    "\n",
    "*remark:* geometrically this is rather interesting to think about in the case 2 dimensional case  \n",
    "\n",
    "(i) if $\\mathbf x= \\mathbf y $ then selecting $\\mathbf w=\\mathbf 0$ gives the result and we are done    \n",
    "(ii) if $\\mathbf x =-\\mathbf y$ then selecting $\\mathbf w = \\mathbf x$ gives the result and we're done  \n",
    "(iii) since $\\mathbf x$ and $\\mathbf y$ have the same lengths and we are working in reals, in all other cases we must have $\\mathbf y$ and $\\mathbf x$ linearly independent.  \n",
    "\n",
    "so consider using part b of this exercise, consider     \n",
    "$\\mathbf x = c\\mathbf w + \\mathbf w'$  and $\\mathbf y = \\mathbf P \\mathbf x = -c\\mathbf w + \\mathbf w'$  \n",
    "\n",
    "then  we have  \n",
    "\n",
    "$\\mathbf x - \\mathbf y = 2c\\mathbf w$  or  \n",
    "$\\mathbf w = \\frac{1}{2c}\\big(\\mathbf x - \\mathbf y\\big)$  \n",
    "and since $\\mathbf w$ has length one this implies  \n",
    "$\\big \\vert c \\big \\vert = \\frac{1}{2}\\big \\Vert \\mathbf x - \\mathbf y\\big \\Vert_2$  \n",
    "the choice of sign for $c$ is arbitrary, so we may select the case of positive $c$ i.e.   \n",
    "$\\mathbf w = \\big(\\mathbf x - \\mathbf y\\big)\\cdot \\big \\Vert \\mathbf x - \\mathbf y \\big \\Vert_2^{-1}$  \n",
    "\n",
    "a quick check then shows that \n",
    "$\\mathbf w' = \\mathbf x + \\mathbf y$ meets the orthogonality requirements and that \n",
    "$\\mathbf x = c\\mathbf w + \\mathbf w'$ and  \n",
    "$\\mathbf y = -c\\mathbf w + \\mathbf w'$  \n",
    "which completes the requirements for this problem  \n",
    "\n",
    "*remark*  \n",
    "There is an interesting and near obvious parallel that comes up when dealing with e.g. iid standard normal r.v.'s $X$ and $Y$.  If we consider  \n",
    "\n",
    "$A=\\frac{1}{\\sqrt{2}}(X+Y)$ and $B=\\frac{1}{\\sqrt{2}}(X-Y)$, these too are iid standard normals  \n",
    "(e.g. recall sums of independent normals give a normal r.v., then check the covariance between $A$ and $B$)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "3+3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*the below works through this in some detail.  It may be skipped.*  \n",
    "The below works through the above in blocked detail, starting with the 'hyper vector' (page 96) given by $\\mathbf B$, though in this case we can be sure that $\\mathbf B$ is real invertible matrix (e.g. created via the basis creation algorithm).  So consider  \n",
    "\n",
    "$\\mathbf B = \n",
    "\\bigg[\\begin{array}{c|c|c|c|c} \n",
    "\\mathbf x  & \\mathbf y & \\mathbf b_{3} & \\cdots & \\mathbf b_n \n",
    "\\end{array}\\bigg]$\n",
    "\n",
    "where $\\det\\big(\\mathbf B\\big) \\neq 0$  \n",
    "(technically invertibility of $\\mathbf B$ is not used in what follows, however we selected it to have a spanning set of linearly independent vectors so as to fit with other common operations on a 'hypervector')  \n",
    "\n",
    "now consider the linear map  \n",
    "\n",
    "$ A :=\\begin{bmatrix}1 & 1 &\\mathbf 0^T \\\\ -1 & 1 & \\mathbf 0^T \\\\ \\mathbf 0 & \\mathbf 0 & \\mathbf I_{n-2}\\end{bmatrix}$   \n",
    "\n",
    "multiplying on the right of $\\mathbf B$ gives  \n",
    "$\\mathbf B  A = \\bigg[\\begin{array}{c|c|c|c|c} \n",
    "\\mathbf x -\\mathbf y  &\\mathbf x + \\mathbf y & \\mathbf b_{3} & \\cdots & \\mathbf b_n \n",
    "\\end{array}\\bigg] = \\bigg[\\begin{array}{c|c|c|c|c} \n",
    "\\alpha\\mathbf w  & \\mathbf w_2 & \\mathbf b_{3} & \\cdots & \\mathbf b_n \n",
    "\\end{array}\\bigg] $  \n",
    "\n",
    "where $\\alpha \\mathbf w^T\\mathbf w_2=\\big(\\mathbf x-\\mathbf y\\big)^T\\big(\\mathbf x + \\mathbf y\\big) = \\mathbf x^T\\mathbf x + \\mathbf x^T \\mathbf y -\\mathbf y^T \\mathbf x -\\mathbf y^T\\mathbf y = 0$  \n",
    "by symmetry of the real (inner) dot product and the fact that $\\mathbf x$ and $\\mathbf y$ have the same lengths. Hence we know that $\\big(\\mathbf x+\\mathbf y\\big)$ and $\\big(\\mathbf x - \\mathbf y\\big)$ are orthogonal, and $\\mathbf w$ has norm one, hence has some scalar $\\alpha$ in front of it-- and of course $\\vert \\alpha \\vert =\\big\\Vert \\mathbf x - \\mathbf y\\big \\Vert_2^{-1}$ which agrees with what we had above.  \n",
    "\n",
    "inverting $A$ gives  \n",
    "$\\bigg[\\begin{array}{c|c|c} \n",
    "\\mathbf x  & \\mathbf y & *\n",
    "\\end{array}\\bigg] $  \n",
    "$ = \\bigg[\\begin{array}{c|c|c} \n",
    "\\mathbf x -\\mathbf y  &\\mathbf x + \\mathbf y & * \n",
    "\\end{array}\\bigg] \\mathbf A^{-1}$  \n",
    "$=\\bigg[\\begin{array}{c|c|c} \n",
    "\\mathbf x -\\mathbf y  &\\mathbf x + \\mathbf y & * \n",
    "\\end{array}\\bigg]\\begin{bmatrix}\\frac{1}{2} & \\frac{-1}{2} &\\mathbf 0^T \\\\ \\frac{1}{2} & \\frac{1}{2} & \\mathbf 0^T \\\\ \\mathbf 0 & \\mathbf 0 & \\mathbf I_{n-2}\\end{bmatrix}$  \n",
    "$=\\bigg[\\begin{array}{c|c|c} \n",
    "\\alpha \\mathbf w  &\\mathbf x + \\mathbf y & *  \n",
    "\\end{array}\\bigg]\\begin{bmatrix}\\frac{1}{2} & \\frac{-1}{2} &\\mathbf 0^T \\\\ \\frac{1}{2} & \\frac{1}{2} & \\mathbf 0^T \\\\ \\mathbf 0 & \\mathbf 0 & \\mathbf I_{n-2}\\end{bmatrix}    \n",
    "=\\bigg[\\begin{array}{c|c|c} \n",
    "\\frac{1}{2}\\alpha\\mathbf w +\\frac{1}{2}\\beta \\mathbf w_2  &-\\frac{1}{2}\\alpha\\mathbf w +\\frac{1}{2}\\beta \\mathbf w_2 & *\n",
    "\\end{array}\\bigg] $  \n",
    "\n",
    "using a change of variables since the length of $\\mathbf w'$ is not specified, consider  \n",
    "$\\mathbf w' := \\frac{1}{2}\\beta \\mathbf w_2$   \n",
    "$\\mathbf x = c \\mathbf w +  \\mathbf w'$  and  \n",
    "$\\mathbf y = -c \\mathbf w +  \\mathbf w'$  \n",
    "as required in this problem  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ex 7.3.9**  \n",
    "considering any orthogonal matrix $\\mathbf Q \\in \\mathbb R^\\text{n x n}$  (where n is some natural number $\\geq 2$)  \n",
    "\n",
    "using results from ex 7.3.8, with orthgonal matrices of the form $\\mathbf P^{(i)} = \\mathbf I -2\\mathbf w_i \\mathbf w_i^T$  \n",
    "we can attack the first column vector $\\mathbf q_1$ so $\\mathbf P^{(1)}\\mathbf q_1 = \\mathbf e_1$ (with standard basis vector $\\mathbf e_1$)  \n",
    "\n",
    "\n",
    "$\\mathbf P^{(1)}\\mathbf Q= \\begin{bmatrix} 1 & \\mathbf 0^T \\\\ \\mathbf 0 & \\mathbf Q^{(2)} \\end{bmatrix}$    \n",
    "where we use the fact that the matrices on the LHS are in the Orthogonal group, so their product must be as well.  The first column of the RHS is $\\mathbf e_1$ by design.  But e.g. from ex 7.2.5, we know that the first row of an orthogonal matrix has norm 1 as well, and since the top left component of the RHS is 1, this implies all other components on the first row are zero and hence the first row is given by $\\mathbf e_1^T$.  Examining the blocked structure (and the fact that the RHS must be orthogonal) we see  $\\big(\\mathbf Q^{(2)}\\big)^T \\mathbf Q^{(2)} = \\mathbf I_{n-1}$, i.e. $\\mathbf Q^{(2)}$ is necessarily orthogonal as well.    \n",
    "\n",
    "To finish this off we may formalize with induction, but it seems preferable to work through this by recursing on the smaller subproblem of making the first column of $\\mathbf Q^{(2)}$ become the first standard basis vector for an n-1 dimensional space, i.e. by applying  \n",
    "$\\begin{bmatrix} 1 & \\mathbf 0^T \\\\ \\mathbf 0 & \\mathbf P^{(2)} \\end{bmatrix}\\mathbf P^{(1)}\\mathbf Q=\\begin{bmatrix} 1 & \\mathbf 0^T \\\\ \\mathbf 0 & \\mathbf P^{(2)} \\end{bmatrix}\\begin{bmatrix} 1 & \\mathbf 0^T \\\\ \\mathbf 0 & \\mathbf Q^{(2)} \\end{bmatrix}=\\begin{bmatrix} 1 & \\mathbf 0^T \\\\ \\mathbf 0 & \\mathbf P^{(2)}\\mathbf Q^{(2)} \\end{bmatrix}$  \n",
    "where this new blocked matrix is necessarily orthogonal as well.  The $\\text{n-1 x n-1}$ submatrix in the bottom corner is given by  \n",
    "$\\mathbf P^{(2)}\\mathbf Q^{(2)} =   \\begin{bmatrix} 1 & \\mathbf 0^T \\\\ \\mathbf 0 & \\mathbf Q^{(3)} \\end{bmatrix}$  \n",
    "\n",
    "for avoidance of doubt notice:  \n",
    "$\\begin{bmatrix} 1 & \\mathbf 0^T \\\\ \\mathbf 0 & \\mathbf P^{(2)} \\end{bmatrix}= \\mathbf I - 2\\mathbf {xx}^T$  where $\\mathbf x = \\sum_{k=2}^n \\alpha_k\\mathbf e_k$ so the above blocked matrix is still conforming in structure (i.e. it is still Householder, as we will shortly observe).  \n",
    "\n",
    "Then we keep repeating by attacking the first column of $\\mathbf Q^{(r)}$ at each the $r$th iteration and map it to $\\mathbf e_1 \\in \\mathbb R^{n-r+1}$.  At each pass, we make incremental progress and can assert that the columns $j\\in\\{1,2,...,r\\}$ of the RHS are $\\mathbf e_j \\in \\mathbb R^n$  and hence after at most $n$ applications we have recovered the identity matrix. That is, we've proven, for any orthogonal $\\mathbf Q \\in \\mathbf R^\\text{n x n}$,  \n",
    "\n",
    "$\\mathbf P^{(k)}...\\mathbf P^{(2)}\\mathbf P^{(1)}\\mathbf Q = \\mathbf I$ or  \n",
    "$\\mathbf Q=\\big(\\mathbf P^{(k)}...\\mathbf P^{(2)}\\mathbf P^{(1)}\\big)^T$  \n",
    "for some $1\\leq k \\leq n$ \n",
    "\n",
    "Matrices of the form $\\mathbf P^{(i)}$ are known as Householder Matrix / Householder Reflections, which in addition to being geometrically interesting, have uses in numerical linear algebra   \n",
    "\n",
    "**corollary**  \n",
    "These Householder matrices, i.e. matrices of the form $\\mathbf P^{(i)}$ are the *generators* of the orthogonal group.  In particular this tells us that if $\\det\\big(\\mathbf Q\\big )=1$ it is in $\\text{SO}(n)$ and it may be decomposed into in an even number of products of $\\mathbf P^{(i)}\\neq \\mathbf I$, so $\\det\\big(\\mathbf P^{(i)}\\big) = -1$ .  It we examine two of these at at time, we have  \n",
    "\n",
    "$\\mathbf P^{(i)}\\mathbf P^{(j)} = \\big(\\mathbf I -2\\mathbf w_1\\mathbf w_1^T\\big)\\big(\\mathbf I -2\\mathbf w_2\\mathbf w_2^T\\big) $  \n",
    "\n",
    "in the case of dimension $n\\geq 3$  since $\\mathbf P^{(i)}$  and $\\mathbf P^{(j)}$  each have some $\\mathbf w_i$ and $\\mathbf w_j$, but there are at least 3 linearly independent vectors in this space, with some 3rd linear independent vector $\\mathbf y$ (use e.g. the basis creation algorithm from chapter 3 to find it), and after running Gram Schmidt we have $\\mathbf y\\mapsto \\mathbf z$ such that $\\mathbf z$ is orthonormal to $\\mathbf w_i$ and $\\mathbf w_j$,  then consider \n",
    "\n",
    "$\\mathbf P^{(i)}(\\tau) = \\mathbf I - 2\\mathbf v_i(\\tau)\\mathbf v_i(\\tau)^T$  \n",
    "where for $\\tau \\in [0,1]$  \n",
    "$\\mathbf v_i(\\tau) = \\sqrt{1-\\tau}\\cdot\\mathbf w_i + \\sqrt{\\tau}\\cdot \\mathbf z$  \n",
    "and  \n",
    "$\\big\\Vert \\mathbf v_i(\\tau)\\big\\Vert_2^2 = (1-\\tau)\\cdot\\big\\Vert \\mathbf w_i \\big\\Vert_2^2 +\\tau\\cdot\\big\\Vert \\mathbf z \\big\\Vert_2^2+ 2\\sqrt{\\tau(1-\\tau)}\\cdot\\mathbf w_i^T\\mathbf z = (1-\\tau)\\cdot 1 + \\tau\\cdot 1 + 0 = 1$  \n",
    "so $\\mathbf v_i(\\tau)$ is unit length and thus $\\mathbf P^{(i)}(\\tau)$ is a Householder matrix for all $\\tau$ in our domain.  \n",
    "\n",
    "This then implies at $\\tau =0$  \n",
    "$\\mathbf P^{(i)}(0)\\mathbf P^{(j)}(0) = \\mathbf P^{(i)}\\mathbf P^{(j)}$  \n",
    "and at $\\tau = 1$  \n",
    "$\\mathbf P^{(i)}(1)\\mathbf P^{(j)}(1) = \\big(\\mathbf I - 2\\mathbf {zz}^T\\big)^2 = \\mathbf I$  \n",
    "\n",
    "hence any arbitrary pair of Householder matrices in dimension $n\\geq 3$ is path connected to the identity.  Using results from ex 2.misc.8 we can then infer that any matrix generated by even number $r$ Householder matrices is path connected to the identity. *And* we can infer that $r$ is even *iff* the determinant of an orthogonal matrix is 1-- which is to say we've proven that $\\text{SO}(n)$ is path connected (since every element is path connected to the identity and this is an equivalence relation).    \n",
    "\n",
    "Note: the above does not consider the $n=2$ case (and the $n=1$ case trivially holds true).  For 2 dimensions there is a simple and direct argument for path connectedness of $\\text{SO(n)}$ for the $\\text{2 x 2}$  case, in particular write out any matrix (e.g. revisiting Artin chapter 5 if needed) as   \n",
    "\n",
    "$\\mathbf Q_\\theta = \\begin{bmatrix} \\cos(\\theta) & -\\sin(\\theta) \\\\ \\sin(\\theta) & \\cos(\\theta) \\end{bmatrix} $   \n",
    "for $\\theta \\in [0,2\\pi)$  \n",
    "and directly consider the path given by any $\\mathbf Q_\\theta \\to \\mathbf I$ as $\\theta \\to 0$  \n",
    "\n",
    "**closing remark on uniqueness of real rotation matrix representation in 2 x 2 case**  \n",
    "consider that all matrices in SO(2) come in the form  \n",
    "$\\mathbf Q_\\theta = \\begin{bmatrix} \\cos(\\theta) & -\\sin(\\theta) \\\\ \\sin(\\theta) & \\cos(\\theta) \\end{bmatrix} $   \n",
    "\n",
    "by first supposing we fix the first row as \n",
    "\n",
    "$\\begin{bmatrix} \\mathbf v_1^T \\end{bmatrix} $   \n",
    "where $\\big\\Vert \\mathbf v_1\\big \\Vert_2 =1$  \n",
    "then there is exactly one linearly independent vector $\\mathbf v_2$ in the nullspace by rank-nullity.  \n",
    "\n",
    "In fact we have 2 unique homongenous solutions to $\\mathbf v_1^T\\mathbf x = 0$     \n",
    "when we insist on $\\big \\Vert \\mathbf x\\big \\Vert_2 =1$ \n",
    "i.e. our choice $\\mathbf v_2$ is unique except it may be rescaled by +1 or -1.  The selection of which scalar is then fixed when we require \n",
    "$\\det\\left(\\begin{bmatrix} \\mathbf v_1^T\\\\ \\mathbf v_2^T \\end{bmatrix}\\right) =1$   \n",
    "\n",
    "working backwards, we can be sure that \n",
    "$\\mathbf v_1^T =\\begin{bmatrix} \\cos(\\theta) & -\\sin(\\theta)\\end{bmatrix} $   \n",
    "since the sum of squares is one and the sign of each component enforces a quadrant in the unit circle and the modulus of sine (and cosine) are surjective and injective maps to $[0,1]$, once we've specified a quadrant.  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
