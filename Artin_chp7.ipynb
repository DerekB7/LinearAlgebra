{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "add complex case of Sylvester's Law of Intertia  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ex 7.1.6**  \n",
    "let $\\langle, \\rangle$ be a symmetric bilinear form on a vector space $V$ over a field $F$ not of characteristic 2.   \n",
    "The function $q: V\\longrightarrow F$ defined by   \n",
    "$q\\big(\\mathbf v\\big) = \\langle \\mathbf v, \\mathbf v \\rangle$  \n",
    "is called the **quadratic form** associated to the bilinear form.  Show how to recover the bilinear form from the quadratic form.  \n",
    "\n",
    "$q\\big(\\mathbf v + \\mathbf w\\big) $  \n",
    "$= \\langle \\mathbf v +\\mathbf w, \\mathbf v+\\mathbf w \\rangle $  \n",
    "$= \\langle \\mathbf w , \\mathbf v+\\mathbf w \\rangle + \\langle \\mathbf v , \\mathbf v+\\mathbf w \\rangle$  \n",
    "$= \\langle \\mathbf w , \\mathbf v \\rangle  +\\langle \\mathbf w , \\mathbf w \\rangle + \\langle \\mathbf v , \\mathbf v \\rangle + \\langle \\mathbf v , \\mathbf w \\rangle$  \n",
    "$= \\Big(\\langle \\mathbf w , \\mathbf v \\rangle +  \\langle \\mathbf v , \\mathbf w \\rangle\\Big) +\\Big(\\langle \\mathbf w , \\mathbf w \\rangle + \\langle \\mathbf v , \\mathbf v \\rangle\\Big)$  \n",
    "$= 2\\Big(\\langle \\mathbf w , \\mathbf v \\rangle \\Big) +q\\big(\\mathbf w, \\mathbf w\\big) + q\\big(\\mathbf v, \\mathbf v\\big)$  \n",
    "\n",
    "thus  \n",
    "$\\langle \\mathbf w , \\mathbf v \\rangle = \\frac{1}{2}\\Big(q\\big(\\mathbf v + \\mathbf w\\big) - q\\big(\\mathbf w, \\mathbf w\\big) - q\\big(\\mathbf v, \\mathbf v\\big)\\Big)$  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ex 7.3.8**  \n",
    "for some unit vector $\\mathbf w \\in \\mathbb R^n$  \n",
    "**(a)** prove that the matrix $\\mathbf P = \\mathbf I - 2\\mathbf {ww}^T$ is orthgonal  \n",
    "*by inspection or diagonalization, or:*  since $\\mathbf P$ is real symmetric, it suffices to chec k $\\mathbf P^2= \\mathbf I$ (i.e. an involution), so \n",
    "$\\mathbf P^2 = \\mathbf I -2\\mathbf {ww}^T-2\\mathbf {ww}^T + 4\\mathbf w \\big(\\mathbf w^T\\mathbf w\\big)\\mathbf {w}^T = \\mathbf I - 4 \\mathbf {ww}^T +4 \\mathbf {ww}^T =\\mathbf I$   \n",
    "note: this confirms that $\\mathbf P$ is real orthogonal, and making use of this we know all eigenvalues of $\\mathbf P$ are on the unit circle -- but in fact +1 or -1 since $\\mathbf P$ is involutive-- then taking the trace we see  \n",
    "$\\text{trace}\\big(\\mathbf P\\big)= \\text{trace}\\big(\\mathbf I\\big) + - 2\\cdot\\text{trace}\\big(\\mathbf {ww}^T\\big)= n -2$  which means that $\\mathbf P$ has $(n-1)$ eigenvalues of $(+1)$ and $1$ eigenvalue of $(-1)$-- there are of course many ways at this result. But this also means that $\\det\\big(\\mathbf P\\big) = -1$ which is something we will exploit as a corollary to ex 7.3.9  \n",
    "\n",
    "**(b)**  Prove that multiplication by $\\mathbf P$ is a reflection through the space orthogonal to $\\mathbf w$.  I.e. if $\\mathbf v = c\\mathbf w + \\mathbf w'$ for $\\mathbf w'\\perp \\mathbf w$, then \n",
    "$\\mathbf P\\mathbf v = c\\mathbf P\\mathbf w + \\mathbf P\\mathbf w' = c\\big(\\mathbf I - 2\\mathbf {ww}^T\\big)\\mathbf w +\\big(\\mathbf I - 2\\mathbf {ww}^T\\big)\\mathbf w'= c\\big(\\mathbf w - 2\\mathbf w\\big) + \\mathbf I\\mathbf w'+ \\mathbf 0 = -c\\mathbf w + \\mathbf w' $     \n",
    "\n",
    "**(c)** let $\\mathbf x$ and $\\mathbf y$ be arbitrary vectors in $\\mathbb R^n$ with the same length (using standard metric).  \n",
    "Select a vector $\\mathbf w$ such that $\\mathbf P = \\mathbf I - 2\\mathbf {ww}^T$ is orthogonal and $\\mathbf P\\mathbf x = \\mathbf y$.  (Note this argues for existence of $\\mathbf P$ *not* uniqueness.)  \n",
    "\n",
    "in the case that $\\mathbf x = \\mathbf 0$ then $\\mathbf y$ is necessarily zero as well (e.g. because it has same length as $\\mathbf x$). \n",
    "\n",
    "*remark:* geometrically this is rather interesting to think about in the case 2 dimensional case  \n",
    "\n",
    "(i) if $\\mathbf x= \\mathbf y $ then selecting $\\mathbf w\\perp \\mathbf x$ gives the result and we are done  \n",
    "(ii) if $\\mathbf x =-\\mathbf y$ then selecting $\\mathbf w = \\mathbf x$ gives the result and we're done  \n",
    "(iii) since $\\mathbf x$ and $\\mathbf y$ have the same lengths and we are working in reals, in all other cases we must have $\\mathbf y$ and $\\mathbf x$ linearly independent.  \n",
    "\n",
    "so consider using part b of this exercise, consider     \n",
    "$\\mathbf x = c\\mathbf w + \\mathbf w'$  and $\\mathbf y = \\mathbf P \\mathbf x = -c\\mathbf w + \\mathbf w'$  \n",
    "\n",
    "then  we have  \n",
    "\n",
    "$\\mathbf x - \\mathbf y = 2c\\mathbf w$  or  \n",
    "$\\mathbf w = \\frac{1}{2c}\\big(\\mathbf x - \\mathbf y\\big)$  \n",
    "and since $\\mathbf w$ has length one this implies  \n",
    "$\\big \\vert c \\big \\vert = \\frac{1}{2}\\big \\Vert \\mathbf x - \\mathbf y\\big \\Vert_2$  \n",
    "the choice of sign for $c$ is arbitrary, so we may select the case of positive $c$ i.e.   \n",
    "$\\mathbf w = \\big(\\mathbf x - \\mathbf y\\big)\\cdot \\big \\Vert \\mathbf x - \\mathbf y \\big \\Vert_2^{-1}$  \n",
    "\n",
    "a quick check then shows that \n",
    "$\\mathbf w' = \\mathbf x + \\mathbf y$ meets the orthogonality requirements and that \n",
    "$\\mathbf x = c\\mathbf w + \\mathbf w'$ and  \n",
    "$\\mathbf y = -c\\mathbf w + \\mathbf w'$  \n",
    "which completes the requirements for this problem  \n",
    "\n",
    "*remark*  \n",
    "There is an interesting and near obvious parallel that comes up when dealing with e.g. iid standard normal r.v.'s $X$ and $Y$.  If we consider  \n",
    "\n",
    "$A=\\frac{1}{\\sqrt{2}}(X+Y)$ and $B=\\frac{1}{\\sqrt{2}}(X-Y)$, these too are iid standard normals  \n",
    "(e.g. recall sums of independent normals give a normal r.v., then check the covariance between $A$ and $B$)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*the below works through this in some detail.  It may be skipped.*  \n",
    "The below works through the above in blocked detail, starting with the 'hyper vector' (page 96) given by $\\mathbf B$, though in this case we can be sure that $\\mathbf B$ is real invertible matrix (e.g. created via the basis creation algorithm).  So consider  \n",
    "\n",
    "$\\mathbf B = \n",
    "\\bigg[\\begin{array}{c|c|c|c|c} \n",
    "\\mathbf x  & \\mathbf y & \\mathbf b_{3} & \\cdots & \\mathbf b_n \n",
    "\\end{array}\\bigg]$\n",
    "\n",
    "where $\\det\\big(\\mathbf B\\big) \\neq 0$  \n",
    "(technically invertibility of $\\mathbf B$ is not used in what follows, however we selected it to have a spanning set of linearly independent vectors so as to fit with other common operations on a 'hypervector')  \n",
    "\n",
    "now consider the linear map  \n",
    "\n",
    "$ A :=\\begin{bmatrix}1 & 1 &\\mathbf 0^T \\\\ -1 & 1 & \\mathbf 0^T \\\\ \\mathbf 0 & \\mathbf 0 & \\mathbf I_{n-2}\\end{bmatrix}$   \n",
    "\n",
    "multiplying on the right of $\\mathbf B$ gives  \n",
    "$\\mathbf B  A = \\bigg[\\begin{array}{c|c|c|c|c} \n",
    "\\mathbf x -\\mathbf y  &\\mathbf x + \\mathbf y & \\mathbf b_{3} & \\cdots & \\mathbf b_n \n",
    "\\end{array}\\bigg] = \\bigg[\\begin{array}{c|c|c|c|c} \n",
    "\\alpha\\mathbf w  & \\mathbf w_2 & \\mathbf b_{3} & \\cdots & \\mathbf b_n \n",
    "\\end{array}\\bigg] $  \n",
    "\n",
    "where $\\alpha \\mathbf w^T\\mathbf w_2=\\big(\\mathbf x-\\mathbf y\\big)^T\\big(\\mathbf x + \\mathbf y\\big) = \\mathbf x^T\\mathbf x + \\mathbf x^T \\mathbf y -\\mathbf y^T \\mathbf x -\\mathbf y^T\\mathbf y = 0$  \n",
    "by symmetry of the real (inner) dot product and the fact that $\\mathbf x$ and $\\mathbf y$ have the same lengths. Hence we know that $\\big(\\mathbf x+\\mathbf y\\big)$ and $\\big(\\mathbf x - \\mathbf y\\big)$ are orthogonal, and $\\mathbf w$ has norm one, hence has some scalar $\\alpha$ in front of it-- and of course $\\vert \\alpha \\vert =\\big\\Vert \\mathbf x - \\mathbf y\\big \\Vert_2^{-1}$ which agrees with what we had above.  \n",
    "\n",
    "inverting $A$ gives  \n",
    "$\\bigg[\\begin{array}{c|c|c} \n",
    "\\mathbf x  & \\mathbf y & *\n",
    "\\end{array}\\bigg] $  \n",
    "$ = \\bigg[\\begin{array}{c|c|c} \n",
    "\\mathbf x -\\mathbf y  &\\mathbf x + \\mathbf y & * \n",
    "\\end{array}\\bigg] \\mathbf A^{-1}$  \n",
    "$=\\bigg[\\begin{array}{c|c|c} \n",
    "\\mathbf x -\\mathbf y  &\\mathbf x + \\mathbf y & * \n",
    "\\end{array}\\bigg]\\begin{bmatrix}\\frac{1}{2} & \\frac{-1}{2} &\\mathbf 0^T \\\\ \\frac{1}{2} & \\frac{1}{2} & \\mathbf 0^T \\\\ \\mathbf 0 & \\mathbf 0 & \\mathbf I_{n-2}\\end{bmatrix}$  \n",
    "$=\\bigg[\\begin{array}{c|c|c} \n",
    "\\alpha \\mathbf w  &\\mathbf w_2& *  \n",
    "\\end{array}\\bigg]\\begin{bmatrix}\\frac{1}{2} & \\frac{-1}{2} &\\mathbf 0^T \\\\ \\frac{1}{2} & \\frac{1}{2} & \\mathbf 0^T \\\\ \\mathbf 0 & \\mathbf 0 & \\mathbf I_{n-2}\\end{bmatrix}    \n",
    "=\\bigg[\\begin{array}{c|c|c} \n",
    "\\frac{1}{2}\\alpha\\mathbf w +\\frac{1}{2}\\beta \\mathbf w_2  &-\\frac{1}{2}\\alpha\\mathbf w +\\frac{1}{2}\\beta \\mathbf w_2 & *\n",
    "\\end{array}\\bigg] $  \n",
    "\n",
    "using a change of variables since the length of $\\mathbf w'$ is not specified, consider  \n",
    "$\\mathbf w' := \\frac{1}{2}\\beta \\mathbf w_2$   \n",
    "$\\mathbf x = c \\mathbf w +  \\mathbf w'$  and  \n",
    "$\\mathbf y = -c \\mathbf w +  \\mathbf w'$  \n",
    "as required in this problem  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**extending the result for Unitary Matrices:** if we were to work in $\\mathbb C$ with an eye to applying this to unitary matrices, the above wouldn't literally work as \n",
    "\n",
    "to simplify the calculations, assume WLOG that  \n",
    "$\\big \\Vert \\mathbf x \\big \\Vert_2 = \\big \\Vert \\mathbf y \\big \\Vert_2=1$   \n",
    "\n",
    "$\\big(\\mathbf x- \\mathbf y\\big)^* \\big(\\mathbf x+ \\mathbf y\\big) = \\mathbf x^*\\mathbf x+\\mathbf y^*\\mathbf y-\\mathbf y^*\\mathbf x+\\mathbf x^*\\mathbf y = 2\\cdot\\text{im}\\big(\\mathbf x^*\\mathbf y\\big)$   \n",
    "the result can be saved via the use of polarization, i.e. instead considering  \n",
    "$\\big(\\mathbf x- \\alpha\\mathbf y\\big)^* \\big(\\mathbf x+ \\alpha\\mathbf y\\big)$  \n",
    "if $\\mathbf x^*\\mathbf y=0$ (or in fact any real number), then select $\\alpha :=1$, otherwise selection \n",
    "$\\alpha = e^{i\\theta}$ where $\\theta$  is the polar angle of $\\big(\\mathbf x^*\\mathbf y\\big)^*=\\mathbf y^*\\mathbf x$ -- in words, this is the value of $\\mathbf x^*\\mathbf y$, with the length shrunk to one, and then conjugating the value so $\\alpha \\cdot \\mathbf x^*\\mathbf y = \\big \\vert \\mathbf x^*\\mathbf y\\big \\vert \\in \\mathbb R$ (positivity is an added bonus though not really needed)  \n",
    "\n",
    "so  \n",
    "$\\text{im}\\big(\\mathbf x^*\\alpha\\mathbf y\\big)=0\\longrightarrow \\big(\\mathbf x- \\alpha\\mathbf y\\big)^* \\big(\\mathbf x+ \\alpha\\mathbf y\\big)=0$  \n",
    "or by direct computation  \n",
    "$\\big(\\mathbf x- \\alpha\\mathbf y\\big)^* \\big(\\mathbf x+ \\alpha\\mathbf y\\big) = \\mathbf x^*\\mathbf x- \\big\\vert \\alpha\\big\\vert^2 \\cdot \\mathbf y^*\\mathbf y-(\\bar{\\alpha}\\cdot\\mathbf y)^*\\mathbf x+\\mathbf x^*(\\alpha\\cdot\\mathbf y)  = -\\big(\\mathbf x^* \\alpha\\cdot\\mathbf y\\big)^*+\\mathbf x^*\\alpha\\cdot \\mathbf y = -\\big(\\mathbf x^* \\alpha\\cdot\\mathbf y\\big)+\\mathbf x^*\\alpha\\cdot \\mathbf y =0 $  \n",
    "because taking the conjugate transpose of a real scalar gives that same real scalar   \n",
    "\n",
    "so select $\\mathbf w\\propto \\mathbf x-\\alpha\\mathbf y $  and revisit  \n",
    "$\\mathbf x = c\\mathbf w + \\mathbf w'$  and $\\alpha\\mathbf y = \\mathbf P \\mathbf x = -c\\mathbf w + \\mathbf w'$  \n",
    "to see  \n",
    "$\\mathbf x - \\alpha\\mathbf y = 2 c \\mathbf w$ as desired  \n",
    "and $\\mathbf w'$ is orthogonal to $\\mathbf w$ and its length isn't constrained to one, so we may rescale as needed, e.g. with  \n",
    "$c\\mathbf w =\\gamma(\\mathbf x-\\alpha\\mathbf y) $   \n",
    "$\\mathbf w' = \\gamma\\big(\\mathbf x+ \\alpha\\mathbf y\\big)$    \n",
    "$\\mathbf x = 2\\gamma \\mathbf x$ \n",
    "(implying $\\gamma =\\frac{1}{2}$)  \n",
    "\n",
    "$\\alpha \\mathbf y = -c\\mathbf w + \\mathbf w'= -\\gamma(\\mathbf x-\\alpha\\mathbf y) + \\gamma\\big(\\mathbf x+ \\alpha\\mathbf y\\big)=2\\gamma\\cdot\\alpha \\mathbf y$, again implying $\\gamma = \\frac{1}{2}$ so we are consistent.  \n",
    "\n",
    "*extension:*  \n",
    "if we follow the below ex 7.3.9 and we *could*, mostly, adapt it to unitary matrices, what it tell us is  \n",
    "with unitary matrices of the form $\\mathbf P^{(i)} = \\mathbf I -2\\mathbf w_i \\mathbf w_i^*$  \n",
    "we can attack the first column vector $\\mathbf q_1$ so $\\mathbf P^{(1)}\\mathbf q_1 = \\alpha_1\\mathbf e_1$ (with standard basis vector $\\mathbf e_1$)  \n",
    "\n",
    "\n",
    "$\\mathbf P^{(1)}\\mathbf Q= \\begin{bmatrix} \\alpha_1 & \\mathbf 0^T \\\\ \\mathbf 0 & \\mathbf Q^{(2)} \\end{bmatrix}$   and the ending would be  \n",
    "$\\mathbf P^{(k)}...\\mathbf P^{(2)}\\mathbf P^{(1)}\\mathbf Q = \\mathbf D$ where $\\mathbf D$ is diagonal and unitary, i.e. $d_{i,i} =\\alpha_i$  so  \n",
    "$ \\mathbf D^*\\mathbf P^{(k)}...\\mathbf P^{(2)}\\mathbf P^{(1)}\\mathbf Q = \\mathbf I$  \n",
    "\n",
    "path connectedness then follows for dimension $n\\geq$ with even $k$ by the same argument as below (and $\\mathbf D^*$ is easily connected to the identity by 'rotating' each diagonal component over the unit circle).  \n",
    "\n",
    "However the above portion isn't really needed when working in the complex plane.  We could prove directly that a single unitary Householder matrix is path connected to the identity by using a one-size fits all argument to show that *any* unitary matrix is path connected to the identity.  Making use of the normality of unitary matrices, we have    \n",
    "$\\mathbf Q = \\mathbf U\\mathbf \\Lambda \\mathbf U^*$  \n",
    "where $\\mathbf U$ is unitary and all eigenvalues are on the unit circle, so $\\mathbf \\Lambda$ is unitary and the product is unitary for any arbitrary $\\lambda_i$.  Then we merely need to 'rotate' the values of $\\lambda_i$ along the unit circle to be one at $\\tau =1$ implying $\\mathbf Q(1) = \\mathbf U\\mathbf I  \\mathbf U^* = \\mathbf I$  \n",
    "This is a much easier result than in the real orthogonal matrix case-- in such a case while the matrices are diagonalizable over $\\mathbb C$ we couldn't guarantee that any path to the identity stays real (and in fact if the determinant of a real orthogonal matrix is $-1$ this is impossible by e.g. intermediate value theorem, though the below shows that if the determinant is +1, then the result *is* possible)     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ex 7.3.9**  \n",
    "considering any orthogonal matrix $\\mathbf Q \\in \\mathbb R^\\text{n x n}$  (where n is some natural number $\\geq 2$)  \n",
    "\n",
    "using results from ex 7.3.8, with orthgonal matrices of the form $\\mathbf P^{(i)} = \\mathbf I -2\\mathbf w_i \\mathbf w_i^T$  \n",
    "we can attack the first column vector $\\mathbf q_1$ so $\\mathbf P^{(1)}\\mathbf q_1 = \\mathbf e_1$ (with standard basis vector $\\mathbf e_1$)  \n",
    "\n",
    "\n",
    "$\\mathbf P^{(1)}\\mathbf Q= \\begin{bmatrix} 1 & \\mathbf 0^T \\\\ \\mathbf 0 & \\mathbf Q^{(2)} \\end{bmatrix}$    \n",
    "where we use the fact that the matrices on the LHS are in the Orthogonal group, so their product must be as well.  The first column of the RHS is $\\mathbf e_1$ by design.  But e.g. from ex 7.2.5, we know that the first row of an orthogonal matrix has norm 1 as well, and since the top left component of the RHS is 1, this implies all other components on the first row are zero and hence the first row is given by $\\mathbf e_1^T$.  Examining the blocked structure (and the fact that the RHS must be orthogonal) we see  $\\big(\\mathbf Q^{(2)}\\big)^T \\mathbf Q^{(2)} = \\mathbf I_{n-1}$, i.e. $\\mathbf Q^{(2)}$ is necessarily orthogonal as well.    \n",
    "\n",
    "To finish this off we may formalize with induction, but it seems preferable to work through this by recursing on the smaller subproblem of making the first column of $\\mathbf Q^{(2)}$ become the first standard basis vector for an n-1 dimensional space, i.e. by applying  \n",
    "$\\begin{bmatrix} 1 & \\mathbf 0^T \\\\ \\mathbf 0 & \\mathbf P^{(2)} \\end{bmatrix}\\mathbf P^{(1)}\\mathbf Q=\\begin{bmatrix} 1 & \\mathbf 0^T \\\\ \\mathbf 0 & \\mathbf P^{(2)} \\end{bmatrix}\\begin{bmatrix} 1 & \\mathbf 0^T \\\\ \\mathbf 0 & \\mathbf Q^{(2)} \\end{bmatrix}=\\begin{bmatrix} 1 & \\mathbf 0^T \\\\ \\mathbf 0 & \\mathbf P^{(2)}\\mathbf Q^{(2)} \\end{bmatrix}$  \n",
    "where this new blocked matrix is necessarily orthogonal as well.  The $\\text{n-1 x n-1}$ submatrix in the bottom corner is given by  \n",
    "$\\mathbf P^{(2)}\\mathbf Q^{(2)} =   \\begin{bmatrix} 1 & \\mathbf 0^T \\\\ \\mathbf 0 & \\mathbf Q^{(3)} \\end{bmatrix}$  \n",
    "\n",
    "for avoidance of doubt notice:  \n",
    "$\\begin{bmatrix} 1 & \\mathbf 0^T \\\\ \\mathbf 0 & \\mathbf P^{(2)} \\end{bmatrix}= \\mathbf I - 2\\mathbf {xx}^T$  where $\\mathbf x = \\sum_{k=2}^n \\alpha_k\\mathbf e_k$ so the above blocked matrix is still conforming in structure (i.e. it is still Householder, as we will shortly observe).  \n",
    "\n",
    "Then we keep repeating by attacking the first column of $\\mathbf Q^{(r)}$ at each the $r$th iteration and map it to $\\mathbf e_1 \\in \\mathbb R^{n-r+1}$.  At each pass, we make incremental progress and can assert that the columns $j\\in\\{1,2,...,r\\}$ of the RHS are $\\mathbf e_j \\in \\mathbb R^n$  and hence after at most $n$ applications we have recovered the identity matrix. That is, we've proven, for any orthogonal $\\mathbf Q \\in \\mathbf R^\\text{n x n}$,  \n",
    "\n",
    "$\\mathbf P^{(k)}...\\mathbf P^{(2)}\\mathbf P^{(1)}\\mathbf Q = \\mathbf I$ or  \n",
    "$\\mathbf Q=\\big(\\mathbf P^{(k)}...\\mathbf P^{(2)}\\mathbf P^{(1)}\\big)^T$  \n",
    "for some $1\\leq k \\leq n$ \n",
    "\n",
    "Matrices of the form $\\mathbf P^{(i)}$ are known as Householder Matrix / Householder Reflections, which in addition to being geometrically interesting, have uses in numerical linear algebra   \n",
    "\n",
    "**note:**  \n",
    "if $k=n$ then the blocked multiplication implies the final Householder matrix is a simple reflection, i.e.    \n",
    "$\\mathbf P^{(n)} =\\begin{bmatrix} \\mathbf I_{n-1} & \\mathbf 0_{n-1} \\\\  \\mathbf 0_{n-1}^T & -1 \\end{bmatrix}$  \n",
    "\n",
    "this isn't directly useful now, but it is something we may exploit in section 4 of chapter 8.  \n",
    "\n",
    "**corollary**  \n",
    "These Householder matrices, i.e. matrices of the form $\\mathbf P^{(i)}$ are the *generators* of the orthogonal group.  In particular this tells us that if $\\det\\big(\\mathbf Q\\big )=1$ it is in $\\text{SO}(n)$ and it may be decomposed into in an even number of products of $\\mathbf P^{(i)}$, with each $\\det\\big(\\mathbf P^{(i)}\\big) = -1$ .  It we examine two of these at at time, we have  \n",
    "\n",
    "$\\mathbf P^{(i)}\\mathbf P^{(j)} = \\big(\\mathbf I -2\\mathbf w_1\\mathbf w_1^T\\big)\\big(\\mathbf I -2\\mathbf w_2\\mathbf w_2^T\\big) $  \n",
    "\n",
    "in the case of dimension $n\\geq 3$  since $\\mathbf P^{(i)}$  and $\\mathbf P^{(j)}$  each have some $\\mathbf w_i$ and $\\mathbf w_j$, but there are at least 3 linearly independent vectors in this space, with some 3rd linear independent vector $\\mathbf y$ (use e.g. the basis creation algorithm from chapter 3 to find it), and after running Gram Schmidt (with respect to the standard inner product) we have $\\mathbf y\\mapsto \\mathbf z$ such that $\\mathbf z$ is orthonormal to $\\mathbf w_i$ and $\\mathbf w_j$,  then consider \n",
    "\n",
    "$\\mathbf P^{(i)}(\\tau) = \\mathbf I - 2\\mathbf v_i(\\tau)\\mathbf v_i(\\tau)^T$  \n",
    "where for $\\tau \\in [0,1]$  \n",
    "$\\mathbf v_i(\\tau) = \\sqrt{1-\\tau}\\cdot\\mathbf w_i + \\sqrt{\\tau}\\cdot \\mathbf z$  \n",
    "and  \n",
    "$\\big\\Vert \\mathbf v_i(\\tau)\\big\\Vert_2^2 = (1-\\tau)\\cdot\\big\\Vert \\mathbf w_i \\big\\Vert_2^2 +\\tau\\cdot\\big\\Vert \\mathbf z \\big\\Vert_2^2+ 2\\sqrt{\\tau(1-\\tau)}\\cdot\\mathbf w_i^T\\mathbf z = (1-\\tau)\\cdot 1 + \\tau\\cdot 1 + 0 = 1$  \n",
    "so $\\mathbf v_i(\\tau)$ is unit length and thus $\\mathbf P^{(i)}(\\tau)$ is a Householder matrix for all $\\tau$ in our domain.  \n",
    "\n",
    "This then implies at $\\tau =0$  \n",
    "$\\mathbf P^{(i)}(0)\\mathbf P^{(j)}(0) = \\mathbf P^{(i)}\\mathbf P^{(j)}$  \n",
    "and at $\\tau = 1$  \n",
    "$\\mathbf P^{(i)}(1)\\mathbf P^{(j)}(1) = \\big(\\mathbf I - 2\\mathbf {zz}^T\\big)^2 = \\mathbf I$  \n",
    "\n",
    "hence any arbitrary pair of Householder matrices in dimension $n\\geq 3$ is path connected to the identity.  Using results from ex 2.misc.8 we can then infer that any matrix generated by even number $r$ Householder matrices is path connected to the identity. *And* we can infer that $r$ is even *iff* the determinant of an orthogonal matrix is 1-- which is to say we've proven that $\\text{SO}(n)$ is path connected (since every element is path connected to the identity and this is an equivalence relation).    \n",
    "\n",
    "Note: the above does not consider the $n=2$ case (and the $n=1$ case trivially holds true).  For 2 dimensions there is a simple and direct argument for path connectedness of $\\text{SO(n)}$ for the $\\text{2 x 2}$  case, in particular write out any matrix (e.g. revisiting Artin chapter 5 if needed) as   \n",
    "\n",
    "$\\mathbf Q_\\theta = \\begin{bmatrix} \\cos(\\theta) & -\\sin(\\theta) \\\\ \\sin(\\theta) & \\cos(\\theta) \\end{bmatrix} $   \n",
    "for $\\theta \\in [0,2\\pi)$  \n",
    "and directly consider the path given by any $\\mathbf Q_\\theta \\to \\mathbf I$ as $\\theta \\to 0$  \n",
    "\n",
    "**closing remark on uniqueness of real rotation matrix representation in 2 x 2 case**  \n",
    "consider that all matrices in SO(2) come in the form  \n",
    "$\\mathbf Q_\\theta = \\begin{bmatrix} \\cos(\\theta) & -\\sin(\\theta) \\\\ \\sin(\\theta) & \\cos(\\theta) \\end{bmatrix} $   \n",
    "\n",
    "by first supposing we fix the first row as \n",
    "\n",
    "$\\begin{bmatrix} \\mathbf v_1^T \\end{bmatrix} $   \n",
    "where $\\big\\Vert \\mathbf v_1\\big \\Vert_2 =1$  \n",
    "then there is exactly one linearly independent vector $\\mathbf v_2$ in the nullspace by rank-nullity.  \n",
    "\n",
    "In fact we have 2 unique homongenous solutions to $\\mathbf v_1^T\\mathbf x = 0$     \n",
    "when we insist on $\\big \\Vert \\mathbf x\\big \\Vert_2 =1$ \n",
    "i.e. our choice $\\mathbf v_2$ is unique except it may be rescaled by +1 or -1.  The selection of which scalar is then fixed when we require \n",
    "$\\det\\left(\\begin{bmatrix} \\mathbf v_1^T\\\\ \\mathbf v_2^T \\end{bmatrix}\\right) =1$   \n",
    "\n",
    "working backwards, we can be sure that \n",
    "$\\mathbf v_1^T =\\begin{bmatrix} \\cos(\\theta) & -\\sin(\\theta)\\end{bmatrix} $   \n",
    "since the sum of squares is one and the sign of each component enforces a quadrant in the unit circle and the modulus of sine (and cosine) are surjective and injective maps to $[0,1]$, once we've specified a quadrant.  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**extension**  \n",
    "consider any $\\mathbf B \\in GL_n\\big(\\mathbb C\\big)$  (or reals if preferred).  We multiply on the right by a diagonal matrix if needed so that the resulting matrix has columns with lenght (2 norm) of 1.  \n",
    "\n",
    "$\\mathbf A = \\mathbf B\\mathbf \\Sigma$   \n",
    "\n",
    "if we mimic the above blocked structure proof, we can apply at most n Householder matrices to reduce $\\mathbf A$ to an upper triangular matrix.  The composition of $n$ Householder matrices gives a unitary matrix.  Hence  \n",
    "\n",
    "$\\mathbf A = \\mathbf Q\\mathbf R$  or  \n",
    "$\\mathbf B = \\mathbf Q\\big(\\mathbf R\\mathbf \\Sigma^{-1}\\big) = \\mathbf Q \\mathbf R'$    \n",
    "\n",
    "\n",
    "this is a simple way of deriving the matrix form of QR factorization, and your author believes, this is close to one of the preferred numerical routines for computing QR factorization on a computer.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ex 7.4.4**  \n",
    "prove that if   \n",
    "$\\mathbf x^* \\mathbf A \\mathbf x \\in \\mathbb R$  for all $\\mathbf x \\in \\mathbb C^n\\longrightarrow \\mathbf A = \\mathbf A^*$  \n",
    "\n",
    "1.) write  \n",
    " $\\mathbf A = \\frac{1}{2}\\big(\\mathbf A + \\mathbf A^*\\big)+\\frac{1}{2}\\big(\\mathbf A - \\mathbf A^*\\big)= \\mathbf H + \\mathbf S$   \n",
    "i.e. decompose $\\mathbf A$ into a Hermitian and skew-Hermitian part  \n",
    "\n",
    "now we want to estimate the skew-Hermitian part.  \n",
    "\n",
    "so consider   \n",
    "$\\mathbf x^* \\mathbf A \\mathbf x \\in \\mathbb R \\longrightarrow \\mathbf x^* \\mathbf A \\mathbf x = \\big(\\mathbf x^* \\mathbf A \\mathbf x\\big)^* = \\mathbf x^* \\mathbf A^* \\mathbf x \\longrightarrow \\mathbf x^* \\mathbf A^* \\mathbf x -\\mathbf x^* \\mathbf A^* \\mathbf x = \\mathbf x^*\\big(\\mathbf A -\\mathbf A^*\\big) \\mathbf x = \\mathbf x^* \\big(2\\mathbf S\\big)\\mathbf x = 0$  \n",
    "\n",
    "This implies that any eigenvalue of the matrix $\\mathbf S$ must be zero.  We can finish by observing that skew-Hermitian matrices are diagonalizable since they are normal (normal matrices are developed in section 7 of this chapter), and hence $\\mathbf S$ is the zero matrix, or by considering the positive-definiteness of the (squared) Frobenius norm  \n",
    "$\\big \\Vert \\mathbf S\\big \\Vert_F^2 = \\text{trace}\\big(\\mathbf S^*\\mathbf S\\big) =  -1\\cdot\\text{trace}\\big(\\mathbf S^2\\big)= -1 \\cdot \\sum_{k=1}^n \\lambda_k^2 = -1 \\cdot \\sum_{k=1}^n 0 = 0\\longrightarrow \\mathbf S = \\mathbf 0$  \n",
    "\n",
    "since the skew Hermitian matrix is zero, we have  \n",
    "$\\mathbf A = \\frac{1}{2}\\big(\\mathbf A + \\mathbf A^*\\big)+0 = \\frac{1}{2}\\big(\\mathbf A + \\mathbf A^*\\big)\\longrightarrow \\frac{1}{2}\\mathbf A = \\frac{1}{2}\\mathbf A^*\\longrightarrow \\mathbf A = \\mathbf A^* $    \n",
    "so $\\mathbf A$ is Hermitian  \n",
    "\n",
    "\n",
    "*remark*  \n",
    "This also proves that for a matrix to be positive (semi)definite over a domain of $\\mathbb C^n$, it must be Hermitian (since having a real-nonegative quadratic form is a subset of having a real quadratic form)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7.4.16**  \n",
    "\n",
    "let $P$ be the (complex) vector space of polynomials with degree $\\leq n$  \n",
    "\n",
    "**(a)** show that  \n",
    "$\\langle f, g\\rangle = \\int_0^{2\\pi}\\bar{f(e^{i\\theta}})g(e^{i\\theta})d\\theta= \\int_0^{2\\pi}\\text{conj}\\big({f(e^{i\\theta}}\\big)g(e^{i\\theta})d\\theta$   \n",
    "is a positive definite hermitian form on $P$  \n",
    "(note the conjugation bar should be over all of $f(e^{i\\theta})$ though it doesn't seems to be rendering that way)  \n",
    "a positive definite hermitian form is equivalent to saying that we have an inner product in a complex space  \n",
    "\n",
    "so we confirm  \n",
    "(i) linearity in the second variable -- immediate  \n",
    "\n",
    "(ii) conjugate linearity under second variable --immediate  \n",
    "*note: this is the way Artin defines the inner product... the more common/standard definition is to linear in first argument and conjugate linear in the second argument*   \n",
    "\n",
    "(iii)  conjugate symmetry  \n",
    "${\\langle g, f\\rangle}=\\text{conj}\\big(\\langle f, g\\rangle\\big)$  \n",
    "$\\text{conj}\\big(\\langle f, g\\rangle\\big) = \\text{conj}\\big(\\int_0^{2\\pi}\\text{conj}\\big({f(e^{i\\theta}}\\big)g(e^{i\\theta})d\\theta\\big)= \\int_0^{2\\pi}\\text{conj}\\big({g(e^{i\\theta}}\\big)g(e^{i\\theta})d\\theta={\\langle g, f\\rangle}$  \n",
    "(writing this out as a limit of Riemann sums over real and imaginary parts can be helpful if needed)  \n",
    "\n",
    "(iv) we finally need to confirm this is positive definte / an actual inner product, i.e.  \n",
    "$\\langle g, g\\rangle \\geq 0$ with equality *iff* $g$ is the zero polynomial  \n",
    "\n",
    "first observe  \n",
    "$\\langle g, g\\rangle = \\int_0^{2\\pi}\\bar{g(e^{i\\theta}})g(e^{i\\theta})d\\theta= \\int_0^{2\\pi}\\big \\vert g(e^{i\\theta})\\big \\vert^2 d\\theta$  \n",
    "\n",
    "if $g$ is identically zero then the integral evaluates to zero.  Suppose $g$ is not identically zero, then it has at most $n$ distinct roots.  Using linearity, we have  \n",
    "\n",
    "$\\int_0^{2\\pi}\\big \\vert g(e^{i\\theta})\\big \\vert^2 d\\theta = \\sum_{k=0}^{n}\\int_{\\frac{k}{n+1}\\cdot 2\\pi}^{\\frac{k+1}{n+1}2\\pi}\\big \\vert g(e^{i\\theta})\\big \\vert^2 d\\theta$    \n",
    "\n",
    "where each of the n+1 intervals $[\\frac{k}{n+1}2\\pi,\\frac{k+1}{n+1}2\\pi]$  and being continuous (and real valued) $\\big \\vert g(e^{i\\theta})\\big \\vert^2$ has a minimum over each -- in all cases each the minimum is at least zero, and in at least one of them the minimum is strictly positive, thus the integral is strictly positive, as required.  \n",
    "**(b)**  \n",
    "find an orthonormal basis for this form.  We do this abstractly with the 'hyper-vector'  \n",
    "\n",
    "*remark:*  \n",
    "the below essentially works as written though there are a few zero vs one indexing issues  \n",
    "- - - - \n",
    "\n",
    "\n",
    "$\\mathbf B = \n",
    "\\bigg[\\begin{array}{c|c|c|c|c} \n",
    "\\mathbf b_0  & \\mathbf b_1 & \\mathbf b_2 & \\cdots & \\mathbf b_n \n",
    "\\end{array}\\bigg]=\\bigg[\\begin{array}{c|c|c|c|c} \n",
    "\\mathbf 1  & \\mathbf x & \\mathbf x^2 & \\cdots & \\mathbf x^n \n",
    "\\end{array}\\bigg]$  \n",
    "\n",
    "via applying Gram Schmidt we have  \n",
    "$\\mathbf q_0' = \\mathbf b_0\\longrightarrow \\mathbf q_0 = \\frac{1}{\\sqrt{2\\pi}}$  \n",
    "and for $k\\in\\{1,2,...,n\\}$  \n",
    "$\\mathbf q_k' = \\mathbf b_k - \\sum_{j=1}^{k-1}\\langle \\mathbf q_j, \\mathbf b_k \\rangle\\mathbf q_j$  \n",
    "(orthogonalization)  \n",
    "and  \n",
    "$\\mathbf q_k = \\alpha_k \\mathbf q_k' = \\frac{\\mathbf q_k'}{\\langle \\mathbf q_k', \\mathbf q_k' \\rangle^\\frac{1}{2}}=\\frac{\\mathbf q_k'}{\\big \\Vert \\mathbf q_k'\\big \\Vert_2}$  \n",
    "(normalization)  \n",
    "\n",
    "\n",
    "(i) this is an orthonormal sequence because if we select any $\\mathbf q_k$, then  \n",
    "direct checking of $\\mathbf q_0$ and $\\mathbf q_1$ tells us  \n",
    "$\\alpha_1 \\langle \\mathbf q_0,\\mathbf q_1\\rangle= \\langle \\mathbf q_0,\\mathbf q_1'\\rangle= \\langle \\mathbf q_0,\\mathbf b_1\\rangle- \\langle \\mathbf q_0, \\mathbf b_1 \\rangle\\cdot \\langle \\mathbf q_0,\\mathbf q_0\\rangle=\\langle \\mathbf q_0,\\mathbf b_1\\rangle- \\langle \\mathbf q_0, \\mathbf b_1\\rangle =0$  \n",
    "\n",
    "\n",
    "for $i \\neq k$  \n",
    "since conjugation doesn't change whether the result is zero or not we may confirm / assume WLOG that $i\\lt k$ due to conjugate symmetry of ${\\langle g, f\\rangle}=\\text{conj}\\big(\\langle f, g\\rangle\\big)$ \n",
    "we may also rescale the inner product by non-zero values -- this too doesn't change whether the inner product is zero   \n",
    "\n",
    "for $i=0, 1, 2,...., n-1$  \n",
    "   for $k = i+1,1,2,...,n$  \n",
    "$\\alpha_k \\langle \\mathbf q_i,\\mathbf q_k\\rangle= \\langle \\mathbf q_i,\\mathbf q_k'\\rangle= \\langle \\mathbf q_i,\\mathbf b_k\\rangle- \\sum_{j=1}^{k-1}\\langle \\mathbf q_j, \\mathbf b_k \\rangle\\cdot \\langle \\mathbf q_i,\\mathbf q_j\\rangle= \\langle \\mathbf q_i,\\mathbf b_k\\rangle- \\langle \\mathbf q_i, \\mathbf b_k \\rangle= 0$    \n",
    "noting that  \n",
    "$\\sum_{j=1}^{k-1}\\langle \\mathbf q_j, \\mathbf b_k \\rangle\\cdot \\langle \\mathbf q_i,\\mathbf q_j\\rangle = \\langle \\mathbf q_i, \\mathbf b_k \\rangle$  \n",
    "because $\\langle \\mathbf q_i,\\mathbf q_j\\rangle=1$ if $i=j$ and \n",
    "$\\langle \\mathbf q_i,\\mathbf q_j\\rangle=0$ if $i\\neq j$ by (in particular $i\\lt j$) induction hypothesis  \n",
    "\n",
    "if we revisit the equation  \n",
    "$\\mathbf q_k' = \\mathbf b_k - \\sum_{j=1}^{k-1}\\langle \\mathbf q_j, \\mathbf b_k \\rangle\\mathbf q_j $  \n",
    "$\\longrightarrow  \\mathbf b_k = \\mathbf q_k' + \\sum_{j=1}^{k-1}\\langle \\mathbf q_j, \\mathbf b_k \\rangle\\mathbf q_j$  \n",
    "or  \n",
    "$\\mathbf b_k = \\gamma_k \\cdot \\mathbf q_k + \\sum_{j=1}^{k-1}\\langle \\mathbf q_j, \\mathbf b_k \\rangle\\mathbf q_j$  \n",
    "and considering $\\langle \\mathbf q_k, \\mathbf b_k\\rangle $  we see   \n",
    "$\\langle \\mathbf q_k, \\mathbf b_k\\rangle  = \\gamma_k \\cdot 1 + \\sum_{j=1}^{k-1}\\langle \\mathbf q_j, \\mathbf b_k \\rangle\\cdot 0 \\longrightarrow \\gamma_k =\\langle \\mathbf q_k, \\mathbf b_k\\rangle$  \n",
    "thus  \n",
    "$\\mathbf b_k = \\langle \\mathbf q_k, \\mathbf b_k\\rangle \\cdot \\mathbf q_k + \\sum_{j=1}^{k-1}\\langle \\mathbf q_j, \\mathbf b_k \\rangle\\mathbf q_j =   r_{k,k}\\mathbf q_k + \\sum_{j=1}^{k-1} r_{j,k}\\mathbf q_j $     \n",
    "\n",
    "i.e.  \n",
    "$\\mathbf B = \n",
    "\\bigg[\\begin{array}{c|c|c|c|c} \n",
    "\\mathbf 1  & \\mathbf x & \\mathbf x^2 & \\cdots & \\mathbf x^n \n",
    "\\end{array}\\bigg] = \\bigg[\\begin{array}{c|c|c|c|c} \n",
    "\\mathbf q_0  & \\mathbf q_1 & \\mathbf q_2 & \\cdots & \\mathbf q_n \n",
    "\\end{array}\\bigg] \\begin{bmatrix}\n",
    "r_{0,0} & r_{0,1}& r_{0,2}&\\dots & r_{0,n} \\\\ \n",
    "0 & r_{1,1}& r_{1,2}&\\dots & r_{1,n}\\\\  \n",
    "0 & 0 & r_{2,2}&\\dots & r_{2,n} \\\\\n",
    "\\vdots & \\vdots & \\dots & \\ddots& \\vdots \\\\ \n",
    "0 & 0 & 0&\\dots & r_{n,n} \\\\ \n",
    "\\end{bmatrix}= \\mathbf {Q}R$  \n",
    "\n",
    "This *is* **QR factorization** and agrees with the procedure e.g. on page 242  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7.5.13**  \n",
    "Suppose $\\mathbf A$ is Hermitian. Prove there is some unitary $\\mathbf Q$ with determinant 1, such that $\\mathbf {QAQ}^* = \\mathbf D$  for diagonal matrix $\\mathbf D$   \n",
    "\n",
    "*proof:*  \n",
    "by spectral theorem we have  \n",
    "$\\mathbf {UAU}^* = \\mathbf D$  \n",
    "with unitary $\\mathbf U$  \n",
    "\n",
    "and we know $\\det\\big(\\mathbf U\\big) = \\gamma$  for some $\\gamma$ on the unit circle   \n",
    "\n",
    "now let $\\alpha^n = \\gamma$  \n",
    "(i.e. $\\alpha$ isn't unique per se -- there are $n$ distinct values on the unit circle that will suffice)   \n",
    "\n",
    "then consider  \n",
    "$\\mathbf Q :=\\bar{\\alpha}\\mathbf U$  \n",
    "\n",
    "so  \n",
    "$\\mathbf {QAQ}^* =\\big(\\bar{\\alpha}\\mathbf U\\big) \\mathbf A \\big(\\bar{\\alpha}\\mathbf U\\big)^* =\\bar{\\alpha} \\mathbf {UAU}^*\\alpha=\\bar{\\alpha}\\alpha\\cdot \\mathbf {UAU}^*= 1\\cdot\\mathbf {UAU}^* = \\mathbf D$  \n",
    "and  \n",
    "$\\det\\big(\\mathbf Q\\big) =\\det\\big(\\bar{\\alpha}\\mathbf I\\mathbf U\\big)=\\det\\big(\\bar{\\alpha}\\mathbf I\\big)\\det\\big(\\mathbf U\\big)=(\\bar{\\alpha})^n \\cdot \\gamma = \\bar{\\gamma}\\cdot \\gamma = 1$  \n",
    "as desired  \n",
    "\n",
    "alternatively, instead of taking nth roots, this may be proven by using elementary matrices of the 3rd type, with the top left element being equal to $\\gamma$ and then $\\mathbf Q:= \\mathbf U \\mathbf E$, so   \n",
    "$\\mathbf A = \\mathbf U\\mathbf D \\mathbf U^*=\\mathbf U\\mathbf I\\mathbf D \\mathbf U^*= \\mathbf U\\mathbf E\\mathbf E^*\\mathbf D \\mathbf U^*=\\mathbf U\\mathbf E\\mathbf D \\mathbf E^*\\mathbf U^*= \\big(\\mathbf U\\mathbf E\\big)\\mathbf D \\big(\\mathbf U\\mathbf E\\big)^* = \\mathbf Q \\mathbf D \\mathbf Q^*$   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7.5.14**  \n",
    "*two n x n Hermitian matrices commute iff they are simultaneously diagonalizable*  \n",
    "\n",
    "\n",
    "collect the eigenvectors of $\\mathbf A$  \n",
    "$\\mathbf V = \\bigg[\\begin{array}{c|c|c|c|c|c} \n",
    "\\mathbf v_1  &  \\mathbf v_2 & \\mathbf v_3 &\\mathbf v_4&\\cdots & \\mathbf v_n\n",
    "\\end{array}\\bigg]$  \n",
    "\n",
    "in general the argument runs with \n",
    "$\\mathbf {AB}_\\delta$  \n",
    "for some $\\delta \\in \\mathbb F$ \n",
    "(e.g. to maintain Hermicity we'd restrict to $\\delta \\in \\mathbb R$)  \n",
    "\n",
    "1.) start off by selecting $\\delta$ to so that $\\mathbf B$ is invertible.  Note that for any choice of $\\delta$ we have  \n",
    "$\\mathbf {AB}_\\delta$ = $\\mathbf {B}_\\delta\\mathbf A$  \n",
    "so $\\delta$ in some sense is a free parameter  \n",
    "\n",
    "by a similar argument we assume $\\mathbf A$ to be invertible throughout this post -- except there is no free parameter for $\\mathbf A$, if necessary at time zero we have $\\mathbf A' := \\mathbf A + \\eta \\mathbf I$ such that $\\mathbf A'$ is invertible and $\\eta$ is fixed.  \n",
    "\n",
    "- - - - \n",
    "*leg one: designing a bilinear form around the eigenvectors of A*  \n",
    "to prove the desired result we define a *new (conjugate) Bilinear form* also known as Hermitian Form or Sesquilinear Form   \n",
    "\n",
    "$\\langle \\mathbf x, \\mathbf y\\rangle := \\mathbf x^*\\mathbf A \\mathbf B_\\delta\\mathbf y= \\mathbf x^*\\mathbf B_\\delta \\mathbf A\\mathbf y$  \n",
    "\n",
    "remark:  it may be cleaner to denote this as   \n",
    "$\\langle \\mathbf x, \\mathbf y\\rangle_\\delta$  \n",
    "though the choice of $\\delta$ is arbitrary and does not impact the key orthogonality relations derived below, so we omit subscript  \n",
    "\n",
    "now partition the $d$ distinct eigenvalues $\\mathbf A$ into $d$ subspaces  \n",
    "$W_1, W_2, W_3,  ... , W_d$  \n",
    "\n",
    "and e.g. \n",
    "$W_1^\\perp = W_2 \\oplus W_3  \\oplus  ...  \\oplus W_d$   \n",
    "i.e. it is generated by the union of the generators for all subspaces except $W_1$  \n",
    "\n",
    "note that with respect to our newly created form, these subspaces are orthogonal, because for $\\mathbf w \\in W_j$ and $\\mathbf z \\in W_k$, $k\\neq j$    \n",
    "\n",
    "$\\langle \\mathbf z, \\mathbf w\\rangle = \\lambda_k\\mathbf z\\mathbf B_\\delta \\mathbf w  = \\big(\\mathbf z^* \\mathbf A\\big)\\mathbf B_\\delta \\mathbf w= \\mathbf z^*\\mathbf B_\\delta\\big(\\mathbf A \\mathbf w\\big)=\\lambda_j\\mathbf z^*\\mathbf B_\\delta \\mathbf w  $  \n",
    "\n",
    "rearranging terms and recognizing $(\\lambda_k-\\lambda_{j}) \\neq 0$    \n",
    "$(\\lambda_j-\\lambda_{k})\\mathbf z^*\\mathbf B_\\delta \\mathbf w = 0\\longrightarrow \\mathbf z^*\\mathbf B_\\delta \\mathbf w = 0\\longrightarrow \\langle \\mathbf z, \\mathbf w\\rangle =0$  \n",
    "\n",
    "in general, for $\\mathbf w \\in W_j$ and $\\mathbf z \\in W^\\perp$, using (conjugate) linearity     \n",
    "$\\langle \\mathbf z, \\mathbf w\\rangle= \\langle \\sum_{k\\neq j} \\alpha_k \\mathbf w^{(k)}, \\mathbf w\\rangle = \\sum_{k\\neq j}\\bar{\\alpha_k}\\langle   \\mathbf w^{(k)}, \\mathbf w\\rangle = 0$  \n",
    "\n",
    "*an optional but nice corollary:*    \n",
    "the orthogonality of these subspaces is closed under multiplication by $\\mathbf {AB}_\\delta$  \n",
    "if $\\mathbf w \\in W_j$ then $\\mathbf {AB}\\mathbf w \\in W_j$  \n",
    "i.e. our invariant subspaces are closed under multiplication by $\\mathbf {AB}_\\delta$  \n",
    "this follows by repeating the argument \n",
    "for $\\mathbf w \\in W_j$ and $\\mathbf z \\in W_k$, $k\\neq j$    \n",
    "$\\langle \\mathbf z, \\mathbf {AB}_\\delta\\mathbf w\\rangle = \\lambda_k^2\\mathbf z^*\\mathbf B_\\delta^2 \\mathbf w  = \\big(\\mathbf z^* \\mathbf A^2\\big)\\mathbf B_\\delta^2 \\mathbf w= \\lambda_k\\lambda_j\\mathbf z\\mathbf B_\\delta^2 \\mathbf w =(\\mathbf z\\mathbf A)^*\\mathbf B_\\delta^2\\big(\\mathbf A \\mathbf w\\big)=\\mathbf z^*\\mathbf B_\\delta^2\\big(\\mathbf A^2 \\mathbf w\\big)=\\lambda_j^2\\mathbf z^*\\mathbf B_\\delta^2 \\mathbf w =0$  \n",
    "since $\\lambda_j^2 \\neq \\lambda_j\\lambda_k \\neq \\lambda_k^2$ the form must be zero  \n",
    "\n",
    "and in general  \n",
    "$\\langle \\mathbf z, \\mathbf {AB}_\\delta^m\\mathbf w\\rangle =0$  \n",
    "because $\\lambda_j^{m-p}\\lambda_k^p$ is not constant for all natural numbers $p$    \n",
    "\n",
    "\n",
    "*leg two: examining the eigenvectors of B and making use of free parameter* $\\delta$     \n",
    "\n",
    "since the eigenvectors of $\\mathbf B$ also form a basis, we can iterate through each each of $\\mathbf B$'s $r$ distinct eigenvalues $\\gamma_i$, and know their algebraic and geometric multiplicities are equal, so:  \n",
    "\n",
    "for $i = 1:r$    \n",
    "set $\\delta:= \\gamma_i$ and apply  \n",
    "\n",
    "$\\mathbf {AB}_{\\gamma_i}\\mathbf V = \\bigg[\\begin{array}{c|c|c|c|c|c} \n",
    "\\mathbf {AB}_{\\gamma_i}\\mathbf v_1  &  \\mathbf {AB}_{\\gamma_i}\\mathbf v_2 & \\mathbf {AB}_{\\gamma_i}\\mathbf v_3 &\\cdots & \\mathbf {AB}_{\\gamma_i}\\mathbf v_n  \\end{array}\\bigg]$   \n",
    "\n",
    "$\\text{rank}\\big(\\mathbf {AB}_{\\gamma_i}\\mathbf V\\big) = \\text{rank}\\big(\\mathbf {B}_{\\gamma_i}\\mathbf V\\big) = \\text{rank}\\big(\\mathbf {B}_{\\gamma_i}\\big) = n -\\text{algebraic multiplicity of }\\gamma_i$  \n",
    "or  \n",
    "$\\text{dim ker}\\big(\\mathbf {AB}_{\\gamma_i}\\mathbf V\\big) = \\text{algebraic multiplicity of }\\gamma_i$  \n",
    "our claim as that the linear dependencies induced by \n",
    "$\\big\\{\\mathbf v_1, \\mathbf v_2, ..., \\mathbf v_n\\}\\mapsto  \\big\\{\\mathbf {AB}_{\\gamma_i}\\mathbf v_1, \\mathbf {AB}_{\\gamma_i}\\mathbf v_2, ..., \\mathbf {AB}_{\\gamma_i}\\mathbf v_n\\}$  \n",
    "\n",
    "are isolated to distinct $W_k$,  i.e. suppose for a contradiction that there is some non-trivial linear combination between subspaces that is zero, i.e.  \n",
    "\n",
    "$\\mathbf 0 = \\mathbf {AB}_{\\gamma_i} \\mathbf w + \\mathbf {AB}_{\\gamma_i}\\mathbf z$  \n",
    "for non-zero $\\mathbf w \\in W_j$ and non-zero $\\mathbf z \\in W^\\perp$, each of which are non-zero under the image of $\\mathbf {AB}_{\\gamma_i}$  (and of course $\\mathbf w$ and $\\mathbf z$ are linearly independent themselves)    \n",
    "\n",
    "Then  \n",
    "$\\mathbf {AB}_{\\gamma_i} \\mathbf w = -\\mathbf {AB}_{\\gamma_i}\\mathbf z$   \n",
    "But this implies, for each $\\mathbf v_k$ (which as a reminder, are the eigenvectors of $\\mathbf A$)   \n",
    "\n",
    "$\\langle \\mathbf v_k, \\mathbf w\\rangle = \\mathbf v_k^*\\mathbf {AB}_{\\gamma_i} \\mathbf w = -\\mathbf v_k^*\\mathbf {AB}_{\\gamma_i}\\mathbf z = \\langle \\mathbf v_k, \\mathbf z\\rangle = 0$   \n",
    "because  \n",
    "$\\langle \\mathbf v_k, \\mathbf z\\rangle = 0$ if $\\mathbf v_k \\in W_j$ and  \n",
    "$\\langle \\mathbf v_k, \\mathbf w\\rangle = 0$ if $\\mathbf v_k \\in W_j^\\perp$  \n",
    "\n",
    "thus $\\mathbf v_k^*\\mathbf {AB}_{\\gamma_i} \\mathbf w$ for all $k$, so  \n",
    "\n",
    "$\\mathbf V^*\\mathbf {AB}_{\\gamma_i} \\mathbf w = \\mathbf 0 \\longrightarrow \\mathbf {AB}_{\\gamma_i} \\mathbf w = \\mathbf 0$  \n",
    "since $\\mathbf V^*$ is invertible  \n",
    "\n",
    "Thus *every* eigenvector of $\\mathbf B$ may be selected so that it is contained in exactly one $W_j$ i.e. is an eigenvector with eigenvalue $\\lambda_j$ for $\\mathbf A$.  And since $\\mathbf B$ is diagonalizable, its eigenvectors form a basis, which we may collect in a matrix $\\mathbf S$, which we may use to simultaneously diagonalize both $\\mathbf A$ and $\\mathbf B$.    \n",
    "- - -  -  \n",
    "for an alternative finish that makes explicit use of Hermicity and uses a norm, consider  \n",
    "$\\mathbf {AB}_{\\gamma_i} \\mathbf w = -\\mathbf {AB}_{\\gamma_i}\\mathbf z$   \n",
    "then  \n",
    "$\\Big\\Vert \\mathbf {AB}_{\\gamma_i} \\mathbf w \\Big\\Vert_F^2=  \\langle \\mathbf {AB}_{\\gamma_i} \\mathbf w, \\mathbf w\\rangle =   -\\langle \\mathbf {AB}_{\\gamma_i} \\mathbf w, \\mathbf z\\rangle = 0$  \n",
    "- - - - - \n",
    "*extension:*  \n",
    "This argument works nearly verbatim for two arbitrary commuting diagonalizable matrices $\\mathbf A$ and $\\mathbf B$, except we'd need to be more careful in defining orthogonality relations for $W_j$. In such a case we'd tailor our (conjugate) bilinear form to still be \n",
    "\n",
    "$\\langle \\mathbf x, \\mathbf y\\rangle := \\mathbf x^*\\mathbf A \\mathbf B_\\delta\\mathbf y= \\mathbf x^*\\mathbf B_\\delta \\mathbf A\\mathbf y$ \n",
    "\n",
    "but we'd consider orthogonality by $\\mathbf w \\in W_j$ and $\\mathbf z \\in W_k^{(L)}$ for $k\\neq j$   \n",
    "where $W_j$ has $\\mathbf A$'s (right) eigenvectors associated with eigenvalue $\\lambda_j$ as before and $W_k^{(L)}$ has the *left eigenvectors* of $\\mathbf A$ associated with $\\lambda_k$, then as before we get    \n",
    "\n",
    "$\\langle \\mathbf z, \\mathbf w\\rangle = \\lambda_k \\mathbf z^*\\mathbf B_\\delta\\mathbf w = \\big(\\mathbf z^*\\mathbf A\\big) \\mathbf B_\\delta\\mathbf w= \\mathbf z^* \\mathbf B_\\delta\\big(\\mathbf A\\mathbf w\\big)= \\lambda_j \\mathbf z^*\\mathbf B_\\delta\\mathbf w = 0$ \n",
    "\n",
    "note: it is easy to stumble on conjugation when working with left eigenvectors.  For clarity, we define a right eigenvector with eigenvalue $\\lambda$ to be $\\mathbf A\\mathbf x = \\lambda \\mathbf x$ and a left eigenvector to be $\\mathbf y^*\\mathbf A = \\lambda \\mathbf y^*$  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7.7.1**  \n",
    "in $\\mathbb C$  \n",
    "$\\mathbf A \\mathbf x = \\mathbf 0$  *iff*  $0 =\\big\\Vert\\mathbf A \\mathbf x\\big \\Vert_2^2$ but this implies  \n",
    "$0 =\\big\\Vert\\mathbf A \\mathbf x\\big \\Vert_2^2=\\mathbf x^*\\mathbf A^*\\mathbf A\\mathbf x =\\mathbf x^*\\mathbf A\\mathbf A^*\\mathbf x =  \\big\\Vert\\mathbf A^* \\mathbf x\\big \\Vert_2^2 \\longrightarrow \\mathbf A^* \\mathbf x =\\mathbf 0 $  \n",
    "now use results from ex 7.5.10   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7.7.10**  \n",
    "Let $V$ be finite dim complex vector space with positive definite Hermitian form (i.e. inner product) $\\langle, \\rangle$  \n",
    "and let $T\\longrightarrow V$ be a linear operator on $V$.  Let $A$ be the matrix of $T$ with respect to the orthonormal basis $\\mathbf B$.  The *adjoint operator* $T^*$ is *defined* as the operator whose matrix with respect to the same basis is $A^*$.  \n",
    "\n",
    "**a.)** Prove that $T$ and $T^*$ are related by the equations $\\langle T\\mathbf v, \\mathbf w\\rangle=\\langle \\mathbf v,T^* \\mathbf w\\rangle$ and $\\langle \\mathbf v, T\\mathbf w\\rangle=\\langle T^*\\mathbf v, \\mathbf w\\rangle$ for all $\\mathbf v, \\mathbf w\\in V$.  Prove that the first of these equations characterizes $T^*$  \n",
    "\n",
    "**b.)**   \n",
    "prove that the definition of $T^*$ doesn't depend on choice of orthonormal basis  \n",
    "*remark:*  this is similar to the way the determinant was defined, by fixing a basis and then showing it is invariant to a change of basis. In this argument we fix an arbitrary orthonormal basis and show that the definition of $T^*$ is invariant to change in orthonormal basis.   \n",
    "\n",
    "\n",
    "*proof for a and b*  \n",
    "for orthonormal $\\mathbf B$  \n",
    "$\\mathbf v = \\mathbf B \\mathbf x$  \n",
    "$\\mathbf w = \\mathbf B \\mathbf y$  \n",
    "\n",
    "$\\langle T\\mathbf v, \\mathbf w\\rangle$  \n",
    "$=\\langle T\\mathbf B\\mathbf x, \\mathbf B\\mathbf y\\rangle$  \n",
    "$=\\langle \\mathbf B (A\\mathbf x), \\mathbf B\\mathbf y\\rangle$  \n",
    "$=\\langle \\mathbf B \\mathbf z, \\mathbf B\\mathbf y\\rangle$  \n",
    "$=\\langle \\sum_{k=1}^n  \\mathbf b_k z_k , \\sum_{i=1}^n  \\mathbf b_i y_i\\rangle$  \n",
    "$=\\sum_{k=1}^n \\bar{z_k}\\langle  \\mathbf b_k  , \\sum_{i=1}^n  \\mathbf b_i y_i\\rangle$  \n",
    "$=\\sum_{k=1}^n \\bar{z_k}\\langle  \\mathbf b_k  ,  \\mathbf b_k y_k\\rangle$  \n",
    "$=\\sum_{k=1}^n \\bar{z_k}y_k\\langle  \\mathbf b_k  ,  \\mathbf b_k \\rangle$  \n",
    "$=\\sum_{k=1}^n \\bar{z_k}y_k$  \n",
    "$=\\mathbf z^*\\mathbf y$  \n",
    "$=\\mathbf x^* \\mathbf A^*\\mathbf y$  \n",
    "$=\\mathbf x^* \\big(\\mathbf A^*\\mathbf y\\big)$  \n",
    "$=\\langle \\mathbf B\\mathbf x, \\mathbf B\\mathbf A^*\\mathbf y\\rangle$  \n",
    "$=\\langle \\mathbf B\\mathbf x, T^*\\mathbf B\\mathbf y\\rangle$  \n",
    "$=\\langle \\mathbf v, T^*\\mathbf w\\rangle$  \n",
    "\n",
    "to prove *b.)* note that we may select a different orthonormal basis \n",
    "$\\mathbf v = \\mathbf Q \\mathbf x'$  \n",
    "$\\mathbf w = \\mathbf Q \\mathbf y'$  \n",
    "and rerun the above argument verbatim to get the same result.\n",
    "\n",
    "As a **not fully developed alternative approach**  since B and Q are both (orthonormal) bases there is some linear mapping between $\\mathbf Q$ and $\\mathbf B$.  In particular  \n",
    "$\\mathbf B = \\mathbf Q\\mathbf U$  \n",
    "and since the LHS is orthonormal, $\\mathbf U$ is as well this implies $\\mathbf U$ is unitary  \n",
    "\n",
    "$\\mathbf v = \\mathbf Q \\mathbf x'= \\mathbf Q\\mathbf U\\mathbf U^{-1} \\mathbf x' =\\mathbf B \\mathbf U^{-1}\\mathbf x'=\\mathbf B \\mathbf x$  \n",
    "\n",
    "note that for any $\\mathbf v \\in V$  \n",
    "\n",
    "so $\\mathbf U^{-1}\\mathbf x' =\\mathbf x\\longrightarrow \\mathbf x' =\\mathbf U\\mathbf x$  \n",
    "and the same applies for relating $\\mathbf y'$ and $\\mathbf y$  \n",
    "$\\langle \\mathbf v, \\mathbf v\\rangle = \\big\\Vert \\mathbf x\\big\\Vert_2^2 = \\langle \\mathbf B\\mathbf x, \\mathbf B\\mathbf x\\rangle =\\langle \\mathbf Q\\mathbf x', \\mathbf Q\\mathbf x'\\rangle=\\langle \\mathbf Q \\mathbf U\\mathbf x, \\mathbf Q\\mathbf U\\mathbf x\\rangle = \\big \\Vert \\mathbf U \\mathbf x\\big\\Vert_2^2$  \n",
    "but the choice of $\\mathbf x$ was arbitrary so $\\mathbf U$ must be unitary, e.g. via application of Cauchy- Schwarz, we have  \n",
    "\n",
    "$\\Big \\Vert \\mathbf x\\big \\Vert_2^2 = \\langle \\mathbf Q \\mathbf U\\mathbf x, \\mathbf Q\\mathbf U\\mathbf x\\rangle = \\langle  \\mathbf U\\mathbf x, \\mathbf U\\mathbf x\\rangle = \\langle  \\mathbf x, \\mathbf U^*\\mathbf U\\mathbf x\\rangle\\leq \\Big \\Vert \\mathbf x\\big \\Vert_2\\Big \\Vert \\mathbf U^*\\mathbf U \\mathbf x\\big \\Vert_2 \\leq \\Big \\Vert \\mathbf x\\big \\Vert_2^2$    \n",
    "and this is met with equality, so $\\mathbf U^*\\mathbf U = \\alpha\\mathbf I$  for some $\\alpha$ on the unit circle, and $\\mathbf U^*\\mathbf U$ necessarily has non-negative numbers on its diagonal so $\\alpha = 1$.  \n",
    "\n",
    "note: the 2nd inequality needs justification -- this too follows from Cauchy Schwarz  \n",
    "$\\Big \\Vert \\mathbf U^*\\mathbf x\\Big \\Vert_2^2 = \\langle \\mathbf U^* \\mathbf x,\\mathbf U^*\\mathbf x\\rangle= \\langle\\mathbf x,\\mathbf U\\mathbf U^*\\mathbf x\\rangle \\leq \\Big \\Vert \\mathbf x\\Big\\Vert_2\\cdot \\Big \\Vert \\mathbf U\\mathbf U^*\\mathbf x\\Big\\Vert_2=\\Big \\Vert \\mathbf x\\Big\\Vert_2\\cdot \\Big \\Vert \\mathbf U^*\\mathbf x\\Big\\Vert_2\\longrightarrow \\Big \\Vert \\mathbf U^*\\mathbf x\\Big \\Vert_2 \\leq \\Big \\Vert \\mathbf x\\Big\\Vert_2$   \n",
    "\n",
    "then  \n",
    "$\\langle T\\mathbf v, \\mathbf w\\rangle$  \n",
    "$=\\langle T\\mathbf Q\\mathbf x', \\mathbf Q\\mathbf y'\\rangle$  \n",
    "$=\\big(\\mathbf x'\\big)^* \\mathbf A'\\big(\\mathbf y'\\big)$  \n",
    "$=\\big(\\mathbf U\\mathbf x\\big)^* \\mathbf A'\\big(\\mathbf U\\mathbf y\\big)$  \n",
    "$=\\mathbf x^* \\mathbf U^*\\mathbf A'\\mathbf U\\mathbf y$  \n",
    "$=\\mathbf x^*\\big( \\mathbf U^{-1}\\mathbf A'\\mathbf U\\big)\\mathbf y$  \n",
    "$=\\mathbf x^* \\mathbf A\\mathbf y$  \n",
    "$=\\langle \\mathbf v, T^*\\mathbf w\\rangle$  \n",
    "\n",
    "so an orthonormal change of of basis preserves the positive definite Hermitian product and thus the orthonormal selection of basis used in coming up with a matrix whose conjugate transpose defines $T^*$ does not matter.  \n",
    "\n",
    "\n",
    "\n",
    "**c.)**  \n",
    "let $\\mathbf v$ be an eigenvector for $T$ with eigenvalue $\\lambda$ and $W$ be the subspace orthogonal to $\\mathbf v$.  prove that $W$ is $T^*$ invariant.  For $\\mathbf w\\in W$  \n",
    "\n",
    "$\\lambda\\cdot \\langle \\mathbf v, \\mathbf w\\rangle = \\langle T\\mathbf v, \\mathbf w\\rangle  = \\langle \\mathbf v, T^*\\mathbf w\\rangle $  \n",
    "\n",
    "and the definition of $T^*$ invariant, adapted from pages 116-117 would be $T^*W \\subset W$ i.e. $T(\\mathbf w)\\in W$ for all $\\mathbf w \\in W$  \n",
    "\n",
    "and $W$ is being the subspace consisting of all $\\mathbf w$ such that\n",
    "$\\langle \\mathbf v, \\mathbf w\\rangle = 0$  \n",
    "\n",
    "and \n",
    "$0 =\\lambda \\cdot 0 = \\lambda \\cdot \\langle \\mathbf v, \\mathbf w\\rangle = \\langle T\\mathbf v, \\mathbf w\\rangle =\\langle \\mathbf v, T^*\\mathbf w\\rangle$  \n",
    "so $T^*\\mathbf w\\subset W$ \n",
    "i.e. $T^*$ does not alter the fact that any $\\mathbf w \\in W$ is orthogonal to $\\mathbf v$  \n",
    "\n",
    "\n",
    "**11**  \n",
    "Prove that for any linear operator $T$, $TT^*$ is Hermitian  \n",
    "**tbc**   \n",
    "the definition of Hermitian linear operator is given on page 253 as \n",
    "$ \\langle T'\\mathbf v, \\mathbf w\\rangle = \\langle \\mathbf v, T'\\mathbf w\\rangle $  for all $\\mathbf v, \\mathbf w\\in V$  \n",
    "\n",
    "selecting $T' = TT^*$ and using our identity from the prior exercise, we have  \n",
    " $\\langle TT^*\\mathbf v, \\mathbf w\\rangle = \\langle \\mathbf v, (TT^*)^*\\mathbf w\\rangle = \\langle \\mathbf v, TT^*\\mathbf w\\rangle$  \n",
    " so the operator is Hermitian  \n",
    "\n",
    "**12**   \n",
    "**pending**  \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7.7.12**  \n",
    "\n",
    "a normal operator $T:V\\longrightarrow V$ is defined as normal *iff* $TT^* = T^*T$  \n",
    "\n",
    "(a)  \n",
    "(b) assume $T$ is normal with eigenvector $\\mathbf v$.  Prove $\\mathbf v$ is an eigenvector for $T^*$ and compute the eigenvalue  \n",
    "\n",
    "\n",
    "$\\big\\Vert T^*\\mathbf v\\big\\Vert_2^2 = \\langle T^*\\mathbf v, T^*\\mathbf v\\rangle=  \\langle \\mathbf v, TT^*\\mathbf v\\rangle\\leq \\big\\Vert \\mathbf v\\big\\Vert_2 \\big\\Vert TT^*\\mathbf v\\big\\Vert_2=\\big\\Vert \\mathbf v\\big\\Vert_2 \\big\\Vert T^*T\\mathbf v\\big\\Vert_2= \\vert\\lambda\\vert\\big\\Vert \\mathbf v\\big\\Vert_2 \\big\\Vert T^*\\mathbf v\\big\\Vert_2 $  \n",
    "\n",
    "By Cauchy-Schwarz, with equality *iff* $ T^*\\mathbf v \\propto  \\mathbf v$  \n",
    "but the upper bound is met with equality because   \n",
    "$\\big\\Vert T^*\\mathbf v\\big\\Vert_2 = \\langle T^*\\mathbf v, T^*\\mathbf v\\rangle^\\frac{1}{2}= \\langle T\\mathbf v, T\\mathbf v\\rangle^\\frac{1}{2}=\\big\\Vert T\\mathbf v\\big\\Vert_2 = \\vert \\lambda\\vert \\big\\Vert \\mathbf v\\big\\Vert_2  $  \n",
    "hence $\\mathbf v$ is an eigenvector for $T^*$ with eigenvalue $\\gamma$  \n",
    "pluggin this into our original expression we have  \n",
    "\n",
    "$\\vert \\lambda\\vert^2 \\big\\Vert \\mathbf v\\big\\Vert_2^2 =  \\langle T\\mathbf v, T\\mathbf v\\rangle=  \\lambda\\cdot\\langle T\\mathbf v, \\mathbf v\\rangle=  \\lambda\\cdot\\langle \\mathbf v, T^*\\mathbf v\\rangle=\\lambda\\cdot\\gamma \\cdot \\langle \\mathbf v, \\mathbf v\\rangle$  \n",
    "thus  \n",
    "$\\gamma = \\bar{\\lambda}$  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*note on representing complex numbers*  \n",
    "$a_1 + b_1 i$ as scalars  \n",
    "is equivalent to the matrix representation  \n",
    "\n",
    "$\\begin{bmatrix} \n",
    "a_1 & -b_1\\\\ \n",
    "b_1 & a_1 \n",
    "\\end{bmatrix}= a_1 \n",
    "\\begin{bmatrix} \n",
    "1 & 0\\\\ \n",
    "0 & 1 \n",
    "\\end{bmatrix} + \n",
    "b_1 \\begin{bmatrix} \n",
    "0 & -1\\\\ \n",
    "1 & 0 \n",
    "\\end{bmatrix} = a_1 \\mathbf I + b_1 \\mathbf i$   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7.8.3**  \n",
    "re: skew symmetric matrices  \n",
    "\n",
    "$\\det\\big(\\mathbf A\\big)= \\det\\big(\\mathbf A^T \\big) = \\det\\big(-\\mathbf A\\big) = \\det\\big(-\\mathbf I \\mathbf A\\big) = \\det\\big(-\\mathbf I\\big) \\det\\big(\\mathbf A\\big) =(-1)^n \\det\\big(\\mathbf A\\big)$  \n",
    "implies $\\mathbf A$ is singular if n is odd, and we are not in a field of characteristic 2.  \n",
    "(The decomposition derived in ex 7.8.7 clarifies that this result also holds in fields of characteristic 2.)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7.8.4**  \n",
    "*claim:* The eigenvalues of a real skew matrix $\\mathbf A$  are purely imaginary   \n",
    "\n",
    "observe    \n",
    "$-\\mathbf A^2 =\\mathbf A^T \\mathbf A \\succeq 0$  \n",
    "and $\\mathbf A^T \\mathbf A$ only has real non-negative eigenvalues, which means $\\mathbf A^2$ only has real non-positive eigenvalues.  This means that $\\mathbf A$ necessarily has purely imaginary eigenvalues.  (For 2 other, different proofs, see the \"Schur's Inequality\" notebook.)   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7.8.5**\n",
    "*Cayley Transform*   \n",
    "Let $\\mathbf S$ be a *real* skew symmetric matrix   \n",
    "*i.)* prove $\\big(\\mathbf I + \\mathbf S\\big)$ is invertible  \n",
    "$\\big(\\mathbf I + \\mathbf S\\big)^T\\big(\\mathbf I + \\mathbf S\\big) = \\big(\\mathbf I + \\mathbf S^T\\big)\\big(\\mathbf I + \\mathbf S\\big)= \\mathbf I + \\mathbf S + \\mathbf S^T +\\mathbf S^T\\mathbf S = \\mathbf I + \\mathbf S - \\mathbf S +\\mathbf S^T\\mathbf S = \\mathbf I +\\mathbf S^T\\mathbf S$  \n",
    "and the addition of a positive definite matrix ($\\mathbf I$) plus a positive semi-definite matrix is positive definite and hence non-singular.  Thus $\\big(\\mathbf I + \\mathbf S\\big)$ is nonsingular.  \n",
    "\n",
    "*note:*    \n",
    "$\\big(\\mathbf I - \\mathbf S\\big)= \\big(\\mathbf I  +\\mathbf S^T\\big) =\\big(\\mathbf I + \\mathbf S\\big)^T  $   \n",
    "so the above argument *also* proves that $\\big(\\mathbf I - \\mathbf S\\big)$ is non-singular  \n",
    "and it also shows that $\\big(\\mathbf I - \\mathbf S\\big)^{-1} = \\big(\\mathbf I + \\mathbf S\\big)^{-T}$  \n",
    "\n",
    "\n",
    "*ii.)* prove $\\mathbf Q:=\\big(\\mathbf I - \\mathbf S\\big)\\big(\\mathbf I + \\mathbf S\\big)^{-1}$ is orthogonal  \n",
    "$\\Big(\\big(\\mathbf I - \\mathbf S\\big)\\big(\\mathbf I + \\mathbf S\\big)^{-1}\\Big)^T\\big(\\mathbf I - \\mathbf S\\big)\\big(\\mathbf I + \\mathbf S\\big)^{-1}$  \n",
    "$=\\big(\\mathbf I + \\mathbf S\\big)^{-T}\\big(\\mathbf I - \\mathbf S^T\\big)\\big(\\mathbf I - \\mathbf S\\big)\\big(\\mathbf I + \\mathbf S\\big)^{-1}$   \n",
    "$=\\big(\\mathbf I + \\mathbf S\\big)^{-T}\\big(\\mathbf I + \\mathbf S\\big)\\big(\\mathbf I - \\mathbf S\\big)\\big(\\mathbf I + \\mathbf S\\big)^{-1}$   \n",
    "$=\\big(\\mathbf I + \\mathbf S\\big)^{-T}\\big(\\mathbf I - \\mathbf S\\big)\\big(\\mathbf I + \\mathbf S\\big)\\big(\\mathbf I + \\mathbf S\\big)^{-1}$   \n",
    "$=\\big(\\mathbf I + \\mathbf S\\big)^{-T}\\big(\\mathbf I - \\mathbf S\\big)$   \n",
    "$=\\mathbf I$  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**remark**  \n",
    "This problem is generalized as 'misc' problem in chapter 8.  The approach taken here also works for a complex skew symmetric (not hermitian) matrix and applying the aboce Cayley Transform gives a complex orthogonal (not unitary) matrix.  The proof follows in exactly the same way as in part ii of the above problem.  However in such a case there is a significant problem that needs addressed -- $\\big(\\mathbf I + \\mathbf S\\big)$ need not be invertible.  See chapter 8 notebook for more information.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**additional remark**  \n",
    "the above algebraic approach will generalize in the chapter 8 misc problem to other kinds of matrices giving the more general Cayley Transform.  However it doesn't necessarily give us insight into where the Cayley Transform comes from.  A different viewpoint is to change our field to $\\mathbb C$ and still have a real skew symmetric $\\mathbb S$ so  \n",
    "\n",
    "$\\mathbf S = -\\mathbf S^T = \\mathbf S^*$  \n",
    "per ex 7.8.4 we know that $\\mathbf S$ has purely imaginary eigenvalues  \n",
    "we may prove that $\\mathbf Q$ is orthogonal, by first proving it is unitary, and then recognizing it is real so it is in fact a real orthogonal matrix  \n",
    "\n",
    "being normal we have  \n",
    "$\\mathbf S = \\mathbf U\\mathbf D \\mathbf U^*$  \n",
    "where $\\mathbf D$ has purely imaginary eigenvalues   \n",
    "for some unitary $\\mathbf U$  \n",
    "so  \n",
    "$\\mathbf Q$  \n",
    "$= \\big(\\mathbf I - \\mathbf S\\big)\\big(\\mathbf I + \\mathbf S\\big)^{-1} $  \n",
    "$= \\big(\\mathbf I - \\mathbf U\\mathbf D \\mathbf U^*\\big)\\big(\\mathbf I + \\mathbf U\\mathbf D \\mathbf U^*\\big)^{-1} $   \n",
    "$= \\Big(\\mathbf U \\big(\\mathbf I - \\mathbf D\\big) \\mathbf U^*\\Big)\\Big(\\mathbf U \\big(\\mathbf I + \\mathbf D\\big) \\mathbf U^*\\Big)^{-1}$  \n",
    "$= \\mathbf U\\big(\\mathbf I - \\mathbf D\\big) \\mathbf U^*\\mathbf U \\big(\\mathbf I + \\mathbf D\\big)^{-1} \\mathbf U^*$   \n",
    "$= \\mathbf U\\Big(\\big(\\mathbf I - \\mathbf D\\big) \\big(\\mathbf I + \\mathbf D\\big)^{-1}\\Big) \\mathbf U^*$  \n",
    "$= \\mathbf U\\Big(\\mathbf \\Lambda \\Big) \\mathbf U^*$  \n",
    "\n",
    "we claim that $\\mathbf \\Lambda =\\big(\\mathbf I - \\mathbf D\\big) \\big(\\mathbf I + \\mathbf D\\big)^{-1}$  is unitary,  and hence $\\mathbf Q$ is the product of 3 unitary matrices so it is unitary which then implies $\\mathbf Q$, being real valued, is real orthogonal.  \n",
    "\n",
    "To finish the proof, observe that $\\mathbf \\Lambda$ is a diagonal matrix since it is the product of two diagonal matrices, and hence we only need to prove that its diagonal elements are on the unit circle. Now  \n",
    "\n",
    "for some $y \\in \\mathbb R$  \n",
    "$\\lambda_i = \\frac{1-y i}{1+yi}$  \n",
    "and the numerator is the conjugate of the denominator, hence $\\vert \\lambda_i\\vert  =1$  as desired.    \n",
    "\n",
    "**extension**  \n",
    "If we use insights from ex 8.Misc.3  \n",
    "we see that the Cayley Transform is involutive and quite general.  However combining those insights with this exercise gives us some ideas as to where the Cayley Transform came from.  In particular consider the  2 x 2 real skew symmetric $\\mathbf S$. \n",
    "\n",
    "From ex 8.Misc.3 we see \n",
    "\n",
    "$\\mathbf S \\mapsto \\mathbf Q \\mapsto \\mathbf S$  \n",
    "and in particular in the chapter 8 notebook, under ex 8.4.4 \"*alternative approach to proving path connectivity for*  $SO_3\\big(\\mathbb C\\big)$  we verify that  \n",
    "if we start with some orthogonal $\\mathbf Q$  \n",
    "$\\mathbf Q \\mapsto \\mathbf S$  \n",
    "for now we are interested in the easier and more geometrically pleasant case of $\\mathbf Q\\in SO_n(\\mathbb R)$ (which is a subgroup of the more difficult $SO_n(\\mathbb C)$) and in particular of $n=2$  \n",
    "\n",
    "focusing on one of the eigenalues of our real 2x2 $\\mathbf S$, recalling that any complex number   \n",
    "$z = x_1 + y_1 i $  \n",
    "\n",
    "may be uniquely specified or even defined as  \n",
    "$\\begin{bmatrix} \n",
    "x_1 & -y_1\\\\ \n",
    "y_1 & x_1 \n",
    "\\end{bmatrix}= x_1 \n",
    "\\begin{bmatrix} \n",
    "1 & 0\\\\ \n",
    "0 & 1 \n",
    "\\end{bmatrix} + \n",
    "y_1 \\begin{bmatrix} \n",
    "0 & -1\\\\ \n",
    "1 & 0 \n",
    "\\end{bmatrix} = x_1 \\mathbf I + y_1 \\mathbf i$  \n",
    "\n",
    "so our skew symmetric matrix $\\mathbf S =  y_1 \\mathbf i= y \\mathbf i$  \n",
    "\n",
    "what the Cayley Transform in effect tells us is for $y \\in \\mathbb R$   \n",
    "\n",
    "$\\big(1,y\\big)\\mapsto \\lambda \\in \\mathbb C$ where $\\vert \\lambda \\vert =1$  \n",
    "i.e. if we draw the Argand Diagram / Complex plane, we map points from the line running through $x =1$ to points on the unit circle.  Now if we start with any $\\lambda'\\neq -1$ on the unit circle, we get  \n",
    "$\\lambda'\\mapsto   \\big(1,y'\\big)$   \n",
    "as noted in the chapter 8 notebook, the Cayley Transform doesn't exist for any eigenvalues $= -1$.  \n",
    "It may be helpful to consider the point on the unit circle $\\lambda' = 1$ or $(1,0)$ in rectangular coordinates.  Applying the Cayley Transform we see $\\lambda'\\mapsto (1,0)$, i.e. it is a fixed point.        \n",
    "\n",
    "Now stepping back: aside from the single point of singularity on the unit circle given by $\\lambda =-1$, we have a continuous map that has a continuous inverse, i.e. we have a homeomorphism.  And this pushes us to consider that the Cayley Transform, in this simplified setting, is an instance of the Stereographic Projection from the one sphere, with the 'north pole' at $(-1,0)$  deleted, to the line running through $(1,y)$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*further*  \n",
    "now rather than focusing on a single insight, if we view our 2x2 real skew symmetrix $\\mathbf S$ as a generalization/or representation of a purely imaginary number, and observe that under the image of the Cayley Transform, we recover a matrix in $SO_2\\big(\\mathbb R\\big)$ (check the mapping of conjugate pairs of eigenvalues under of $\\mathbf S$ this implies a determinant of 1 for the resulting orthogonal matrix) and matrices in $SO_2\\big(\\mathbb R\\big)$ may be viewed as uniquely identifying with a single point on the unit circle -- i.e. they are 2d rotations and geometrically act the same as points on the unit circle.  And *all* matrices in $SO_2\\big(\\mathbb R\\big)$ are 'reachable' under the Cayley Transform, *except* those with eigenvalue -1, but having determinant of 1, this necessarily refers to a matrix with two eigenvalues of -1, which necessarily means $-I$ (check the trace and look at implied column norms for an orthogonal matrix, or observe that matrices in $SO_2\\big(\\mathbb R\\big)$ are diagonalizable over $\\mathbb C$ so the matrix is similar to the negative identity matrix and hence is the negative identity matrix).  And of course $-I_2$ may be interpretted, geometrically as uniquely representing the value of $-1$ on the unit circle.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7.8.6**\n",
    "Let $\\mathbf A$ be a real skew symmetric matrix  \n",
    "(a) prove $\\det\\big(\\mathbf A\\big)\\geq 0$ \n",
    "*a common approach invokes the fact that $\\mathbf A$ has purely imaginary eigenvalues which come in conjugate pairs if non-zero*  \n",
    "an alternative approach uses the fact that (8.5 on page 261 -- the proof of which was left as an exercise... see ex 7.8.7 below to derive this congruence)  \n",
    "\n",
    "if $\\mathbf A$ is singular then $\\det\\big(\\mathbf A\\big) =0$.  Supposing $\\mathbf A$ is nonsingular, we have  \n",
    "\n",
    "via congruence transforms we have  \n",
    "$\\mathbf M^T\\mathbf A\\mathbf M = \\begin{bmatrix}\\mathbf 0 & \\mathbf I_r \\\\ -\\mathbf I_r & \\mathbf 0\\end{bmatrix}$  \n",
    "and  \n",
    "$\\det\\Big(\\begin{bmatrix}\\mathbf 0 & \\mathbf I_\\frac{r}{2} \\\\ -\\mathbf I_\\frac{r}{2} & \\mathbf 0\\end{bmatrix}\\Big)  =\\det\\Big(\\begin{bmatrix}\\mathbf I_\\frac{r}{2} & \\mathbf 0 \\\\ \\mathbf 0 & -\\mathbf I_\\frac{r}{2}\\end{bmatrix}\\begin{bmatrix}\\mathbf 0 & \\mathbf I_\\frac{r}{2} \\\\ \\mathbf I_\\frac{r}{2} & \\mathbf 0\\end{bmatrix}\\Big)=\\det\\Big(\\begin{bmatrix}\\mathbf I_\\frac{r}{2} & \\mathbf 0 \\\\ \\mathbf 0 & -\\mathbf I_\\frac{r}{2}\\end{bmatrix}\\big)\\det\\big(\\begin{bmatrix}\\mathbf 0 & \\mathbf I_\\frac{r}{2} \\\\ \\mathbf I_\\frac{r}{2} & \\mathbf 0\\end{bmatrix}\\Big) =(-1)^\\frac{r}{2} \\cdot (-1)^\\frac{r}{2} = (-1)^{r}=  1$  \n",
    "\n",
    "where $r$ is even  \n",
    "\n",
    "The diagonal matrix determinant is immediate and second matrix is permutation and symmetric... referencing the discussion of Householder matrices we can observe it has the same number of +1 and -1 eigenvalues -- i.e. $\\frac{r}{2}$ of them so the determinant of that too is $(-1)^\\frac{r}{2}$. An alternative approach is to recognize that up to graph isomorphism, i.e. permutation matrix similarity, we have the standard sympletic basis being given by a block diagonal matrix with each block consisting of     \n",
    "$\\begin{bmatrix} 0 & 1 \\\\ -1 & 0\\end{bmatrix}$ and *this* has a determinant of 1.  Earlier work (chapter 2) on block triangular matrices then implies the determinant is 1.    \n",
    "\n",
    "so  \n",
    "$1 =\\det\\big(\\mathbf M^T\\mathbf A\\mathbf M \\big)=\\det\\big(\\mathbf M^T\\big)\\det\\big(\\mathbf A\\big)\\det\\big(\\mathbf M \\big)=\\det\\big(\\mathbf A\\big)\\det\\big(\\mathbf M \\big)^2$ and being real \n",
    "$\\det\\big(\\mathbf M \\big)^2\\gt 0\\longrightarrow \\det\\big(\\mathbf A\\big) \\gt 0$ in the nonsingular case \n",
    "\n",
    "(b) This is a corollary to the above work. Suppose $\\mathbf A$ has integer entries.  Prove its determinant is an integer, squared.  If $\\mathbf A$ is singular, the result is immediate.  Suppose it is non-singular, then our prior work tells us:  \n",
    "$\\det\\big(\\mathbf A\\big) = \\det\\big(\\mathbf M^{-1} \\big)^2\\cdot 1= \\det\\big(\\mathbf M^{-1} \\big)^2$  \n",
    "The LHS is an integer because $\\mathbf A$ has integer entries  \n",
    "This implies  $\\det\\big(\\mathbf M^{-1} \\big) \\in \\mathbb N$  \n",
    "i.e. we may change our the field here to be $\\mathbb Q$.  The process follows identically and we still have this decomposition.  Now  \n",
    "if \n",
    "$\\det\\big(\\mathbf M^{-1} \\big) = \\alpha$ and $\\alpha \\not \\in \\mathbb N$, then $\\alpha$ is a fraction with some prime in the denominator that is not in the numerator (Fundamental Theorem of Arithmetic -- see appendix of text). Then this implies $\\det\\big(\\mathbf A\\big) =\\alpha^2 \\not \\in \\mathbb N$ which is a contradiction.  \n",
    "   \n",
    "The positive integer given by $\\det\\big(A\\big)^\\frac{1}{2}$ is called the Pfaffian.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7.8.7**  \n",
    "for a skew-symmetric form defined as  \n",
    "$V \\text{ x } V\\longrightarrow \\mathbb F$  \n",
    "$\\langle \\mathbf v, \\mathbf v \\rangle = 0$   \n",
    "for all $\\mathbf v \\in V$  \n",
    "\n",
    "we define *orthogonal* with respect to skew symmetric bilinear forms to mean $\\langle \\mathbf v, \\mathbf x \\rangle = 0$.  This implies all vectors are self orthogonal in a skew symmetric form (in some ways the 'opposite' of when we have a positive definite symmetric form -- standard inner products).  \n",
    "\n",
    "we define null space to be the set of $\\mathbf v' \\in V$ such that \n",
    "$\\langle \\mathbf v, \\mathbf v' \\rangle = 0$  for all $v \\in V$.  \n",
    "i.e. $\\mathbf v'$ are *orthogonal to everything*  \n",
    "once we introduce bases and have a skew symmetric matrix $\\mathbf A$ to effect our bilinear form, these will be vectors in the nullspace of $\\mathbf A$   \n",
    "\n",
    "a vector space with a skew symmetric bilinear form may be bipartitioned into a portion that is degenerate and a portion that is non-degenerate. Degenerate subspaces include the zero vector and all all $\\mathbf v'$ in the nullspace.  The non-degenerate form / associated subspace includes the zero vector and for all  $\\mathbf 0 \\neq \\mathbf v' \\in V$ there exists some $v \\in V$ such that $\\langle \\mathbf v, \\mathbf v' \\rangle \\neq 0$.  Note that  $\\mathbf v' \\neq \\alpha \\mathbf v$ (for some scalar $\\alpha \\in \\mathbb F$) because if it did, \n",
    "$\\langle \\mathbf v, \\mathbf v' \\rangle=\\langle \\alpha \\mathbf v', \\mathbf v' \\rangle=\\alpha \\langle \\mathbf v', \\mathbf v' \\rangle = 0$ since all vectors are self-orthogonal in a skew symmetric form.   \n",
    "\n",
    "**a)**  \n",
    "a form is nondegenerate *iff* the matrix with respect to any basis is invertible.  It's for n x n $\\mathbf A$ the result is immediate-- if $\\text{rank}\\big(A\\big) = n$, this is preserved under change of basis (congruence transforms), so for non-zero $\\mathbf x$ , in particular with coordinate k being nonzero, we have  \n",
    "$\\mathbf x^T \\mathbf A \\mathbf y\\neq 0$ for some $\\mathbf y$, in particular with $\\mathbf y:= \\mathbf A^{-1}\\mathbf e_k$ the bilinear form evaluates to $x_k \\neq 0$.  On the other hand, if $\\text{rank}\\big(A\\big) \\leq n-1$, this too is preserved under congruence transforms and implies there is some $\\mathbf y \\neq \\mathbf 0$ such that $\\mathbf {Ay} =\\mathbf 0$ and hence $\\mathbf x^T \\mathbf 0 = \\mathbf x^T\\mathbf {Ay} = 0$ for all $\\mathbf x$ hence the form is degenerate.  \n",
    "\n",
    "**b)**  \n",
    "if $W$ is the subspace containing all $v\\in V$ such that the form is non-degenerate, then $V = W \\oplus W^\\perp$  its immediate that $W\\cap W^\\perp = \\{0\\}$ since if $\\mathbf w \\neq \\mathbf 0$ and it is an element of $W\\cap W^\\perp$ then because $\\mathbf w \\in W$ there is some $\\mathbf v$ such that $\\langle \\mathbf w, \\mathbf v\\rangle \\neq 0$ but because $\\mathbf \\in W^\\perp$, then $\\langle \\mathbf w, \\mathbf v\\rangle \\neq 0$ for all $\\mathbf v \\in V$ (this is our definition of degenerate).  Thus it can only be the case that the zero vector is in both.  To show that this direct sum spans all of $V$ requires a little more care than was given in the book on top of page 244... the span really comes from the construction of $W$ and $W^\\perp$.  We place the zero vector in each.  Now for any non-zero $\\mathbf v \\in V$ if it is degenerate with respect to the skew symmetric form then we assign it to $\\mathbf W^\\perp$ and if not we assign it to $W$.  \n",
    "\n",
    "**c,d)**  \n",
    "if the form is identically zero then all $\\mathbf v\\in V$ are in the nullspace.  If the form is not identically zero, then not all $\\mathbf v \\in V$ are in the nullspace.  So there is some $\\mathbf w_1$ and some $\\mathbf w_2$ such that  $\\langle \\mathbf w_1, \\mathbf w_2 \\rangle = \\gamma \\neq 0$.  I.e. this implies the existence of 2 linearly independent vectors (reference the above $\\mathbf v' \\neq \\alpha \\mathbf v$).  \n",
    "\n",
    "we may now consider $\\mathbf w_1' = \\mathbf w_1$ and $\\mathbf w_2' = \\gamma^{-1}\\mathbf w_2$ so we have  \n",
    "$\\langle \\mathbf w_1', \\mathbf w_2' \\rangle=1$ and of course, being skew symmetric, $\\langle \\mathbf w_2', \\mathbf w_1' \\rangle=-1$ \n",
    "\n",
    "this forms a non-degenerate subspace of $\\alpha_1\\mathbf w_1' + \\alpha_2\\mathbf w_2'$  \n",
    "\n",
    "let $W$ be the subspace of $V$ that has all non-degenerate vectors and $W^\\perp$ be the subspace of degenerate vectors.  $\\mathbf w_1, \\mathbf w_2\\in W$ and if $\\dim W = 2$ then we are done.  Suppose $\\dim W = r \\gt 2$.  Then this implies the existence of some non-degenerate $\\mathbf w_3$ that is linearly independent of $\\mathbf w_1', \\mathbf w_2'$.  So looking at our hyper-vector we have  \n",
    "\n",
    "\n",
    "$\\mathbf B = \n",
    "\\bigg[\\begin{array}{c|c|c|c||c|c|c|c} \n",
    "\\mathbf w_1  & \\mathbf w_2 & \\mathbf w_{3} & \\cdots&\\mathbf w_r &\\mathbf w^\\perp_1 & ...&  \\mathbf w^\\perp_{m} \n",
    "\\end{array}\\bigg]$  \n",
    "where by construction these vectors are linearly independent (and hence generate $V$)  \n",
    "\n",
    "now consider   \n",
    "$\\mathbf w_{3}^* := \\mathbf w_3 - \\alpha_1\\mathbf w_1+\\alpha_2\\mathbf w_2 + \\sum_{k=1}^m \\alpha_k^\\perp\\mathbf w^\\perp_k$\n",
    "and observe it is still linearly independent i.e.  \n",
    "\n",
    "$\\bigg[\\begin{array}{c|c|c|c||c|c|c|c} \n",
    "\\mathbf w_1  & \\mathbf w_2 & \\mathbf w_{3} & \\cdots&\\mathbf w_r &\\mathbf w^\\perp_1 & ...&  \\mathbf w^\\perp_{m} \n",
    "\\end{array}\\bigg]A= \\bigg[\\begin{array}{c|c|c|c||c|c|c|c} \\mathbf w_1  & \\mathbf w_2 & \\mathbf w_{3}^* & \\cdots&\\mathbf w_r &\\mathbf w^\\perp_1 & ...&  \\mathbf w^\\perp_{m} \n",
    "\\end{array}\\bigg]$  \n",
    "with invertible $A$ (which is the identity matrix except column 3 is allowed to have off diagonal entries hence $\\det\\big(A) = 1$)  \n",
    "\n",
    "\n",
    "we now in effect run through 'QR factorization' but using 'skewed Gram Schmidt'  \n",
    "first compute      \n",
    "$\\langle \\mathbf w_3, \\mathbf w_1' \\rangle = \\alpha_1$  \n",
    "$\\langle \\mathbf w_3, \\mathbf w_2' \\rangle = \\alpha_2$  \n",
    "\n",
    "Then set  \n",
    "$\\mathbf w_3^* := \\mathbf w_3 -\\alpha_2\\mathbf w_1' + \\alpha_1 \\mathbf w_2'$  \n",
    "which gives  \n",
    "$\\langle \\mathbf w_3^*, \\mathbf w_1' \\rangle = \\langle \\mathbf w_3, \\mathbf w_1' \\rangle - \\alpha_2\\langle \\mathbf w_1', \\mathbf w_1' \\rangle - \\alpha_1\\langle \\mathbf w_2', \\mathbf w_1' \\rangle = \\alpha_1 + 0 + \\alpha_1\\cdot (-1) = 0$  \n",
    "and \n",
    "$\\langle \\mathbf w_3^*, \\mathbf w_2' \\rangle = \\langle \\mathbf w_3, \\mathbf w_2' \\rangle - \\alpha_2\\langle \\mathbf w_1', \\mathbf w_2' \\rangle - \\alpha_1\\langle \\mathbf w_2', \\mathbf w_2' \\rangle = \\alpha_2 - \\alpha_2 \\cdot (+1)  + 0 = 0$  \n",
    "\n",
    "however $W$ is a subspace of non-degenerate vectors (and $\\mathbf w_3^*$ is linearly independent of $\\mathbf w_1$ and $\\mathbf w_2$ so it is non-zero), thus this implies the existence of some 4th linearly independent vector \n",
    "$\\mathbf w_4 \\in W$ such that $\\langle \\mathbf w_3^*, \\mathbf w_4 \\rangle =\\gamma \\neq 0$. For avoidance of doubt, if $\\mathbf w_4$ didn't exist, and e.g. $\\dim W =  3$, then $\\langle \\mathbf w_3^*, \\mathbf w \\rangle =0$, by using linearity of the second argument and writing $\\mathbf w$ as a linear combination of $\\mathbf w_1, \\mathbf w_2, \\mathbf w_3$.   for all $\\mathbf w \\in W$ which would contradict the definition of $\\mathbf w$.  But $\\dim W =  3$ is unnecessary here -- if $\\mathbf w_4$ didn't exists we could still run the same argument on the generators of $W$ and see $\\mathbf w_3^*$ is orthogonal to everything in $W$, implying degeneracy and contradicting our definition of $W$.    \n",
    "\n",
    "We repeat the above process to create $\\mathbf w_4^*$ so $\\langle \\mathbf w_4^*, \\mathbf w_1' \\rangle = 0 = \\langle \\mathbf w_4^*, \\mathbf w_2'\\rangle$.  But using bilinearity we can observe that  $\\langle \\mathbf w_3^*, \\mathbf w_4 \\rangle= \\langle \\mathbf w_3^*, \\mathbf w_4^* \\rangle =\\gamma \\neq 0$.  Now we do the 'normalization' so  \n",
    "$\\mathbf w_3' := \\mathbf w_3^*$  and $\\mathbf w_4' := \\gamma^{-1}\\mathbf w_4^*$  \n",
    "\n",
    "if $\\dim W=r =4$ we are done.  If $d\\gt 4$ repeat the process above.  And continue repeating until we have a 'skewed orthogonal set' that spans $W$. Notice at each stage there necessarily are 2 new vectors introduced $\\big\\{\\mathbf w_j', \\mathbf w_{j+1}'\\big\\}$ for $j$ odd (and since d is finite this process stops after finitely many iterations).  This tells us $d\\text{ % } 2 = 0$ i.e. $d$ is even.  \n",
    "\n",
    "We thus have  \n",
    "\n",
    "$\\mathbf BR' = \n",
    "\\bigg[\\begin{array}{c|c|c|c||c|c|c|c} \n",
    "\\mathbf w_1  & \\mathbf w_2 & \\mathbf w_{3} & \\cdots&\\mathbf w_r &\\mathbf w^\\perp_1 & ...&  \\mathbf w^\\perp_{m} \n",
    "\\end{array}\\bigg]R' = \n",
    "\\bigg[\\begin{array}{c|c|c|c||c|c|c|c} \n",
    "\\mathbf w_1'  & \\mathbf w_2' & \\mathbf w_{3}' & \\cdots&\\mathbf w_r' &\\mathbf w^\\perp_1 & ...&  \\mathbf w^\\perp_{m} \n",
    "\\end{array}\\bigg]$  \n",
    "\n",
    "where \n",
    "$R' =\\begin{bmatrix}R_0 & \\mathbf 0 \\\\ \\mathbf 0 & \\mathbf I_m \\end{bmatrix}$   \n",
    "where $R_0$ is upper triangular with no zeros on the diagonal (and indeed we know $r_{j,j}=1$ for off j)  \n",
    "\n",
    "or with  \n",
    "$R:= (R')^{-1}$  \n",
    "\n",
    "$B = \\bigg[\\begin{array}{c|c|c|c||c|c|c|c} \n",
    "\\mathbf w_1  & \\mathbf w_2 & \\mathbf w_{3} & \\cdots&\\mathbf w_r &\\mathbf w^\\perp_1 & ...&  \\mathbf w^\\perp_{m} \n",
    "\\end{array}\\bigg] = \n",
    "\\bigg[\\begin{array}{c|c|c|c||c|c|c|c} \n",
    "\\mathbf w_1'  & \\mathbf w_2' & \\mathbf w_{3}' & \\cdots&\\mathbf w_r' \n",
    "&\\mathbf w^\\perp_1 & ...&  \\mathbf w^\\perp_{m} \n",
    "\\end{array}\\bigg]R$   \n",
    "which emphasizes this / comparability with QR factorization  \n",
    "\n",
    "where for $j\\in\\{1,3,5,..., r-1\\}$  \n",
    "(recalling again that r must be even)   \n",
    "$\\langle \\mathbf w_j', \\mathbf w_{k}' \\rangle = 0$ for $k\\neq j+1$  and  \n",
    "$\\langle \\mathbf w_j', \\mathbf w_{j+1}' \\rangle = 1$  \n",
    "(skew symmetry then implies the result for even j)  \n",
    "\n",
    "\n",
    "*to close out loose ends*  \n",
    "using the first form, if we have   \n",
    "$\\mathbf BR' =\n",
    "\\bigg[\\begin{array}{c|c|c|c||c|c|c|c} \n",
    "\\mathbf w_1'  & \\mathbf w_2' & \\mathbf w_{3}' & \\cdots&\\mathbf w_r' &\\mathbf w^\\perp_1 & ...&  \\mathbf w^\\perp_{m} \n",
    "\\end{array}\\bigg]$  \n",
    "as our 'orthonormal' basis, then  (referencing page 239)  \n",
    "\n",
    "with \n",
    "$\\mathbf v = \\big(\\mathbf BR'\\big)\\mathbf x$ and  \n",
    "$\\mathbf z = \\big(\\mathbf BR'\\big)\\mathbf y$  \n",
    "\n",
    "$\\langle \\mathbf v, \\mathbf z\\rangle$  \n",
    "$= \\langle \\big(\\sum_{i=1}^r x_i\\mathbf w_i'\\big)+\\big(\\sum_{i=1}^m x_{n+i}\\mathbf w_i^\\perp\\big), \\big(\\big(\\sum_{k=1}^r y_i\\mathbf w_k'\\big)+\\big(\\sum_{k=1}^m y_{n+k}\\mathbf w_k^\\perp\\big)\\rangle$  \n",
    "$=\\langle \\sum_{i=1}^r x_i\\mathbf w_i', \\sum_{k=1}^r y_k\\mathbf w_k'\\rangle$  \n",
    "$=\\sum_{i=1}^r x_i\\langle \\mathbf w_i', \\sum_{k=1}^r y_k\\mathbf w_k'\\rangle$  \n",
    "$=\\sum_{i=1}^r\\sum_{k=1}^r y_k x_i\\langle \\mathbf w_i', \\mathbf w_k'\\rangle$  \n",
    "$=\\sum_{i=1}^r\\sum_{k=1}^r y_k x_i \\mathbf A_\\text{non-degenerate}$  \n",
    "\n",
    "with, for $i,k \\in\\{1,2,...,r\\}$\n",
    "$a_{i,k} = \\langle \\mathbf w_i', \\mathbf w_k'\\rangle$ and  \n",
    "$a_{i,k} :=0$ otherwise  \n",
    "\n",
    "Thus we have  \n",
    "$\\mathbf C := \\begin{bmatrix} 0 & 1 \\\\ -1 & 0\\end{bmatrix}$  \n",
    "and  \n",
    "\n",
    "$\\mathbf A :=\\begin{bmatrix}\\mathbf A_\\text{non-degenerate} &\\mathbf 0 \\\\ \\mathbf 0 & \\mathbf 0\\end{bmatrix}$  \n",
    "\n",
    "where $\\mathbf A \\in \\mathbb F^\\text{(r + m) x (r+m)}$ and  \n",
    "\n",
    "$\\mathbf A_\\text{non-degenerate} \\in \\mathbb F^\\text{r x r}$  \n",
    "and it is block diagonal with $\\frac{r}{2}$  blocks of $\\mathbf C$ \n",
    "\n",
    "note that up to graph isomorphism  \n",
    "$\\mathbf A_\\text{non-degenerate}$ is equivalent to \n",
    "\n",
    "$\\mathbf J_{2n} = \\begin{bmatrix}\\mathbf 0 & \\mathbf I_\\frac{r}{2} \\\\ -\\mathbf I_\\frac{r}{2} & \\mathbf 0\\end{bmatrix}$  \n",
    "\n",
    "which is the standard symplectic basis (see page 261)    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*remark:*  \n",
    "if we work through an extremely close process for symmetric forms over reals (or conjugate symmetric / Hermitian forms over $\\mathbb C$) and we recover congruence to   \n",
    "$\\begin{bmatrix}\\mathbf I_p & \\mathbf 0 &\\mathbf 0 \\\\ \\mathbf 0 & -\\mathbf I_m & \\mathbf 0 \\\\ \\mathbf 0 & \\mathbf 0 & \\mathbf 0\\end{bmatrix}$  \n",
    "\n",
    "where the number of positive elements and negative elements (and for dimension reasons 0s on the diagonal as well) is an invariant, per Sylvester's Law of Inertia.  The problem with trying to replicate the above argument when our field is $\\mathbb Q$ or $\\mathbb F_p$ is that in general the normalization stage, when dealing with the non-degenerate portion of symmetric form -- selecting $\\alpha$ such that $\\langle \\alpha\\mathbf w_j, \\alpha \\mathbf w_j\\rangle = \\vert \\alpha\\vert^2 \\cdot \\langle \\mathbf w_j, \\mathbf w_j\\rangle = 1$  implies that, up to a sign, $ \\alpha = \\big(\\langle \\mathbf w_j, \\mathbf w_j\\rangle\\big)^\\frac{1}{2}$  but for other fields, that square root may not exist.  \n",
    "\n",
    "\n",
    "note in the curious case of a complex (not Hermitian) symmetric bilinear form, we recover congruence to \n",
    "\n",
    "$\\begin{bmatrix}\\mathbf D & \\mathbf 0 \\\\\\mathbf 0 & \\mathbf 0\\end{bmatrix}$  \n",
    "where $\\mathbf D$  has points on the unit circle.  The (not unique) square roots of $\\mathbf D$ exist, so this gives us the answer to *ex 7.2.12* -- every complex symmetric non-singular matrix $\\mathbf A = \\mathbf P^T\\mathbf P$  (i.e. take some branch of square root and $\\mathbf P: = \\mathbf D^\\frac{1}{2}\\mathbf M^{-1}$ where $\\mathbf M$ is a congruence transform... note Artin uses $\\mathbf Q$ instead of $\\mathbf M$ though this can be confusing since $\\mathbf Q$ typically indicates unitary/orthogonal matrices but that is *not* the case for this congruence transform.)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7.9.6**  \n",
    "*Shown how to describe any element of $SO_4$ in terms of rotations of two orthogonal planes in* $\\mathbb R^4$  \n",
    "the end goal is to somehow represent the matrix in a form like that on page 125   \n",
    "\n",
    "for the rest of this writeup we use $\\mathbf A \\in \\mathbb R^\\text{n x n}$  where $\\mathbf A^T\\mathbf A = \\mathbf I$ and $\\det\\big(\\mathbf A\\big) = 1$  \n",
    "\n",
    "\n",
    "*commentary*  \n",
    "$\\mathbf A = \\frac{1}{2}\\big(\\mathbf A + \\mathbf A^T\\big)+ \\frac{1}{2}\\big(\\mathbf A - \\mathbf A^T\\big)$  \n",
    "i.e. $\\mathbf A$ is the sum of a real symmetric and skew symmetric matrix, so we can develop the result by trying to combine tools and techniques associated with symmetric and skew forms from this chapter.  \n",
    "\n",
    "\n",
    "\n",
    "We can approach this problem by developing a bilinear form on an orthonormal set of vectors.  The do this in $\\mathbb R^n$ with $n=4$ since that is what the problem specifies though it easily generalizes to any $n$  \n",
    "\n",
    "The goal is to build out the *real orthogonal* matrix  \n",
    "$\\mathbf V = \\bigg[\\begin{array}{c|c|c|c} \n",
    "\\mathbf v_1  &  \\mathbf v_2 & \\mathbf v_3 & \\mathbf v_4 \n",
    "\\end{array}\\bigg]$  \n",
    "\n",
    "\n",
    "what we want is to approach this 2 at a time, reminiscent of how we did it in the skew symmetric form case, where we want  \n",
    "$\\mathbf v_2^T\\mathbf v_1 =0$ via the standard inner product   \n",
    "and further we want to 'compartmentalize / isolate'  dot products as follows   \n",
    "$\\mathbf v_1^T\\mathbf A\\mathbf v_1 = \\frac{1}{2}\\mathbf v_1^T\\big(\\mathbf A + \\mathbf A^T\\big)\\mathbf v_1+ \\frac{1}{2}\\mathbf v_1^T\\big(\\mathbf A - \\mathbf A^T\\big)\\mathbf v_1 = \\frac{1}{2}\\mathbf v_1^T\\big(\\mathbf A + \\mathbf A^T\\big)\\mathbf v_1 + 0 = \\langle \\mathbf v_1, \\mathbf v_1\\rangle_\\text{sym}$  \n",
    "\n",
    "$\\mathbf v_2^T\\mathbf A\\mathbf v_1 = \\frac{1}{2}\\mathbf v_2^T\\big(\\mathbf A + \\mathbf A^T\\big)\\mathbf v_1+ \\frac{1}{2}\\mathbf v_2^T\\big(\\mathbf A - \\mathbf A^T\\big)\\mathbf v_1 = 0 +\\frac{1}{2}\\mathbf v_2^T\\big(\\mathbf A - \\mathbf A^T\\big)\\mathbf v_1 = \\langle \\mathbf v_2, \\mathbf v_1\\rangle_\\text{skew}$  \n",
    " \n",
    "now recall that we always have the first identity, since the real quadratic form of a real skew matrix is always zero.  But we can even ensure the second result -- \n",
    "$\\big(\\mathbf A + \\mathbf A^T\\big)$ is real symmetric and thus has real eigenvectors that may be selected to be mutually orthonormal so if we select $\\mathbf V$ to contain our eigenvectors we get the second identity as well.  \n",
    "\n",
    "The details follow.  First we 'attack' an eigenvector $\\mathbf v$ of $\\big(\\mathbf A +\\mathbf A^T\\big)$, and see how it relates to $\\mathbf A$ and $\\mathbf A^T$ individually       \n",
    "\n",
    "$\\mathbf A \\mathbf v = \\alpha_1\\mathbf v + \\alpha'\\mathbf v'$  \n",
    "where $\\mathbf v' \\perp \\mathbf v$  and has unit length (*all* vectors have unit length unless indicated otherwise).  Note if if $\\alpha_1 =0$ or $\\alpha' =0$ then the desired result trivially holds -- so we omit these cases.  \n",
    "\n",
    "and left multiplying by $\\mathbf A^T$  \n",
    "$\\mathbf v = \\mathbf A^T\\mathbf A \\mathbf v = \\alpha_1\\mathbf A^T\\mathbf v + \\alpha'\\mathbf A^T\\mathbf v'$  \n",
    "$\\longrightarrow  \\mathbf A^T\\mathbf v =\\alpha_1^{-1}\\mathbf v -\\alpha_1^{-1}\\alpha'\\mathbf A^T\\mathbf v'$  \n",
    "\n",
    "adding the two lines,  \n",
    "$\\lambda_1 \\mathbf v =\\big(\\mathbf A + \\mathbf A^T\\big) \\mathbf v =\\mathbf A \\mathbf v + \\mathbf A^T \\mathbf v = \\alpha_1\\mathbf v + \\alpha'\\mathbf v' + \\alpha_1^{-1}\\mathbf v -\\alpha_1^{-1}\\alpha'\\mathbf A^T\\mathbf v' = \\gamma_1\\mathbf v + \\big(\\alpha'\\mathbf v'-\\alpha_1^{-1}\\alpha'\\mathbf A^T\\mathbf v'\\big)$  \n",
    "\n",
    "the right hand side tells us  \n",
    "$\\mathbf A^T \\mathbf v' = \\beta'\\mathbf v'+\\beta_1 \\mathbf v $  \n",
    "so we have a closed system.  \n",
    "\n",
    "equivalently we have  \n",
    "$\\mathbf 0 = (-\\lambda_1+\\gamma_1)\\mathbf v + \\alpha'\\mathbf v'-\\alpha_1^{-1}\\alpha'\\mathbf A^T\\mathbf v'$  \n",
    "which cannot be zero unless $\\mathbf A^T\\mathbf v'$ can be written as a linear combination $\\mathbf v$ and $\\mathbf v'$   \n",
    "- - - -  \n",
    "\n",
    "Left multiplying $\\mathbf A^T \\mathbf v' = \\beta'\\mathbf v'+\\beta_1 \\mathbf v $  by $\\mathbf A$ and collecting terms we see   \n",
    "$\\mathbf A\\mathbf v' = \\eta' \\mathbf v' + \\eta_1 \\mathbf v$  \n",
    "\n",
    "it remains to confirm that $\\mathbf v'$ is in fact an eigenvector of   \n",
    "$\\big(\\mathbf A + \\mathbf A^T\\big)$   \n",
    "\n",
    "i.e. we want to show   \n",
    "$\\big(\\mathbf A+\\mathbf A^T\\big) \\mathbf v' = (\\eta'+\\beta')\\mathbf v'+ (\\eta_1+\\beta_1) \\mathbf v =\\big(\\eta'+\\beta'\\big)\\mathbf v'$   \n",
    "or equivalently that  $\\eta_1+\\beta_1 = 0$   \n",
    "\n",
    "this is true, e.g. making use of $\\mathbf v ^T \\mathbf v'= 0$ and that $\\mathbf v$ is an eigenvector of a symmetric matrix, we compute the same thing two different ways  \n",
    "$0=\\lambda_1 \\mathbf v^T\\mathbf v' = \\Big(\\mathbf v^T\\big(\\mathbf A+\\mathbf A^T\\big)\\Big) \\mathbf v'=\\mathbf v^T\\Big(\\big(\\mathbf A+\\mathbf A^T\\big) \\mathbf v'\\Big) =\\mathbf v^T\\big((\\eta'+\\beta') \\mathbf v' +  (\\eta_1+\\beta_1) \\mathbf v\\big) =  (\\eta_1+\\beta_1)  \\big\\Vert \\mathbf v\\big\\Vert_2^2 =  (\\eta_1+\\beta_1)  $  \n",
    "as desired  \n",
    "\n",
    "\n",
    "we may re-label \n",
    "$\\mathbf v_1:=\\mathbf v$  \n",
    "$\\mathbf v_2:=\\mathbf v'$  \n",
    "\n",
    "and now select a new eigenvector of  \n",
    "$\\big(\\mathbf A+\\mathbf A^T\\big)$, and call it $\\mathbf v_3$  \n",
    "\n",
    "and see  \n",
    "$\\mathbf A \\mathbf v_3 = \\alpha\\mathbf v_3 + \\alpha'\\mathbf v'$    \n",
    "(we will relabel it $\\mathbf v_4$ at the end)  \n",
    "\n",
    "and repeat the before process.  \n",
    "\n",
    "Now for avoidance of doubt, we *know* $\\mathbf v'\\perp \\mathbf v_1, \\mathbf v_2$  \n",
    "For a simple argument, consider  \n",
    "$\\mathbf A\\mathbf V = \\bigg[\\begin{array}{c|c|c|c} \n",
    "\\mathbf v_1  &  \\mathbf v_2 & \\cdots & \\mathbf v_n\n",
    "\\end{array}\\bigg]\\mathbf D = \\mathbf V\\begin{bmatrix}  \\langle \\mathbf v_1, \\mathbf v_1\\rangle_\\text{sym} &\\langle \\mathbf v_2, \\mathbf v_1\\rangle_\\text{skew} &\\mathbf x_1^T \\\\  \\langle \\mathbf v_1, \\mathbf v_2\\rangle_\\text{skew} &\\langle \\mathbf v_2, \\mathbf v_2\\rangle_\\text{sym} & \\mathbf x_2^T \\\\ \\mathbf 0 & \\mathbf 0 & \\mathbf *\\end{bmatrix}= \\mathbf V\\begin{bmatrix}  \\langle \\mathbf v_1, \\mathbf v_1\\rangle_\\text{sym} &\\langle \\mathbf v_2, \\mathbf v_1\\rangle_\\text{skew} &\\mathbf 0^T \\\\  \\langle \\mathbf v_1, \\mathbf v_2\\rangle_\\text{skew} &\\langle \\mathbf v_2, \\mathbf v_2\\rangle_\\text{sym} & \\mathbf 0^T \\\\ \\mathbf 0 & \\mathbf 0 & \\mathbf *\\end{bmatrix}$    \n",
    "\n",
    "\n",
    "$\\mathbf V^T\\mathbf A\\mathbf V =\\mathbf D$ is an orthogonal matrix, thus the first column has length one which implies the first row has length one as well, and the same for the second column and second row, and thus  $\\mathbf D$ is a block diagonal matrix real orthogonal matrix.    \n",
    "\n",
    "The blocks each are orthogonal so they come in the form  \n",
    "\n",
    " $ \\begin{bmatrix}\\langle \\mathbf v_1, \\mathbf v_1\\rangle_\\text{sym} & \\langle \\mathbf v_2, \\mathbf v_1\\rangle_\\text{skew} \\\\\\langle \\mathbf v_1, \\mathbf v_2\\rangle_\\text{skew} & \\langle \\mathbf v_2, \\mathbf v_2\\rangle_\\text{sym} \\end{bmatrix} = \\begin{bmatrix} \\cos(\\theta) & -\\sin(\\theta) \\\\ \\sin(\\theta) & \\cos(\\theta) \\end{bmatrix}$  \n",
    " \n",
    "The above process terminates now in the case of $n=4$ but may be repeated as needed to fill out the 2x2 blocks for the case of an arbitrary $n$.  (Note: in the trivial cases that we neglected, this results in 1 x 1 blocks containing a plus 1 or minus 1.)     \n",
    "\n",
    "**extension**  \n",
    "we rarely made explicit use of othogonality of $\\mathbf A$.  The first time was early on where we left multiplied by $\\mathbf A^T$ to get  \n",
    "$\\mathbf v = \\mathbf A^T\\mathbf A \\mathbf v = \\alpha_1\\mathbf A^T\\mathbf v + \\alpha'\\mathbf A^T\\mathbf v'\\longrightarrow  \\mathbf A^T\\mathbf v =\\alpha_1^{-1}\\mathbf v -\\alpha_1^{-1}\\alpha'\\mathbf A^T\\mathbf v'$  \n",
    "\n",
    "however for any *normal* $\\mathbf A$, we know that $\\mathbf A + \\mathbf A^T$ has the same eigenvectors as $\\mathbf A\\mathbf A^T$ so if we were working more generally with a normal matrix, we could have written  \n",
    "$\\mathbf A^T\\mathbf A \\mathbf v = \\sigma^2\\mathbf v$    \n",
    "\n",
    "(as will be expanded on in the below box)  \n",
    "if $\\gamma$ is an eigenvalue of normal $\\mathbf A$, then what we get with  \n",
    "\n",
    "$\\mathbf A^T\\mathbf A$ is $\\big \\vert \\gamma\\big\\vert^2 = \\sigma^2$  \n",
    "and what we get with \n",
    "\n",
    "$\\big(\\mathbf A + \\mathbf A^T\\big) =\\big(\\mathbf A + \\mathbf A^*\\big)$ is $ \\gamma + \\bar{\\gamma} = 2\\cdot \\text{real}\\big(\\gamma\\big)$  \n",
    "(SVD and Polar Decomposition are other ways of bridging the gap to relate the eigenvectors and singular vectors of $\\mathbf A$)  \n",
    "\n",
    "and the only other time we made use of orthogonality of $\\mathbf A$ was  \n",
    "$\\mathbf A^T \\mathbf v' = \\beta'\\mathbf v'+\\beta_1 \\mathbf v  \\longrightarrow \\mathbf A\\mathbf A^T \\mathbf v' = \\beta'\\mathbf A\\mathbf v'+\\beta_1 \\mathbf A\\mathbf v $  \n",
    "and we used $\\mathbf A\\mathbf A^T = \\mathbf I$   \n",
    "however for normal $\\mathbf A$, writing $\\mathbf A\\mathbf v'$ as a linear combination of  \n",
    "$\\mathbf A\\mathbf A^T \\mathbf v'$ and $\\beta_1 \\mathbf A\\mathbf v $   \n",
    "would work perfectly well for our purposes -- we could still add $\\mathbf A \\mathbf v' +\\mathbf A^T \\mathbf v'$ and run the same othogonality argument as before -- then being able to relate eigenvectors of $\\big(\\mathbf A + \\mathbf A^*\\big)$ and $\\big(\\mathbf A\\mathbf A^*\\big)$ would give the result. The argument would proceed as before until we have  \n",
    "\n",
    "\n",
    "and \n",
    "$\\mathbf A \\mathbf v_3 = \\alpha\\mathbf v_3 + \\alpha'\\mathbf v'$   \n",
    "where $\\mathbf v'$ is *linearly independent* from $\\mathbf v_3$, and running Gram Schmidt we could decompose it into a linear combination of $\\{\\mathbf v_1,\\mathbf v_2, \\mathbf v_3, \\mathbf v''\\}$ where $\\mathbf v''$ is orthogonal to all the preceding vectors so we know    \n",
    "\n",
    "$\\mathbf A =  \\mathbf V\\begin{bmatrix}  \\langle \\mathbf v_1, \\mathbf v_1\\rangle_\\text{sym} &\\langle \\mathbf v_2, \\mathbf v_1\\rangle_\\text{skew} &\\mathbf x_1^T \\\\  \\langle \\mathbf v_1, \\mathbf v_2\\rangle_\\text{skew} &\\langle \\mathbf v_2, \\mathbf v_2\\rangle_\\text{sym} & \\mathbf x_2^T \\\\ \\mathbf 0 & \\mathbf 0 & \\mathbf *\\end{bmatrix}\\mathbf V^T = \\mathbf V\\mathbf R  \\mathbf V^T$  \n",
    "\n",
    "and we'd like to confirm the block triangular structure -- that $\\mathbf x_1^T$ and $\\mathbf x_2^T$ are in fact equal to the zero vector.  \n",
    "\n",
    "Since $\\mathbf A$ is normal -- this implies $\\mathbf R$ is normal, so we can mimic the proof midway through the Schur's Inequality notebook regarding the commuting of an upper triangular matrix and its (conjugate) transpose, except here we need only focus just on rows, cols 1,1 and 2,2.     \n",
    "\n",
    "$\\mathbf R = \n",
    "\\bigg[\\begin{array}{c|c|c|c}\n",
    "\\mathbf r_1 & \\mathbf r_2 &\\cdots & \\mathbf r_{n}\n",
    "\\end{array}\\bigg]$  \n",
    "and  \n",
    "$\\mathbf R= \n",
    "\\begin{bmatrix}\n",
    "\\tilde{ \\mathbf r_1}^T \\\\\n",
    "\\tilde{ \\mathbf r_2}^T \\\\ \n",
    "\\vdots\\\\ \n",
    "\\tilde{ \\mathbf r}_{n-1}^T \\\\ \n",
    "\\tilde{ \\mathbf r_n}^T\n",
    "\\end{bmatrix}$  \n",
    "\n",
    "but \n",
    "$\\mathbf R^T \\mathbf R = \\mathbf R \\mathbf R^T$  \n",
    "so focusing on the diagonal, we see  \n",
    "\n",
    "$\\big\\Vert\\mathbf r_1\\big\\Vert_2^2 =\\langle \\mathbf v_1, \\mathbf v_1\\rangle_\\text{sym}^2 + \\langle \\mathbf v_1, \\mathbf v_2\\rangle_\\text{skew}^2= \\langle \\mathbf v_1, \\mathbf v_1\\rangle_\\text{sym}^2 + \\langle \\mathbf v_2, \\mathbf v_1\\rangle_\\text{skew}^2 =  \\langle \\mathbf v_1, \\mathbf v_1\\rangle_\\text{sym}^2 + \\langle \\mathbf v_2, \\mathbf v_1\\rangle_\\text{skew}^2+\\big\\Vert \\mathbf x_1^T\\big\\Vert_2^2 =  \\big\\Vert\\ \\tilde{ \\mathbf r_1}^T\\big\\Vert_2^2$  \n",
    "and by positive definiteness of the 2 norm, we see $\\big\\Vert \\mathbf x_1^T\\big\\Vert_2^2 = 0$  \n",
    "\n",
    "This argument may be repeated as needed for $\\mathbf x_2^T$ then $\\mathbf x_3^T$ and so on, which confirms that $\\mathbf D$ is block diagonal.  \n",
    "\n",
    "*remark:*  \n",
    "This derivation works on any normal matrix $\\mathbf A$.  A closely related idea is the Real Schur Triangularization $\\mathbf B = \\mathbf V \\mathbf R \\mathbf V^T$ for any square real $\\mathbf B$ and $\\mathbf R$ being essentially upper triangular (with perhaps 2x2 blocks on the diagonal).  However the above stated approach does not seem to immediately yield the general Real Schur Triangularization-- it only yields the result for the the 'special case' when $\\mathbf B$ is normal.  \n",
    "\n",
    "*additional remark:*  \n",
    "if we revisit the orthogonal matrix that is considered in the problem (and consider the 2 trivial cases that were omitted) we see up to graph isomorphism  \n",
    "$\\mathbf V^T\\mathbf A \\mathbf V = \n",
    "\\begin{bmatrix}D_{2m} & \\mathbf 0 &\\mathbf 0 \\\\ \\mathbf 0 & -\\mathbf I_{r} & \\mathbf 0 \\\\ \\mathbf 0 & \\mathbf 0 & \\mathbf I_{n-2m -r}\\end{bmatrix}$  \n",
    "\n",
    "where  \n",
    "$\\mathbf D_{2m}$  \n",
    "is block diagonal with 2x2 blocks of the form  \n",
    "\n",
    "$\\begin{bmatrix}\\langle \\mathbf v_k, \\mathbf v_k\\rangle_\\text{sym} & \\langle \\mathbf v_{k+1}, \\mathbf v_{k}\\rangle_\\text{skew} \\\\\\langle \\mathbf v_k, \\mathbf v_{k+1}\\rangle_\\text{skew} & \\langle \\mathbf v_{k+1}, \\mathbf v_{k+1}\\rangle_\\text{sym} \\end{bmatrix} = \\begin{bmatrix} \\cos(\\theta_k) & -\\sin(\\theta_k) \\\\ \\sin(\\theta_k) & \\cos(\\theta_k) \\end{bmatrix}$  \n",
    "\n",
    "each 2 x 2 block has a determinant of 1, so  \n",
    "$\\det\\big(\\mathbf A\\big)=\\det\\big(-\\mathbf I_{r}\\big) = (-1)^r$  \n",
    "therefore if the determinant is 1 (i.e. the case of $SO_n$) then $r$ is even.  This means that  \n",
    "\n",
    "$-\\mathbf I_{r} = \\mathbf D_r'$  \n",
    "where is again block diagonal with 2x2 blocks of the form  \n",
    "$\\begin{bmatrix} \\cos(\\theta) & -\\sin(\\theta) \\\\ \\sin(\\theta) & \\cos(\\theta) \\end{bmatrix}$  \n",
    "except we have $\\theta = \\pi$  \n",
    "\n",
    "This gives us yet another proof that $SO_n$ is path connected to the identity -- this time for the same reason that any element of $S0_2$ is path connected to the identity.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*a different approach to the exercise*  \n",
    "\n",
    "another way to the core of the above results comes from changing the field. Our real orthogonal matrix may be considered a complex unitary matrix, which, being normal, is unitarily diagonalizable per spectral theorem.  *In the interest of a crisp argument we consider only the case where* $\\mathbf A$ *has distinct eigenvalues* -- the 'more general' case with repeated eigenvalues may be dealt with by more carefully working through the algebra, or via perturbations starting with a distinct eigenvalue case and considering the general case as a limit (where compactness of the orthogonal / unitary groups implies a convergent subsequence for $\\mathbf V_k$ that we can exploit).    \n",
    "\n",
    "so working over $\\mathbb C$ we have  \n",
    "$\\mathbf A = \\mathbf Q\\mathbf \\Lambda \\mathbf Q^*$  \n",
    "\n",
    "and all $\\lambda_k$ are on the unit circle.  Any eigenvalues that are real are +1 or -1 and are no issue for solving the problem.  The issue comes from non-real eigenvalues of $\\mathbf A$ which come in conjugate pairs, and typically have non-real eigenvectors. So consider non-real $\\lambda_1$, and associated eigenvector $\\mathbf q_1$   and \n",
    "\n",
    "the matrix  \n",
    "$\\mathbf A - \\lambda_1\\mathbf I$  \n",
    "is still normal, so we know from ex *7.7.1* that it has the same kernel as $\\big(\\mathbf A - \\lambda_1\\mathbf I\\big)^*$ i.e. any vector in one is in both, so $\\mathbf q_1$ is in both.  However when we add the two matrices we see that $\\mathbf q_2$, associated with $\\lambda_2=\\bar{\\lambda_1}$ is in the kernel as well, i.e.  \n",
    "\n",
    "$\\Big(\\mathbf A - \\lambda_1\\mathbf I + \\big(\\mathbf A - \\lambda_1\\mathbf I\\big)^*\\Big)\\mathbf q_2= \\Big(\\mathbf A + \\mathbf A^*- \\lambda_1\\mathbf I  - \\bar{\\lambda_1}\\mathbf I\\Big)\\mathbf q_2 = \\mathbf A\\mathbf q_2 + \\mathbf A^*\\mathbf q_2 - 2re(\\lambda_1)\\mathbf q_2= \\lambda_2\\mathbf q_2 + \\bar{\\lambda_2}\\mathbf q_2 - 2re(\\lambda_1)\\mathbf q_2 = \\mathbf 0$  \n",
    "(the argument can be easier to follow if we suppose WLOG that A is diagonal)   \n",
    "\n",
    "\n",
    "$\\mathbf q_1, \\mathbf q_2 \\in \\ker\\Big(\\mathbf A - \\lambda_1\\mathbf I + \\big(\\mathbf A - \\lambda_1\\mathbf I\\big)^*\\Big)=\\ker\\Big(\\mathbf A + \\mathbf A^* - 2\\cdot re(\\lambda_1)\\mathbf I\\Big)=\\ker\\Big(\\mathbf A + \\mathbf A^T - \\lambda_{A+A^T}\\mathbf I\\Big)$  \n",
    "\n",
    "so  \n",
    "\n",
    "$\\mathbf Q^*\\mathbf A\\mathbf Q + \\mathbf Q^*\\mathbf A^*\\mathbf Q -2\\cdot re(\\lambda_1)\\mathbf I = \\mathbf \\Lambda+\\mathbf \\Lambda^*-2\\cdot re(\\lambda_1)\\mathbf I$  \n",
    "has a 2 dimensional kernel (since all eigenvalues are distinct and on the unit circle)  \n",
    "\n",
    "but this is equivalent to saying  \n",
    "$\\dim \\ker\\Big(\\mathbf A + \\mathbf A^T - \\lambda_{A+A^T}\\mathbf I\\Big)=2$   \n",
    "\n",
    "and this holds whether we keep the field to be $\\mathbb C$ or change it to be $\\mathbb R$.  \n",
    "i.e. to make this explicit: by rank-nullity it is sufficient to prove that the rank of $\\big(\\mathbf A + \\mathbf A^T - \\lambda_{A+A^T}\\mathbf I\\big)$ stays constant at $n-2$ and e.g. from ex 4.Misc.10 we know know that in $\\mathbb C$ our matrix has an n-2 x n-2 submatrix with non-zero determinant and no larger submatrix with nonzero determinant but these polynomials have entirely real components so they don't change when we change the field from $\\mathbb C$ to $\\mathbb R$.  \n",
    "\n",
    "and we know $\\big(\\mathbf A+\\mathbf A^T\\big)$ has real mutually orthogonal vectors $\\mathbf v_1$, $\\mathbf v_2$ in its kernel. Since the both form a basis for the kernel (over $\\mathbb C$) we have  \n",
    "\n",
    "$\\bigg[\\begin{array}{c|c|c|c} \n",
    "\\mathbf q_1  &\\mathbf q_2& \\cdots &\\mathbf q_n  \n",
    "\\end{array}\\bigg]\\begin{bmatrix}X & * \\\\ \\mathbf 0 & *\\end{bmatrix} = \\bigg[\\begin{array}{c|c|c|c} \n",
    "\\mathbf q_1  &\\mathbf q_2& \\cdots &\\mathbf q_n  \n",
    "\\end{array}\\bigg]\\begin{bmatrix}X & \\mathbf 0 \\\\ \\mathbf 0 & *\\end{bmatrix} = \\bigg[\\begin{array}{c|c|c|c} \n",
    "\\mathbf v_1  &\\mathbf v_2& \\cdots &\\mathbf v_n  \n",
    "\\end{array}\\bigg]$      \n",
    "\n",
    "where $\\mathbf Q$ and $\\mathbf V$ are unitary matrices which implies our mapping between them is as well (so everything on the first two rows, but not in $X$ is zero for length reasons).  \n",
    "\n",
    "so in some sense our problem is to go from  \n",
    "\n",
    "$\\mathbf A = \\mathbf Q\\mathbf \\Lambda \\mathbf Q^* = \\Big(\\mathbf Q \\begin{bmatrix}X & \\mathbf 0 \\\\ \\mathbf 0 & *\\end{bmatrix}\\Big)\\begin{bmatrix}X & \\mathbf 0 \\\\ \\mathbf 0 & *\\end{bmatrix}^* \\mathbf \\Lambda \\begin{bmatrix}X & \\mathbf 0 \\\\ \\mathbf 0 & *\\end{bmatrix}\\Big(\\begin{bmatrix}X & \\mathbf 0 \\\\ \\mathbf 0 & *\\end{bmatrix}^*\\mathbf Q^*\\Big)=\\mathbf V \\Big(\\begin{bmatrix}X & \\mathbf 0 \\\\ \\mathbf 0 & *\\end{bmatrix}^* \\mathbf \\Lambda \\begin{bmatrix}X & \\mathbf 0 \\\\ \\mathbf 0 & *\\end{bmatrix}\\Big)\\mathbf V^T$  \n",
    "(where this slightly overloads notation as we have * inside the matrix denoting entries we aren't concerned about and * to the top right of a matrix denoting conjugate transpose)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**remark:**  \n",
    "\n",
    "when viewing n x n matrices as living in a vector space, we may consider the symmetric bilinear form given by  \n",
    "$\\langle A, B\\rangle := \\text{trace}\\big(AB\\big)$   \n",
    "\n",
    "this is *not* an inner product but nevertheless is a symmetric bilinear form that is commonly used  \n",
    "(ref e.g. Killing Form on page 304 and some other items in chapter 8)  \n",
    "\n",
    "and if we wanted to e.g. characterize the dimension of trace zero matrices, we could define a 1 dimensional subspace  \n",
    "\n",
    "$\\alpha I \\in W$ so the orthgonal complement is defined by  \n",
    "$A \\in W^\\perp$ *iff*  \n",
    "$0 =\\langle A,  I\\rangle$  \n",
    "now in general n x n matrices live in a vector space with dimension $n^2$ and $W$ has dimension 1, but  \n",
    "$W \\oplus W^\\perp = V$  so  \n",
    "$n^2 = \\dim W + \\dim W^\\perp = 1 + \\dim W^\\perp \\longrightarrow \\dim W^\\perp = n^2-1$    \n",
    "\n",
    "which in some sense is a formal way of proving the obvious that trace zero matrices live in a vector space with dimension $n^2-1$  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
