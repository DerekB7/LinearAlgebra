{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where the 2nd moments exists and both random variables have zero mean, and $Y$ has unit variance remark: \n",
    "\n",
    "\n",
    "$\\text{Cov}\\big(Y,X\\big) = \\text{Cov}\\big(Y, X - E[X\\big \\vert Y\\big] \\big) + \\text{Cov}\\big(Y, E[X\\big \\vert Y\\big] \\big) = 0+\\text{Cov}\\big(Y, E[X\\big \\vert Y\\big] \\big) = \\text{Cov}\\big(Y, E[X\\big \\vert Y\\big] \\big)$  \n",
    "\n",
    "\n",
    "Covariance is a bilinear function, so we know the residual has zero covariance, because  \n",
    "\n",
    "$\\text{Cov}\\big(Y,X - E[X\\big \\vert Y\\big] \\big)$  \n",
    "$= E\\Big[Y\\big(X - E[X\\big \\vert Y\\big]\\big)\\Big] - E\\Big[Y\\Big]\\cdot E\\Big[X - E\\big[X\\big \\vert Y\\big]\\Big] $  \n",
    "$=  E\\Big[XY - Y\\cdot E\\big[X\\big \\vert Y\\big]\\Big] - 0 $  \n",
    "$= E\\Big[XY\\Big] - E\\Big[Y \\cdot E\\big[X\\big \\vert Y\\big]\\Big]$  \n",
    "$= E\\Big[E\\big[XY\\big \\vert Y \\big]\\Big] - E\\Big[E\\big[XY\\big \\vert X\\big]\\Big]$  \n",
    "$= 0$  \n",
    "where the 2nd to last line applied the Tower Property  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a mix of things from Ross & Pekoz, vol1 Karlin & Taylor, and perhaps other sources.  \n",
    "\n",
    "This will be updated, slowly, over time.  A *very* large amount of this is related to Azuma Hoeffding, and Hoeffding's Lemma in particular as your author way unhappy with available proofs on the internet as well as in Ross & Pekoz.  \n",
    "\n",
    "A very long (and tedious) original proof is first given and then under the heading of \"a radical streamlining\" the proof is given in merely a few lines, by a simple insight your author had upon finishing the first proof -- work in logspace and use cumulants, then wield linear lower bound property for convex functions-- problem solved.  Your author has, for now, chosen to leave in the long cumbersome original proof, though as it is frequently helpful for others to leave the 'scaffolding' in place.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Azuma Hoeffding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Hoeffding's lemma:**   \n",
    "\n",
    "**note the approach is original, and rather long.  A sketch of key steps is as follows:**  \n",
    "\n",
    "1.) we want to prove, with centered (zero mean) Bernouli random variable $Z= X - \\bar{X}$  \n",
    "$E\\big[e^{tZ}\\big] \\leq \\exp\\big(\\frac{t^2}{8}\\big)$    \n",
    "\n",
    "for $t \\gt 0$, though we'll prove the slightly more general case for $t\\geq 0$  \n",
    "\n",
    "2.)  note that by convexity $1 = \\exp\\big({E[tZ]}\\big) \\leq E\\big[e^{tZ}\\big] $ and because $0\\leq \\frac{t^2}{8}$ we have $1 \\leq \\exp\\big(\\frac{t^2}{8}\\big)$  \n",
    "\n",
    "3.) further note that for $t=0$ we have $E\\big[e^{tZ}\\big] = \\exp\\big(\\frac{t^2}{8}\\big)$ -- i.e. the inequality is an equality and $= 1$ (or in logspace equals 0).  \n",
    "\n",
    "4.) Since both sides are always positive we may choose to work in logspace\n",
    "\n",
    "5.) In essence consider the function $f_p(t) =\\exp\\big(\\frac{t^2}{8}\\big) - E\\big[e^{tZ}\\big]$ and prove it is non-negative for some closed interval including zero -- in this case for $t \\in [0,2]$.  The method used for this will not directly generalize for all positive $t$ but this positive interval is enough to kickstart the process.  Note that $\\exp\\big(\\frac{t^2}{8}\\big)$ is invariant to choice of $p$.  If we want to show that the minimum of $f_p(t)\\geq 0$ for $t \\in [0,2]$ it's enough to do so when selecting the maximizing value of $p$.  We prove that the inequality is satisfied for the \"easy case\" of $q \\in [0, \\frac{1}{2}]$ and then prove that the maximum must be in $q \\in [0, \\frac{1}{2}]$ proves the claim for any choice of $p$ for any $t \\in [0,2]$.  \n",
    "\n",
    "6.)  Taking advantange of positivity, we recognize that we could also 'merely' prove \n",
    "$\\frac{\\exp\\big(\\frac{t^2}{8}\\big)}{E\\big[e^{tZ}\\big]}\\geq 1$.  We've already done this for $t\\in [0,2]$ in step 5 (i.e. both terms are know to be non-negative in step 3, and step 5 shows that the numerator is larger than the denominator at least for $t \\in [0,2]$.  So we work in logspace with cumulants, instead of MGF and focus on proving  \n",
    "\n",
    "$\\phi(t) =  \\log\\Big(\\frac{\\exp \\big(\\frac{t^2}{8}\\big)}{E\\big[e^{tZ}\\big]}\\Big) = \\frac{t^2}{8} - \\log\\Big(E\\big[e^{tZ}\\big]\\Big) = \\frac{t^2}{8} - \\gamma_Z(t)\\geq 0$  \n",
    "\n",
    "We can apply Mean Value Theorem to see that there exists a $u \\in (0,2)$ such that $\\phi'(u) \\geq 0$.  And by work in $5$ we know that for any $u \\in (0,2)$ $\\phi(u) \\geq 0$.  If $\\phi$ is convex, we thus have the makings of a linear lower bound that is non-negative for $t\\geq u$ which includes all of $t\\geq 2$ -- giving us the desired result for all $t\\geq 0$.  \n",
    "\n",
    "7.) To verify the convexity of $\\phi$ we differentiate twice -- some additional maneuvering is needed to tease out the positivity.  \n",
    "\n",
    "- - - - -\n",
    "\n",
    "consider a centered Bernouli random variable $Y = X - \\bar{X} = X - p$  and its negative $Z = -Y$ which is also a centered Bernouli random variable -- which is positive with probability $(1-p)$.  \n",
    "\n",
    "The key idea is, to first tackle $p \\in \\big[\\frac{1}{2}, 1]$ then do the other portion later. \n",
    "\n",
    "So for $t \\gt 0$, \n",
    "\n",
    "though we start by considering $t\\in [0,2]$:  \n",
    "\n",
    "$E\\big[e^{tZ}\\big] = E\\big[e^{-t Y}\\big]$    \n",
    "$=E\\big[f(-t)\\big] $  \n",
    "$=  E\\big[f(0) +f'(0)(-t-0)+ \\frac{f''(0)(-t-0)^2}{2!} + R_3\\big] $  \n",
    "$=  E\\big[f(0) +f'(0)(-t)+ \\frac{f''(0)(t)^2}{2!} + R_3\\big] $  \n",
    "$=  E\\big[f(0)\\big] +E\\big[f'(0)\\big](-t)+ E\\big[f''(0)\\big]\\frac{(t)^2}{2!} + E\\big[R_3\\big]$  \n",
    "$=  1 + 0 + E\\big[Y^2\\big]\\frac{t^2}{2} + E\\big[R_3\\big]$  \n",
    "$=  1 + \\text{var}\\big(Y^2\\big)\\frac{t^2}{2} + E\\big[R_3\\big]$  \n",
    "$\\leq  1 + \\frac{t^2}{8} + E\\big[R_3\\big]$  \n",
    "(by Grues's Inequality)  \n",
    "\n",
    "what we want to show is that for the cubic remainder term for our Taylor Polynomial $E\\big[R_3\\big]\\leq 0$ \n",
    "\n",
    "giving us \n",
    "\n",
    "$E\\big[e^{tZ}\\big] \\leq  1 + \\frac{t^2}{8} + E\\big[R_3\\big] \\leq 1 + \\frac{t^2}{8} \\leq e^{\\frac{t^2}{8}}$  \n",
    "\n",
    "which is the Hoeffding Lemma, finished off by the familiar fact that $1 + s \\leq e^s$  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$E\\big[R_3\\big] = E\\big[f'''(u)\\frac{(-t)^3}{3!}\\big]= -\\frac{t^3}{3!} E\\big[f'''(u)\\big]\\leq 0$   \n",
    "for some $ u\\in [-t, 0]$  \n",
    "hence our goal is to prove that \n",
    "\n",
    "$E\\big[f'''(u)\\big] \\leq 0$  \n",
    "**potential cleanup item: is this backwards? should I have said the below?**  \n",
    "$E\\big[f'''(u)\\big] \\geq 0$  \n",
    "\n",
    "so we have \n",
    "\n",
    "$E\\big[f'''(u)\\big] = p\\cdot(-(1-\\bar{X}))^3e^{(1-\\bar {X})u} + (1-p)\\cdot(0--\\bar{X})^3e^{(0--\\bar {X})u}$  \n",
    "$ = -p\\cdot(1-p)^3e^{-(1-p)\\vert u\\vert} + (1-p)\\cdot(p)^3e^{-p\\vert u\\vert}$   \n",
    "$= p(1-p)\\big(-(1-p)^2 e^{-(1-p)\\vert u\\vert} + p^2 e^{-p\\vert u\\vert}\\big)$   \n",
    "$= p(1-p)e^{-p\\vert u\\vert}\\big(-(1-p)^2 e^{(2p-1)\\vert u\\vert} + p^2 \\big)$   \n",
    "$= p(1-p)e^{-p\\vert u\\vert}\\big(p -(1-p) e^{\\frac{1}{2}(2p-1)\\vert u\\vert}  \\big)\\big(p +(1-p) e^{\\frac{1}{2}(2p-1)\\vert u\\vert}  \\big)$   \n",
    "\n",
    "hence the claim reduces to proving  \n",
    "$p(1+e^{\\frac{1}{2}(2p-1)\\vert u\\vert}) -e^{\\frac{1}{2}(2p-1)\\vert u\\vert}= p -(1-p) e^{\\frac{1}{2}(2p-1)\\vert u\\vert}\\geq 0$   \n",
    "\n",
    "or via positive re-scaling, that \n",
    "\n",
    "$ p\\big(e^{\\frac{1}{2}(1-2p)\\vert u\\vert} - \\frac{1}{p}+ 1\\big) \\geq 0$   \n",
    "\n",
    "which, for instance, gives us \n",
    "\n",
    "$ p\\big(e^{\\frac{1}{2}(1-2p)\\vert u\\vert} - \\frac{1}{p}+ 1\\big) \\geq p\\big(1 + \\frac{1}{2}(1-2p)\\vert u\\vert - \\frac{1}{p}+ 1\\big) \\geq p\\big(1 + \\frac{1}{2}(1-2p) - \\frac{1}{p}+ 1\\big)  =  \\big(\\frac{5p}{2}-p^2 - 1 \\big)   \\geq 0$   \n",
    "\n",
    "\n",
    "noting that $\\big(\\frac{5p}{2}-p^2 - 1 \\big) $ is concave with roots at $p = \\frac{1}{2}$ and $p=2$ and hence non-negative for $p \\in [\\frac{1}{2},1]$.  With $t\\in [0,1]$ we know $\\vert u \\vert \\leq 1$ and have thus proven the desired result.\n",
    "\n",
    "\n",
    "note: because we ran this analysis on $Z = -Y$ we in fact have proven that it holds for $t\\in [0,1]$ for a centered Bernouli with success parameter $q \\in [0, \\frac{1}{2}]$.  \n",
    "\n",
    "*another look: why this approach can't be directly used to prove for all t*  \n",
    "re-visiting the line:  \n",
    "$p(1-p)\\big(-(1-p)^2 e^{-(1-p)\\vert u\\vert} + p^2 e^{-p\\vert u\\vert}\\big)$   \n",
    "\n",
    "we want to show that this is always non-negative for $p\\in [\\frac{1}{2}, 1]$.  Equivalently, we want to show \n",
    "\n",
    "$p^2 e^{-p\\vert u\\vert}\\geq (1-p)^2 e^{-(1-p)\\vert u\\vert}$  \n",
    "\n",
    "or, taking advantage of positivity, we may re-arrange terms to see  \n",
    "\n",
    "$e^{-p\\vert u\\vert}e^{(1-p)\\vert u\\vert}\\geq \\big(\\frac{1-p}{p}\\big)^2 $   \n",
    "$e^{(1-2p)\\vert u\\vert}  \\geq  \\big(\\frac{1-p}{p}\\big)^2 $   \n",
    "\n",
    "since we've constrained ourself to $p \\in [\\frac{1}{2}, 1]$  we have \n",
    "\n",
    "if $p = \\frac{1}{2}$ then we have   \n",
    "$e^{(1-2p)}=e^{(1-2p)\\vert u \\vert} = 1= \\big(\\frac{1-p}{p}\\big)^2 $   \n",
    "\n",
    "for any $\\vert u\\vert$  \n",
    "\n",
    "now for $p \\in (\\frac{1}{2}, 1]$  we see \n",
    "\n",
    "$e^{(1-2p)}\\lt 1$ \n",
    "\n",
    "which qualitatively means that if we set $\\vert u \\vert \\gt 1$ (equivalently: loosening our restriction on the domain of $t$), we have \n",
    "\n",
    "$e^{(1-2p)\\vert u \\vert}\\leq e^{(1-2p)}\\leq 1$  \n",
    "\n",
    "which is a problem, because the Left Hand Side may be made arbitrarily close to zero by changing $u$ while $\\big(\\frac{1-p}{p}\\big)^2$ has some fixed positive value for a given choice of $p$.  Ultimately it tells us a different approach is needed for $t \\gt 1$.  We've thus proven the inequality holds for $t \\in [0,1]$ so long as $p\\in [\\frac{1}{2},1]$.  \n",
    "\n",
    "- - - \n",
    "**leg 2**  \n",
    "we now seek to prove that $E\\big[e^{tZ}\\big]$ attains a maximum with a probability parameter $q \\in [0, \\frac{1}{2}]$ for any $t \\in [0,1]$.  Note that we have a compact domain (i.e. any $q \\in [0,1]$) and so we know that a maximum exists for any given $t$.  We can see that *any* choice of $q$ is ok for $t=0$ and the desired inequality holds -- i.e. we have $E\\big[e^{tZ}\\big] = 1\\leq 1 = \\exp\\big(\\frac{0^2}{8}\\big)$.  Now for all other choices of $t \\in (0,1]$  we need to prove \n",
    "\n",
    "$M(t) = E\\big[e^{tZ}\\big] \\leq E\\big[e^{tZ^*}\\big]  = \\text{max  } M(t) \\leq \\exp\\big(\\frac{t^2}{8}\\big)$  \n",
    "where $Z^*$ is the centered Bernoui random variable with maximum MGF value for some chosen $t \\in (0,1]$.  Notation could perhaps be cleaned up around here.      \n",
    "\n",
    "Since we know that the maximum exists for any choice of $t \\in (0,1]$, the goal is to prove that this maximum cannot exist for $q \\in (\\frac{1}{2}, 1]$ and hence our earlier bound holds because $Z^*$ must have $q \\in [0, \\frac{1}{2}]$.  \n",
    "\n",
    "- - - - \n",
    "for avodiance of doubt, we have   \n",
    "$g_t(q) = E\\big[e^{tZ}\\big] = q \\cdot e^{t(1-q)} + (1-q) \\cdot e^{-tq}$  \n",
    "\n",
    "$g_t(1) = 1$  \n",
    "$g_t(0) = 1$   \n",
    "but by strict convexity of the exponential map, we know, that for any $t \\in (0,1]$ and $q \\in(0,1)$, we have   \n",
    "$ 1 = \\exp\\Big(t \\cdot 0\\Big) = \\exp\\Big(t \\cdot E\\big[Z\\big]\\Big)= \\exp\\Big(E\\big[tZ\\big]\\Big) \\lt E\\Big[\\exp\\big(tZ\\big)\\Big]=  E\\big[e^{tZ}\\big]$  \n",
    "\n",
    "so the maximum cannot occur with $q =1$ or $q=0$.  \n",
    "\n",
    "Put differently, what this tells us is that for any choice of $t \\gt 0$, \n",
    "\n",
    "$g_t(q)$  attains a maximum at $q \\in(0,1)$, and the maximum occurs when the derivative of $g$ with respect to $q$ is zero.  However we'll show that this derivative is always negative for $q\\in(\\frac{1}{2},1)$ which implies that $q^* \\in (0, \\frac{1}{2}]$  \n",
    "\n",
    "\n",
    "- - - - \n",
    "\n",
    "The above function has two terms and consists of multiplication and the exponential function -- it is differentiable as many times as we wish.  \n",
    "\n",
    "Our plan of attack is to consider $q \\in (\\frac{1}{2}, 1)$  by showing that the derivative with respect to $q$ is negative there, for any given $t$ and hence the function cannot attain a maximum there.  \n",
    "\n",
    "$g_t'(q) = e^{-tp}\\big(-(1-q)t +e^t(1-qt) -1 \\big)$  \n",
    "\n",
    "since $e^{-tp} \\gt 0$  we thus want to show that the component inside the parenthesis is negative for any $t \\gt 0 $ and $q \\in (\\frac{1}{2}, 1)$  \n",
    "\n",
    "$\\big(-(1-q)t +e^t(1-qt) -1 \\big) $  \n",
    "$= \\big(-t+qt +e^t(1-qt) -1 \\big) $  \n",
    "$= \\big(-t-1(1 - qt) +e^t(1-qt)  \\big) $  \n",
    "$= (e^t - 1)(1-qt) -t  $   \n",
    "$\\lt (e^t - 1)(1-\\frac{t}{2}) -t =\\text{upper bound} = h(t)$   \n",
    "\n",
    "i.e. $(e^t - 1)(1-\\frac{t}{2})  \\gt 0$  and monotone decreases as $q$ increases for any choice of $t$, and $q = \\frac{1}{2}$ is the infimum over our domain in consideration. This gives us a clue on why the partition of $q$ into $\\leq \\frac{1}{2}$ and $\\gt \\frac{1}{2}$ matters and it allows us to get rid of our dependence on $q$.    \n",
    "\n",
    "\n",
    "if we consider $t=0$ we have   \n",
    "$h(0) = (e^0 - 1)(1-\\frac{0}{2}) -0 =  0$   \n",
    "$h(1) = (e^1 - 1)(1-\\frac{1}{2}) -1 \\lt 0$   \n",
    "\n",
    "\n",
    "We aim to show that the upper bound is never positive for $t \\gt 0$, because it's maximum occurs at 0.  We show that by examining the derivative of the $h$, showing that it is always negative for $t \\gt 0$, and hence $h(t)$ is never increasing. (Equivalently, for $t \\in [0,1]$ we consider that we have a compact domain of $t \\in[0,1]$ and we know that $h(t)$ obtains a maximum. It is either in the interior where $h'(t) = 0$ for $t \\in(0,1)$ or it is at $\\max\\big(h(0), h(1)\\big) = h(0)$.  But as we show below, $h'(t)\\lt 0$ for all $t \\in(0,1)$ hence the maximum occurs as $h(0) = 0$ -- i.e. we have $h(t) \\leq 0$ for $t \\in [0,1]$.)  \n",
    "\n",
    "for $t \\gt 0$:  \n",
    "$h'(t) = \\frac{1}{2}\\big(-e^t(t-1) -1\\big)$  \n",
    "$= \\frac{1}{2}\\big(e^t(1-t) -1\\big)$  \n",
    "$\\lt \\frac{1}{2}\\big(e^{t-t} -1\\big)$  \n",
    "$=\\frac{1}{2}\\big(1 -1\\big)$  \n",
    "$ = 0$  \n",
    "\n",
    "\n",
    "where we used   \n",
    "$(1+a) \\lt e^a$  for $a \\neq 0$, then rescaled by $e^t$   \n",
    "$e^t(1+a) \\lt e^t e^a = e^{t+a}$  \n",
    "and set $a = -t$   \n",
    "$e^t(1-t) \\lt e^{t-t} = 1$   \n",
    "- - - - \n",
    "This completes the proof that for $0 \\lt t \\leq 1$, for all $q \\in (\\frac{1}{2},1)$  \n",
    "\n",
    "$g_t'(q) = e^{-tp}\\big(-(1-q)t +e^t(1-qt) -1 \\big) \\lt h(t) \\lt 0$  \n",
    "\n",
    "and that $\\text{max  } M(t)$ occurs for any $t\\in (0,1]$ with $q \\in (0, \\frac{1}{2}]$.  And again for avoidance of doubt, with $t=0$. $\\text{max  } M(0)$ occurs for every choice of $q$ including $q \\in (0, \\frac{1}{2}]$ (which is in effect weak dominance).  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**generalizing the result for**  $t\\gt 1$  \n",
    "Thus far we've proven for $t\\in[0,1]$ \n",
    "\n",
    "$1 \\leq E\\big[\\exp\\big(t Z\\big)\\big]\\leq \\exp\\big(\\frac{t^2}{8}\\big)$  \n",
    "\n",
    "However, it becomes convenient to work in logspace here.   \n",
    "\n",
    "$\\gamma_Z(t) = \\log\\Big(E\\big[\\exp\\big(t Z\\big)\\big]\\Big) = \\log\\Big(E\\big[\\exp\\big(-t \\bar{X}\\big)\\big]\\Big) +\\log\\Big(E\\big[\\exp\\big(t X\\big)\\big]\\Big) = -pt+\\log\\Big(\\big[\\exp\\big(t X\\big)\\big]\\Big) \\leq \\log\\Big(\\exp\\big(\\frac{t^2}{8}\\big)\\Big)=\\frac{t^2}{8}$   \n",
    "\n",
    "- - - - \n",
    "where $-\\bar{X}$ is a deterministic random variable and hence independent from $X$.  By independence, their cumulants add.  \n",
    "\n",
    "$\\gamma_Z(t) = \\gamma_{-\\bar{X}}(t) + \\gamma_X(t)$  \n",
    "- - - - \n",
    "\n",
    "while we have proven this is true for $t \\in[0,1]$, what we want to prove is that for *all* $t$ \n",
    "\n",
    "$\\gamma_Z(t) \\leq \\frac{t^2}{8}$  \n",
    "notice that both sides are convex functions of $t$ -- cumulants always are, and so are quadratics.  \n",
    "\n",
    "re-arranging terms:  \n",
    "$0 \\leq \\frac{t^2}{8}- \\gamma_Z(t)$  \n",
    "\n",
    "\n",
    "\n",
    "We proceed indirectly when we consider the function $\\phi$ given by \n",
    "\n",
    "$\\phi(t) =  \\frac{t^2}{8} - \\gamma_Z(t)$  \n",
    "\n",
    "we know \n",
    "\n",
    "$\\phi(0) = \\log(1) - \\log(1) = 0$  \n",
    "\n",
    "and  \n",
    "\n",
    "$\\phi(1) = \\log\\Big(\\exp\\big(\\frac{1}{8}\\big)\\Big)- \\log\\Big(E\\big[\\exp\\big(t Z\\big)\\big]\\Big) = m \\geq 0$  \n",
    "\n",
    "however, by Mean Value Theorem we know that \n",
    "\n",
    "$0 \\leq m = \\frac{\\phi(1)-\\phi(0)}{1- 0} = \\phi'(u)$  for some $u \\in(0,1)$  \n",
    "\n",
    "but we also know that $\\phi(u) = b\\geq 0$  hence we have \n",
    "\n",
    "$0\\leq  m\\cdot t + b$ \n",
    "\n",
    "for \n",
    "\n",
    "$u \\lt 1 \\leq t$  \n",
    "  \n",
    "The final linkage is, we need to prove that $\\phi$ is convex, and hence the above tangent line at $u$ is a linear lower bound for the entire function.  We've already proven $0\\leq \\phi(t)$ for $t\\in [0,1]$ and this non-negative linear lower bound then extends the non-negative of $\\phi$ to all $t\\in [1, \\infty)$ and the union gives the result for all $t \\geq 0$.  \n",
    "\n",
    "We prove the convexity of $\\phi$ by careful examination of the second derivative. We wish to prove\n",
    "\n",
    "$0 \\leq \\phi''(t) $ or equivalently  \n",
    "$\\gamma_Z''(t)\\leq \\frac{d^2}{dt^2}\\big(\\frac{t^2}{8}\\big) $     \n",
    "\n",
    "After twice differentiating, the right hand side\n",
    "$\\frac{d^2}{dt^2}\\frac{t^2}{8} \\to \\frac{1}{4}$   \n",
    "which is a nice result so characteristic of quadratics:  the second derivative is constant and no longer depends on $t$.  \n",
    "\n",
    "Hence we must prove $\\gamma_Z''(t) \\leq \\frac{1}{4}$   \n",
    "\n",
    "\n",
    "note that by linearity \n",
    "\n",
    "$\\frac{d^2}{dt^2}\\gamma_Z(t) = \\frac{d^2}{dt^2}\\big(\\gamma_{-\\bar{X}}(t) + \\gamma_X(t)\\big)= \\frac{d^2}{dt^2}\\gamma_{-\\bar{X}}(t) + \\frac{d^2}{dt^2}\\gamma_X(t) = \\gamma_X(t)$  \n",
    "\n",
    "(where the first term is linear in $t$ and hence gets mapped to zero.) \n",
    "\n",
    "Differentiating is mechanically straightforward\n",
    "\n",
    "https://www.wolframalpha.com/input/?i=d%5E2%2Fdx%5E2+log(p+exp(x)+%2B(1-p))   \n",
    "\n",
    "(and e.g. in chp9_DiscStochProcesses.ipynb we see a general formula for the second derivative of cumulants)  \n",
    "\n",
    "\n",
    "$\\frac{d^2}{dt^2}\\gamma_Z(t) = \\frac{(1-p)pe^t}{(p(e^t-1)+1)^2}= \\frac{(1-p)pe^t}{(pe^t+1-p)^2}= \\frac{(1-p)pe^t}{\\big((1-p)+pe^t\\big)^2}$  \n",
    "\n",
    "however it is not immediately clear why this must be less than $\\frac{1}{4}$.  \n",
    "\n",
    "Note: the below works through a long messy polynomial factorization to confirm the result.  \n",
    "- - - -  \n",
    "But first an **interlude:** a much better approach is to do a simple sanity check and try to confirm \n",
    "\n",
    "$\\frac{(1-p)pe^t}{\\big( (1-p)+pe^t\\big)^2}\\leq 1$  \n",
    "\n",
    "for $p\\in(0,1)$.  How doe we know this? In essence because of $\\text{GM}\\leq \\text{AM}$.    \n",
    "\n",
    "in particular, consider \n",
    "\n",
    "$0\\leq (1-p)^\\frac{1}{2}(pe^t)^\\frac{1}{2} \\leq \\frac{1}{2}\\big((1-p)+(pe^t)\\big)$  \n",
    "\n",
    "which gives us a way of bounding the numerator in terms of the denominator.  Taking advantage of positivity, we may square both sides to see  \n",
    "\n",
    "$0\\leq (1-p)(pe^t) \\leq \\frac{1}{4}\\big((1-p)+(pe^t)\\big)^2$  \n",
    "and dividing by $\\big((1-p)+(pe^t)\\big)^2$  gives the desired upper bound  \n",
    "\n",
    "$0\\leq \\frac{(1-p)(pe^t)}{\\big((1-p)+(pe^t)\\big)^2} \\leq \\frac{1}{4}$  \n",
    "\n",
    "this completes the proof of the Hoeffding Lemma.  \n",
    "- - - -  \n",
    "**Detour:  A much better finish**  \n",
    "recalling the work through in 'chp9_DiscStochProcesses.ipynb' \n",
    "\n",
    "we can see that \n",
    "$\\frac{d^2}{dt^2}\\gamma_Z(t) =  \\text{var}_g\\big(Z\\big)  = E_g\\big[Z^2\\big] - E_g\\big[Z\\big]^2$  \n",
    "\n",
    "i.e. it is variance with respect to the tilted probability measure of \n",
    "$g(z) = \\frac{f(z)\\exp(tZ)}{E [\\exp(tZ)]}$  \n",
    "\n",
    "instead of $f(x)$ (of course we recover the 'regular' variance by setting $t:=0$).  However, Gruess's Inequality tells us that *any* random variable bounded between $[a, A]$, which in our case, is an interval of length one (i.e. $A-a = 1$), which gives us      \n",
    "\n",
    "$\\frac{d^2}{dt^2}\\gamma_Z(t) = \\text{var}_g\\big(Z\\big) \\leq \\frac{1}{4}\\big(A-a\\big) = \\frac{1}{4}$  \n",
    "\n",
    "This is strongly suggestive that we may refine the bound in Hoeffding's Lemma via the use of a tighter bound on $\\text{var}_g\\big(Z\\big)$ for arbitrary bounded random variables (i.e. Gruess's Inequality is loose if the variable does not have all of its mass concentrated at end points).  \n",
    "\n",
    "# a radical streamlining of the proof  \n",
    "\n",
    "This is also suggestive of a *much* shorter proof of the existing setup.  \n",
    "\n",
    "chp9_DiscStochProcesses.ipynb also tells us  \n",
    "$\\gamma'_X(r) = \\frac{g'_X(r)}{g_X(r)} = E_g\\big[X\\big]$  \n",
    "\n",
    "The streamlined setup is thus:  Verify upper bound and lower bound are $\\geq 1$ in general and $=1$ when $t=0$.  Then work in logspace, using $\\phi(t)$. As a technical nit, since our random variable is bounded, we can verify that $\\phi(t)$ exists for $t \\in (-a,a)$ for some $a \\gt 0$ i.e. while we are interested in $t\\geq 0$ it is useful to extend the domain in question to $(-\\epsilon, \\infty)$ that way we have bonafide derivatives at $t=0$.  \n",
    "\n",
    "we know  \n",
    "$\\phi(0) = 0$  \n",
    "we *also* know \n",
    "\n",
    "$\\phi'(t) = \\frac{t}{4} - \\gamma'_X(r) = \\frac{t}{4} - E_g\\big[X\\big] $\n",
    "\n",
    "and thus  \n",
    "$\\phi'(0) = 0$   \n",
    "\n",
    "But using the above we can easily prove  \n",
    "$\\phi''(t) \\geq 0$ for all $t$   \n",
    "which confirms that $\\phi$ is convex.  \n",
    "\n",
    "This tells us that we have a linear lower bound given by the tangent line \n",
    "\n",
    "$y = mx + b$  \n",
    "\n",
    "where  \n",
    "$m= \\phi'(0) = 0$  \n",
    "$b= \\phi(0)$  \n",
    "hence  \n",
    "$\\phi(t) \\geq 0$ for all $t \\geq 0$  \n",
    "\n",
    "which proves the Hoeffding Lemma.  \n",
    "\n",
    "**end Detour**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other approach to finishing the proof comes from a clever decomposition curtiosy of Wikipedia  \n",
    "\n",
    "$\\frac{d^2}{dt^2}\\gamma_Z(t) = a\\cdot(1-a)  = \\frac{b}{(p(e^t-1)+1)}(1-a) = \\frac{b}{(p(e^t-1)+1)}\\big(1-\\frac{b}{(p(e^t-1)+1)}\\big) $   \n",
    "\n",
    "for some $a \\in (0,1)$  and hence by $\\text{GM}\\leq \\text{AM}$ we know \n",
    "\n",
    "$0\\leq a^\\frac{1}{2}(1-a)^\\frac{1}{2} \\leq \\frac{1}{2}\\big(a + 1-a) = \\frac{1}{2}$  \n",
    "and squaring both sides tells us that   \n",
    "$a(1-a)\\leq \\frac{1}{4}$  \n",
    "\n",
    "so we want to find $a$ via brute force polynomial factorization:  \n",
    "\n",
    "$\\frac{(1-p)pe^t}{(pe^t+1-p)^2} = \\frac{d}{c ^2} = a(1-a) = \\frac{b}{c}\\big(1-\\frac{b}{c}\\big)= \\frac{b}{c}\\big(\\frac{c-b}{c}\\big) $   \n",
    "\n",
    "which tells us that for the factorization to work, \n",
    "\n",
    "$(1-p)pe^t = d = bc - b^2 = b(p(e^t-1)+1) - b^2$    \n",
    "\n",
    "i.e. we need to solve for a positive root of \n",
    "\n",
    "$b^2 - b(p(e^t-1)+1) + (1-p)pe^t$  \n",
    "\n",
    "and confirm $\\frac{b}{c}^* \\in (0,1)$\n",
    "\n",
    "more simply consider \n",
    "\n",
    "$b^2 - bc + d = 0$  \n",
    "\n",
    "thankfully this is a quadratic which means a very simple root structure and it has a real roots at \n",
    "\n",
    "$b^* = \\frac{1}{2}\\big(c-\\sqrt{c^2 - 4d}\\big)$   \n",
    "and  \n",
    "$b^* = \\frac{1}{2}\\big(c+\\sqrt{c^2 - 4d}\\big)$   \n",
    "\n",
    "The symbol manipulations and simplifications are tedious however and sympy was not of much help in doing them.  \n",
    "\n",
    "However wolfram did a better job and \n",
    "https://www.wolframalpha.com/input/?i=(p*(exp(t)+-+1)+%2B+sqrt(-4*p*(-p+%2B+1)*exp(t)+%2B+(p*(exp(t)+-+1)+%2B+1)**2)+%2B+1)%2F2\n",
    "\n",
    "gives the desired result about halfway through.  Of course once we have the result, the messy means of finding it don't matter so much -- we can verify directly via algebraic manipulations that it holds true -- which appears to be what was done on wikipedia.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ross and Pekoz Problems \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8**  \n",
    "with iid $X_i$ that are zero mean, with a second moment, if $T$ is a valid stopping time (i.e. $E\\big[T\\big]\\lt \\infty$), prove:\n",
    "\n",
    "$\\text{Var}\\Big(\\sum_{i=1}^T X_i\\Big) = \\sigma^2 \\cdot E\\big[T\\big]$  \n",
    "\n",
    "*remark:*  \n",
    "this is the martingale approach to deriving one of the key results that falls under Wald's Identity\n",
    "\n",
    "*proof:*  \n",
    "$\\text{Var}\\Big(\\sum_{i=1}^T X_i\\Big)$  \n",
    "$= \\sum_{i=1}^T \\text{Var}\\Big(X_i\\Big) + \\sum_{j}\\sum_{k\\neq j} \\text{Cov}\\big({X_j,X_k}\\big)$  \n",
    "$= \\sum_{i=1}^T \\text{Var}\\Big(X_i\\Big) + 0$  \n",
    "$= \\sum_{i=1}^T E\\Big[X_i^2\\Big]$  \n",
    "$= E\\big[S_T^2\\big]$  \n",
    "$= E\\Big[X_i^2\\Big]\\cdot E\\big[T\\big]$  \n",
    "$= \\sigma^2\\cdot E\\big[T\\big]$  \n",
    "\n",
    "where we made use of zero covariance between iid random variables, the fact that variance of a zero mean random variable is its second moment, and finally the Wald Equation.  \n",
    "\n",
    "*remark:*  \n",
    "This is a quite useful result when dealing with zero mean random variables amenable to the Wald Equation -- that, as a first step, let's us exploit zero mean to get the probability of hitting a left vs right barrier.  However the Wald Equation remains silent on expected time until 'absorbtion' in the zero mean case -- this follow-up result allows us to get the expected time until absorbtion -- specifically we know the thresholds (hopefully without overshoot problems!) and can square them and take a convex combination with weights $p$ and $1-p$ and to get $ E\\big[S_T^2\\big]$.  \n",
    "**This should be cleaned up and needs a little more thought.**  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# remark on Markov Chains with Martingales embedded  \n",
    "\n",
    "This is a mixture of ideas from e.g. problems 31, 32, 33 in section 11.2 (page 431) of Grinstead and Snell, as well as e.g. page 399 of Feller vol 1 (3rd edition), with some ideas from the Martingales chapter of Ross & Pekoz.  In addition, in here and in the section below of problem from Karlin and Taylor, there are numerous closely related extensions -- as they make this a big focal point in their chapter on martingales.\n",
    "\n",
    "The ideas here can extend to the countably infinite markov chain case, however that introduces a lot of subtleties that need to be addressed related to convergence, as well as meaning of 'fair game' etc. There are a large class of interesting problems, e.g. gambler's ruin, that may be modelled as a time homogenous $n$ state markov chain\n",
    "\n",
    "from Feller (p. 399, with a slight generalization): \n",
    "suppose we have states labelled $0, 1, 2, ...., r$ and states $0$ and $r$ are absorbing, with all other states being transient.  \n",
    "\n",
    "our transition matrix $\\mathbf P$ is row stochastic, we multiply on the right to make use of it as an expectations operatior.  \n",
    "\n",
    "supposed we find some harmonic function where $f(i) = \\sum_k p_{i,k} \\cdot f(k)$ or in matrix notation: $\\mathbf {Pf} = \\mathbf f$   \n",
    "\n",
    "so we have easy pointwise convergence as we take a limit in the number of transitions in our chain.  I.e. \n",
    "\n",
    "$\\mathbf f = \\mathbf P^n \\mathbf f$  \n",
    "for all $n$, hence for any $\\epsilon \\gt 0$ we have \n",
    "\n",
    "$\\big \\Vert \\mathbf P^n \\mathbf f - \\mathbf f \\big \\Vert_2 \\lt \\epsilon $  \n",
    "for $n \\geq 1$  (i.e. any natural number is trivially large enough here).  \n",
    "\n",
    "hence \n",
    "\n",
    "$\\lim_{n \\to \\infty }\\mathbf P^n \\mathbf f = \\mathbf f$  \n",
    "\n",
    "or we have, in Feller's setup, \n",
    "\n",
    "$ p_{i,0}^{(n)}\\cdot f(0) + p_{i,r}^{(n)}\\cdot f(r) + \\sum_{k=1}^{r-1} p_{i,k}^{(n)} \\cdot f(k)=  \\sum_{k=0}^r p_{i,k}^{(n)} \\cdot f(k) = f(i)$   \n",
    "\n",
    "for all $i$, but since the 'middle' states are transient, we know  for $k=1,2, .., r-1$ that \n",
    "$p_{i,k}^{(n)} \\to 0$, so we know  \n",
    "\n",
    "$p_{i,0}^{(\\infty)}\\cdot f(0) + p_{i,r}^{(\\infty)}\\cdot f(r) = p_{i,0}^{(\\infty)}\\cdot f(0) + p_{i,r}^{(\\infty)}\\cdot f(r) +\\big(0\\big)  =  \\lim_{n \\to \\infty}\\Big( p_{i,0}^{(\\infty)}\\cdot f(0) + p_{i,r}^{(\\infty)}\\cdot f(r) + \\big(\\sum_{k=1}^{r-1} p_{i,k}^{(n)} \\cdot f(k)\\big)\\Big) = f(i)$   \n",
    "\n",
    "but since these absorbing states are reached with probability 1, we know \n",
    "\n",
    "$p_{i,0}^{(\\infty)} + p_{i,r}^{(\\infty)} = 1$     \n",
    "\n",
    "thus  \n",
    "$ p_{i,r}^{(\\infty)} = 1- p_{i,0}^{(\\infty)} $  \n",
    "\n",
    "so, we typically have this harmonic function in hand and want to compute the limitting probabilities, which gives:  \n",
    "\n",
    "$f(i) = p_{i,0}^{(\\infty)}\\cdot f(0) + p_{i,r}^{(\\infty)}\\cdot f(r) = p_{i,0}^{(\\infty)}\\cdot f(0) + \\big(1-p_{i,0}^{(\\infty)}\\big)\\cdot f(r) = p_{i,0}^{(\\infty)}\\big( f(0) - f(r)\\big) + f(r)  $  \n",
    "\n",
    "$\\frac{f(i)- f(r)}{f(0) - f(r)} = p_{i,0}^{(\\infty)}$  \n",
    "\n",
    "(supposing $f(0)\\neq  f(r)$)  \n",
    "\n",
    "or equivalently, the finish is to solve \n",
    "\n",
    "$\\begin{bmatrix}\n",
    "f(0)  & f(r)  \\\\ \n",
    "1 & 1\n",
    "\\end{bmatrix}\\begin{bmatrix}\n",
    "p_{i,0}^{(\\infty)}   \\\\ \n",
    "p_{i,r}^{(\\infty)}\n",
    "\\end{bmatrix} =\\begin{bmatrix}\n",
    "f(i)  \\\\ \n",
    "1\n",
    "\\end{bmatrix}$  \n",
    "\n",
    "where the solution is exists and is unique if $ f(0) \\neq  f(r) $ -- in the equality case, another approach is needed (e.g. perhaps using Wald's Equation).  \n",
    "\n",
    "- - - - \n",
    "The exact same thing, in short, can be said in Martingale notation, here using some ideas from Karlin & Taylor, and Ross & Pekoz.  \n",
    "\n",
    "where we have random variable $Y_k$ this a markov chain as in the above  \n",
    "\n",
    "and for each $Y_k = i$  we have a harmonic function   \n",
    "$f(i) = \\lambda \\sum_{j} P_{i,j} f(j)$    \n",
    "\n",
    "where we assign  \n",
    "\n",
    "$X_n := \\lambda^{-n} f(Y_n)$  \n",
    "\n",
    "(note: for the case of main interest, i.e. the above, we have $\\lambda =1$)\n",
    "\n",
    "This is a martingale \n",
    "\n",
    "$E\\Big[ X_{n+1} \\big \\vert Y_0 , ... Y_n\\Big] $  \n",
    "$= E\\Big[\\lambda^{-(n+1)}  f(Y_{n+1}) \\big \\vert Y_n\\Big]$  \n",
    "$= \\lambda^{-(n+1)}\\cdot E\\Big[ f(Y_{n+1}) \\big \\vert Y_n\\Big]$  \n",
    "- - - -  \n",
    "which is a random variable that conditioned on $Y_n = i$, which occurs with probability $P(Y_n = i)$, we have the markov property, so \n",
    "$E\\Big[ f(Y_{n+1}) \\big \\vert Y_n = i\\Big] = \\sum_{j}P_{i,j}f(j) = \\lambda f(i)$  \n",
    "- - - -  \n",
    "$= \\lambda^{-(n+1)}\\cdot \\lambda f\\big(Y_n\\big)$  \n",
    "$= \\lambda^{-n}f\\big(Y_n\\big)$  \n",
    "$=X_n$  \n",
    "\n",
    "Now in our case we have these two absorbing states, so we set up a stopping time with random variable $N$, we confirm a sufficient condition for the Martingale Stopping Theorem ( e.g. in this case we could use uniformly bounded $X_n$ which implies the $X_{min(N,n)}$ are uniformly bounded, or alternatively use $E\\big[N\\big] \\lt \\infty$ and that $\\big \\vert X_{n+1} - X_n\\big \\vert $ is bounded by some constant $M$ for all $n$ , no matter what we condition on (and hence the expectation is bounded).  e.g. see bottom of page 88 of Ross & Pekoz.  \n",
    "\n",
    "Thus after checking that we've met sufficient conditions for the stopping theorem, we have \n",
    "\n",
    "$p_{i,0}^{(\\infty)}\\cdot f(0) + p_{i,r}^{(\\infty)}\\cdot f(r) = E\\big[X_N\\big] = E\\big[X_0\\big] = f(i)$  \n",
    "as before   \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**An open idea, for finite state markov chains:**   \n",
    "Take a recurrent chain that is time reversible (and hence easy to guess/find the steady state) \n",
    "then convert the two (or more) \"end states\" into absorbing states \n",
    "and use time reversibility to get a martingale that we can then use to get absorbiton probabilities \n",
    "\n",
    "keep in mind that for steady state we have $\\mathbf \\pi^T \\mathbf P = \\mathbf \\pi^T$ though this is typically just written as  \n",
    "$\\mathbf \\pi \\mathbf P = \\mathbf \\pi$   \n",
    "\n",
    "i.e. we are using Left side of (row stochastic) transition matrix, whereas for Martingale we are using  \n",
    "$\\mathbf {Pf} = \\mathbf f$  \n",
    "i.e. we are using the right, expectations side of the transition matrix, or equivalently  \n",
    "$\\mathbf f^T\\mathbf {P}^T = \\mathbf f^T$  \n",
    "\n",
    "However if the process is in fact time reversible, that allows us to relate $\\mathbf P$ to its transpose  \n",
    "$\\mathbf P^* =  \\mathbf D \\mathbf P^T \\mathbf D^{-1}$  \n",
    "(see discussion in Feller chp 15 notes -- the discussion there was for reversing a chain that was not time reversible... the results *much* nicer when the chain is time reversible)   \n",
    "\n",
    "where $\\mathbf P^*$ is the reversed chain.    \n",
    "\n",
    "hence if \n",
    "$\\mathbf \\pi_{\\text{rev}}$ is the steady state for the reversed chain, then   \n",
    "\n",
    "$\\mathbf \\pi_{\\text{rev}} = \\mathbf \\pi_{\\text{rev}}\\mathbf P^* =  \\mathbf \\pi_{\\text{rev}}\\mathbf D \\mathbf P^T \\mathbf D^{-1}$  \n",
    "\n",
    "letting   \n",
    "$\\mathbf v^T = \\mathbf \\pi_{\\text{rev}}\\mathbf D$  \n",
    "or  \n",
    "$\\mathbf v^T\\mathbf D^{-1} = \\mathbf \\pi_{\\text{rev}}$  \n",
    "\n",
    "we can confirm that $\\mathbf v$ is an eigenvector of $\\mathbf P$ (this *is* basic similarity transform stuff) because   \n",
    "\n",
    "$ \\mathbf v^T\\mathbf D^{-1} = \\mathbf \\pi_{\\text{rev}} = \\mathbf \\pi_{\\text{rev}}\\mathbf P^*  = \\mathbf v^T\\mathbf D^{-1}\\mathbf D \\mathbf P^T \\mathbf D^{-1} = \\mathbf v^T \\mathbf P^T \\mathbf D^{-1}$  \n",
    "\n",
    "i.e. multiplying each side on the right by $\\mathbf D$ we get  \n",
    "$\\mathbf v^T = \\mathbf v^T \\mathbf P^T$  \n",
    "or  \n",
    "$\\mathbf {Pv} = \\mathbf v$  \n",
    "\n",
    "**lingering concern -- this may be just telling us the obvious -- that the vector here is really the ones vector and our matrix is row stochastic**  \n",
    "\n",
    "The real technique / issue here is not the above manipulations... it is probably going from the time reversible steady state case to the absorbing state case...  \n",
    "\n",
    "Some kind of fundamental bridging concept seems relevant here....  a suspected bridge is:   \n",
    "\n",
    "\n",
    "time reversible steady state   \n",
    "$\\longrightarrow$  non-reversible steady state where we tweak the two end points to flow pro-rata into the others to maintain the same steady state \n",
    "\n",
    "then use results on flows in recurrent events/ renewal rewards, to get absorbtion probabilities... seems great, though it isn't clear what this has to do with martingales...  \n",
    "\n",
    "- - - - \n",
    "it may just be the case that there is some discernable pattern... if the pattern is 'good' enough, then we can explicitly solve small $n$ x $n$ matrices in many cases -- look at (right) null space of absorbing chain after subtracting the identity matrix.... then induct that the pattern holds for arbitrary $n$, and then tease out the solution... \n",
    "\n",
    "Another approach would be to find the nodes directly connected to states $0$ and $N$ when in time reversible, recurrent form. We may be able to 'pinch' the results and use renewal type arguments to get the absorbing states.... \n",
    "\n",
    "another, looser, setup is to think of the problem as a casino and figure out what is needed to make the game 'fair'... in some sense this is the de Moivre approach to the gambler's ruin problem (see e.g. page 487, -- 497 of 520 in Grinstead and Snell).  The 'lets make it fair approach' may be amenable to first (or perhaps last) step analysis.  \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Gambler's Ruin -- Pending item: can i do a markov chain / martingale method for an inhomogenous process, e.g. problem 61 in Ross Intro Probability Models, (markov chains chapter)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gambler's Ruin  \n",
    "\n",
    "To really nail home the technique, vs alternatives, the below (large) section solves the Gambler's Ruin problem from *many* different perspectives, about half of which use martingale techniques, though there are numerous other ways of approaching the problem.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below consider the finite state Gambler's Ruin Problems \n",
    "\n",
    "**Martingale methods**  \n",
    "The easiest method to apply is when the game is already fair! i.e. when each gamble is zero mean, we have \n",
    "\n",
    "$S_T = Y_1 + Y_2 +... + Y_T$  \n",
    "\n",
    "where $Y_i$ are the iid (Rademacher) random variables that payoff $+1$ with probability $p$ and $-1$ with probability $q = 1-p$.  In this setup, we look at payoffs relative to some starting wealth value of $i$ \n",
    "\n",
    "Since this may be embedded in a finite state markov chain, we know $E\\big[T\\big] \\lt \\infty$ and the payoffs/ states themselves are bounded.  Hence we may apply Wald's Equation to see \n",
    "\n",
    "$E\\big[S_T\\big] = E\\big[Y_1 + Y_2 +... + Y_T\\big] = E\\big[T\\big] E\\big[Y_1\\big] = E\\big[T\\big]0 = 0$ \n",
    "\n",
    "but we know that the termination points of weatlh will be reached WP1 (and there are no overshoots), so \n",
    "\n",
    "$p_{win} \\cdot (N-i) + (1-p_{win}) (-i) = E\\big[S_T\\big] = 0 $   \n",
    "$p_{win} \\cdot (N-i) + p_{win} \\cdot i = i$  \n",
    "$p_{win} \\cdot N = i$  \n",
    "$p_{win} = \\frac{i}{N}$  \n",
    "\n",
    "if we want to recover the time for $E\\big[T\\big]$ we can apply Wald's Identity (page 435 of Gallgher) or the results from chapter 3 ex 8 of Ross & Pekoz to see \n",
    "\n",
    "$ E\\big[S_T^2\\big] = \\text{Var}\\big(S_T\\big) =\\sigma_{Y_i}\\cdot E\\big[T\\big]  = E\\big[T\\big]$  \n",
    "where recall that $S_T$ is zero mean and that $\\sigma_{Y_i} = 1$ (zero mean random variable taking squared payofss of +1)\n",
    "\n",
    "With our stopping rule in place, we can see \n",
    "\n",
    "$E\\big[S_T^2\\big] = p_{win} \\cdot (N-i)^2 + (1-p_{win}) (-i)^2 = \\frac{i}{N}(N-i)^2 + \\frac{N-i}{N}i^2= iN -i^2 = i(N-i)$   \n",
    "\n",
    "**Alternative methods for Expected Time of Fair Game**  \n",
    "an alternative finish is to consider \n",
    "\n",
    "$\\big(\\mathbf I - \\mathbf B \\big) \\mathbf m = \\mathbf N^{-1}\\mathbf m = \\mathbf 1$   \n",
    "\n",
    "where $\\mathbf B$ is the partitioned part of our transition matrix for the markov chain, referencing the transition probabilities for transient states (i.e. the non-absorbing states).  \n",
    "\n",
    "$\\mathbf N^{-1} = \n",
    "\\left[\\begin{matrix}\n",
    "1 & -\\frac{1}{2}& 0 & 0 & 0 & \\dots & 0 \\\\\n",
    "-\\frac{1}{2} & 1 & -\\frac{1}{2} & 0 & 0 & \\dots & 0\\\\\n",
    "0 & \\frac{1}{2} & 1 & -\\frac{1}{2} & 0 & \\dots & 0\\\\\n",
    "0 & 0& -\\frac{1}{2} & 1 &-\\frac{1}{2}  & \\dots & 0\\\\\n",
    "0 & 0 & 0 & -\\frac{1}{2} & 1 & \\dots & 0\\\\\n",
    "\\vdots & \\vdots & \\vdots & \\vdots & \\ddots & \\ddots & \\vdots\\\\\n",
    "0 & 0 & 0 & 0 & 0 & -\\frac{1}{2} & 1\\end{matrix}\\right]$  \n",
    "\n",
    "There are two primary interpretations for this: (1.) As a system of linear equations associated with boundary conditions + a recurrence (2.)  (see absorbing state markov chains writeup) $\\mathbf N$ is the fundamdental matrix that is given by summing over the geometric series of $\\mathbf B^k$ (i.e. in effect summing over the complementary CDF for each state)  \n",
    "\n",
    "noting that $N = \\mathbf 1^T\\mathbf 1$  \n",
    "\n",
    "There is special structure in this problem -- notice that every column, except the first and last, sums to 0--  and we can telescope a solution by left multiplying our matrix vector equation by $\\mathbf 1^T$, which gives:  \n",
    "\n",
    "$m_{N-1} = m_1 = \\frac{1}{2} m_1 + \\frac{1}{2}m_{N-1} =  \\mathbf 1^T \\mathbf N^{-1}\\mathbf m = \\mathbf 1^T\\mathbf 1 = N$ \n",
    "\n",
    "because by symmetry, the expected time until absorbtion from state $1$ must be the same as the expected time until absorbtion from state $N-1$.  In many cases this information in itself is enough and what we are interested in.  \n",
    "\n",
    "We can also apply this to state $2$ (and also $N-2$) to see that the above system of equations on row 1 implies  \n",
    "\n",
    "$1 \\cdot N  -\\frac{1}{2}m_2 = 1\\cdot m_1 -\\frac{1}{2}m_2 =  1$  \n",
    "or  \n",
    "$2(N-1) = m_2 $  \n",
    "\n",
    "and we can plug this in, in line 2, and to see \n",
    "$3 (N-3) = m_3 $   \n",
    "\n",
    "and in general, for $i \\in \\{3, 4, ..., N-3,\\}$   we have  \n",
    "\n",
    "$-\\frac{1}{2}\\cdot m_{i-1} + 1 \\cdot m_i -\\frac{1}{2}\\cdot m_{i+1} = 1$   \n",
    "or  \n",
    "$-\\frac{1}{2}\\cdot m_{i+1} = 1 + \\frac{1}{2}\\cdot m_{i-1} - 1\\cdot  m_i $   \n",
    "or  \n",
    "$ m_{i+1} = -2 -m_{i-1} + 2\\cdot  m_i $   \n",
    "\n",
    "but  \n",
    "$m_i = i(N-i)$  \n",
    "and   \n",
    "$m_{i-1} = (i-1)(N-(i-1))$  \n",
    "by strong inductive hypothesis, which gives  \n",
    "$ m_{i+1} $  \n",
    "$= -2 -(i-1)(N-(i-1)) + 2\\cdot  i(N-i) $  \n",
    "$= - 2 - (i-1)N + (i-1)^2 + 2iN - 2i^2 $  \n",
    "$= -2 -iN +N + i^2 -2i + 1 + 2iN - 2i^2 $    \n",
    "$= -1 + N + -2i + iN  - i^2$    \n",
    "$=(i+1)(N-(i+1))$  \n",
    "\n",
    "which completes the inductive proof of the formula for expected time until absorbtion   \n",
    "\n",
    "*remark:*  \n",
    "given the symmetry, which among other things gives us the fact that each time we solve for state $i$ we get a solution for state $N-i$, it may be more natural to apply something akin to leap forward, fall backward induction.  *This is at present an open item*.  \n",
    "\n",
    "\n",
    "- - - - -\n",
    "Using the Wald Equality is very easy in the zero mean case, i.e. where $p = 0.5 = q$.  All of the problems below assume that $p\\neq q$.  \n",
    "\n",
    "**Martingale method -- with matrices **  \n",
    "this is inspired by page 492, number 9 in Grinstead and Snell, though it may be different as it was hard to parse what exactly the authors wanted here.  (problem 33 on page 431 also ties in with this, though it says to merely confirm the value and does not say where the value came from)  \n",
    "\n",
    "With the above matrix multiplication take on martingales, consider, a Gambler's Ruin problem, shown below with states $\\{0,1,2,3,4, N=5\\}$  \n",
    "\n",
    "$\\mathbf P_{\\text{Absorbing}} = \\left[\\begin{matrix}1 & 0 & 0 & 0 & 0 & 0\\\\q & 0 & p & 0 & 0 & 0\\\\0 & q & 0 & p & 0 & 0\\\\0 & 0 & q & 0 & p & 0\\\\0 & 0 & 0 & q & 0 & p\\\\0 & 0 & 0 & 0 & 0 & 1\\end{matrix}\\right]$  \n",
    "\n",
    "Employing a common tactic in stochastics, we do a \"first step analysis\", conditioning on the event that the first round is a win, denoted by $A_1$ we have, for $i=1,2,3..., N-1$   \n",
    "\n",
    "In this setup, rather than looking at payoffs from $Y_i$, we look at the *gambler's wealth* at time $0$, with some current value after a first step, given where $X_2 = i$.  \n",
    "\n",
    "$f(i) = E\\Big[f(X_2)\\Big] = E\\Big[E\\big[f(X_{2})\\big \\vert X_1\\big]\\Big] = q\\cdot E\\Big[f(X_2)\\big \\vert X_1 = i-1 \\Big] +p\\cdot E\\Big[f(X_2)\\big \\vert X_1 = i+1 \\Big] = q\\cdot(f-i) + p\\cdot f(i)  $    \n",
    "\n",
    "*above needs cleaned up-- it would almost make more sense to set this up as a backwards martingale...*   \n",
    "\n",
    "Wt this point we may see a companion system for a linear recurrence that we can re-arrange, and solve though this is mechanical and not that probabilistically insightful (for this approach, see \"Yet another solution to Gambler's Ruin\") which follows in a few subsections.  However, knowing what the solutions to such systems frequently look like, we may try to *guess* that each state here has some geometric value with some $\\delta \\gt 0$ (for lots of other motivation, consider e.g. that this is what an MGF or OGF looks like and how roots there behave, or read the \"first step analysis\" section)  \n",
    "\n",
    "- - - - \n",
    "the underlying point is that we typically have a sum or product martingale, and the product form is going to be something like \n",
    "\n",
    "$\\pi = p_0 \\pi^{0} + p_1 \\pi^{1} + ... + p_n \\pi^{n}  = E\\big[\\pi^Y\\big]$  \n",
    "\n",
    "i.e. this:  \n",
    "$E\\big[\\pi^Y\\big]$  \n",
    "is a common form, so looking for some value to take powers of is a *very* reasonable guess. (Working through cumulants tells us why and is addressed from slightly different perspective in the sub-section that follows called \"*Martingale method -- Ross and Pekoz* \")   \n",
    "- - - - \n",
    "\n",
    "$\\delta^i = q \\delta^{i-1} +\\delta^{i+1} p$  \n",
    "\n",
    "note-- as we've seen elsewhere with say generating functions and martingales for branching processes, $\\delta =1$ always trivially satisfies the above, irrespective of $p$, so that is not an interesting solution.  So we can simplify this to \n",
    "\n",
    "$\\delta = (1-p) +\\delta^{2} p$  \n",
    "or  \n",
    "$0=  \\delta^{2} p - \\delta+ (1-p)$  \n",
    "\n",
    "which wolfram \n",
    "https://www.wolframalpha.com/input/?i=delta%5E2+p+-+delta+%2B(1-p)  \n",
    "\n",
    "says has solution of $\\delta = 1$ and $\\delta = \\frac{1-p}{p}$  \n",
    "(note again in the $p=0.5=q$ case how both roots are one, which is uninformative and tells us in a different way, why we need to treat the zero mean case separately)  \n",
    "\n",
    "\n",
    "so, selecting $\\delta = \\frac{1-p}{p}$, we see a martingale with   \n",
    "\n",
    "$\\mathbf f= \\left[\\begin{matrix}1 \\\\ \\delta \\\\\\delta^2 \\\\ \\vdots \\\\\\delta^{N-1}\\\\\\delta^N \\end{matrix}\\right]= \n",
    "\\left[\\begin{matrix}1 \\\\ \\big(\\frac{1-p}{p}\\big) \\\\\\big(\\frac{1-p}{p}\\big)^2 \\\\ \\vdots \\\\ \\big(\\frac{1-p}{p}\\big)^{N-1}\\\\\\big(\\frac{1-p}{p}\\big)^N \\end{matrix}\\right]$   \n",
    "\n",
    "and we can thus see that \n",
    "\n",
    "$\\mathbf P_{\\text{Absorbing}}\\mathbf f = \\mathbf f$ \n",
    "\n",
    "even in the case of row $0$ and row $N$. Hence we do have a martingale here.   \n",
    "\n",
    "and we can plug in the above result of $\\frac{f(i)- f(r)}{f(0) - f(r)} = p_{i,0}^{(\\infty)}$  \n",
    "which can be restated in our notation as   \n",
    "$\\frac{f(i)- f(N)}{f(0) - f(N)} = p_{i,0}^{(\\infty)}$ \n",
    "$\\frac{f(i)- f(N)}{f(0) - f(N)} = 1- p_{i,N}^{(\\infty)}$ \n",
    "\n",
    "$ p_{i,N}^{(\\infty)} = 1 - \\frac{f(i)- f(N)}{f(0) - f(N)} =  \\frac{f(0)- f(N)}{f(0) - f(N)} + \\frac{-f(i)+ f(N)}{f(0) - f(N)} =  \\frac{f(0)- f(i)}{f(0) - f(N)} = \\frac{1-\\big(\\frac{1-p}{p}\\big)^i }{1-\\big(\\frac{1-p}{p}\\big)^N } $   \n",
    "\n",
    "which is the standard result to Gambler's Ruin.  \n",
    "- - - - \n",
    "Now changing our perspective from the stock variable of Gambler's Wealth to the flow variable of total winnings, i.e. rather than quitting at 0 or $N$, given initial wealth of $i$, we now subtract $i$ from each side of our equation, and quit when accumulated winnings are $-i$ or $N-i$  (this in effect moves us from an affine space to a vector space -- but the wealth perspective was useful for other reasons -- it was real non-negative with a \"proper\" zero -- both perspectives can be useful) \n",
    "\n",
    "applying the Wald Equation: \n",
    "\n",
    "\n",
    "$E\\big[S_T\\big] = E\\big[X_1 + X_2 +... + X_T\\big] = E\\big[T\\big] E\\big[X\\big] \\neq  0$  \n",
    "we can solve for the expected number of turns as   \n",
    "$\\frac{E[S_T]}{\\bar{X}}= E\\big[T\\big]$   \n",
    "\n",
    "so  \n",
    "$\\frac{1}{\\bar{X}}E\\big[S_T\\big] = \\frac{1}{\\bar{X}}\\big(p_{win} \\cdot (N-i) + (1-p_{win}) \\cdot -i\\big) =  \\frac{1}{\\bar{X}}\\big( p_{win} - i\\big) = \\frac{1}{p-q}\\big( p_{win} - i\\big) = E\\big[T\\big]$    \n",
    "\n",
    "\n",
    "- - - - - \n",
    "*extension*  \n",
    "Suppose want an alternative method beside guessing to find the above martingale function $f$.  \n",
    "\n",
    "If we re-visit \"markov_chains_absorbing_state_recut.ipynb\", the above method, for highly structured problems would allow us to, partition out states 1 through N-1.  (Note that matrix has a canonical form of all absorbing states in bottom right cortner -- this is a graph isomorphism--- whereas the common form for martingales with finite matrices seems to be one absorbing state up top and one in the bottom.)    \n",
    "\n",
    "Using this, we could select a few small cases, i.e. ##N=\\{4,5,6,7\\}## which after removing the absorbing states means 2-4 transient states.  Then we can solve for the 'Red' portion, knowing that solutions come in the form \n",
    "\n",
    "$\\frac{f(0)- f(i)}{f(0) - f(N)}$  \n",
    "\n",
    "Holding N constant, we can divide one solution by the other (they are all positive) to isolate $\\frac{f(0)- f(i)}{f(0)- f(i+1)}$ for some starting state $i$ vs $i+1$.  From here, we hopefully, find some value $f(0)$ that is common in both, numerator and denominator and then isolate $f(i)$ vs $f(i+1)$.  If there is a pattern that holds over multiple values of $i$, we may have a hypothesis of how the problem works, and see if the pattern holds for other $N$ values.  \n",
    "\n",
    "\n",
    "**Martingale method -- Ross and Pekoz **  \n",
    "use product form of Tilted martingale  \n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With some facts in hand about Moment Generating Functions and Cumulants, perhaps the most succinct approach is to use *tilting* and martingale techniques, as shown on page 94 of Ross and Pekoz.  In particular, we can consider our gambler's ruin problem as being \n",
    "\n",
    "$S_T = X_1 + X_2 + ... +X_T$  \n",
    "\n",
    "where we quit when we've lost $i$ dollars our have gained $N-i$ dollars and each $X_i$ is an iid r.v. with payoffs of $+/-1$ and loss/wins of $q/p$.  Via a finite state (absorbing) markov chain embedding argument we know that this will be absorbed geometrically fast and hance $E\\big[T\\big] \\lt \\infty$.  \n",
    "\n",
    "As mentionedm, in the case where $\\bar{X} =0$ we can directly apply Wald's Equation to get the probabilities of win and loss. However when $\\bar{X}\\neq 0$ we can exploit this information in combination with tilting.  In particular, consider the MGF\n",
    "\n",
    "$E\\big[e^{\\theta X_i}\\big] :=1$  \n",
    "so  \n",
    "\n",
    "$E\\big[e^{\\theta \\sum_{i=1}^T X_i}\\big]=\\prod_{i=1}^T E\\big[e^{\\theta  X_i}\\big] = 1$  \n",
    "\n",
    "(where we make use of the fact that expectations of indepdendent random variables $e^{\\theta X_i}$ multiply).  Hence we have a product form martingale.  \n",
    "\n",
    "It may be convenient to consider this from the perspective of a cumulant so  \n",
    "\n",
    "$\\log\\Big(E\\big[e^{\\theta X_i}\\big]\\Big) =0$  \n",
    "\n",
    "but from our writeup _____ *(insert writeup name)*  \n",
    "we know that the cumulant has value of 0, slope of $\\bar{X}$ when $\\theta = 0$ and is convex. This implies it goes under the $y$ access if we follow the direction of the slope -- which updates to become the slope of the implied titled random variable for different values of $\\theta$.  The technique applies in general for any open interval in which the MGF exists -- Ross and Pekoz give boundedness (which applies here) as strong sufficent condition -- which allows us to allocate arbitrarily large amounts of probability mass to positive or negative values in our case for the tilted random variable (while guaranteeing no 'blow up to infinity'), which qualitatively tells us that at some point the tilted r.v. must have a mean (and hence cumulant slope) that is opposite of the original value.  This is a linear lower bound on a function that exists, in the bounded and other 'nice' cases, over the entire real line, hence we find that there must be one and only one other crossing of the y axis.  (Note at the origin our slope is a linear lower bound preventing another root from existing when we go 'the other way' and hence there will be exactly 2 roots.) Since there are two roots for the cumulant, this means that we must have $E\\big[e^{\\theta X_i}\\big] :=1$ for $\\theta \\in \\{0, \\theta^*\\}$  \n",
    "\n",
    "As noted elswhere, the value $\\theta =0$ is uninformative because all that tells us is the underlying probability distribution integrates to one (all values get mapped to 0, then exponentiated to become 1 when taking the expectation).  On the other hand we see that $\\theta^*$ does *not* do this and is interesting. Using our new tilted product form of a martingale, we have  \n",
    "\n",
    "$1 = E\\big[e^{\\theta^* S_T}\\big]$  \n",
    "\n",
    "since there are no overshoots here, we can nicely apply total expectation to see  \n",
    "\n",
    "$1 = E\\big[e^{\\theta^* S_T}\\big] = p_0 \\cdot E\\big[e^{\\theta^* S_T \\vert S_T = -i}\\big] + (1-p_0) \\cdot E\\big[e^{\\theta^* S_T \\vert S_T = N-i}\\big]$    \n",
    "\n",
    "and it comes as no surpise that for our probelm we recover \n",
    "https://www.wolframalpha.com/input/?i=(1-p)+exp(-t)+%2B+p+*+exp(t)+%3D+1  \n",
    "\n",
    "$\\theta^* = \\log\\big(\\frac{1-p}{p}\\big)$  \n",
    "\n",
    "so  \n",
    "\n",
    "$1 = p_0 \\cdot \\big(\\frac{1-p}{p}\\big)^{-i} + (1-p_0) \\big(\\frac{1-p}{p}\\big)^{N-i}$  \n",
    "\n",
    "$1-\\big(\\frac{1-p}{p}\\big)^{N-i} = p_0 \\cdot \\Big(\\big(\\frac{1-p}{p}\\big)^{-i} - \\big(\\frac{1-p}{p}\\big)^{N-i}\\Big)$  \n",
    "\n",
    "or, after multiplying each side by $\\big(\\frac{1-p}{p}\\big)^{-(N-i)}$\n",
    "\n",
    "$\\big(\\frac{1-p}{p}\\big)^{-(N-i)}-1 = p_0 \\cdot \\Big(\\big(\\frac{1-p}{p}\\big)^{-N} - 1\\Big)$  \n",
    "\n",
    "$\\frac{1-\\big(\\frac{1-p}{p}\\big)^{-(N-i)}}{1 - \\big(\\frac{1-p}{p}\\big)^{-N}} = p_0 $  \n",
    "as desired  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First Step Analysis:**   \n",
    "\n",
    "Dealing with one barrier is sometimes easier than two, and frequently limits can simplify messy problems, so suppose we consider the case of a countable state markov chain, where we start at state 1 and there is an absorbing state at 0.  \n",
    "\n",
    "We may reason that the probability of absorbtion is given by \n",
    "\n",
    "$(\\text{prob absorb from 1}) = q + p\\cdot (\\text{prob absorb from 1})^2$  \n",
    "based on the first step of absorbing with probability $q$ and and with probability $p$ moving to the right one, but from there, based on the markov property, we have $\\text{p absorb from 1}$ of ever getting from $2 \\to 1$ and from there we have the independent probability (again markov property) of $\\text{p absorb from 1}$ of ever being absorbed.  So if we work through the roots of \n",
    "\n",
    "$0 = p\\cdot (\\text{prob absorb from 1})^2 - (\\text{prob absorb from 1}) +  (1-p)$    \n",
    "\n",
    "we see roots of \n",
    "$\\lambda = 1$ and $\\lambda = \\frac{1-p}{p}$    \n",
    "https://www.wolframalpha.com/input/?i=(1-p)+%2B+px%5E2+-+x  \n",
    "\n",
    "If we are familiar with martingales, these roots from First step analysis are enough to show transience when $\\frac{1-p}{p} \\lt 1$ and recurrence when $\\frac{1-p}{p}\\geq 1$  by plugging these roots in to $\\mathbf f$ and seeing $\\mathbf {Pf} = \\mathbf f$ -- which would tell us the probability of \"escape\" is $\\lt 1$ precisely when $\\frac{1-p}{p} \\lt 1$ (use limit theorem with constant expected value for non-negative martingale).  This also helps motivate our 'guess' in the earlier section.  \n",
    "\n",
    "If we didn't know martingales, we could reason through using some mixture of the following.  For the 50:50 case, the probability of return is $1$ in either case.  For the case of $p\\lt \\frac{1}{2}$ the other root is $\\gt 1$ and hence not a probability -- hence we return WP1.  (We could also invoke the strong or weak laws of large numbers here.)  \n",
    "\n",
    "The typical point of contention is for $p\\gt \\frac{1}{2}$ -- which root to select?  We can look at a boundary case of $p \\to 1^{-}$ and see probability of not being absorbed $\\to 0$.  As done on pages 271, 272, 280 of Feller vol 1, we *could* use a continuity arument here (though the overall setup is via a generating function in that case) -- i.e. the probability of ruin would not vary continuously with $p$ if we ever use the first root for $p\\gt \\frac{1}{2}$. Another approach would be to use monotonicity of the roots -- hence the smallest ones have preference (though the details are coming to your author and in any case they can be exchanged for the same but more powerful result via use of Martingales).  A final approach, using knowledge of SLLN and basic facts about countable state markov chains -- i.e. consider the same chain in \"recurrent form\", where state $0 \\to 1$ with weight one, we could reason that in the case of $p \\gt \\frac{1}{2}$ there is positive expected value / drift per each turn, so the SLLN tells us the the expeced value $\\to \\infty$ as n growns large and hence and state value like $0$ will be visiting finitely many times -- but being state 0 being visited only finitely times means the chain in this form is not recurrenct, and hence in absorbing state form, $0$ is not persistent.  We could *also* look at $\\mathbf P \\big(\\mathbf 1 -\\mathbf f\\big) = \\big(\\mathbf 1 -\\mathbf f\\big)$ which has a zero on top row, but not elsewhere (and all values are in $[0,1]$), then delete first row of each vector and first row and column of $\\mathbf P$ and referencing top of page 402 of Feller vol 1, we can be certain that state 0, the deleted one, is not persistent and hence the probability of return $\\lt 1$.   Yet one more may of justifying this other root would be to run separate analysis on a Ballot problem (see cell below).  \n",
    "\n",
    "So in the case of $p \\gt \\frac{1}{2}$ we see non-zero probability of 'wandering off' / 'escaping' the absorbing state.  What does this have to do with Gambler's Ruin?  Using conditional probability (and the markov property), we can write  \n",
    "\n",
    "$1 - \\frac{1-p}{p} = \\text{prob escape from 1} = 1 - \\text{prob absorb from 1} = \\text{prob of 1st hitting state N before 0}\\cdot \\text{prob escape from state N}$  \n",
    "\n",
    "but we can see from our earlier argument that  \n",
    "$\\text{prob absorb from state N} = \\big(\\text{prob absorb from 1}\\big)^N = \\big(\\frac{1-p}{p}\\big)^N$, so \n",
    "\n",
    "$\\text{prob escape from state N} = 1-  \\big(\\frac{1-p}{p}\\big)^N$ $\n",
    "\n",
    "then dividing both sides by this, we have \n",
    "\n",
    "$\\text{prob of 1st hitting state N before 0} = \\frac{1 - \\frac{1-p}{p} }{1-  \\big(\\frac{1-p}{p}\\big)^N}$  \n",
    "\n",
    "we can re-run the exact same argument, starting from state $1\\lt i \\lt N$ to get \n",
    "\n",
    "$\\text{prob of 1st hitting state N before 0 given start at i} = \\frac{1 - \\big(\\frac{1-p}{p}\\big)^i }{1-  \\big(\\frac{1-p}{p}\\big)^N}$  \n",
    "\n",
    "which is the solution to Gambler's Ruin for $p\\gt \\frac{1}{2}$.  Now making use of symmetry, we can 'flip' the indexing to a chain that has maximal state $N$ and counts down toward $-\\infty$ and re-run the above argument when $p\\lt \\frac{1}{2}$  (or better, recognize this mirrored chain is identical to the original, with some relabelling, and once again positive drift away from the absorbing state -- with a change of variables so $p_{new} = q_{old}$).  \n",
    "\n",
    "And with some work, we end up with the same solution as before:  \n",
    "\n",
    "$\\text{prob of 1st hitting state N before 0 given start at i} = \\frac{1 - \\big(\\frac{1-p}{p}\\big)^i }{1-  \\big(\\frac{1-p}{p}\\big)^N}$  \n",
    "\n",
    "As has been observed, we *still* have a nuisance case when $p=\\frac{1}{2}$.  As noted on page 345 of Feller vol 1, we *could* take limits as $p \\to \\frac{1}{2}$ (from left and right) via rule of L'Hopital to recover the correct solution in this case.  Your author's preference *still* is to deal with this via the Wald Equation, which we may know from Martingales, *or* from Renewal Theory (though it is not covered in Feller Vol 1 it is covered in most all stochastics texts covering renewal theory as it underpins the elementary renewal theorem).    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Yet another solution to Gambler's Ruin**  \n",
    "Consider setting this problem up as a simple linear recurrence (as done on pages 344 and 345 of Feller Vol 1, though the payoffs are flipped in this case)  \n",
    "\n",
    "If we assign a payoff of 1 at state $N$ and payoff of 0 at state $0$, we can set up a linear recurrence -- again via the use of first step analysis.  We have \n",
    "\n",
    "$f(i) = q \\cdot f(i-1) + p \\cdot f(i+1)$  \n",
    "\n",
    "or  \n",
    "\n",
    "$ f(i+1) = -\\frac{q}{p} f(i-1) + \\frac{1}{p} f(i) +$  \n",
    "\n",
    "in (companion) matrix form \n",
    "\n",
    "$\\mathbf C \\mathbf f =\n",
    "\\left[\\begin{matrix}\n",
    "f(i)\\\\\n",
    "f(i+1)\\end{matrix}\\right] = \\left[\\begin{matrix}\n",
    "0 & 1\\\\\n",
    "-\\frac{q}{p} & \\frac{1}{p}\\end{matrix}\\right]\\left[\\begin{matrix}\n",
    "f(i-1)\\\\\n",
    "f(i)\\end{matrix}\\right]$  \n",
    "\n",
    "and in particular  \n",
    "\n",
    "$\\mathbf C^k \\mathbf f_{\\text{start}} =\n",
    "\\left[\\begin{matrix}\n",
    "f(k)\\\\\n",
    "f(k+1)\\end{matrix}\\right] = \\left[\\begin{matrix}\n",
    "0 & 1\\\\\n",
    "-\\frac{q}{p} & \\frac{1}{p}\\end{matrix}\\right]^k\\left[\\begin{matrix}\n",
    "1\\\\\n",
    "f(1)\\end{matrix}\\right]$  \n",
    "\n",
    "unfortunately, working in this form is a bit difficult.  (Your author's original take was to use something akin to a forward-backward algorithm to take these values forward to $f(N) =1$, then work backwards to find the implied $f(1)$ however this becomes suprisingly messy.) Instead, it is better to pursue the method of particular solutions, as done in Feller -- for a typical linear algebra take using the companion matrix (with vandermonde matrix as eigenvectors of course) refer to the subsection called \"Detour into Solving Linear Recurrences with boundary conditions\" in \"Vandermonde_Matrices_Permutations_and_Discrete_Fourier_Transform.ipynb\" in the Linear Algebra Folder.  \n",
    "\n",
    "We can eyeball the trace and determinant here, and not surprisingly given our work through of first step analysis, we find eigenvalues of  $\\lambda_1 = 1$, $\\lambda_2 = \\frac{1-p}{p}$   \n",
    "\n",
    "and we can then say we know $a_0 = 0$ and $a_N = 1$ so, $a_k = b_1\\lambda_1^{n-1+k} + b_2\\lambda_{2}^{n-1+k}$   \n",
    "\n",
    "\n",
    "$\\begin{bmatrix}\n",
    "a_0 \\\\ \n",
    "a_N \\\\ \n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "0 \\\\ \n",
    "1 \\\\ \n",
    "\\end{bmatrix} =  \\begin{bmatrix}\n",
    "1 & 1 \\\\ \n",
    "\\lambda_1^N & \\lambda_2^N \\\\ \n",
    "\\end{bmatrix} \\begin{bmatrix}\n",
    "b_1 \\\\ \n",
    "b_1 \\\\ \n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "1 & 1 \\\\ \n",
    "1 & (\\frac{1-p}{p})^N \\\\ \n",
    "\\end{bmatrix} \\begin{bmatrix}\n",
    "b_1 \\\\ \n",
    "b_2 \\\\ \n",
    "\\end{bmatrix}$  \n",
    "\n",
    "which is exactly solvable when $p\\neq 1-p$  (check the determinant), and we recover the same solution as in the above.  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem of Runs with Martingales**  \n",
    "(an expansion and adaptation of pages 91, 92 in Ross & Pekoz)  \n",
    "\n",
    "The problem of Runs can be modelled, and very flexibly so, using a fair casino and a sequence of gamblers.  He illustrate this problem with the 'pure' streak or say $HHHHHH$ as a complementary look to what is in Ross & Pekoz.  \n",
    "\n",
    "*setup 1st Moment:*  \n",
    "consider a sequence of gamblers betting on a pattern appearing $HHHHHH$, betting on one item at at time.  The game stops (and cashes out all players) when one player has successfully has bet on a full sequence of 6 items and the pattern is exactly those 6 items.  \n",
    "\n",
    "each player $i$ has starting stake of $a_i =1$ and bets it.  But we may vefity that  \n",
    "$a = p\\cdot \\frac{a}{p} + (1-p)\\cdot 0$  i.e. $1 = p\\cdot \\frac{1}{p} + (1-p)\\cdot 0$.  \n",
    "So we see it is a fair game.  \n",
    "\n",
    "we define $Z_n$ as the casino's winnings after $n$ bets.  We can use condition 3 of Theorem 3.14 -- in the difference in payoffs $\\big \\vert Z_{n+1} - Z_n\\big\\vert$  are uniformly bounded (and we know $E\\big[N\\big] \\lt \\infty$  via a finite state markov chain embedding argument), so we have a valid stopping time, and see that \n",
    "\n",
    "$Z_T  = T - \\big(\\frac{1}{p}\\big)^6 - \\big(\\frac{1}{p}\\big)^5 - \\big(\\frac{1}{p}\\big)^4 -....- \\big(\\frac{1}{p}\\big)^1$  \n",
    "$Z_T  = T - \\frac{1}{p} \\Big(\\frac{1}{p}^5 + \\big(\\frac{1}{p}\\big)^4 + \\big(\\frac{1}{p}\\big)^3 + ....+ 1\\Big)$  \n",
    "$Z_T  = T - \\frac{1}{p} \\Big(\\frac{1 - \\frac{1}{p^6}}{1 - \\frac{1}{p}}\\Big)$  \n",
    "$Z_T  = T - \\frac{1 - \\frac{1}{p^6}}{p - 1}$  \n",
    "$Z_T  = T - \\frac{p^6 - 1}{(p - 1)p^6}$  \n",
    "$Z_T  = T - \\frac{1 - p^6}{(1 - p)p^6}$  \n",
    "\n",
    "which reads, for any $T=t$ that exists with positive probability, we *know* that there will be $6$ gamblers in the trailing \"queue\" i.e. the one who saw the full patter, the one who bet on all but one of the elements of the pattern, the one who bet on all but 2 elements of the pattern, and on down to one who bet on only 1 element of the pattern, and those are payouts made by the casino (hence negative), but for any $T=t$ exactly $t$ games have been played and hence there have been $t$ gamblers in the sequence, each of whom paid $1$ on the initial bet.  (For avoidance of doubt in a simplified example of a single element pattern, if $t =1$ then casion collected $1$ and paid out $\\frac{1}{p}$)  \n",
    "\n",
    "But, this *is* a fair game and meets one of our sufficient conditionsm, and hence  \n",
    "$0 = E\\big[Z_T\\big]  = E\\big[T\\big] - \\frac{1 - p^6}{(1 - p)p^6}$  \n",
    "\n",
    "$E\\big[T\\big]=  \\frac{1 - p^6}{(1 - p)p^6}$  \n",
    "\n",
    "- - - - \n",
    "What is interesting is we have an extra parameter that we may use here -- we can change the amount of the starting wager of each player $i$.  In the prior problem we had it flat at $a_i = 1$, but if we set $a_i = i$ we have a in increasing sequence, that recovers a triangular number in the amount bet.   \n",
    "\n",
    "if we do that *and ignoring convergence issues for the time being*, we have \n",
    "\n",
    "$Z_T = \\big(1 + 2 + ... + T\\big) -  \\Big((T-6)\\big(\\frac{1}{p}\\big)^6 - (T-5)\\big(\\frac{1}{p}\\big)^5 - (T-4)\\big(\\frac{1}{p}\\big)^4 -....- 1\\big(\\frac{1}{p}\\big)^1\\Big)$ \n",
    "\n",
    "$Z_T = \\frac{T^2 + T }{2} - T\\frac{1 - p^6}{(1 - p)p^6} +   \\frac{1}{p}\\Big(6\\big(\\frac{1}{p}\\big)^5  + 5\\big(\\frac{1}{p}\\big)^6 + 4\\big(\\frac{1}{p}\\big)^3 +....+1\\Big)$  \n",
    "\n",
    "$Z_T =  \\frac{T^2 + T }{2} - T\\frac{1 - p^6}{(1 - p)p^6} +   \\frac{1}{p}\\Big(\\sum_{i=1}^n i \\cdot p^{i-1}\\Big)$  \n",
    "\n",
    "and hence assuming we have a valid stopping time, we still have a fair game and   \n",
    "$ 0 = E\\big[Z_T\\big] = \\frac{1}{2}\\big(E\\big[T^2] + E\\big[T\\big]\\big)  - E \\big[T\\big]\\frac{1 - p^6}{(1 - p)p^6}  + \\frac{k \\cdot p^{k+1} - (k+1)p^k  +1}{(1-p)^2 p}$  \n",
    "\n",
    "(where in our problem $k=6$ and we used wolfram to help simplify for the expected value of a truncated geometric random variable)  \n",
    "https://www.wolframalpha.com/input/?i=%5Cfrac%7BT%5E2+%2B+T+%7D%7B2%7D+-+T%5Cfrac%7B1+-+p%5E6%7D%7B(1+-+p)p%5E6%7D+%2B+++%5Cfrac%7B1%7D%7Bp%7D%5CBig(%5Csum_%7Bi%3D1%7D%5En+i+%5Ccdot+p%5E%7Bi-1%7D%5CBig)  \n",
    "\n",
    "$ E\\big[T^2\\big] =  - E\\big[T\\big]  + 2 \\cdot E \\big[T\\big]\\frac{1 - p^6}{(1 - p)p^6}  - 2 \\frac{k \\cdot p^{k+1} - (k+1)p^k+ 1}{(1-p)^2 p}$  \n",
    "\n",
    "*remark:*  \n",
    "This shows the power of having an additional, flexible parameter, namely the bet size $a_i$, and we could in principle recover higher moments further expoiting this.  However, the formula is already rather messy.  \n",
    "\n",
    "*re: convergence*  \n",
    "if we tried to apply sufficient condition 3, we'd run in to a problem where the difference in bet sizes is no longer bounded. We do know that $E\\big[T^2\\big]\\lt \\infty$ again by a finite state markov chain embedding argument.  Ross and Pekoz note that the result stands but to not prove / supply justifiction.  \n",
    "\n",
    "*an open item* \n",
    "\n",
    "consider a truncated form of the martingale (e.g. as in Lawler) where we use $T \\text{^} n$ (i.e. the first of the stopping time and $n$), can we examine:  \n",
    "\n",
    "$\\frac{(T \\text{^} n)^2 + T \\text{^} n }{2} - (T \\text{^} n)\\frac{1 - p^6}{(1 - p)p^6} \\leq  Z_{T \\text{^} n} \\leq   \\frac{(T \\text{^} n)^2 + T \\text{^} n }{2} - (T \\text{^} n)\\frac{1 - p^6}{(1 - p)p^6} +   \\frac{1}{p}\\Big(\\sum_{i=1}^n i \\cdot p^{i-1}\\Big)$  \n",
    "\n",
    "and starting with suffficicently large $n$, apply monotone convergence theorem to each of the bounds? \n",
    "\n",
    "*additional remark:*  \n",
    "we may of course derive these results via other means -- e.g. via generating functions.  We can find variance of renewal epochs given in (7.7) on page 324 of Feller vo1 1, 3rd edition.  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should be able to adapt sufficient condition 3 in Ross & Pekoz to this problem.  In particular, we know \n",
    "\n",
    "$E\\big[T^2\\big] \\lt \\infty$  \n",
    "because of a finite state markov chain argument (and said chain is stochastically smaller than some geometrically distributed random variable)  \n",
    "\n",
    "and we observe that with the sequence of bets increasning linearly with $a_i = i$ we have \n",
    "\n",
    "$E\\Big[\\big\\vert Z_{t+1} - Z_t\\big \\vert \\Big] \\lt M \\cdot i$  \n",
    "\n",
    "and revisiting the bottom lines of page 89,  (and using a corollary of montone convergence for the second line) \n",
    "\n",
    "$E\\Big[\\sum_{i=1}^\\infty \\mathbb I_{T\\geq i} \\cdot \\big \\vert Z_i - Z_{i-1}\\big \\vert \\Big]$  \n",
    "$= \\sum_{i=1}^\\infty E\\Big[\\mathbb I_{T\\geq i} \\cdot \\big \\vert Z_i - Z_{i-1}\\big \\vert \\Big]$  \n",
    "$= \\sum_{i=1}^\\infty E\\Big[E\\big[\\mathbb I_{T\\geq i} \\cdot \\big \\vert Z_i - Z_{i-1}\\big \\vert \\big \\vert  \\mathcal{F}_{i-1}\\big] \\Big]$  \n",
    "$= \\sum_{i=1}^\\infty E\\Big[\\mathbb I_{T\\geq i} \\cdot E\\big[ \\big \\vert Z_i - Z_{i-1}\\big \\vert \\big \\vert  \\mathcal{F}_{i-1}\\big] \\Big]$  \n",
    "$\\leq \\sum_{i=1}^\\infty E\\Big[\\mathbb I_{T\\geq i} \\cdot i \\cdot M \\Big]$  \n",
    "$= M \\cdot \\sum_{i=1}^\\infty i \\cdot E\\Big[\\mathbb I_{T\\geq i}\\Big]$  \n",
    "$= M \\cdot \\sum_{i=0}^\\infty (i+1) \\cdot E\\Big[\\mathbb I_{T\\gt i}\\Big]$  \n",
    "$= M \\cdot \\sum_{i=0}^\\infty (i+1) \\cdot Pr\\{T\\gt i\\}$  \n",
    "$= M \\cdot\\Big\\{ \\big(\\sum_{i=0}^\\infty i \\cdot Pr\\{T\\gt i\\}\\big)  + \\big(\\sum_{i=0}^\\infty 1 \\cdot Pr\\{T\\gt i\\}\\big) \\Big\\}$  \n",
    "$= M \\cdot\\Big\\{ \\big(\\sum_{i=0}^\\infty i \\cdot Pr\\{T\\gt i\\}\\big)  + E\\big[T\\big] \\Big\\}$  \n",
    "$\\leq M \\cdot\\Big\\{\\big( E\\big[T^2\\big] + E\\big[T\\big]\\big) + E\\big[T\\big] \\Big\\}$  \n",
    "$ = M \\cdot E\\big[T^2\\big]+2M\\cdot E\\big[T\\big] $   \n",
    "$\\lt \\infty $  \n",
    "\n",
    "where the second to last inequality is justified in either absorbing state markov chain writeup, or in Feller chp 13 notes -- the exact value of the summation depends on whether the underlying distribution is natural number denominated (like this problem) or otherwise though in either case the second moment plus the mean is sufficient to be an upper bound\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ross and Pekoz  \n",
    "chapter 3, problem 23  \n",
    "\n",
    "3 person, variant of gamlbers ruin-- a fair game where each player has starting stakes $a,b,c$ and in each round a gambler is selected uniformlyat random to give up a chip and one of the other gamblers is selected uniformly at random to receive said chip.  The game terminates once on player has been knocked out and only 2 remain.  \n",
    "\n",
    "Letting $X_n, Y_n, Z_n$ denote the stakes of the 3 players after round n, with  \n",
    "$\\big(X_0, Y_0, Z_0\\big) = (a,b,c)$    \n",
    "\n",
    "*(a)* compute  \n",
    "$E\\Big[\\big(X_{n+1}, Y_{n+1}, Z_{n+1}\\big)\\big \\vert \\big(X_{n}, Y_{n}, Z_{n}\\big) = (x,y,z)\\Big]$   \n",
    "*answer:*  \n",
    "$E\\Big[\\big(X_{n+1}, Y_{n+1}, Z_{n+1}\\big)\\big \\vert \\big(X_{n}, Y_{n}, Z_{n}\\big) = (x,y,z)\\Big]$   \n",
    "$=  E\\Big[(x,y,z) + \\mathbf r_n \\big \\vert \\big(X_{n}, Y_{n}, Z_{n}\\big) = (x,y,z)\\Big]$  \n",
    "$= (x,y,z) + E\\Big[\\mathbf r_n \\big \\vert \\big(X_{n}, Y_{n}, Z_{n}\\big) = (x,y,z)\\Big]$  \n",
    "$= (x,y,z) + E\\Big[\\mathbf r_n \\Big]$  \n",
    "$= (x,y,z) $  \n",
    "with $\\mathbf r_n$   taking on values uniformly at random in \n",
    "\n",
    "$\\big\\{(1,-1,0), -(1,-1,0),(1,0,-1), -(1,0,-1), (0,1,-1), - (0,1,-1)\\big\\}$\n",
    "and is independent of the stakes of the players (i.e. it is the iid random variable that we observe at each random of the game)  \n",
    "\n",
    "*(b)* show that \n",
    "$M_n = X_n Y_n Z_n + n\\frac{a+b+c}{3}$  \n",
    "is a martingale  \n",
    "\n",
    "*convergence for martingale stopping theorem:*  \n",
    "$X_n Y_n Z_n \\leq \\max\\{a,b,c\\}^3$  \n",
    "i.e. it is uniformly bounded, and $(n+1)\\frac{a+b+c}{3} - n\\frac{a+b+c}{3} = \\frac{a+b+c}{3}$  \n",
    "hence we have a sufficient condition 3 from page 88, Theorem 3.14, where we know \n",
    "$E\\big[N\\big ] \\lt \\infty $  \n",
    "(due to a finite state markov chain embedding argument, we know that all moments exist for $N$)   \n",
    "\n",
    "and we have  \n",
    "$E\\Big[\\big \\vert M_{n+1} - M_n\\big \\vert\\big \\vert \\mathcal{F}_{n}\\Big ] \\leq M = \\max\\{a,b,c\\}^3 + \\frac{a+b+c}{3} \\lt \\infty$  \n",
    "\n",
    "*note*\n",
    "this also addressed convergence required to be a martingale, for all $n$ \n",
    "$E\\Big[\\big \\vert M_{n+1} \\big \\vert \\big \\vert \\mathcal{F}_{n}\\Big ] \\leq  \\max\\{a,b,c\\}^3   \\big \\vert n\\frac{a+b+c}{3} \\big \\vert \\lt \\infty$  \n",
    "by application of triangle inequality\n",
    "\n",
    "\n",
    "*conditional expectations criterion*    \n",
    "$E\\Big[M_{n+1} \\big \\vert M_{n}=m_n\\Big]$   \n",
    "$=  E\\Big[X_{n+1} Y_{n+1} Z_{n+1} + (n+1)\\frac{a+b+c}{3}  \\big \\vert M_{n}=m_n\\Big]$  \n",
    "$=(n)\\frac{a+b+c}{3} + \\frac{a+b+c}{3} +   E\\Big[X_{n+1} Y_{n+1} Z_{n+1}  \\big \\vert M_{n}m_n\\Big]$  \n",
    "$=(n)\\frac{a+b+c}{3} + \\frac{a+b+c}{3} +   E\\Big[e_3\\big(\\big(X_{n+1}, Y_{n+1}, Z_{n+1}\\big)\\big)  \\big \\vert M_{n}=m_n\\Big]$  \n",
    "$=(n)\\frac{a+b+c}{3} + \\frac{a+b+c}{3} +   E\\Big[e_3\\big(\\big(X_{n}, Y_{n}, Z_{n}\\big) + \\mathbf r_n\\big) \\big \\vert M_{n}=m_n\\Big]$  \n",
    "\n",
    "\n",
    "where for non-stopped game, we have     \n",
    "$E\\Big[e_3\\big(\\big(X_{n}, Y_{n}, Z_{n}\\big) + \\mathbf r_n\\big) \\big \\vert M_{n}m_n\\Big]$  \n",
    "$= \\frac{1}{6}\\big\\{e_3(x_n + 1,y_n-1,z_n) + e_3(x_n-1,y_n+1,z_n) + e_3(x_n+1,y_n,z_n-1) + e_3(x_n-1,y_n,z_n+1)+ e_3(x_n ,y_n + 1,z_n-1) + e_3(x_n ,y_n - 1,z_n+1)  \\big\\}\\big\\vert M_n = m)n$  \n",
    "\n",
    "given the symmetry, it is useful to consider the entire function by looking WLOG at the first two components, and making use of multi-linearity    \n",
    "$e_3(x_n + 1,y_n-1,z_n) + e_3(x_n-1,y_n+1,z_n)$  \n",
    "$= e_3(1,y_n-1,z_n) + e_3(x_n ,y_n-1,z_n) + e_3(x_n,y_n+1,z_n) + e_3(-1,y_n+1,z_n) $  \n",
    "$= e_3(1,y_n,z_n) + e_3(1,-1,z_n) + e_3(x_n, -1,z_n) + e_3(x_n ,y_n,z_n) + e_3(x_n,1,z_n) + e_3(x_n,y_n,z_n)  + e_3(-1,1,z_n) + e_3(-1,y_n,z_n) $  \n",
    "$= -2 z_n + 2 e_3(x_n ,y_n,z_n) $   \n",
    "\n",
    "so we have   \n",
    "$=(n)\\frac{a+b+c}{3} + \\frac{a+b+c}{3} +   E\\Big[e_3\\big(\\big(X_{n}, Y_{n}, Z_{n}\\big) + \\mathbf r_n\\big) \\big \\vert M_{n}=m_n\\Big]$   \n",
    "$=(n)\\frac{a+b+c}{3} + \\frac{a+b+c}{3} +   E\\Big[\\frac{1}{6}\\big\\{ -2 z_n + 2 e_3(x_n ,y_n,z_n) -2 y_n + 2 e_3(x_n ,y_n,z_n) -  2 x_n + 2 e_3(x_n ,y_n,z_n) \\big \\vert M_{n}\\Big]$   \n",
    "\n",
    "$=(n)\\frac{a+b+c}{3} + \\frac{a+b+c}{3} -  \\frac{1}{3} E\\Big[(x_n + y_n + z_n) \\big \\vert M_{n}=m_n\\Big] + E\\Big[ e_3(x_n,y_n,z_n) \\big \\vert M_{n}=m_n\\Big]$   \n",
    "\n",
    "$=(n)\\frac{a+b+c}{3} + \\frac{a+b+c}{3} - \\frac{1}{3} E\\Big[(a + b +c ) \\big \\vert M_{n}=m_n\\Big] + m_n - (n)\\frac{a+b+c}{3} $   \n",
    "$= m_n $   \n",
    "\n",
    "which of course tells us that  \n",
    "$E\\Big[M_{n+1} \\big \\vert M_{n}\\Big]$   \n",
    "$=  E\\Big[X_{n+1} Y_{n+1} Z_{n+1} + (n+1)\\frac{a+b+c}{3}  \\big \\vert M_{n}\\Big]$  \n",
    "$=M_n$  \n",
    "\n",
    "and confirms the martingale property  \n",
    "\n",
    "(c)  \n",
    "*application of Tower Property*   \n",
    "$E\\big[M_n\\big] = E\\big[M_1\\big] = E\\big[M_0\\big] = abc$  \n",
    "\n",
    "but by application of the Martingale Stopping Theorem  \n",
    "$abc = E\\big[M_N\\big] = E\\big[X_N Y_N Z_N + N\\frac{(a+b+c)}{3}\\big] = E\\big[X_N Y_N Z_N \\big]+ E\\big[N\\big]\\frac{(a+b+c)}{3}= E\\big[N\\big]\\frac{(a+b+c)}{3} $  \n",
    "$E\\big[N\\big] = \\frac{3 abc}{a+b+c}$  \n",
    "\n",
    "\n",
    "*remarks:*  \n",
    "comment on where did this come from and how might we guess this result?   \n",
    "The underlying idea is that we have fair game, and in the pending part (c) we are looking to compute expected time until a player is knocked out -- thus a product $X_N Y_N Z_N = 0$ if the process has stopped.  So we may try to condition on some knowledge of that product -- i.e. applying the 3rd elementary symmetric function to the vector containing those random variables, and then as is often the case (e.g. with second moment for a fair game, ref page 88) there is a traling component that is has some constant fixed by the game that is scaled by $n$.  \n",
    "\n",
    "there should be a better way to handle this with multilinearity or a clever inequality or inclusion exclusion...  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**extension:**  'Gambling' - problem 53 in Bollobas's *The Art of Mathematics: Coffee Time in Memphis*  \n",
    "\n",
    "Part $(iii)$ of this problem is the same as the above problem 23 in Ross and Pekoz.  \n",
    "\n",
    "Parts $(i)$ and $(ii)$ are similar, except they have $m$ gamblers and the game does not end when the first gambler is out -- it instead ends once all but 1 gamblers have been knocked out (i.e. only one gamblers is left).  \n",
    "\n",
    "So we now have \n",
    "\n",
    "$\\big(X_0^{(1)}, X_0^{(2)}, ..., X_0^{(m)}\\big) = (a_1,a_2,...,a_m)$    \n",
    "\n",
    "\n",
    "$(i)$  \n",
    "what is the expected time until the completion of this game?  \n",
    "\n",
    "Borrowing a lot of ideas from the prior problem, we'll try to solve this with a martingale, observing that \n",
    "$e_2\\big(X_n^{(1)}, X_n^{(2)}, ..., X_n^{(m)}\\big)$  \n",
    "equals 0 when the game is over and is positive when the game has not stopped.  \n",
    "\n",
    "So consider \n",
    "\n",
    "$M_n = e_2\\big(X_n^{(1)}, X_n^{(2)}, ..., X_n^{(m)}\\big) + n \\cdot \\rho$  \n",
    "where $\\rho$ is some constant which we'll decide on later   \n",
    "\n",
    "based on the prior analysis in ross & pekoz problem we can see that \n",
    "$E\\Big[\\big \\vert  M_{n+1} - M_n\\big \\vert \\Big] \\leq (a_1+a_2+...+a_m)^2 \\cdot \\binom{m}{2} +\\rho$  \n",
    "\n",
    "and again based on a finite state markov chain argument we know $E\\big[N\\big] \\lt \\infty$  \n",
    "which gives us sufficient conditions for use of martingale stopping theorem, and \n",
    "similarly we have  \n",
    "$E\\Big[\\big \\vert M_n\\big \\vert \\Big]\\lt \\infty$    \n",
    "\n",
    "streamlining the argument, we have  \n",
    "\n",
    "$E\\Big[M_{n+1} \\big \\vert M_{n}=m_n \\Big]$  \n",
    "$= n \\cdot \\rho + \\rho + E\\Big[ e_2\\big(X_{n+1}^{(1)}, X_{n+1}^{(2)}, ..., X_{n+1}^{(m)}\\big)\\big \\vert M_{n}=m_n\\Big]$  \n",
    "\n",
    "since the ordering of the stakes in the vector is arbitrary, we once again focus, WLOG on the case where player 1 and 2 win/lose\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using newton's identities  \n",
    "$e_2\\big(X_{n}^{(1)}+1, X_{n}^{(2)}-1, ..., X_{n}^{(m)}\\big) + e_2\\big(X_{n}^{(1)}-1, X_{n}^{(2)}+1, ..., X_{n}^{(m)}\\big)$  \n",
    "$ = -\\frac{1}{2}\\Big\\{-\\big(X_{n}^{(1)}+1+X_{n}^{(2)}-1+ ...+ X_{n}^{(m)}\\big)^2 +\\big((X_{n}^{(1)}+1)^2+(X_{n}^{(2)}-1)^2+ ...+ (X_{n}^{(m)})^2\\big) + -\\big(X_{n}^{(1)}-1+X_{n}^{(2)}+1+ ...+ X_{n}^{(m)}\\big)^2 +\\big((X_{n}^{(1)}-1)^2+(X_{n}^{(2)}+1)^2+ ...+ (X_{n}^{(m)})^2\\big)\\Big\\}$  \n",
    "\n",
    "$ = -\\frac{1}{2}\\Big\\{-2 \\cdot \\big(X_{n}^{(1)}+X_{n}^{(2)}+ ...+ X_{n}^{(m)}\\big)^2 +\\big((X_{n}^{(1)}+1)^2+(X_{n}^{(2)}-1)^2+ ...+ (X_{n}^{(m)})^2\\big) + \\big((X_{n}^{(1)}-1)^2+(X_{n}^{(2)}+1)^2+ ...+ (X_{n}^{(m)})^2\\big)\\Big\\}$  \n",
    "\n",
    "$ = -\\frac{1}{2}\\Big\\{-2 \\cdot \\big(X_{n}^{(1)}+X_{n}^{(2)}+ ...+ X_{n}^{(m)}\\big)^2 + 2\\big((X_{n}^{(1)})^2+(X_{n}^{(2)})^2+ ...+ (X_{n}^{(m)})^2\\big) - 2 (X_{n})^{(1)} + 2 (X_{n})^{(2)} + 2 (X_{n})^{(1)} - 2 (X_{n})^{(2)}  + 4 \\Big\\}$  \n",
    "\n",
    "$ = -\\frac{1}{2}\\Big\\{-2 \\cdot \\big(X_{n}^{(1)}+X_{n}^{(2)}+ ...+ X_{n}^{(m)}\\big)^2 + 2\\big((X_{n}^{(1)})^2+(X_{n}^{(2)})^2+ ...+ (X_{n}^{(m)})^2\\big) +4 \\Big\\} $  \n",
    "\n",
    "$ = 2 \\cdot e_2\\big(X_{n}^{(1)}, X_{n}^{(2)}, ..., X_{n}^{(m)}\\big) - 2 $  \n",
    "\n",
    "and each occurs with probability  $\\frac{1}{2}$  \n",
    "so conditioned on the first two players winning /losing, then the expected value is (we are in effect using another level of conditioning /iterated expectations here, though it is hard to show notationaly)  \n",
    "\n",
    "$ = e_2\\big(X_{n}^{(1)}, X_{n}^{(2)}, ..., X_{n}^{(m)}\\big) - 1 $  \n",
    "\n",
    "there are $\\binom{m}{2}$ of these pairings when all players are in the game and $\\binom{j}{2}$ pairing when there are $j$ players in the game, with each pairing equally likely.  However in *all* cases that we condition on, the result is $m_n +1 $.  This means  \n",
    "\n",
    "$E\\Big[M_{n+1} \\big \\vert M_{n}=m_n \\Big]$   \n",
    "$= n \\cdot \\rho + \\rho + E\\Big[ e_2\\big(X_{n+1}^{(1)}, X_{n+1}^{(2)}, ..., X_{n+1}^{(m)}\\big)\\big \\vert M_{n}=m_n\\Big]$  \n",
    "$= n \\cdot \\rho + \\rho + e_2\\big(X_{n}^{(1)}, X_{n}^{(2)}, ..., X_{n}^{(m)}\\big) - 1$  \n",
    "and if we select $\\rho := 1$, we get  \n",
    "$= e_2\\big(X_{n}^{(1)}, X_{n}^{(2)}, ..., X_{n}^{(m)}\\big) +n $  \n",
    "$=m_n$  \n",
    "\n",
    "or more abstracly, we say that \n",
    "\n",
    "$M_n = e_2\\big(X_n^{(1)}, X_n^{(2)}, ..., X_n^{(m)}\\big) + n $    \n",
    "\n",
    "is a martingale since    \n",
    "$E\\Big[M_{n+1} \\big \\vert M_{n}\\Big] = M_n$   \n",
    "\n",
    "applying the martingale stopping theorem, we have  \n",
    "\n",
    "$e_2(a_1,a_2,...,a_m) = e_2\\big(X_0^{(1)}, X_0^{(2)}, ..., X_0^{(m)}\\big) = E\\Big[M_{0}\\Big] = E\\Big[M_{N}\\Big] =  0 + E\\big[N\\big]$   \n",
    "\n",
    "or \n",
    "\n",
    "$E\\big[N\\big] = e_2(a_1,a_2,...,a_m)$   \n",
    "\n",
    "*technical nits*  \n",
    "while we do not need to be concerned about subtle convergence issues thanks to a markov chain embedding argument, the above argument needs cleaned up slightly in that for any non-stopped point during the game, our martingale actually is desiged to involve  \n",
    "\n",
    "$e_2\\big(X_n^{(1)}, X_n^{(2)}, ..., X_n^{(j)}\\big)$  \n",
    "\n",
    "where $2 \\leq j \\leq m$ and $j$ are the actual number of players in the game.  However when we condition on remaining player $i$ winning and remaining player $k$ losing a given round, we get \n",
    "\n",
    "$E\\Big[M_{n+1} \\big \\vert M_{n}=m_n\\Big] = E\\Big[E\\big[M_{n+1} \\mathbb I_A\\big]\\big \\vert M_{n}=m_n\\Big] = m_n$\n",
    "\n",
    "so really we are conditioning on the filtration \n",
    "$\\mathcal{F}_{n} = \\big(X_n^{(1)}, X_n^{(2)}, ..., X_n^{(m)}\\big)$\n",
    "i.e. we actually need to know the 'name's of the remaining players at each step, which is given by the above filtration, but merely conditioning on $M_n$ does not quite give us enough information.  Conversely, knowing $\\mathcal{F}_{n}$ *does* give us enough information to compute $e_2\\big(X_n^{(1)}, X_n^{(2)}, ..., X_n^{(j)}\\big)$.   \n",
    "\n",
    "Further we know that our event $A$ is arbitrarily chosen from any feasible outcome given $\\mathcal{F}_{n}$.   \n",
    "\n",
    "So really two pieces of cleanup are in order.  First the *actual* martingale is   \n",
    "$E\\Big[M_{n+1} \\big \\vert \\mathcal{F}_{n} \\Big] = E\\Big[M_{n+1} \\big \\vert \\big(X_n^{(1)}, X_n^{(2)}, ..., X_n^{(m)}\\big) \\Big] =M_n$  \n",
    "\n",
    "and second, we'd possibly want to introduce notation to clarify *exactly* what vector we are applying the 2nd elementary symmetric function to at any time. Since there are no convergence issues  -- perhaps the biggest pitfall in martingale arguments when one isn't careful-- we leave this notation cleanup/redo as a minor open item.  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ballot Problems**    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Need to include the Feller un fair \"fair game\" (where we only have a first moment)... this fits very nicely in with some convergence issues and how we need to be careful in unbounded cases (in particular without a 2nd moment!)... this is a big idea and should nicely fit in  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Matrix([\n",
       "[     1,      0,      0,      0,      0,      0,      0,      0,      0,      0, 0, 0],\n",
       "[-p + 1,      0,      p,      0,      0,      0,      0,      0,      0,      0, 0, 0],\n",
       "[     0, -p + 1,      0,      p,      0,      0,      0,      0,      0,      0, 0, 0],\n",
       "[     0,      0, -p + 1,      0,      p,      0,      0,      0,      0,      0, 0, 0],\n",
       "[     0,      0,      0, -p + 1,      0,      p,      0,      0,      0,      0, 0, 0],\n",
       "[     0,      0,      0,      0, -p + 1,      0,      p,      0,      0,      0, 0, 0],\n",
       "[     0,      0,      0,      0,      0, -p + 1,      0,      p,      0,      0, 0, 0],\n",
       "[     0,      0,      0,      0,      0,      0, -p + 1,      0,      p,      0, 0, 0],\n",
       "[     0,      0,      0,      0,      0,      0,      0, -p + 1,      0,      p, 0, 0],\n",
       "[     0,      0,      0,      0,      0,      0,      0,      0, -p + 1,      0, p, 0],\n",
       "[     0,      0,      0,      0,      0,      0,      0,      0,      0, -p + 1, 0, p],\n",
       "[     0,      0,      0,      0,      0,      0,      0,      0,      0,      0, 0, 1]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sympy as sp\n",
    "p = sp.Symbol('p')\n",
    "q = sp.Symbol('q')\n",
    "n = 12\n",
    "\n",
    "A = sp.zeros(n,n)\n",
    "A[0,0] = 1\n",
    "A[-1,-1] = 1 \n",
    "for i in range(1, n-1):\n",
    "    A[i,i-1] = q\n",
    "    A[i,i+1] = p\n",
    "\n",
    "# A.nullspace()\n",
    "# print(sp.latex(A))\n",
    "A = A.subs(q, 1-p)\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 10.,  18.,  24.,  28.,  30.,  30.,  28.,  24.,  18.,  10.])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_num = np.array(A.subs(p, 0.5)).astype(np.float64)\n",
    "B = A_num[1:-1,1:-1]\n",
    "N_inv = np.identity(B.shape[0]) - B\n",
    "np.linalg.solve(N_inv, np.ones(B.shape[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1. , -0.5,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ],\n",
       "       [-0.5,  1. , -0.5,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ],\n",
       "       [ 0. , -0.5,  1. , -0.5,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ],\n",
       "       [ 0. ,  0. , -0.5,  1. , -0.5,  0. ,  0. ,  0. ,  0. ,  0. ],\n",
       "       [ 0. ,  0. ,  0. , -0.5,  1. , -0.5,  0. ,  0. ,  0. ,  0. ],\n",
       "       [ 0. ,  0. ,  0. ,  0. , -0.5,  1. , -0.5,  0. ,  0. ,  0. ],\n",
       "       [ 0. ,  0. ,  0. ,  0. ,  0. , -0.5,  1. , -0.5,  0. ,  0. ],\n",
       "       [ 0. ,  0. ,  0. ,  0. ,  0. ,  0. , -0.5,  1. , -0.5,  0. ],\n",
       "       [ 0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. , -0.5,  1. , -0.5],\n",
       "       [ 0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. , -0.5,  1. ]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N_inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2, -1, -2],\n",
       "       [ 1,  0,  0],\n",
       "       [ 0,  0,  1]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C = np.array([[2, -1, -2],[1,0,0],[0,0,1]])\n",
    "C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 1.,  1.,  1.]), array([[  7.071e-01,   7.071e-01,  -7.071e-01],\n",
       "        [  7.071e-01,   7.071e-01,  -7.071e-01],\n",
       "        [  0.000e+00,   0.000e+00,   1.743e-32]]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.eig(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([30, 30,  1])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = np.array([18, 10,1])\n",
    "C @ C @ C @ C @ v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_dim = sp.Symbol('m')\n",
    "k = sp.Symbol('k')\n",
    "\n",
    "C = sp.Matrix([[2, -1, -2],[1,0,0],[0,0,1]])\n",
    "v = sp.Matrix([2*(m_dim-1), m_dim, 1])\n",
    "C*C*C*C*v\n",
    "e_1 = sp.Matrix([1,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eq = sp.simplify((e_1.T * C**k * v)[0])\n",
    "eq.subs(m_dim, 10).subs(k, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\operatorname{PurePoly}{\\left( \\lambda^{3} - 3 \\lambda^{2} + 3 \\lambda - 1, \\lambda, domain=\\mathbb{Z} \\right)}\n"
     ]
    }
   ],
   "source": [
    "print(sp.latex(C.charpoly()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\operatorname{PurePoly}{\\left( \\lambda^{3} - 3 \\lambda^{2} + 3 \\lambda - 1, \\lambda, domain=\\mathbb{Z} \\right)}$ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\left[\\begin{matrix}2.0 & -1.0 & -2.0\\\\1.0 & 0.0 & 0.0\\\\0.0 & 0.0 & 1.0\\end{matrix}\\right]\n"
     ]
    }
   ],
   "source": [
    "print(sp.latex(sp.Matrix(C)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Matrix([\n",
       "[0],\n",
       "[0],\n",
       "[0],\n",
       "[0],\n",
       "[0],\n",
       "[0],\n",
       "[0]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = sp.zeros(n,1)\n",
    "for k in range(n):\n",
    "#     v[k] = (p/(1-p))**k\n",
    "    v[k] = ((1-p)/p)**k\n",
    "\n",
    "result = A*v\n",
    "sp.simplify(result-v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\mathbf f= \\left[\\begin{matrix}1 \\\\ \\delta \\\\\\delta^2 \\\\ \\vdots \\\\\\delta^{N-1}\\\\\\delta^N \\end{matrix}\\right]= \n",
    "\\left[\\begin{matrix}1 \\\\ \\big(\\frac{1-p}{p}\\big) \\\\\\big(\\frac{1-p}{p}\\big)^2 \\\\ \\vdots \\\\ \\big(\\frac{1-p}{p}\\big)^{N-1}\\\\\\big(\\frac{1-p}{p}\\big)^N \\end{matrix}\\right]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\mathbf P_{\\text{Absorbing}} = \\left[\\begin{matrix}1 & 0 & 0 & 0 & 0 & 0\\\\q & 0 & p & 0 & 0 & 0\\\\0 & q & 0 & p & 0 & 0\\\\0 & 0 & q & 0 & p & 0\\\\0 & 0 & 0 & q & 0 & p\\\\0 & 0 & 0 & 0 & 0 & 1\\end{matrix}\\right]$  \n",
    "\n",
    "$\\mathbf P_{\\text{Recurrent}} =  \\left[\\begin{matrix}0 & 1 & 0 & 0 & 0 & 0\\\\q & 0 & p & 0 & 0 & 0\\\\0 & q & 0 & p & 0 & 0\\\\0 & 0 & q & 0 & p & 0\\\\0 & 0 & 0 & q & 0 & p\\\\0 & 0 & 0 & 0 & 1 & 0\\end{matrix}\\right]$  \n",
    "\n",
    "or \n",
    "\n",
    "$\\mathbf P_{\\text{Recurrent}} =  \\left[\\begin{matrix}q & p & 0 & 0 & 0 & 0\\\\q & 0 & p & 0 & 0 & 0\\\\0 & q & 0 & p & 0 & 0\\\\0 & 0 & q & 0 & p & 0\\\\0 & 0 & 0 & q & 0 & p\\\\0 & 0 & 0 & 0 & q & p\\end{matrix}\\right]$  \n",
    "\n",
    "we may select whichever one is easier  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\left[\\begin{matrix}1\\\\\\frac{q \\left(- 2 p q + 1\\right)}{p q \\left(p q - 1\\right) - 2 p q + 1}\\\\\\frac{q^{2} \\left(- p q + 1\\right)}{p q \\left(p q - 1\\right) - 2 p q + 1}\\\\\\frac{q^{3}}{p^{2} q^{2} - 3 p q + 1}\\\\\\frac{q^{4}}{p^{2} q^{2} - 3 p q + 1}\\\\0\\end{matrix}\\right]\n"
     ]
    }
   ],
   "source": [
    "m_1, m_2 =(A-sp.eye(n)).nullspace()\n",
    "m_1 = m_1 / m_1[0]\n",
    "m_2 = m_2 / m_2[-1]\n",
    "# normalize with repct to absorbing states \n",
    "print(sp.latex(sp.simplify(m_1 )))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\left[\\begin{matrix}1\\\\\\frac{q \\left(- 2 p q + 1\\right)}{p q \\left(p q - 1\\right) - 2 p q + 1}\\\\\\frac{q^{2} \\left(- p q + 1\\right)}{p q \\left(p q - 1\\right) - 2 p q + 1}\\\\\\frac{q^{3}}{p^{2} q^{2} - 3 p q + 1}\\\\\\frac{q^{4}}{p^{2} q^{2} - 3 p q + 1}\\\\0\\end{matrix}\\right]\n",
    " + \\left[\\begin{matrix}\\frac{p}{q^{4}} \\left(2 p q - 1\\right)\\\\\\frac{p}{q^{3}} \\left(p q - 1\\right)\\\\- \\frac{p}{q^{2}}\\\\- \\frac{p}{q}\\\\0\\\\1\\end{matrix}\\right]\n",
    "= \\left[\\begin{matrix}\\frac{1}{q^{4}} \\left(p \\left(2 p q - 1\\right) + q^{4}\\right)\\\\\\frac{p \\left(p q - 1\\right) \\left(p q \\left(p q - 1\\right) - 2 p q + 1\\right) + q^{4} \\left(- 2 p q + 1\\right)}{q^{3} \\left(p q \\left(p q - 1\\right) - 2 p q + 1\\right)}\\\\\\frac{- p^{3} q^{2} + 3 p^{2} q - p q^{5} - p + q^{4}}{q^{2} \\left(p^{2} q^{2} - 3 p q + 1\\right)}\\\\\\frac{- p^{3} q^{2} + 3 p^{2} q - p + q^{4}}{q \\left(p^{2} q^{2} - 3 p q + 1\\right)}\\\\\\frac{q^{4}}{p^{2} q^{2} - 3 p q + 1}\\\\1\\end{matrix}\\right]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\left[\\begin{matrix}1\\\\\\frac{- \\frac{p}{q} + \\frac{1}{q^{2}}}{- \\frac{p}{q^{2}} - \\frac{1}{q} \\left(\\frac{p}{q} - \\frac{1}{q^{2}}\\right)}\\\\\\frac{1}{q \\left(- \\frac{p}{q^{2}} - \\frac{1}{q} \\left(\\frac{p}{q} - \\frac{1}{q^{2}}\\right)\\right)}\\\\\\frac{1}{- \\frac{p}{q^{2}} - \\frac{1}{q} \\left(\\frac{p}{q} - \\frac{1}{q^{2}}\\right)}\\\\0\\end{matrix}\\right] +\\left[\\begin{matrix}\\frac{p}{q} \\left(\\frac{p}{q} - \\frac{1}{q^{2}}\\right)\\\\- \\frac{p}{q^{2}}\\\\- \\frac{p}{q}\\\\0\\\\1\\end{matrix}\\right]  =m_1+ m_2 =\\left[\\begin{matrix}\\frac{1}{q^{3}} \\left(p \\left(p q - 1\\right) + q^{3}\\right)\\\\\\frac{1}{q^{2} \\left(2 p q - 1\\right)} \\left(- p \\left(2 p q - 1\\right) + q^{3} \\left(p q - 1\\right)\\right)\\\\- \\frac{p \\left(2 p q - 1\\right) + q^{3}}{q \\left(2 p q - 1\\right)}\\\\- \\frac{q^{3}}{2 p q - 1}\\\\1\\end{matrix}\\right]\n",
    "$  \n",
    "\n",
    "in either case this is something of a mess.... since there is nice structure in the recurrent time reversible chain, we could, it seems get a toehold at state (1) or (n-1) and basically compare inflows from 0 vs n... then use that to solve for the convex combination that gets such a value... i.e. using state $i=1$ \n",
    "we would know that long run absorbtion probability into 0 is (ignoring the singular case, dealt with directly via Wald's Equality)...  \n",
    "\n",
    "$p_1 = \\frac{f(1)}{f(0) - f(n)}$  \n",
    "or something along those lines... so why not set $f(1)=1$ and invert, with  \n",
    "\n",
    "$\\frac{1}{p_1} = f(0)^* - f(n)^*$  \n",
    "\n",
    "$f(n)^* = w_1 f(n)^{(1)} + w_2 f(n)^{(2)}$   \n",
    "$f(0)^* = w_1 f(0)^{(1)} + w_2 f(0)^{(2)}$   \n",
    "\n",
    "$\\frac{1}{p_1} = w_1 f(0)^{(1)}+ w_2 f(0)^{(2)}- (w_1 f(n)^{(1)} + w_2 f(n)^{(2)}) =  (w_1 f(0)^{(1)}- w_1 f(n)^{(1)})+ (w_2 f(0)^{(2)}-   w_2 f(n)^{(2)})$  \n",
    "*(cleanup and check for errors...)*  \n",
    "\n",
    "and then we have \n",
    "\n",
    "$\\begin{bmatrix}\n",
    "f(0)^{(1)}- f(n)^{(1)}  & f(0)^{(2)}  -f(n)^{(2)} \\\\ \n",
    "1 & 1\n",
    "\\end{bmatrix}\\begin{bmatrix}\n",
    "w_1\\\\ \n",
    "w_2\n",
    "\\end{bmatrix} =\\begin{bmatrix}\n",
    "\\frac{1}{p_1}  \\\\ \n",
    "1\n",
    "\\end{bmatrix}$  \n",
    "which is exactly solvable unless the first row is a scalar multiple of the ones vector.  \n",
    "\n",
    "and from here we can get the desired affine combination of the two vectors in the nullspace of $A$  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Matrix([\n",
       "[0],\n",
       "[0],\n",
       "[0],\n",
       "[0],\n",
       "[0],\n",
       "[0]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp.simplify(A * m_1 - m_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "flush out problem 9 (page 492, aka page 502 of 520 in Grinstead and Snell)   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Karlin and Taylor' \"A First Course\" -- Martingales Chapter\n",
    "*\"Elementary Problems\"* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8.)** (Simple) Success runs Markov Chain, where success runs may be arbitrarily long  \n",
    "\n",
    "i.e. the Renewal Matrix- Age chain with homogenization  $q = q_1 = q_2 = ...$  and $1-p = q$ with $q \\in (0,1)$  \n",
    "\n",
    "The typical matrix, in recurrent form, looks like this,  \n",
    "\n",
    "$\\mathbf A =\n",
    "\\left[\\begin{matrix}\n",
    "q & p & 0 & 0 & 0 & 0 & \\dots\\\\\n",
    "q & 0 & p& 0 & 0 & 0 & \\dots\\\\\n",
    "q & 0 & 0 & p & 0 & 0 & \\dots\\\\\n",
    "q & 0 & 0 & 0 & p & 0 & \\dots\\\\\n",
    "q & 0 & 0 & 0 & 0 & p & \\dots\\\\\n",
    "q & 0 & 0 & 0 & 0 & 0 & \\ddots\\\\\n",
    "\\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\ddots\\end{matrix}\\right]$\n",
    "\n",
    "\n",
    "note: with a little insight we can see that this is a geometric distribution with 'success' probability of $q$  hence, starting in state 1 the complementary CDF is given by $\\bar{F}(k) = p^k$, hence from the geometric distribution we know all moments (and in particular the first and second moments) exists.   \n",
    "\n",
    "However, this problem wants us we want to show this with an absorbing state in state zero, which is given by  \n",
    "\n",
    "$\\mathbf A =\n",
    "\\left[\\begin{matrix}\n",
    "1 & 0 & 0 & 0 & 0 & 0 & \\dots\\\\\n",
    "q & 0 & p& 0 & 0 & 0 & \\dots\\\\\n",
    "q & 0 & 0 & p & 0 & 0 & \\dots\\\\\n",
    "q & 0 & 0 & 0 & p & 0 & \\dots\\\\\n",
    "q & 0 & 0 & 0 & 0 & p & \\dots\\\\\n",
    "q & 0 & 0 & 0 & 0 & 0 & \\ddots\\\\\n",
    "\\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\ddots\\end{matrix}\\right]$\n",
    "\n",
    "\n",
    "\n",
    "*claim: *  \n",
    "for arbitrary real $a, b$  \n",
    "\n",
    "$X_n = b $, if $Y(n) = 0$  \n",
    "$X_n = (a-b) \\big(\\frac{1}{p}\\big)^{Y(n)-1} + b $  if, $Y(n)\\gt 0$   \n",
    "\n",
    "then $X_n$ is a martingale  \n",
    "\n",
    "*remark:*   \n",
    "the notation in the text can be hard to parse at times.  Your author reads this as, if in state zero, then consider a payoff of $b$, for all other states we need to show that the current payoff equals the expected payoff one time step out.  \n",
    "\n",
    "*proof:*  \n",
    "\n",
    "using the matrix setup from earlier in the post apply first step analysis:\n",
    "\n",
    "if at state $i$, the expected value equals \n",
    "\n",
    "$(a-b)\\big(\\frac{1}{p}\\big)^{i-1} + qb $  \n",
    "if we compute it directly.  However if we apply first step analysis, we get  \n",
    "\n",
    "$= q \\cdot b + p\\cdot (a-b)\\big(\\frac{1}{p}\\big)^{i} = (a-b) \\big(\\frac{1}{p}\\big)^{i-1}$  \n",
    "\n",
    "and of course a value of $b$ state 0 as required.   \n",
    "\n",
    "*remark:*  \n",
    "it isn't clearn what the purpose of this exercise is, other than computational practice.  We know (e.g. in Feller chp 15) that looking at the series $\\sum_{i=1}^\\infty q$ is what matters for whether or not the chain is transient.  \n",
    " \n",
    "If we naively applied an earlier result \n",
    "\n",
    "$\\frac{f(i)- f(r)}{f(0) - f(r)} = p_{i,0}^{(\\infty)}$  \n",
    "except there is no $r$ in this case, so  \n",
    "$\\frac{f(i)}{f(0)} = p_{i,0}^{(\\infty)}$  \n",
    "\n",
    "we'd have  \n",
    "$\\frac{(a-b)\\big(\\frac{1}{p}\\big)^{i-1} + qb}{b} = p_{i,0}^{(\\infty)}$  \n",
    "\n",
    "but for say $a \\gt b$ the numerator grows arbitrarily large (and much more larger than the denominator) with large enough $i$, hence it cannot be a probability.  An underlying issue is that $\\big(\\frac{1}{p}\\big)^{i-1}$ may become arbitrarily large and we can run into convergence problems.  \n",
    "\n",
    "If, on the other hand truncated the chain to be finite state, so that state $0$ and $r$ are both absorbing states, we could resurrect the martingale by setting $b=0$ to have  \n",
    "\n",
    "$X_n = 0 $, if $Y(n) = 0$  \n",
    "$X_n = 1\\cdot \\big(\\frac{1}{p}\\big)^{Y(n)-1}$  if, $Y(n) \\in \\{1, 2, ,..., r\\}$     \n",
    "(for convenience setting $a =1 $)  \n",
    "\n",
    "and we'd find the probability of absorbtion given by manipulating these equations.  However, the results of interest are *much* more easily found by working directly with geometric probability distributions...  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**15.)** pending \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**18.)**  \n",
    "\n",
    "consider the \"poisson martingale\" where parameter $\\lambda = i$ for row i (where state zero is naturally absorbing)  \n",
    "\n",
    "\n",
    "$\\mathbf A =\n",
    "\\left[\\begin{matrix}\n",
    "1 & 0 & 0 & 0 & 0 & 0 & \\dots\\\\\n",
    "e^{-1} & e^{-1}\\frac{1^1}{1!} & e^{-1}\\frac{1^2}{2!} & e^{-1}\\frac{1^3}{3!} & e^{-1}\\frac{1^4}{4!} & e^{-1}\\frac{1^5}{5!} & \\dots\\\\\n",
    "e^{-2} & e^{-2}\\frac{2^1}{1!} & e^{-2}\\frac{2^2}{2!} & e^{-2}\\frac{2^3}{3!} & e^{-2}\\frac{2^4}{4!} & e^{-2}\\frac{2^5}{5!} &\\dots\\\\\n",
    "e^{-3} & e^{-3}\\frac{3^1}{1!} & e^{-3}\\frac{3^2}{2!} & e^{-3}\\frac{3^3}{3!} & e^{-3}\\frac{3^4}{4!} & e^{-3}\\frac{3^5}{5!} & \\dots\\\\\n",
    "e^{-4} & e^{-4}\\frac{4^1}{1!} & e^{-4}\\frac{4^2}{2!} & e^{-4}\\frac{4^3}{3!} & e^{-4}\\frac{4^4}{4!} & e^{-4}\\frac{4^5}{5!} & \\dots\\\\  \n",
    "e^{-5} & e^{-5}\\frac{5^1}{1!} & e^{-5}\\frac{5^2}{2!} & e^{-5}\\frac{5^3}{3!} & e^{-5}\\frac{5^4}{4!} & e^{-5}\\frac{5^5}{5!} & \\dots\\\\\n",
    "\\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\ddots\\end{matrix}\\right]$\n",
    "\n",
    "a) Verify that $X_n$ is a martingale.  \n",
    "\n",
    "consider the vector \n",
    "\n",
    "$\\mathbf v =\\left[\\begin{matrix}\n",
    "0 \\\\\n",
    "1 \\\\\n",
    "2 \\\\\n",
    "3 \\\\\n",
    "4 \\\\\n",
    "5 \\\\\n",
    "\\vdots \\end{matrix}\\right]$\n",
    "\n",
    "\n",
    "we can see that $\\mathbf {Av} = \\mathbf v$  \n",
    "\n",
    "because for row zero the result is $1 \\cdot 0$ and for for $i\\geq 1$ we have \n",
    "\n",
    "$i = e^{-i} \\sum_{j=1}^\\infty \\frac{i^j}{j!} = e^{-i} \\sum_{j=0}^\\infty \\frac{i^j}{j!}$  \n",
    "which is the formula for the mean $i$ of a Poisson.  \n",
    "\n",
    "*remark:* it isn't immediately clear how to verify convergence issues to be sure that this is a Martingale.  The typical sufficient conditions given on page 88 of Ross & Pekoz do not seem to be enough in this case for applying the stopping theorem. (The problem in the text suggests we address this with a truncation argument, to get an inequality, and then apply it.)  We do know that the martingale itself has  \n",
    "\n",
    "$E\\Big[\\big \\vert X_n\\big \\vert \\Big]= E\\Big[ X_n \\Big]$   \n",
    "But for proving $E\\big[ X_n \\big]\\lt \\infty$, the procedures seems not immediately clear to your author  (which again is not a concern if we use a truncation argument -- because then values become uniformly bounded).  We may take advantage of non-negativity to help streamline *some* of the results in here...  \n",
    "\n",
    "**->  clean this up... there's likely a worked technique in this chapter of Karlin and Taylor **   \n",
    "\n",
    "\n",
    "\n",
    "part (b) is to prove a certain inequality   \n",
    "part(c) Prove that $\\lim_{n\\to}\\infty X_n = 0$  WP1.  A sketch of an idea from your author would be to try to show, via stochastic dominance, that the expected time until absorbtion is bounded above by a geometric series, i.e. \n",
    "\n",
    "$E\\big[T\\big] \\leq \\sum_{i=1}^\\infty i\\cdot e{-i} = \\sum_{i=1}^\\infty i\\cdot e{-1}^i = \\frac{e^{-1}}{(1-e^{-1})^2}$   \n",
    "and *then* apply markov inequality to recover $c$... however the mechanics of showing this is stochastically largest seem elusive.  (An alternative idea would come from the renewal age matrix and that would be the series used for the return state...)  \n",
    "\n",
    "\n",
    "(b) ref p.107 and p.147 of Ross & Pekoz, as well as page 278, 280, 281 of Karlin and Taylor.  \n",
    "for this wee need to reference the Kolmogorov Inequality for super (or sub) martingales (as our martingale is both, though super is in some sense cleaner here).  \n",
    "\n",
    "Here we truncate the chain and put a stopping rulee with T being the time t$n$ that $X_n =0$ \n",
    "\n",
    "This gives us \n",
    "\n",
    "$P\\big(\\max\\{X_1, ..., X_n\\} \\geq a $  \n",
    "$= P\\big(Z_N\\geq a\\big)$  \n",
    "$\\leq \\frac{E[X_T]}{a}$  \n",
    "$\\leq \\frac{E[X_0]}{a}$  \n",
    "which is the Kolmogorov Inequality  \n",
    "$= \\frac{1}{a}$  \n",
    "\n",
    "for any starting state $i$ we may for convenience select $a = i +1$  and find that the probability of threshold crossing is strictly less than one.   \n",
    "\n",
    "Yet if we apply \n",
    "Corollary 3.3.6 on page 107 of Ross and Pekoz (or in Karlin & Taylor: page 278 Theorem 5.1), we have \n",
    "$0\\leq X_n$ \n",
    "and $E\\Big[\\big \\vert X_n\\big \\vert \\Big] = E\\Big[X_n\\Big] = E\\Big[X_0\\Big] = i \\lt \\infty$ \n",
    "\n",
    "so we know   \n",
    "$\\lim_{n \\to \\infty } X_n =_{as} 0$ \n",
    "\n",
    "But all states in the truncated chain, except zero, are transient (because each has non-zero probability of reaching state 0) and further *all* states in the chain are transient except state zero (because state $j$ has probability of reaching state zero that is bounded below by $e^{-j}$), hence for the limit to exist WP1, we know $X_n \\longrightarrow_{as}0$   \n",
    "\n",
    "For a weaker finish, we could directly verify that $X_n \\longrightarrow_{p}0$  noticing that for any given $i$ we can make the probability $X_n$ solely takes on values in the truncated chain for arbitrariliy small by selecting large enough $\\alpha$.  However within the truncated chain, we have a finite state markov chain that converges geometrically with everything being absorbed into state $0$  (in fact as a sweetener we could use a coupling argument to estimate this rate of convergence, or use Gerschgorin Discs to see that while $\\lambda_1 =1$, the subdominant eigenvalue, $\\lambda_2 \\lt 1-e^{-a}$).  Thus $\\text{Probability of Not absorbed} \\leq P\\big(A_1 \\cup A_2\\big) = P\\big(A_1\\big) + P\\big(A_2\\big) \\leq P\\big(A_1\\big)  + \\frac{i}{a}\\leq a \\cdot \\lambda_2^n  + \\frac{i}{a}\\leq \\frac{2i}{a} $  \n",
    "\n",
    "where $A_1$ is event of still being in the absorbded chain after $n$ iterations (they could escape in the future at time after $n$ in this setup but still could be absorbed and and hence total probability with $A_1$ is an upper bound) , and $A_2$ is event of having escaped at time $n$  \n",
    "\n",
    "but the upper bound may be made arbirtrarily close to zero by selecting large enough $a$\n",
    "where we select $a$ and from there select large enough $n$ such that $a \\cdot \\lambda_2^n \\leq \\frac{i}{a}$, i.e. we choose large enough $n$ such that $\\lambda_2^n \\leq \\frac{i}{a^2}$  which is easy enough given monotonicity.  \n",
    "\n",
    "Now, of course convergence almost surely implies convergence in probability.  However in a manner of using a truncation argument to prove WLLN, this additional work supplies us with some additional insights and may be useful in other cases that don't have clean almost sure convegence.  \n",
    "\n",
    "- - - - \n",
    "a related but slightly different way to frame this problem, in the truncated setup is to consider, using the Wald Equation, \n",
    "\n",
    "$p\\dot 0 + q \\cdot \\text{EV at upper threshold r}  = \\text{starting value of i} = i$  \n",
    "(where $1-p =q$ and both non-negative)  \n",
    "\n",
    "so  \n",
    "$q  = \\frac{i}{\\text{EV at upper threshold r}}$    \n",
    "\n",
    "but there may be very large overshoots.  However given that payoff monotonically increase, we can lower bound the EV at upper threshold by the upper threashold value to get. Working with the inverse, this reads as   \n",
    "\n",
    "$\\frac{r}{i} \\leq \\frac{1}{q} = \\frac{\\text{EV at upper threshold r}}{i}$    \n",
    "$q\\leq \\frac{i}{r}$    \n",
    "\n",
    "This gets us to the same end result as kolmogorov inequality, though via an approach closer to Wald's Equation, (e.g. pages 92 and 94 of Ross and Pekoz -- we'd want to verify that conditon 3 on page 88 holds, via an argument close to that of page 90-- the finite state truncated chain converges geometrically fast, and we can bound the expected values of these non-negative random variables...).  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ],\n",
       "       [ 0.4,  0. ,  0.6,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ],\n",
       "       [ 0.4,  0. ,  0. ,  0.6,  0. ,  0. ,  0. ,  0. ,  0. ],\n",
       "       [ 0.4,  0. ,  0. ,  0. ,  0.6,  0. ,  0. ,  0. ,  0. ],\n",
       "       [ 0.4,  0. ,  0. ,  0. ,  0. ,  0.6,  0. ,  0. ,  0. ],\n",
       "       [ 0.4,  0. ,  0. ,  0. ,  0. ,  0. ,  0.6,  0. ,  0. ],\n",
       "       [ 0.4,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0.6,  0. ],\n",
       "       [ 0.4,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0.6],\n",
       "       [ 0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  0. ,  1. ]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(precision = 3, linewidth=180)\n",
    "\n",
    "p = 0.6 \n",
    "\n",
    "r = 8\n",
    "N = r + 1\n",
    "A = np.zeros((N,N))  \n",
    "\n",
    "A[0,0] = 1 \n",
    "A[r,r] = 1 \n",
    "\n",
    "for i in range(1, N-1):  \n",
    "    A[i,0] = 1-p\n",
    "    A[i,i+1]= p\n",
    "A\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0.        ,   1.        ,   1.66666667,   2.77777778,\n",
       "         4.62962963,   7.71604938,  12.8600823 ,  21.43347051,  35.72245085])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = np.zeros(N)\n",
    "v[0] = 0\n",
    "for i in range(1,N):\n",
    "    v[i] = (1/p)**(i-1)\n",
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(A@v - v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ],\n",
       "       [ 0.972,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.028],\n",
       "       [ 0.953,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.047],\n",
       "       [ 0.922,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.078],\n",
       "       [ 0.87 ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.13 ],\n",
       "       [ 0.784,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.216],\n",
       "       [ 0.64 ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.36 ],\n",
       "       [ 0.4  ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.6  ],\n",
       "       [ 0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  1.   ]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.matrix_power(A, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.39999999999999997"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_idx = 7\n",
    "(v[my_idx] - v[-1])/(-v[-1])\n",
    "\n",
    "# (v[my_idx] )/v[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1-p**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{f(i)- f(r)}{f(0) - f(r)} = p_{i,0}^{(\\infty)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "There are basically 2 key types of martingales that come up again and again... esp when there is an underlying Markov Chain...  \n",
    "\n",
    "one is $Z_n$ or $\\alpha^n Z_n$, or something along those lines.  \n",
    "\n",
    "The other makes use of the generating function, in particular in the case of non-negative integers (which rouhgly speaking is the typical case for markov chains) and thus the use of the OGF\n",
    "\n",
    "$E\\big[s^{X_n}\\big]$ or using MGF $E\\big[e^{\\theta X_n}\\big]$    \n",
    "note: the MGF is equivaelent to the OGF, e.g. for $s\\in (0,1]$ is equivalent to selecting $\\theta \\in (-\\infty, 0]$   \n",
    "\n",
    "With respect to the OGF, *assuming there are no convergence problems*, we look for a root (i) $s^* \\in (0,1)$ or in (i)$s^* \\in (1,\\infty)$.  When applying this to problems where state zero is absorbing and all others are transient (countably infinite markov chain in particular), finding (i) tells us that the chain has non zero probability of escape.  We can justify this directly based on properties on minimial roots of an OGF (*though the argument needs cleaned up a bit here*), or (in most cases) we can note a geometric decline in the values of $\\mathbf v = \\mathbf {Pv}$  where state zero is the largest and has some value $v_0$ (typically equal to 1).  From here compute $\\mathbf v^* = v_0\\mathbf 1 - \\mathbf v $ and note that $\\mathbf P \\mathbf v^* = \\mathbf v^*$.  I.e. this is still a right eigenvector, except the payoff for state zero is zero.  Hence we could use $\\mathbf Q$ which is a substochastic matrix where the zeroth row and columen of $\\mathbf P$ are deleted, (and similarly delete row zero in $\\mathbf v^*$)  and see $\\mathbf Q\\mathbf v^* = \\mathbf v^*$.  Referencing p 402 of Feller vol 1, we can see that this proves the chain is not persistent (i.e. there is non zero probability of escape/ \"wandering off\").  While it can be tricky in proving that we have the minimal (or maximal depending on framing) solution, we may can get to the desired result another way:  since our values are real non-negative and upper bounded by a constant, $v_[0]$, we can use that special case of dominanted convergence (i.e. bounded convergence) to interchange limits and expectation so we see that \n",
    "$E\\big[\\lim_{n\\to \\infty}\\big] = \\lim_{n\\to \\infty} E\\big[X_n\\big]$  and thus we know the limit exists by Martingale convergence theorem (e.g. corollary 3.36 in Ross and Pekoz), and since all states other than 0 are transient, the expected value associated with $s^*$ gives us precisly the probability of absorbtion.  \n",
    "\n",
    "We can also see this in what happens to the OGF for sufficiently large $n$, $E\\big[s^{X_n}\\big]$ (and quote a martingale limit theorem that notes a limit exists WP1 since we have a real non-negative martingale with bounded expected value for all $n$).  Some portion (specifically $1 - s^*$) wanders off to infinity though the payoff decreased geometrically.  Note: when using $s^*$ we may thing of there being some function $f$ such that $f\\big(X_n=i \\big) = f(i) = \\big(s^*\\big)^i$  Over non-negative integers, again, we should not that this reverses the ordering amongst $x_i$'s e.g. the raw value value $x_{i} \\gt x_{i-1}\\longrightarrow f(x_{i}) \\lt f(x_{i-1})$    .  Crucically, this re-ordering makes it impossible to e.g. apply the Kolmogorov Inequality to show vanishingly small probability of 'escape'.  We *should* be able to make the argument here that $s^*$ is our probability of being absorbded, and as $s^* \\to 1^{-}$ the probability of escape gets arbitrarily close to zero (i.e. probabiity of absorbtion gets arbitrarily close to 1)  \n",
    "\n",
    "Note again the role of normalization -- a key condition on page 402 of Feller for the persistency criterion of markov chains is that for each $i$ $0\\leq v_i \\leq 1$.  In this case of $s^* \\in (0,1)$ we have an upper bound at state 1 or zero and hence for the satisfying solution can can divide all values by this amount if needed, to make sure that the $0\\leq v_i \\leq 1$.  On the other hand, when consider the case of $s^* \\in (1,\\infty)$ the sequence monotonically increases without bound, so there would be no way to normalize such a vector to have each $0\\leq v_i \\leq 1$.  (Unfortunately this doesn't directly prove that no other  solution vectors could exist  -- more work is needed to prove that and it is stepped through, below.)   \n",
    "\n",
    "In the other extreme, when we find some $s^* \\in (1,\\infty)$ this indicates absorbtion WP1.  \n",
    "Note: That unlike this first component, there may be convergence issues here -- if we have a nice OGF like for a bounded distribution, or something nice like a Poisson distribution, it will not be a problem, but OGF's may break for $s\\gt 1$ in general.  \n",
    "\n",
    "Note: In the Karlin and Taylor Poisson Martingale /Markov chain (problem 18) it was not an $s^*$ here, but nevertheless an increasing sequence in the $\\mathbf v$ where $\\mathbf {Pv} = \\mathbf v$, and we may use this argument in any case where we have an increasing sequence.  Any time we have something like this, we may truncate the chain to have absorbtion at either state 0 or state $r$ and set up a stopping time at trial $n$ where the trial stops at the earlier of $n$ or being absorbed.  From there we may apply Kolmogorov Inequality (i.e. Markov Inequality specialized to stopping trials) and by choosing a sufficiently large $r$, we can show that the probability of exceeding $r$ is very low, and with sufficiently large $n$ almost everything has been absorbed (basic finite state time homogenous markov chain result), and hence up to any $\\epsilon \\gt 0$, we can show all probability is absorbed in state $0$.  The fact is that the martingale convergence theorem tells us that a limit exists WP1, and we may conclude with the above truncation argument that all probability is allocated to state $0$, i.e. that the absorbing state is persistent.  \n",
    "\n",
    "\n",
    "Now, again assuming no convergence issues, we have the one other case where $E\\big[s^{X_n}\\big] =1$ iff $s=1$.  (This is the 'uninteresting root' because it is always true).  However in this case, 1 is a strict linear lower bound of our OGF, with equality iff s = 1, and if we differentiate twice we verify the OGF is convex, which then implies that this is a zero mean random variable/ martingale.  In such a case we may (a) depart from the OGF setup and instead make use of the Wald Equation to exploit the zero mean structure of the martingale.  E.g. setup a truncation argument and find probabilities of absorbtion to states 0 and r, and watch as $r$ becomes arbitrarily large.  \n",
    "\n",
    "Another approach, which is more subtle (and needs some more work), would be to try tease the result out by continuity.  In such a case, we'd want to show that probability of absorbtion varies continuously with $s^*$ and from the right (i.e. $s^* \\in (1,\\infty)$ it is always 1, and from the left (i.e. $s^* \\in (0,1)$) probability of absorbtion gets arbitrarily close to $1$ as $s\\to 1^{-1}$, hence by continuity in the only root is $s=1$ case, we have probability of absorbtion sandwiched between $1-\\epsilon$ and $1$ for any $\\epsilon \\gt 0$, hence it implies probability of absorbtion being equal to $1$.  However, there are *many* moving parts here and it is not clear to your author how to make this argument apply in general markov chain-martingales as there could be numerous structural differences and complications.  Note if we think of this in terms of function $f$ applied to $X_n$, we can see that $f(i)=1$ for all $i$ and hence this is an uniformative martingale.  The fact that the underlying random variable has zero mean seems to be what we should exploit.  (Note that this will mostly refer to null recurrent chains which can require some additional care when handling).  \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**23**  \n",
    "\n",
    "This is something of a (clever) plug and chug exercise with a finite state markov chain  \n",
    "\n",
    "The idea is we have a binomial distribution on each row, but row 0 and N are absorbing \n",
    "\n",
    "$P_{i,j} = \\binom{N}{j}  \\pi_i^j (1-\\pi_i)^{N-j}$  \n",
    "\n",
    "with  \n",
    "$\\pi_i = \\frac{1-e^{-2ai/N}}{1-e^{-2a}}$    \n",
    "\n",
    "we need to verify   \n",
    "$Z_n = e^{-2a X_n}= f(X_n)$   \n",
    "is a martingale \n",
    "\n",
    "i.e. in particular, that key step\n",
    "$E\\Big[ f(X_{n+1}) \\big \\vert X_n = i\\Big] = \\sum_{j}P_{i,j}f(j) = f(i)$  \n",
    "(since it is a finite state chain we may take for granted that expected values are all finite)  \n",
    "\n",
    "or equivalently verify that  \n",
    "$\\mathbf P\\mathbf f = \\mathbf f$  \n",
    "\n",
    "$\\mathbf f =\\left[\\begin{matrix}\n",
    "e^{-2a0} \\\\\n",
    "e^{-2a1} \\\\\n",
    "e^{-2a2} \\\\\n",
    "e^{-2a3} \\\\\n",
    "\\vdots \\\\\n",
    "e^{-2aN}\n",
    "\\end{matrix}\\right] \n",
    "=  \n",
    "\\left[\\begin{matrix}\n",
    "1 \\\\\n",
    "e^{-2a} \\\\\n",
    "e^{-4a} \\\\\n",
    "e^{-6a} \\\\\n",
    "\\vdots \\\\\n",
    "e^{-2Na}\n",
    "\\end{matrix}\\right]$    \n",
    "\n",
    "so, using a binomial expansion technique, we can say that for any given $i$   \n",
    "\n",
    "$\\sum_j \\binom{N}{j} \\pi_i^j(1-\\pi_i)^{N-j}e^{-2aj} $  \n",
    "$= \\big((\\pi_i e^{-2a} + (1-\\pi_i)\\big)^N $  \n",
    "$= \\big((-1(1- e^{-2a})\\pi_i+1)\\big)^N$    \n",
    "$= \\big((-1(1- e^{-2a})\\frac{1-e^{-2ai/N}}{1-e^{-2a}}+1)\\big)^N$  \n",
    "$= \\big((-1)(1-e^{-2ai/N})+1)\\big)^N$  \n",
    "$= \\big(e^{-2ai/N})\\big)^N$  \n",
    "$= e^{-2ai}$  \n",
    "\n",
    "as required  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sympy as sp\n",
    "p = sp.Symbol('p', positive=True)\n",
    "q = sp.Symbol('q', positive=True)\n",
    "\n",
    "v_1 = sp.Symbol('v_1')\n",
    "k = sp.Symbol('k')\n",
    "a = sp.Symbol('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 1, -(p - 1)/p: 1}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = sp.Matrix([[1/p, -(1-p)/p],[1,0]])\n",
    "A.eigenvals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "v = sp.Matrix([v_1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_future = sp.simplify(A**k * v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{v_1: (p*((-p + 1)/p)**k - p - ((-p + 1)/p)**k + 1)/(p*((-p + 1)/p)**k + p - ((-p + 1)/p)**k)}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp.simplify(sp.solve(v_future[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.842950850553191"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp.simplify(result[0] + result[1] * (q/p)**k -(1-(q/p)**k)/(1-(q/p)**a)).subs(q, 1-p).subs(p,0.3).subs(k,7).subs(a, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\frac{p^{- k}}{p^{a} - q^{a}} \\left(p^{a} q^{k} - p^{k} q^{a}\\right)\n"
     ]
    }
   ],
   "source": [
    "W = sp.Matrix([[1,1],[1, (q/p)**a]])\n",
    "result = W.inv()* sp.Matrix([1,0])\n",
    "print(sp.latex(sp.simplify(result[0] + result[1] * (q/p)**k)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Matrix([\n",
       "[1],\n",
       "[0]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp.simplify(W*result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(p**(-a + k)*q**(2*a) + p**(a - k)*q**(2*k) - 2*q**(a + k))/(p**a*q**k + p**k*q**a - p**(a + k) - q**(a + k))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp.simplify(result[0] + result[1] * (q/p)**k - ((q/p)**a - (q/p)**k)/((q/p)**k -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{p^{- k}}{p^{a} - q^{a}} \\left(p^{a} q^{k} - p^{k} q^{a}\\right)\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(p + v_1*(p + ((-p + 1)/p)**k*(p - 1)) =  -((-p + 1)/p)**k*(-p + 1) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  9.25325861e-01,   0.00000000e+00,   2.55782359e-14,\n",
       "         0.00000000e+00,   4.86720080e-14,   0.00000000e+00,\n",
       "         6.90049733e-14,   0.00000000e+00,   8.63779125e-14,\n",
       "         0.00000000e+00,   1.00666831e-13,   0.00000000e+00,\n",
       "         1.11819684e-13,   0.00000000e+00,   1.19851900e-13,\n",
       "         0.00000000e+00,   1.24840874e-13,   0.00000000e+00,\n",
       "         1.26919643e-13,   0.00000000e+00,   1.26269912e-13,\n",
       "         0.00000000e+00,   1.23114635e-13,   0.00000000e+00,\n",
       "         1.17710330e-13,   0.00000000e+00,   1.10339307e-13,\n",
       "         0.00000000e+00,   1.01301971e-13,   0.00000000e+00,\n",
       "         9.09093539e-14,   0.00000000e+00,   7.94759976e-14,\n",
       "         0.00000000e+00,   6.73133215e-14,   0.00000000e+00,\n",
       "         5.47235593e-14,   0.00000000e+00,   4.19943483e-14,\n",
       "         0.00000000e+00,   2.93940307e-14,   0.00000000e+00,\n",
       "         1.71677062e-14,   0.00000000e+00,   5.53405901e-15,\n",
       "         7.46741392e-02])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "n = 45\n",
    "my_p = 0.49\n",
    "\n",
    "A = np.zeros((n+1,n+1))\n",
    "A[0,0] = 1\n",
    "A[-1,-1] = 1 \n",
    "for i in range(1, n):\n",
    "    A[i,i-1] = 1-my_p\n",
    "    A[i,i+1] = my_p\n",
    "\n",
    "# A.nullspace()\n",
    "# print(sp.latex(A))\n",
    "v = np.zeros(n+1)\n",
    "starteridx = 8\n",
    "\n",
    "v[starteridx] = 1\n",
    "\n",
    "v@np.linalg.matrix_power(A,10000)\n",
    "# A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07467413918030136\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.925325860819699"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expression_1 = (1 - ((1-my_p)/my_p)**starteridx)/(1 - ((1-my_p)/my_p)**n)\n",
    "print(expression_1)\n",
    "\n",
    "expression_2 = result[0] + result[1] * (q/p)**k \n",
    "expression_3 = (1-(q/p)**k)/(1-(q/p)**a)\n",
    "\n",
    "expression_2.subs(p, my_p).subs(q,1-my_p).subs(a, n).subs(k, starteridx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0746741391803014"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expression_3.subs(p, my_p).subs(q,1-my_p).subs(a, n).subs(k, starteridx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$p_{i,N}^{(\\infty)} = 1 - \\frac{f(i)- f(N)}{f(0) - f(N)} =  \\frac{f(0)- f(N)}{f(0) - f(N)} + \\frac{-f(i)+ f(N)}{f(0) - f(N)} =  \\frac{f(0)- f(i)}{f(0) - f(N)} = \\frac{1-\\big(\\frac{1-p}{p}\\big)^i }{1-\\big(\\frac{1-p}{p}\\big)^N }$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "       [ 0.38533354,  0.        ,  0.61466646,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.41554805,  0.        ,  0.58445195,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.53041676,  0.        ,  0.46958324],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  1.        ]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "transient_states = 3\n",
    "prob_foward = np.random.random(transient_states +  2)\n",
    "prob_foward[0] = 0\n",
    "prob_foward[-1] =0\n",
    "prob_foward\n",
    "\n",
    "A = np.zeros((transient_states + 2,transient_states + 2))\n",
    "A[-1,-1] = 1\n",
    "A[0,0] = 1\n",
    "for i in range(1, transient_states+1):\n",
    "    A[i,i-1] = 1- prob_foward[i]\n",
    "    A[i,i+1] = prob_foward[i]\n",
    "A\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/d64/anaconda2/envs/py35/lib/python3.5/site-packages/ipykernel/__main__.py:4: RuntimeWarning: divide by zero encountered in true_divide\n"
     ]
    }
   ],
   "source": [
    "running_product = np.ones(transient_states + 2)\n",
    "running_product\n",
    "for k in range(1, running_product.shape[0]):\n",
    "    running_product[k] = running_product[k-1]*(1-prob_foward[k])/[prob_foward[k]]\n",
    "running_product\n",
    "\n",
    "running_product[-1] = 100 # dummy value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0.00000000e+00,   3.24088767e-02,   1.09033232e-01,\n",
       "         4.66912748e+01,   0.00000000e+00])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A @ running_product - running_product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  3.68463842e-01,   0.00000000e+00,   1.24179686e-62,\n",
       "         0.00000000e+00,   6.31536158e-01])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "starting_state = 2\n",
    "v = np.zeros(transient_states + 2)\n",
    "v[2] = 1\n",
    "\n",
    "v@ np.linalg.matrix_power(A,500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.50552759020780946"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formula= (1-((1- prob_foward[starting_state])/prob_foward[starting_state])**starting_state)\n",
    "\n",
    "formula *= 1/(1-((1- prob_foward[starting_state])/prob_foward[starting_state])**starting_state)\n",
    "formula "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Matrix([\n",
       "[1/p, (p - 1)/p, -1/p],\n",
       "[  1,         0,    0],\n",
       "[  0,         0,    1]])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = sp.Matrix([[1/p, -(1-p)/p, -1/p],[1,0,0],[0,0,1]])\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 2, -(p - 1)/p: 1}"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.eigenvals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "n = 50\n",
    "my_p = 0.5\n",
    "\n",
    "A = np.zeros((n+1,n+1))\n",
    "A[0,1] = 1\n",
    "A[-1,-1] = 1 \n",
    "for i in range(1, n):\n",
    "    A[i,i-1] = 1-my_p\n",
    "    A[i,i+1] = my_p\n",
    "\n",
    "# A.nullspace()\n",
    "# print(sp.latex(A))\n",
    "# v = np.zeros(n+1)\n",
    "# starteridx = 8\n",
    "\n",
    "# v[starteridx] = 1\n",
    "\n",
    "# v@np.linalg.matrix_power(A,10000)\n",
    "# A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "d, P = np.linalg.eig(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.19802951,  0.19793179,  0.19763874,  0.19715065,  0.19646799,\n",
       "        0.19559144,  0.19452186,  0.19326032,  0.19180805,  0.19016649,\n",
       "        0.18833725,  0.18632216,  0.18412318,  0.1817425 ,  0.17918246,\n",
       "        0.17644558,  0.17353458,  0.17045232,  0.16720184,  0.16378636,\n",
       "        0.16020924,  0.15647401,  0.15258436,  0.14854413,  0.1443573 ,\n",
       "        0.14002801,  0.13556053,  0.13095926,  0.12622876,  0.12137368,\n",
       "        0.11639882,  0.1113091 ,  0.10610952,  0.10080522,  0.09540144,\n",
       "        0.08990352,  0.08431686,  0.078647  ,  0.07289952,  0.0670801 ,\n",
       "        0.06119448,  0.05524847,  0.04924794,  0.0431988 ,  0.03710703,\n",
       "        0.03097864,  0.02481968,  0.01863622,  0.01243438,  0.00622026])"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perron_vector = P[:,d.argmax()] \n",
    "if perron_vector[0] <0:\n",
    "    perron_vector *= -1 \n",
    "perron_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0. ,  1. ,  0. , ...,  0. ,  0. ,  0. ],\n",
       "       [ 0.5,  0. ,  0.5, ...,  0. ,  0. ,  0. ],\n",
       "       [ 0. ,  0.5,  0. , ...,  0. ,  0. ,  0. ],\n",
       "       ..., \n",
       "       [ 0. ,  0. ,  0. , ...,  0. ,  0.5,  0. ],\n",
       "       [ 0. ,  0. ,  0. , ...,  0.5,  0. ,  0.5],\n",
       "       [ 0. ,  0. ,  0. , ...,  0. ,  0.5,  0. ]])"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.19802951,  0.19793179,  0.19763874,  0.19715065,  0.19646799,\n",
       "        0.19559144,  0.19452186,  0.19326032,  0.19180805,  0.19016649,\n",
       "        0.18833725,  0.18632216,  0.18412318,  0.1817425 ,  0.17918246,\n",
       "        0.17644558,  0.17353458,  0.17045232,  0.16720184,  0.16378636,\n",
       "        0.16020924,  0.15647401,  0.15258436,  0.14854413,  0.1443573 ,\n",
       "        0.14002801,  0.13556053,  0.13095926,  0.12622876,  0.12137368,\n",
       "        0.11639882,  0.1113091 ,  0.10610952,  0.10080522,  0.09540144,\n",
       "        0.08990352,  0.08431686,  0.078647  ,  0.07289952,  0.0670801 ,\n",
       "        0.06119448,  0.05524847,  0.04924794,  0.0431988 ,  0.03710703,\n",
       "        0.03097864,  0.02481968,  0.01863622,  0.01243438,  0.00622026])"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perron_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-ea5a7484f322>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mleft_side_perron\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mleft_side_perron\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft_side_perron\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mleft_side_perron\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "left_side_perron = np.linalg.inv(P)[d.argmax()]\n",
    "left_side_perron *= 1/np.sum(left_side_perron)\n",
    "left_side_perron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0157092553237 0.0157015037545\n",
      "0.0157015037545 0.0156782566967\n",
      "0.0156782566967 0.0156395370925\n",
      "0.0156395370925 0.0155853831533\n",
      "0.0155853831533 0.0155158483226\n",
      "0.0155158483226 0.0154310012229\n",
      "0.0154310012229 0.0153309255879\n",
      "0.0153309255879 0.0152157201804\n",
      "0.0152157201804 0.015085498694\n",
      "0.015085498694 0.0149403896417\n",
      "0.0149403896417 0.0147805362286\n",
      "0.0147805362286 0.0146060962108\n",
      "0.0146060962108 0.0144172417393\n",
      "0.0144172417393 0.0142141591909\n",
      "0.0142141591909 0.0139970489835\n",
      "0.0139970489835 0.0137661253786\n",
      "0.0137661253786 0.01352161627\n",
      "0.01352161627 0.0132637629586\n",
      "0.0132637629586 0.0129928199145\n",
      "0.0129928199145 0.0127090545258\n",
      "0.0127090545258 0.0124127468347\n",
      "0.0124127468347 0.012104189261\n",
      "0.012104189261 0.0117836863139\n",
      "0.0117836863139 0.0114515542911\n",
      "0.0114515542911 0.0111081209668\n",
      "0.0111081209668 0.0107537252681\n",
      "0.0107537252681 0.0103887169409\n",
      "0.0103887169409 0.0100134562044\n",
      "0.0100134562044 0.00962831339551\n",
      "0.00962831339551 0.00923366860375\n",
      "0.00923366860375 0.00882991129587\n",
      "0.00882991129587 0.00841743993159\n",
      "0.00841743993159 0.00799666157034\n",
      "0.00799666157034 0.00756799146958\n",
      "0.00756799146958 0.00713185267493\n",
      "0.00713185267493 0.00668867560273\n",
      "0.00668867560273 0.00623889761524\n",
      "0.00623889761524 0.00578296258903\n",
      "0.00578296258903 0.00532132047694\n",
      "0.00532132047694 0.00485442686399\n",
      "0.00485442686399 0.0043827425178\n",
      "0.0043827425178 0.00390673293389\n",
      "0.00390673293389 0.00342686787623\n",
      "0.00342686787623 0.00294362091371\n",
      "0.00294362091371 0.00245746895274\n",
      "0.00245746895274 0.0019688917666\n",
      "0.0019688917666 0.001478371522\n",
      "0.001478371522 0.000986392303192\n",
      "0.000986392303192 0.000493439634268\n"
     ]
    }
   ],
   "source": [
    "for i in range(left_side_perron.shape[0]):\n",
    "    for j in range(i+1 ,left_side_perron.shape[0]):\n",
    "        if np.isclose(B[i,j] +B[j,i], 0):\n",
    "            continue\n",
    "        print(left_side_perron[i] * B[i,j], left_side_perron[j] * B[j,i] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.377488535866084e-06"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(d)**25000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.19802951,  0.19793179,  0.19763874,  0.19715065,  0.19646799,\n",
       "        0.19559144,  0.19452186,  0.19326032,  0.19180805,  0.19016649,\n",
       "        0.18833725,  0.18632216,  0.18412318,  0.1817425 ,  0.17918246,\n",
       "        0.17644558,  0.17353458,  0.17045232,  0.16720184,  0.16378636,\n",
       "        0.16020924,  0.15647401,  0.15258436,  0.14854413,  0.1443573 ,\n",
       "        0.14002801,  0.13556053,  0.13095926,  0.12622876,  0.12137368,\n",
       "        0.11639882,  0.1113091 ,  0.10610952,  0.10080522,  0.09540144,\n",
       "        0.08990352,  0.08431686,  0.078647  ,  0.07289952,  0.0670801 ,\n",
       "        0.06119448,  0.05524847,  0.04924794,  0.0431988 ,  0.03710703,\n",
       "        0.03097864,  0.02481968,  0.01863622,  0.01243438,  0.00622026])"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perron_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.10789771,  0.10789771,  0.10768479,  0.10747188,  0.1070469 ,\n",
       "        0.10662192,  0.10598654,  0.10535116,  0.1045079 ,  0.10366464,\n",
       "        0.10261682,  0.10156899,  0.10032075,  0.09907251,  0.09762876,\n",
       "        0.09618502,  0.09455148,  0.09291794,  0.09110105,  0.08928415,\n",
       "        0.08729108,  0.085298  ,  0.08313661,  0.08097522,  0.07865404,\n",
       "        0.07633287,  0.07386106,  0.07138926,  0.06877659,  0.06616391,\n",
       "        0.06342068,  0.06067745,  0.05781448,  0.05495152,  0.05198012,\n",
       "        0.04900872,  0.04594061,  0.0428725 ,  0.03971979,  0.03656709,\n",
       "        0.03334222,  0.03011736,  0.02683307,  0.02354878,  0.02021801,\n",
       "        0.01688725,  0.01352317,  0.01015908,  0.00677495,  0.00339082])"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.matrix_power(B, n**2*2).sum(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.57735026919 0.274546367849\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.274546367849 0.233543089741\n",
      "0.0 0.0\n",
      "0.0 0.0\n",
      "0.233543089741 0.169678986818\n",
      "0.0 0.0\n",
      "0.169678986818 0.0892055224433\n"
     ]
    }
   ],
   "source": [
    "for i in range(perron_vector.shape[0]):\n",
    "    for j in range(i + 1, perron_vector.shape[0]):\n",
    "        print(perron_vector[i]*B[i,j], perron_vector[j]*B[j,i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.98768834059513744"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.58328351,  0.54152351,  0.41893615,  0.23013379])"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B @ perron_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.58328351,  0.23599876,  0.58328351, -0.23599876],\n",
       "       [-0.54152351, -0.09074868,  0.54152351, -0.09074868],\n",
       "       [ 0.41893615, -0.17441581,  0.41893615,  0.17441581],\n",
       "       [-0.23013379,  0.23132652,  0.23013379,  0.23132652]])"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B@P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ -5.72534399e-01  -5.55028759e-01  -5.54117865e-01  -5.72534399e-01\n",
      "   -5.55028759e-01]\n",
      " [  5.46632271e-01   3.27483704e-01  -2.72611583e-17  -5.46632271e-01\n",
      "   -3.27483704e-01]\n",
      " [ -4.69202943e-01   1.83345727e-01   5.76734921e-01  -4.69202943e-01\n",
      "    1.83345727e-01]\n",
      " [  3.45292230e-01  -5.61624891e-01  -2.24904556e-16  -3.45292230e-01\n",
      "    5.61624891e-01]\n",
      " [ -1.84443477e-01   4.85447553e-01  -6.00275122e-01  -1.84443477e-01\n",
      "    4.85447553e-01]]\n"
     ]
    }
   ],
   "source": [
    "print(P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -9.54758826e-01,  -5.90030155e-01,  -1.64142383e-16,\n",
       "         9.54758826e-01,   5.90030155e-01])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -2.46440960e-02,  -4.18199066e-01,   4.48519298e-01,\n",
       "        -1.76307518e-04,  -1.30041829e-02,   4.57137310e-01,\n",
       "        -4.62467745e-01,  -2.14214017e-01])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B@P[0]- P[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
