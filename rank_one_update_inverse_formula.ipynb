{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "For some full rank matrix, $\\mathbf A$, and some rank one update given by $\\mathbf {xy}^T$, we have the formula\n",
    "\n",
    "$\\big(\\mathbf A + \\mathbf {xy}^T\\big)^{-1} = \\mathbf A^{-1} - \\alpha \\mathbf A^{-1} \\mathbf {xy}^T \\mathbf A^{-1}$\n",
    "\n",
    "where $\\alpha = \\frac{1}{ 1 + \\mathbf y^T \\mathbf A^{-1} \\mathbf x}$\n",
    "\n",
    "Note that for simplicity this post deals in Reals, with a particular emphasis on things like covariance matrices and Hessians, and hence we assume $\\mathbf A$ is symmetric definite, and indeed positive definite.  \n",
    "\n",
    "$det\\big(\\mathbf A + \\mathbf {xy}^T \\big) =  det\\big(\\mathbf A \\big)\\alpha^{-1} = \\big(\\mathbf A + \\mathbf {xy}^T\\big)$.  See my \"determinant_rank_one_update\" post for more details.  (Technically that posting was concerned with the case where $\\mathbf x = \\mathbf y$ but the analysis ultimately is the same.)\n",
    "\n",
    "**note on Singularity**  \n",
    "If $1 + \\mathbf y^T \\mathbf A^{-1} \\mathbf x = 0$, we can see that the rank one update has made $\\mathbf A$ not invertible.  Why? Notice that in the determinant formula, it would now equal zero, and also notice that $\\alpha$ is undefined.  This challenge goes away if $\\mathbf{xy}^T = \\mathbf {xx}^T$, because for some symmetric positive definite matrix $\\mathbf A$, $\\mathbf x^T \\mathbf {A}^{-1}\\mathbf x \\gt 0$ for all $\\mathbf x \\neq 0$ --and we know $\\mathbf x \\neq 0$ because such an update would be a rank zero update, not a rank one update that this post is concerned with plus it is also trivially easy to do the inversion and verify the formla : $\\big(\\mathbf A + \\mathbf {0}\\big)^{-1} = \\mathbf A^{-1} - \\alpha \\mathbf A^{-1} \\mathbf {0} \\mathbf A^{-1} = \\mathbf A^{-1}$. \n",
    "\n",
    "\n",
    "Proof:\n",
    "\n",
    "$\\big(\\mathbf A + \\mathbf {xy}^T\\big) \\big(\\mathbf A + \\mathbf {xy}^T\\big)^{-1} = \\mathbf I = \\big(\\mathbf A + \\mathbf {xy}^T\\big)\\big(\\mathbf A^{-1} - \\alpha \\mathbf A^{-1} \\mathbf {xy}^T \\mathbf A^{-1}\\big)$\n",
    "\n",
    "$\\mathbf I= \\mathbf A \\big(\\mathbf A^{-1} - \\alpha \\mathbf A^{-1} \\mathbf {xy}^T \\mathbf A^{-1}\\big) + \\mathbf {xy}^T\\big(\\mathbf A^{-1} - \\alpha \\mathbf A^{-1} \\mathbf {xy}^T \\mathbf A^{-1}\\big) $\n",
    "\n",
    "$\\mathbf I = \\mathbf I - \\alpha \\mathbf {xy}^T \\mathbf A^{-1} + \\mathbf {xy}^T\\mathbf A^{-1} - \\alpha \\mathbf {xy}^T\\mathbf A^{-1} \\mathbf {xy}^T \\mathbf A^{-1} = \\mathbf I - \\alpha \\mathbf {xy}^T \\mathbf A^{-1} + \\mathbf {xy}^T\\mathbf A^{-1} - \\alpha \\big(\\mathbf {xy}^T\\mathbf A^{-1}\\big)^2    $\n",
    "\n",
    "subtract $\\mathbf I$ from both sides and we have\n",
    "\n",
    "$\\mathbf 0 = -\\alpha \\mathbf {xy}^T \\mathbf A^{-1} + \\mathbf {xy}^T\\mathbf A^{-1} - \\alpha \\big(\\mathbf {xy}^T\\mathbf A^{-1}\\big)^2 = \\mathbf {xy}^T \\mathbf A^{-1}\\Big( (1-\\alpha) \\mathbf I - \\alpha \\big(\\mathbf {xy}^T\\mathbf A^{-1}\\big)\\Big) $\n",
    "\n",
    "where $\\mathbf {xy}^T \\mathbf A^{-1}$ is a diagonalizable rank one matrix, so long as $trace(\\mathbf {xy}^T \\mathbf A^{-1}) = trace(\\mathbf {y}^T \\mathbf A^{-1} \\mathbf x) \\neq 0$.  In the event $trace(\\mathbf {xy}^T \\mathbf A^{-1}) = 0$, we either have $\\mathbf {xy}^T \\mathbf A^{-1} = \\mathbf 0$ (and hence $\\mathbf {xy}^T \\mathbf A^{-1}$ is actually a rank zero matrix) or else $\\mathbf {xy}^T \\mathbf A^{-1}$ is defective.  The former case, while technically not possible given that we are multiplying a rank one matrix by a full rank matrix, is nevertheless covered in the below diagonalization analysis.  The defective case is covered in the Post-Script.  \n",
    "\n",
    "Thus our matrix $ \\mathbf {xy}^T \\mathbf A^{-1} = \\mathbf{PDP}^{-1}$, where all values in $\\mathbf D$ are zero, except the top left corner has $\\lambda_1 = \\mathbf{y}^T \\mathbf A^{-1} \\mathbf x$.\n",
    "\n",
    "rewriting the above equation we get:\n",
    "\n",
    "$\\mathbf 0 = \\mathbf{PDP}^{-1}\\Big( \\big(1-\\alpha\\big) \\mathbf I - \\alpha \\big(\\mathbf{PDP}^{-1}\\big)\\Big) = \\mathbf{PDP}^{-1}\\Big( \\big(1-\\alpha\\big) \\mathbf{PIP}^{-1} - \\alpha \\big(\\mathbf{PDP}^{-1}\\big)\\Big)= \\mathbf{PD}\\Big( (1-\\alpha) \\mathbf{I} - \\alpha \\big(\\mathbf{D}\\big)\\Big)\\mathbf P^{-1}$\n",
    "\n",
    "$\\mathbf 0= \\mathbf{PD}\\Big( (\\frac{\\mathbf y^T \\mathbf A^{-1} \\mathbf x}{ 1 + \\mathbf y^T \\mathbf A^{-1} \\mathbf x}) \\mathbf{I} - \\frac{1}{ 1 + \\mathbf y^T \\mathbf A^{-1} \\mathbf x} \\big(\\mathbf{D}\\big)\\Big)\\mathbf P^{-1} = \\mathbf{P}\\Big( (\\frac{\\mathbf y^T \\mathbf A^{-1} \\mathbf x}{ 1 + \\mathbf y^T \\mathbf A^{-1} \\mathbf x}) \\mathbf{D} - \\frac{1}{ 1 + \\mathbf y^T \\mathbf A^{-1} \\mathbf x} \\big(\\mathbf{D}^2\\big)\\Big)\\mathbf P^{-1} $  \n",
    "$=  \\mathbf{P}\\Big( \\frac{\\mathbf 1}{ 1 + \\mathbf y^T \\mathbf A^{-1} \\mathbf x} \\mathbf{D}^2 - \\frac{1}{ 1 + \\mathbf y^T \\mathbf A^{-1} \\mathbf x} \\mathbf{D}^2\\Big)\\mathbf P^{-1} = \\mathbf {P0P}^{-1} = \\mathbf 0 $\n",
    "\n",
    "For good measure, one may wish to repeat the entire analysis of:    \n",
    "$\\big(\\mathbf A + \\mathbf {xy}^T\\big)^{-1} = \\mathbf A^{-1} - \\alpha \\mathbf A^{-1} \\mathbf {xy}^T \\mathbf A^{-1}$\n",
    "\n",
    "this time right multiplying by $\\big(\\mathbf A + \\mathbf {xy}^T\\big)$, though it should suffice to show  \n",
    "$\\big(\\mathbf A + \\mathbf {xy}^T\\big)^{-1}\\big(\\mathbf A + \\mathbf {xy}^T\\big) = \\mathbf I = \\mathbf A^{-1}\\big(\\mathbf A + \\mathbf {xy}^T\\big) - \\alpha \\mathbf A^{-1} \\mathbf {xy}^T \\mathbf A^{-1}\\big(\\mathbf A + \\mathbf {xy}^T\\big)$\n",
    "\n",
    "$\\mathbf I =  \\mathbf I+ \\mathbf A^{-1}\\mathbf {xy}^T - \\alpha \\mathbf A^{-1} \\mathbf {xy}^T + \\alpha \\big(\\mathbf A^{-1} \\mathbf {xy}^T\\big)^2$\n",
    "\n",
    "which is nearly equivalent to the equation on the third line that we solved under the heading \"Proof\".  Note that in the special case of particular interest $\\mathbf x = \\mathbf y$, then the equation must be true, because it is merely the tranpose of the above (subtract $\\mathbf I$ from both sides, and the transpose of the zero matrix is still the zero matrix). Of course, since the matrices are all square and we are talking about actual inverses, not pseudo-inverses, we could just define $\\mathbf B = \\big(\\mathbf A + \\mathbf{xy}^T\\big)^{-1}$ and reuse the original work, to show that $\\mathbf B^{-1} \\mathbf B = \\mathbf I$.\n",
    "\n",
    "\n",
    "- - - -\n",
    "**Post-Script:**\n",
    "\n",
    "In the special case where we have a defective rank one matrix given by $\\mathbf {xy}^T \\mathbf A^{-1}$, where the trace must be $trace(\\mathbf {xy}^T \\mathbf A^{-1}) = trace(\\mathbf {y}^T \\mathbf A^{-1} \\mathbf x) = 0$\n",
    "\n",
    "we may use the Jordan form $\\mathbf {xy}^T \\mathbf A^{-1} = \\mathbf{PJP}^{-1}$\n",
    "\n",
    "Where $\\mathbf J$ is the zero matrix, except the second item on the first row is equal to one.  Thus we see that $\\mathbf J^2 = \\mathbf 0$. In such a case our final equation becomes:  \n",
    "\n",
    "$\\mathbf 0= \\mathbf{PJ}\\Big( (\\frac{\\mathbf y^T \\mathbf A^{-1} \\mathbf x}{ 1 + \\mathbf y^T \\mathbf A^{-1} \\mathbf x}) \\mathbf{I} - \\frac{1}{ 1 + \\mathbf y^T \\mathbf A^{-1} \\mathbf x} \\big(\\mathbf{J}\\big)\\Big)\\mathbf P^{-1} = \\mathbf{P}\\Big( (\\frac{0}{ 1}) \\mathbf{J} - \\frac{1}{ 1} \\big(\\mathbf{J}^2\\big)\\Big)\\mathbf P^{-1} $  \n",
    "$=  \\mathbf{P}\\Big( 0* \\mathbf J - \\mathbf{0}\\Big)\\mathbf P^{-1} =  \\mathbf{P}\\Big( \\mathbf 0 - \\mathbf{0}\\Big)\\mathbf P^{-1} = \\mathbf {P0P}^{-1} = \\mathbf 0 $\n",
    "\n",
    "**Alternative approach** \n",
    "\n",
    "Since your author finds Jordan forms to be rather unpleasant, it is also worth pointing out that a Schur Decomposition can be used to prove the same argument. \n",
    "\n",
    "That is $\\mathbf {xy}^T \\mathbf A^{-1} = \\mathbf{QTQ}^H = \\mathbf{QTQ}^{-1}$\n",
    "\n",
    "We may choose $\\mathbf Q, \\mathbf T$ in an optimal way -- that is $\\mathbf Q$ has all othogonal vectors from $\\mathbf A$'s null space, until we get to the final column of $\\mathbf Q$, which will have the one vector orthgonal to all of those in the nullspace.  Thus \n",
    "\n",
    "$\\mathbf T = \n",
    "\\bigg[\\begin{array}{c|c|c|c}\n",
    "\\mathbf t_1 & \\mathbf t_2 &\\cdots & \\mathbf t_{n-1} & \\mathbf t_{n}\n",
    "\\end{array}\\bigg] =  \n",
    "\\bigg[\\begin{array}{c|c|c|c}\n",
    "\\mathbf 0 & \\mathbf 0 &\\cdots & \\mathbf 0 & \\mathbf t_{n}\n",
    "\\end{array}\\bigg]\n",
    "$  \n",
    "\n",
    "we can also partition $\\mathbf T$ horizontally as in the below:\n",
    "\n",
    "$\\mathbf T = \\begin{bmatrix}\n",
    "\\mathbf t_1^{\\sim T}\\\\ \n",
    "\\mathbf t_2^{\\sim T}\\\\ \n",
    "\\vdots  \\\\ \n",
    "\\mathbf t_{n-1}^{\\sim T}\\\\ \n",
    "\\mathbf t_{n}^{\\sim T}\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "\\mathbf t_1^{\\sim T}\\\\ \n",
    "\\mathbf t_2^{\\sim T}\\\\ \n",
    "\\vdots  \\\\ \n",
    "\\mathbf t_{n-1}^{\\sim T}\\\\ \n",
    "\\mathbf 0^T\n",
    "\\end{bmatrix}$\n",
    "\n",
    "Notice we know that the bottom right entry of $\\mathbf T$ is zero because that is the final eigenvalue (given by trace) of the matrix, and since $\\mathbf T$ is upper triangular, this means that the bottom row of $\\mathbf T$ is the zero vector.\n",
    "\n",
    "Thus by outer product interpretation of matrix multiplication,\n",
    "\n",
    "$\\mathbf {TT} = \\mathbf t_1 \\mathbf t_1 ^{\\sim T} + \\mathbf t_2\\mathbf t_2^{\\sim T} + ... + \\mathbf t_{n-1}\\mathbf  t_{n-1}^{\\sim T} + \\mathbf t_{n}\\mathbf t_{n}^{\\sim T} = \\mathbf 0\\mathbf t_1^{\\sim T} + \\mathbf 0\\mathbf t_2^{\\sim T} + ... + \\mathbf 0\\mathbf t_{n-1}^{\\sim T} + \\mathbf t_{n}\\mathbf 0^T = \\mathbf 0$\n",
    "\n",
    "That is $\\mathbf T^2 = \\mathbf 0$.  From here we can use the exact same argument that was used with respect to the Jordan form.  \n",
    "\n",
    "$\\mathbf 0= \\mathbf{QT}\\Big( (\\frac{\\mathbf y^T \\mathbf A^{-1} \\mathbf x}{ 1 + \\mathbf y^T \\mathbf A^{-1} \\mathbf x}) \\mathbf{I} - \\frac{1}{ 1 + \\mathbf y^T \\mathbf A^{-1} \\mathbf x} \\big(\\mathbf{T}\\big)\\Big)\\mathbf Q^{-1} = \\mathbf{Q}\\Big( \\big(\\frac{0}{ 1}\\big) \\mathbf{T} - \\frac{1}{ 1} \\big(\\mathbf{T}^2\\big)\\Big)\\mathbf Q^{-1} $  \n",
    "$=  \\mathbf{Q}\\Big( 0* \\mathbf T - \\mathbf{0}\\Big)\\mathbf Q^{-1} =  \\mathbf{Q}\\Big( \\mathbf 0 - \\mathbf{0}\\Big)\\mathbf Q^{-1} = \\mathbf {Q0Q}^{-1} = \\mathbf 0 $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above formula can be used to quickly and easily get the formula for the determinant of a the rank one update of a matrix (where $\\mathbf A$ is symmetric positive definite, as before).  The below proof can be viewed as an alternative  to my post titled \"determinant_rankone_update\". \n",
    "\n",
    "\n",
    "\n",
    "$\\big(\\mathbf A + \\mathbf {xy}^T\\big)^{-1} = \\mathbf A^{-1} - \\alpha \\mathbf A^{-1} \\mathbf {xy}^T \\mathbf A^{-1}$\n",
    "\n",
    "where $\\alpha = \\frac{1}{ 1 + \\mathbf y^T \\mathbf A^{-1} \\mathbf x}$\n",
    "\n",
    "Thus\n",
    "\n",
    "$det\\Big(\\big(\\mathbf A + \\mathbf {xy}^T\\big)^{-1}\\Big) = det\\Big(\\mathbf A^{-1} - \\alpha \\mathbf A^{-1} \\mathbf {xy}^T \\mathbf A^{-1}\\Big)$\n",
    "\n",
    "$det\\Big(\\mathbf A\\Big)det\\Big(\\big(\\mathbf A + \\mathbf {xy}^T\\big)^{-1}\\Big) = det\\Big(\\mathbf A\\Big) det\\Big(\\mathbf A^{-1} - \\alpha \\mathbf A^{-1} \\mathbf {xy}^T \\mathbf A^{-1}\\Big)$\n",
    "\n",
    "\n",
    "$det\\Big(\\mathbf A\\Big)det\\Big(\\big(\\mathbf A + \\mathbf {xy}^T\\big)^{-1}\\Big) = det\\Big(\\mathbf I - \\alpha \\mathbf {xy}^T\\mathbf A^{-1}\\Big)$\n",
    "\n",
    "$det\\Big(\\mathbf A\\Big)det\\Big(\\big(\\mathbf A + \\mathbf {xy}^T\\big)^{-1}\\Big) = det\\Big(\\mathbf I - \\alpha \\mathbf {QTQ}^{-1} \\Big)$\n",
    "\n",
    "where we use the Schur Decomposition from the Post-Script of the above writeup\n",
    "\n",
    "$det\\Big(\\mathbf A\\Big)det\\Big(\\big(\\mathbf A + \\mathbf {xy}^T\\big)^{-1}\\Big) = det\\Big(\\mathbf {QIQ}^{-1} - \\alpha \\mathbf {QTQ}^{-1}\\Big)$\n",
    "\n",
    "$det\\Big(\\mathbf A\\Big)det\\Big(\\big(\\mathbf A + \\mathbf {xy}^T\\big)^{-1}\\Big) = det\\Big(\\mathbf Q\\Big) det\\Big(\\mathbf {I} - \\alpha \\mathbf {T} \\Big)det\\Big(\\mathbf Q^{-1}\\Big)$\n",
    "\n",
    "$det\\Big(\\mathbf A\\Big)det\\Big(\\big(\\mathbf A + \\mathbf {xy}^T\\big)^{-1}\\Big) = det\\Big(\\mathbf {I} - \\alpha \\mathbf {T} \\Big)$\n",
    "\n",
    "The matrix given by $\\big(\\mathbf {I} - \\alpha \\mathbf {T}\\big)$ **is upper triangular with all ones on its diagonal, except, the bottom left cell** has a value of  \n",
    "$= 1 - \\alpha* trace \\big( \\mathbf T \\big) = 1 - \\alpha * trace \\big( \\mathbf {xy}^T \\mathbf A^{-1} \\big) =  1 - \\alpha* trace \\big( \\mathbf {y}^T \\mathbf A^{-1}\\mathbf x \\big) = 1 - \\alpha* \\big( \\mathbf {y}^T \\mathbf A^{-1}\\mathbf x \\big) $   \n",
    "\n",
    "Thus that bottom left cell is given by:  \n",
    "\n",
    "$\\big(\\mathbf {I} - \\alpha \\mathbf {T}\\big)_{n,n} = 1 - \\alpha  \\big( \\mathbf {y}^T \\mathbf A^{-1}\\mathbf x \\big) = \\frac{1 + \\mathbf y^T \\mathbf A^{-1} \\mathbf x}{ 1 + \\mathbf y^T \\mathbf A^{-1} \\mathbf x} - \\frac{1}{ 1 + \\mathbf y^T \\mathbf A^{-1} \\mathbf x} \\big( \\mathbf {y}^T \\mathbf A^{-1}\\mathbf x \\big)$  \n",
    " $\\big(\\mathbf {I} - \\alpha \\mathbf {T}\\big)_{n,n}= \\frac{1 + \\mathbf y^T \\mathbf A^{-1} \\mathbf x}{ 1 + \\mathbf y^T \\mathbf A^{-1} \\mathbf x} - \\frac{\\mathbf {y}^T \\mathbf A^{-1}\\mathbf x}{ 1 + \\mathbf y^T \\mathbf A^{-1} \\mathbf x} $  \n",
    "\n",
    "$\\big(\\mathbf {I} - \\alpha \\mathbf {T}\\big)_{n,n} =\\frac{1}{ 1 + \\mathbf y^T \\mathbf A^{-1} \\mathbf x} $\n",
    "\n",
    "Since $\\big(\\mathbf {I} - \\alpha \\mathbf {T}\\big)$ is upper triangular, we know that its diagonal entries are its eigenvalues, and since the determinant is computed as being the product of the matrix's eigenvalues, we merely need the product of the diagonals of $\\big(\\mathbf {I} - \\alpha \\mathbf {T}\\big)$ to get the determinant\n",
    "\n",
    "$det\\Big(\\mathbf {I} - \\alpha \\mathbf {T} \\Big) =  1 * 1 * 1 * .... * 1 * \\frac{1}{ 1 + \\mathbf y^T \\mathbf A^{-1} \\mathbf x} = \\frac{1}{ 1 + \\mathbf y^T \\mathbf A^{-1} \\mathbf x}$\n",
    "\n",
    "$det\\Big(\\mathbf A\\Big)det\\Big(\\big(\\mathbf A + \\mathbf {xy}^T\\big)^{-1}\\Big)= det\\Big(\\mathbf {I} - \\alpha \\mathbf {T} \\Big) = \\frac{1}{ 1 + \\mathbf y^T \\mathbf A^{-1} \\mathbf x}$\n",
    "\n",
    "$det\\Big( \\mathbf A^{-1}\\Big) det\\Big(\\mathbf A\\Big)det\\Big(\\big(\\mathbf A + \\mathbf {xy}^T\\big)^{-1}\\Big)= det\\Big( \\mathbf A^{-1}\\Big) \\frac{1}{ 1 + \\mathbf y^T \\mathbf A^{-1} \\mathbf x}$\n",
    "\n",
    "$det\\Big(\\big(\\mathbf A + \\mathbf {xy}^T\\big)\\Big)^{-1} = det\\Big( \\mathbf A\\Big)^{-1} \\frac{1}{ 1 + \\mathbf y^T \\mathbf A^{-1} \\mathbf x}$\n",
    "\n",
    "$det\\Big(\\mathbf A + \\mathbf {xy}^T \\Big) = det\\Big( \\mathbf A \\Big) \\big(1 + \\mathbf y^T \\mathbf A^{-1}\\mathbf x \\big)$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [py35]",
   "language": "python",
   "name": "Python [py35]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
