{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I stumbled onto this inequality while working on problem 8.7 (SVMs) in 'Learning From Data'\n",
    "\n",
    "(all matrices are finitie dimensional and real valued)\n",
    "\n",
    "$\\mathbf 1 =\\begin{bmatrix}\n",
    "1\\\\ \n",
    "1\\\\ \n",
    "\\vdots \\\\ \n",
    "1\n",
    "\\end{bmatrix}$\n",
    "\n",
    "\n",
    "$\\mathbf X^T = \n",
    "\\bigg[\\begin{array}{c|c|c|c}\n",
    "\\mathbf x_1 & \\mathbf x_2 &\\cdots & \\mathbf x_{N}\n",
    "\\end{array}\\bigg]\n",
    "$   \n",
    "\n",
    "\n",
    "Thus $\\mathbf{X X}^T$ is a symmetric, positive semi-definite, $N$x$N$ matrix\n",
    "\n",
    "**claim:**\n",
    "\n",
    "$N * trace\\big(\\mathbf{X X}^T\\big) \\geq \\mathbf 1^T \\mathbf{X X}^T \\mathbf 1 = sum \\big(\\mathbf{X X}^T\\big)$\n",
    "\n",
    "That is, a scalar of $N$ multiplied by the sum of the $N$ diagonal elements that come from $trace\\big(\\mathbf{X X}^T\\big)$ must be at least as big as the sum of the entire $N^2$ elements in $\\mathbf{X X}^T$\n",
    "\n",
    "Note that $\\mathbf {X X}^T$ is symmetric and may be diagonalized.  The degenerate case where $\\mathbf X$ is the zero matrix is trivially true, and excluded from this writeup.\n",
    "\n",
    "note that $trace(\\mathbf {X X}^T) =  \\Sigma_{k=1}^{N} \\mathbf x_k^T \\mathbf x_k = \\mathbf x_1^T \\mathbf x_1 + \\mathbf x_2^T \\mathbf x_2 + ... + \\mathbf x_N^T \\mathbf x_N = \\mathbf 1^T\\big(\\mathbf x_1\\circ \\mathbf x_1 + \\mathbf x_2\\circ \\mathbf x_2 + ... + \\mathbf x_N \\circ \\mathbf x_N\\big) = N \\mathbf 1^T E\\big[ \\mathbf{x} \\circ \\mathbf {x}\\big] $\n",
    "\n",
    "Further, notice that $\\big(\\mathbf 1^T \\mathbf{X}\\big) \\big(\\mathbf X^T \\mathbf 1\\big)= \\big(\\mathbf X^T \\mathbf 1\\big)^T \\big(\\mathbf X^T \\mathbf 1\\big) = E\\big[N\\mathbf x\\big]^TE\\big[N\\mathbf x\\big] = N^2 \\mathbf 1^T \\Big( E\\big[\\mathbf x\\big] \\circ E\\big[\\mathbf x\\big]\\Big) $\n",
    "\n",
    "where $\\circ$ denotes the Hadarmard product, and $E\\big[ \\big]$ denotes taking the expected value / arithmetic mean (we can think of the column vectors in $\\mathbf X^T$ as being uniformly distributed, if helpful)\n",
    "\n",
    "**proof:**  \n",
    "$N * trace\\big(\\mathbf{X X}^T\\big) = N^2 \\mathbf 1^T E\\big[ \\mathbf{x} \\circ \\mathbf {x}\\big] \\geq N^2 \\mathbf 1^T \\Big( E\\big[\\mathbf x\\big] \\circ E\\big[\\mathbf x\\big]\\Big)= \\mathbf 1^T \\mathbf{X X}^T \\mathbf 1$\n",
    "\n",
    "because: \n",
    "\n",
    "$ \\mathbf 1^T E\\big[ \\mathbf{x} \\circ \\mathbf {x}\\big] \\geq \\mathbf 1^T \\Big( E\\big[\\mathbf x\\big] \\circ E\\big[\\mathbf x\\big]\\Big)$\n",
    "\n",
    "equivalently:\n",
    "\n",
    "$ \\big(\\mathbf{e_1 + e_2 + ... + e_N} \\big)^T  E\\big[ \\mathbf{x} \\circ \\mathbf {x}\\big] \\geq \\big(\\mathbf{e_1 + e_2 + ... + e_N} \\big)^T \\Big( E\\big[\\mathbf x\\big] \\circ E\\big[\\mathbf x\\big]\\Big)$\n",
    "\n",
    "where $\\mathbf e_k$ denotes the kth unit standard vector (i.e. column slice of the $N$ x $N$ Identity matrix) for $k = \\{1, 2, ..., N\\}$\n",
    "\n",
    "The above is true due to Jensen's Inequality.  In fact Jensen's Inequality makes an even stronger statement:\n",
    "\n",
    "$ \\mathbf e_k^T \\Big(E\\big[ \\mathbf{x} \\circ \\mathbf {x}\\big]\\Big) \\geq \\mathbf e_k^T \\Big( E\\big[\\mathbf x\\big] \\circ E\\big[\\mathbf x\\big]\\Big)$\n",
    "\n",
    "which can be re-written as:\n",
    "\n",
    "$  E\\big[ \\mathbf{x} \\circ \\mathbf {x}\\big] .\\geq  \\Big( E\\big[\\mathbf x\\big] \\circ E\\big[\\mathbf x\\big]\\Big)$\n",
    "\n",
    "where $.\\geq$ denotes elevement-wise comparisons between the vectors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An alternative proof uses quadratic forms.  As in the above writeup, we banish from discussion the case of $\\mathbf X$ being the zero matrix, hence $ \\big \\vert \\big \\vert \\mathbf X\\big \\vert \\big \\vert_F \\gt 0$.\n",
    "\n",
    "claim: $N * trace\\big(\\mathbf{X X}^T\\big) \\geq \\mathbf 1^T \\mathbf{X X}^T \\mathbf 1 = sum \\big(\\mathbf{X X}^T\\big)$\n",
    "\n",
    "where $\\mathbf 1 =\\begin{bmatrix}\n",
    "1\\\\ \n",
    "1\\\\ \n",
    "\\vdots \\\\ \n",
    "1\n",
    "\\end{bmatrix}$\n",
    "\n",
    "$\\mathbf b  := \\frac{1}{\\sqrt N} \\mathbf 1 $ \n",
    "\n",
    "so $\\mathbf 1^T \\mathbf 1 = N = \\mathbf b^T \\mathbf b (\\sqrt(N))^2 = 1 * N = N$\n",
    "\n",
    "We can factorize $\\big(\\mathbf{X X}^T\\big) = \\mathbf U \\mathbf D \\mathbf U^T$, where $\\mathbf U$ is orthogonal and $\\mathbf D$ is a diagonal matrix with $\\lambda_k$ in position $\\mathbf D_{k,k}$\n",
    "\n",
    "The matrix given by $\\big(\\mathbf{X X}^T\\big)$ is symmetric, positive semi-definite, hence eigenvalues are real and non-negative. Let the eigenvalues be ordered such that $\\mathbf \\lambda_1 \\geq \\mathbf \\lambda_2 \\geq ... \\geq \\lambda_N \\geq 0$\n",
    "\n",
    "\n",
    "consider that $N * trace\\big(\\mathbf{X X}^T\\big) = N * trace\\big(\\mathbf U \\mathbf D \\mathbf U^T\\big) = N * trace\\big(\\mathbf U^T\\mathbf U \\mathbf D \\big)= N * trace\\big(\\mathbf D \\big) =  N * \\big( \\lambda_1 + \\lambda_2 + ... + \\lambda_N \\big) $\n",
    "\n",
    "\n",
    "on the other hand, consider maximizing the quadratic form \n",
    "\n",
    "max $\\mathbf 1^T \\mathbf{X X}^T \\mathbf 1$  \n",
    "max $\\mathbf 1^T \\big(\\mathbf U \\mathbf D \\mathbf U^T\\big) \\mathbf 1$  \n",
    "max $\\sqrt(N) \\mathbf b^T \\big(\\mathbf U \\mathbf D \\mathbf U^T\\big) \\sqrt(N) \\mathbf b$  \n",
    "max $N \\big(\\mathbf b^T \\mathbf U \\big) \\mathbf D \\big(\\mathbf U^T \\mathbf b \\big)$  \n",
    "max $N \\big(\\mathbf U^T \\mathbf b \\big)^T \\mathbf D \\big(\\mathbf U^T \\mathbf b \\big)$\n",
    "  \n",
    "where $\\mathbf U^T \\mathbf b := \\mathbf y$\n",
    "\n",
    "hence $\\big \\vert \\big \\vert \\mathbf y \\big \\vert\\big \\vert_2^2 = \\big \\vert \\big \\vert \\mathbf U^T \\mathbf b \\big \\vert\\big \\vert_2^2 = \\big \\vert \\big \\vert \\mathbf b \\big \\vert\\big \\vert_2^2 = 1$\n",
    "\n",
    "max $N \\big(\\mathbf y \\big)^T \\mathbf D \\big(\\mathbf y \\big)$\n",
    "\n",
    "max $N \\big( \\lambda_1 y_1^2 + \\lambda_2 y_2^2 + ... + \\lambda_N y_N^2\\big)$\n",
    "\n",
    "which is the familiar diagonalization argument for allocating all of $\\mathbf y $ to $y_1$, i.e. $\\mathbf y = \\mathbf e_1$ (the first column slice of the N x N Identity Matrix) -- otherwise we use a familiar exchange argument to prove $\\mathbf y = \\mathbf e_1$ is the (weakly) dominant solution.  \n",
    "\n",
    "Thus we have \n",
    "\n",
    "$N * trace\\big(\\mathbf{X X}^T\\big) =  N * \\big( \\lambda_1 + \\lambda_2 + ... + \\lambda_N \\big) \\gt N \\big(\\lambda_1 \\big)= N \\big( \\lambda_1 1^2 + \\lambda_2 0^2 + ... + \\lambda_N 0^2\\big) = \\mathbf e_1^T D \\mathbf e_1 $\n",
    "\n",
    "Thus we can say \n",
    "\n",
    "$N * trace\\big(\\mathbf{X X}^T\\big) \\gt \\mathbf 1^T \\mathbf{X X}^T\\mathbf 1$ \n",
    "\n",
    "with **equality if and only if** $\\mathbf X$ **is a rank one matrix**, where its dominant (i.e. not in the null space) right singular vector  $\\propto \\mathbf 1$.  More specifically, equality holds iff $\\mathbf X^T$ **is a rank one matrix of the form **\n",
    "\n",
    "$\\mathbf X^T = \n",
    "\\bigg[\\begin{array}{c|c|c|c}\n",
    "\\mathbf x_1 & \\mathbf x_1 &\\cdots & \\mathbf x_{1}\n",
    "\\end{array}\\bigg]\n",
    "= \\mathbf x_1 \\mathbf 1^T$   \n",
    "\n",
    "where, as before, the ones vector, has N scalar values equal to one in it, thus $\\mathbf 1^T \\mathbf 1 = N$\n",
    "\n",
    "\n",
    "Thus $\\mathbf {XX}^T= \\big(\\mathbf {x_1 1}^T\\big)^T \\mathbf x_1 \\mathbf 1^T =\\mathbf {1 x_1}^T \\mathbf x_1 \\mathbf 1^T = (\\mathbf x_1^T \\mathbf x_1) \\mathbf{11}^T$, hence  \n",
    "\n",
    "$N * trace\\big(\\mathbf{X X}^T\\big) = N * trace\\big( (\\mathbf x_1^T \\mathbf x_1) * \\mathbf{11}^T\\big) =  N *(\\mathbf x_1^T \\mathbf x_1) trace\\big(  \\mathbf1^T \\mathbf 1 \\big) = N^2 *(\\mathbf x_1^T \\mathbf x_1) $\n",
    "\n",
    "and equivalently  \n",
    "$\\mathbf 1^T \\mathbf{X X}^T\\mathbf 1 = \\mathbf 1^T  \\big( \\mathbf 1 \\mathbf x_1^T \\mathbf x_1 \\mathbf 1^T\\big) \\mathbf 1 =  \\big(\\mathbf 1^T  \\mathbf 1\\big) \\big(\\mathbf x_1^T \\mathbf x_1 \\big) \\big(\\mathbf 1^T \\mathbf 1\\big) = (N) (\\mathbf x_1^T \\mathbf x_1)(N) = N^2 *(\\mathbf x_1^T \\mathbf x_1)$\n",
    "\n",
    "This has a nice interpretation in terms of Jensen's Inequality, whereby $ \\mathbf 1^T E\\big[ \\mathbf{x} \\circ \\mathbf {x}\\big] = \\mathbf 1^T \\Big( E\\big[\\mathbf x\\big] \\circ E\\big[\\mathbf x\\big]\\Big)$, put differently, $\\mathbf 1^T \\Big(E\\big[ \\mathbf{x} \\circ \\mathbf {x}\\big] - E \\big[\\mathbf x\\big] \\circ E\\big[\\mathbf x\\big]\\Big) = 0$, or  $ \\Big(E\\big[ \\mathbf{x} \\circ \\mathbf {x}\\big] - E \\big[\\mathbf x\\big] \\circ E\\big[\\mathbf x\\big]\\Big) = \\mathbf 0$ \n",
    "\n",
    "because by making every column in $\\mathbf X^T$ the same, we homogenized the distribution such that there is now zero variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [py35]",
   "language": "python",
   "name": "Python [py35]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
