{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ex 12.1.3**  \n",
    "Let $\\phi:V\\longrightarrow W$ be a homomorphism of modules over a ring $R$ and let $V',W'$ be submodules of $V$,$W$ respectively.  Prove that (i) $\\phi\\big(V'\\big)$ is a submodule of $W$ and that (ii) $\\phi^{-1}\\big(W'\\big)$ (i.e. the pre-image of $W'$) is a submodule of $V$   \n",
    "\n",
    "**proof:**  \n",
    "This comes down to applying the definition of homomorphism (p. 451)-- which essentially tells us that $\\phi$ behaves as a linear map-- and the definition of submodule (given also on p. 451).  \n",
    "\"*A submodule of an R-module V is a non-empty subset that is closed under addition and scalar multiplication.  We have seen submodules in one case before, namely ideals.*\"    \n",
    "\n",
    "*note:* these ideas both get their inspiration from vector subspaces  \n",
    "\n",
    "i.) \n",
    "It is enough to consider 2 elements-- and use associativity / bunching / induction.  \n",
    "Consider $v_1, v_2 \\in V'$ with associated 'scalars' from the ring, given by $\\alpha_1, \\alpha_2$.  Then  \n",
    "$\\Big(\\alpha_1 \\cdot v_1+\\alpha_2 \\cdot v_2\\Big) \\in V'$ \n",
    "by definition of $V'$  and applying $\\phi$ two different ways   \n",
    "$\\alpha_1 \\cdot \\phi\\Big(v_1\\Big)+\\alpha_2 \\cdot \\phi\\Big(v_2\\Big)=\\phi\\Big(\\big(\\alpha_1 \\cdot v_1+\\alpha_2 \\cdot v_2\\big)\\Big) \\in \\phi\\Big(V'\\Big)$  \n",
    "hence the RHS is closed under linear combinations (with scalars in ring $R$) of elements from $V'$ under the image of $\\phi$, i.e. it is a submodule.  \n",
    "\n",
    "\n",
    "ii.)  \n",
    "the proof is essentially the same as in the first case though a couple of comments are in order.  \n",
    "Note that $W'$ contains $\\mathbf 0$ since it is a submodule, and $\\mathbf 0$ must exist in the preimage as well since $\\mathbf 0$ is mapped to $\\mathbf 0$ under a module homomorphism so the preimage cannot be empty.  Since $\\phi^{-1}$ refers to the preimage and $\\phi$ need not be injective, it's worth noting that $\\phi^{-1}\\Big( w_j'\\Big)$ is technically a set with multiple elements in it, *however* in the one line directly below we should interpret this notation interpret it to refer to one specific arbitrarily chosen element in such a set.  \n",
    "\n",
    "$\\gamma_1\\cdot\\phi^{-1}\\Big( w_1'\\Big) + \\gamma_2\\cdot \\phi^{-1}\\Big(w_2'\\Big) = \\phi^{-1}\\Big(\\gamma_1\\cdot w_1' + \\gamma_2\\cdot w_2'\\Big) = \\phi^{-1}\\Big(\\big(\\gamma_1\\cdot w_1' + \\gamma_2\\cdot w_2'\\big)\\Big)\\in \\phi^{-1}\\Big(W'\\Big)$      \n",
    "\n",
    "\n",
    "\n",
    "**special cases of interest:**  \n",
    "If we let $W'= \\{\\mathbf 0\\}$ which is a submodule in a trivial sense, then this tells us that the pre-image is a submodule, i.e. $\\ker \\phi\\big(V\\big)$ is a submodule.  \n",
    "\n",
    "And if we let $V':=V$ then this tells us that $\\text{image } \\phi\\big(V\\big)$ is a submodule  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ex 12.1.6**  \n",
    "A module is called *simple* if it is not the zero module and it has no proper submodule.  (This is essentially the idea of being irreducible and nontrivial.)   \n",
    "\n",
    "*(a)*  Prove that any simple module is isomorphic to $R/M$ where $M$ is a maximal ideal.  \n",
    "\n",
    "*(b)* Prove Schur's Lemma:  Let $\\phi: S\\longrightarrow S'$ be a homomorphism of simple modules.  Then either $\\phi$ is zero, or else it is an isomorphism.  \n",
    "\n",
    "This can be evaluated with the help of a $2\\times 2$ table  \n",
    "\n",
    "$\\begin{bmatrix} & \\phi \\text{ is injective} & \\phi \\text{ is not injective} \\\\  \\phi \\text{ is surjective} & \\phi  \\text{ is an isomorphism} & \\text{impossible} \\\\\\phi \\text{ is not surjective} & \\text{impossible} & \\text{its kernel must be all of S so it is the zero map} \\\\ \\end{bmatrix}$  \n",
    "\n",
    "\n",
    "note: if we check the definitions, using linearity, $\\phi$ is injective *iff* it has a trivial kernel (again just like linear maps)   \n",
    "\n",
    "For the left column, since $S$ is simple it is non-zero, so if $\\phi$ is injective, then $\\phi\\big(S\\big)$ is non-zero but it is a submodule of $S'$ (again per ex 12.1.5) so it must be equal to all of $S'$ since $S'$ is simple and has no (non-zero) *proper* submodules.  \n",
    "\n",
    "For the right column.  If $\\phi$ is not injective, then per ex 12.1.5 we see its kernel is a non-zero submodule.  But $S$ is simple hence the kernel must be all of $S$.  This implies the bottom right corner, since the image of $\\phi$ is zero, and $S'$ is simple, hence non-zero, so the map cannot be surjective.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ex 12.2.4** \n",
    "\n",
    "Let R be a ring and V be a free R-module of finite rank.  *Prove or disprove*  \n",
    "**(a)** Every set of generators contains a basis  \n",
    "this isn't true, and depending on the exact interpretation of the question we may offer various counter examples.  E.g. consider a submodule with a set of $m$ generators that are nilpotent.  This is not the zero submodule yet has zero linearly independent elements in it so there can be no basis, despite having a set of generators.   \n",
    "\n",
    "remark: reminiscent of the the bilinear forms chapter, the difficult task lies in dealing with things that are self linearly dependent (or self-orthogonal in bilinear forms chapter).  \n",
    "\n",
    "Counterexample:  \n",
    "$\\mathbb C[x,y]$ which can be interpreted as an R-module over itself.  So it has a basis given by $1$.  But if we consider the non-principal ideal $(x,y)$ then this does not have a basis (ex 12.2.1) so despite having generators for this submodule, we cannot find a basis for it.  \n",
    "\n",
    "**(b)** Every linearly independent set can be extended to a basis  \n",
    "for concreteness consider $R:=\\mathbb Z$ and up to isomorphism we are working in $\\mathbb Z^n$.  \n",
    "now the set $\\big(2\\mathbf e_1\\big)$ is linearly independent.  We may extend this to a maximally sized linearly independent set, collected as a hyper-vector, and indeed a matrix,  \n",
    "\n",
    "$\\mathbf B'=\\begin{bmatrix}2 &*\\\\ \\mathbf 0_{n-1} &B_{n-1}\\end{bmatrix}$  \n",
    "but \n",
    "$\\Big\\vert \\det\\big(\\mathbf  B'\\big)\\Big\\vert = 2 \\cdot \\Big\\vert\\det\\big(\\mathbf  B_{n-1}'\\big)\\Big\\vert \\geq 2$  \n",
    "since $\\big(\\mathbf  B_{n-1}\\big) \\in \\mathbb Z -\\{0\\}$  \n",
    "(we can reason over $\\mathbb Q$ if needed for why $\\big(\\mathbf  B_{n-1}\\big) \\neq 0$, and clear denominators as needed for linear dependence arguments)  \n",
    "\n",
    "Thus $\\det\\big(\\mathbf B'\\big)$ is not a unit and hence the matrix, viewed as a homomorphism, would be injective, but not surjective (see ex 12.2.7 for more color)  \n",
    "\n",
    "Equivalently, we know that the standard basis $\\mathbf B$ is a basis for our free module, but if $\\mathbf B'$ were a basis for our space as well, then per page 455, there would be an invertible matrix $A$ such that \n",
    "\n",
    "$\\mathbf B A =\\mathbf B'$  \n",
    "but taking determinants  \n",
    "$\\det\\big(\\mathbf B A\\big) = \\det\\big(\\mathbf B\\big)\\cdot \\det\\big( A\\big)=1\\cdot \\det\\big( A\\big) =\\det\\big(\\mathbf B'\\big)=\\text{not a unit}$   \n",
    "thus $A$ has non-unit determinant which is necessary and sufficient to be invertible (ref. page 453) which contradicts the fact that it is invertible.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ex 12.2.7**  \n",
    "\n",
    "Let $A$ be the matrix of a homomorphism $\\phi: \\mathbb Z^n \\longrightarrow \\mathbb Z^m$  between free modules  \n",
    "*note: this mean A has n columns and m rows*  \n",
    "\n",
    "*(a)* Prove that $\\phi$ is injective *iff* $\\text{rank}\\big(A\\big)= n$  \n",
    "The proof is essentially the same as when working over a field.  \n",
    "$\\text{rank}\\big(A\\big)= n\\implies \\text{all columns independent}\\implies \\ker\\big(A\\big)=\\{0\\} \\implies \\ker\\big(\\phi\\big)=\\{0\\}\\implies \\phi \\text{ injective} $  \n",
    "$\\phi\\text{ injective} \\implies \\ker\\big(\\phi\\big)=\\{0\\} \\implies \\ker\\big(A\\big)=\\{0\\} \\implies \\text{all columns independent}\\implies\\text{rank}\\big(A\\big)= n $   \n",
    "note: we are not using or assuming rank-nullity here.  We are just working with the definition the independence of modules given on page 454.  And no-nontrivial linear combination of $A$'s columns evaluating to zero  is equivalent to a trivial kernel for $A$  \n",
    "\n",
    "*(b)* Prove that $\\phi$ is surjective *iff* the GCD of the determinants of the $m \\times m$ submatrices of $A$ is 1. \n",
    "$\\phi$ is surjective *iff* $A$, so the below develops the problem in terms of $A$   \n",
    "\n",
    "In note that these determinants take values in $\\mathbb Z$ which is a principal ideal domain (chp 11).  Thus the determinants having gcd $\\gt  1$ is equivalent to the minors having a common (non-unit) factor $r$ and being in the principal ideal $(r)$.  And the determinants having a gcd of $1$ is equivalent to them being in the unit ideal.  (We can ignore the trivial case of all determinants being zero and hence living in the zero ideal as this case implies that no matter how we select $r$ columns in $A$ we never have $r$ linearly independent ones -- working over $\\mathbb Q$ if needed for justification-- hence for reason of our guiding rank inequality the map cannot be surjective.)  \n",
    "\n",
    "Thus if we let $A_Z$ be the $m\\times m$ submatrix of $A$ with column set $Z$ our problem is to prove that $A$ is surjective *iff* it $m \\times m$ minors generate the unit ideal in $\\mathbb Z$.   \n",
    "\n",
    "or $A$ is surjective *iff*  \n",
    "$S \\in \\binom{[n]}{m}$  $n_k\\in \\mathbb Z$   \n",
    "$\\sum_{S} n_k\\det\\big(A_{[m],S}\\big)=1$   \n",
    "\n",
    "which reads as the set of all subsets of size $m$  of set $[n]$ --- which is the set $\\{1, 2, ..., n\\}$, so we want to choose subsets of cardinality $m$ from this   \n",
    "(ref 'cauchy_binet.ipynb' or page 232 of *Proofs from THE Book* for discussion of this or comparable notation. These two items also are useful as a reference to for 2nd leg of the proof re: necessity which uses Cauchy-Binet)   \n",
    "\n",
    "**sufficiency:**    \n",
    "$\\text{m x m minors of A generating unit ideal} \\implies \\text{A is surjective}$  \n",
    "to show that $A$ is surjective, it is enough to show that for any standard basis vector $\\mathbf e_j$ in the codomain, there is a non-empty preimage i.e. that for any $\\mathbf e_j$ there is some $\\mathbf x$ such that \n",
    "$A\\mathbf x = \\mathbf e_j$  \n",
    "\n",
    "Take our equation $\\sum_{S} n_k\\det\\big(A_{[m],S}\\big)=1$ and multiply each side by $\\mathbf e_j$ to get  \n",
    "$\\sum_{S} n_k\\det\\big(A_{[m],S}\\big)\\mathbf e_j=\\mathbf e_j$  \n",
    "\n",
    "Thus  \n",
    "$A\\mathbf x = A\\big(\\sum_{S} \\mathbf x^{(k)}\\big) =\\big(\\sum_{S} A\\mathbf x^{(k)}\\big) = \\sum_{S} n_k\\det\\big(A_{[m],S}\\big)\\mathbf e_j = \\mathbf e_j$  \n",
    "\n",
    "and for each item in the summation we may solve  \n",
    "$A\\mathbf x^{(k)}=\\mathbf b = n_k\\det\\big(A_{[m],S}\\big)\\mathbf e_j$   \n",
    "by setting $ x_i^{(k)}:=\\mathbf 0$ if the RHS is zero and otherwise by using Cramer's rule.  I.e. for reasons that amount to graph isomorphism, we need only show this for the first minor of $A$ i.e. the leading m x m submatrix in it.  We set $x_i^{(k)} :=0$ for $i\\gt m$.  Then for $i\\in{1,2,....,m}$  we apply Cramer's Rule (classical form of solving for one component at a time, not the adjugate form that Artin uses)  \n",
    "to get  \n",
    "$x_i^{(k)} := \\frac{\\det\\big(A_{[m],S}[\\mathbf b]\\text{ (swapped in col i) } \\big)}{\\det\\big(A_{[m],S}\\big)} = n_k\\cdot\\det\\big(A_{[m],S}\\big)\\cdot \\frac{\\det\\big(A_{[m],S}[\\mathbf e_j]\\text{ (swapped in col i) } \\big)}{\\det\\big(A_{[m],S}\\big)}=n_k\\cdot \\det\\big(A_{[m],S}[\\mathbf e_j]\\text{ (swapped in col i)}\\big)$  \n",
    "\n",
    "hence we've constructed a solution for  \n",
    "$A\\mathbf x^{(k)}=\\mathbf b = n_k\\det\\big(A_{[m],S}\\big)\\mathbf e_j$   \n",
    "\n",
    "and using linearity, we may sum over this solution set to get  \n",
    "$A\\mathbf x = A\\big(\\sum_{S} \\mathbf x^{(k)}\\big) =\\big(\\sum_{S} A\\mathbf x^{(k)}\\big) = \\sum_{S} n_k\\det\\big(A_{[m],S}\\big)\\mathbf e_j = \\mathbf e_j$  \n",
    "\n",
    "which proves that $A$ is surjective.  \n",
    "\n",
    "*note:* if the nominal division used in Cramer's rule makes the reader uncomfortable, we may temporarily relax/extend the ring from $\\mathbb Z$ to $\\mathbb Q$ and solve for each $x_i$ there, then follow the implications through and finally work backwards.  Alternatively, we could apply Cramer's Rule as  \n",
    "$\\det\\big(A_{[m],S}\\big) \\cdot x_i^{(k)} := \\det\\big(A_{[m],S}[\\mathbf b]\\text{ (swapped in col i) } \\big) = \\det\\big(A_{[m],S}\\big)\\cdot n_k \\cdot \\det\\big(A_{[m],S}[\\mathbf e_j]\\text{ (swapped in col i) } \\big)$   \n",
    "\n",
    "Then noting this allows a solution of $x_i^{(k)} := n_k\\cdot \\det\\big(A_{[m],S}[\\mathbf e_j]\\text{ (swapped in col i) } $   \n",
    "where e.g. we don't even contemplate being in an integral domain.  \n",
    "(The validity of Cramer's Rule for modules is addressed in ex 12.3.1)    \n",
    "\n",
    "**necessity:**    \n",
    "$\\text{A is surjective} \\implies \\text{m x m minors of A generating unit ideal}$   \n",
    "if $A$ is surjective, we may always solve $A\\mathbf x = \\mathbf e_j$, thus if $A$ is surjective, there exists $X \\in \\mathbb Z^{n\\times m}$ such that \n",
    "$I_m = AX$    \n",
    "\n",
    "taking determinants of each side we have   \n",
    "$1 = \\det\\big(I_m\\big) = \\det\\big(AX\\big) = \\sum_{S} \\det\\big(A_{[m],S}\\big)\\det\\big(X_{S,[m]}\\big) = \\sum_{S} \\det\\big(A_{[m],S}\\big)\\cdot n_k$    \n",
    "by Cauchy-Binet.  Hence the determinants of the $m\\times m$ submatrices in $A$ must generate the unit ideal.  \n",
    "\n",
    "(Cauchy-Binet may be justified by considering the matrices as having integer coefficients in $\\mathbb Q$, or likely by applying the below Principle of Permanence of Identities though this requires some careful checking).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**12.3 Principle of Permance of Identities**  \n",
    "This is my notes / re-rendering of pages 456-457, which while short, gives some of the most powerful insights in the book.  The principle goes as follows.  \n",
    "\n",
    "**0.) Background**  \n",
    "*viewpoint 1*  \n",
    "Lemma: For the integer ring $\\mathbb Z$, where $\\mathbb Z\\big[\\mathbf w\\big]$  is a polynomial in $w_i$ for $i\\in \\{1,2,..., m\\}$  \n",
    "$\\phi: \\mathbb Z\\big[\\mathbf w\\big]\\longrightarrow \\Re$ is *injective*   \n",
    "where $\\Re$ is the ring of *polynomial functions* with coefficients in $\\mathbb F$, when $\\mathbb F$ is a field of **characteristic zero** (e.g. $\\mathbb C$ or $\\mathbb Q$).  Note that this implies that $\\mathbb F$ has infinite cardinality).  \n",
    "(this is my adaptation of prop 3.8 on page 355)  \n",
    "\n",
    "To prove injectivity, we check the kernel of $\\phi$.  \n",
    "The zero polynomial is mapped to zero under any ring homomorphism.  \n",
    " \n",
    "i) if $p\\in \\mathbb Z\\big[\\mathbf w\\big]$ where $\\text{degree}(p) = 0$  \n",
    "(i.e. non-zero constant polynomial, which means $p\\in \\mathbb Z$)\n",
    "\n",
    "Then recalling prop 3.9 (p. 355) there is exactly one homomorphism from $\\mathbb Z$ to any ring, so, for $n\\in \\mathbb Z_\\gt0$ \n",
    "$\\phi(n) = \\sum_{k=1}^n \\phi(1) = n\\cdot 1 =n\\neq 0$  and  to deal with negative integers:  \n",
    "$\\phi(-n) = \\sum_{k=1}^n \\phi(-1) = n\\cdot -1 = -n\\neq 0$  \n",
    "\n",
    "since $\\mathbb F$ has characteristic zero.  Thus we've checked the degree 0 case and prove no degree 0 polynomial is mapped to zero.  \n",
    "\n",
    "ii.) if $p\\in \\mathbb Z\\big[\\mathbf w\\big]$ where $\\text{degree}(p)  = d \\geq 1$   \n",
    "then consider a subset $S\\in \\mathbb F$ where $\\big \\vert S\\big \\vert = t$, then by Schwarz-Zippel, the probability that a point selected uniformly at random in $S^{m}$ is zero under the image of $\\phi(p)$, our *polynomial function* , is $\\leq\\frac{d}{t}$.  Hence for any $d$, we may set $t:=d+1$  (since $\\mathbb F$ has infinite cardinality), and $\\phi(p)$ is not zero everywhere on $S$ which proves that $\\phi(p) \\neq 0$.  \n",
    "\n",
    "Thus  $\\phi$ is injective.  \n",
    "\n",
    "*viewpoint 2*  \n",
    "For our purposes, an equivalent way of formulating this is, the degree zero case is obvious.  Now for any polynomial $p\\in \\mathbb Z\\big[\\mathbf w\\big]$ with degree such that $1\\leq \\text{degree}(p) \\leq d $, to determine whether or not $d$ is the zero polynomial, it suffices to evaluate under the image of $t$ well chosen \n",
    "*distinct substitution homomorphisms*   \n",
    "\n",
    "$\\Phi_k: \\mathbb Z\\big[\\mathbf w\\big]\\longrightarrow \\mathbb F$  \n",
    "for $1\\leq k \\leq t^m$, where again the $\\Phi_k$ are chosen to be distinct and in particular evaluate at points in $S^{m}$ where $S\\in \\mathbb F$ and $\\big \\vert S\\big \\vert = t \\geq d+1$.  Per Schwartz-Zippel, the only polynomial that will evaluate to zero under every point in $S$ is the zero polynomial (where recalling that the zero polynomial has degree $-\\infty$ under some conventions, though others would say it has undefined degree).  This viewpoint is the essence of Miniature 24 in Matousek's *Thirty-three Miniatures*.  \n",
    "\n",
    "**1.) Main Argument**  \n",
    "When we consider an $r \\times n$ matrix with components some ring $R$, we may wonder whether results that we are familiar with from Linear Algebra still hold in $R$.  The answer supplied via application of the Principle of Permanence of Identities is: if we can map the problem to one involving the polynomial ring  $\\mathbb Z\\big[\\mathbf w\\big]$, then the answer is yes.  The problem explicitly shown in the chapter is to consider two $n\\times n$ matrices $X,Y$ and ask whether  \n",
    "$\\det\\big(XY\\big)=\\det\\big(X\\big)\\det\\big(Y\\big)$  \n",
    "\n",
    "This does still hold and the proof is essentially to re-write it as follows  \n",
    "i.) consider each component in $X$ as a formal variable $x_{i,j} \\in \\mathbb Z\\big[\\mathbf x\\big]$  and for $Y$ $y_{i,j} \\in \\mathbb Z\\big[\\mathbf y\\big]$. For convenience we may further relabel these so they both take values in $\\in \\mathbb Z\\big[\\mathbf w\\big]$, where $\\mathbf w$ has $2\\cdot n$ components, the first n being reserved for $x_{i,j}$ and the remaining reserved for $y_{i,j}$     \n",
    "\n",
    "ii.) Make it a question about the zero polynomial.  So we ask, is    \n",
    "$\\det\\big(XY\\big)-\\det\\big(X\\big)\\det\\big(Y\\big) =0$  \n",
    "where the LHS is the addition of two polynomials $\\in \\mathbb Z\\big[\\mathbf w\\big]$ and hence is some polynomial $p\\in \\mathbb Z\\big[\\mathbf w\\big]$.  \n",
    "\n",
    "Then we can uniformly bound the degree of $p$ as being $\\leq d:= n$.  So using viewpoint 2, it suffices to evaluate this under $t^m$  well chosen substitution homomorphisms in a convenient field of characteristic zero, say $\\mathbb C$ or $\\mathbb Q$ or $\\mathbb R$.  Working with $\\mathbb C$, we know from chapters 1-4 that  \n",
    "$\\det\\big(XY\\big)-\\det\\big(X\\big)\\det\\big(Y\\big) =0$  \n",
    "*everywhere* $in \\mathbb C$ which implies it is zero under all of our substitution homomorphisms, and hence we have confirmed that $p$ is the zero polynomial.  Finally we may may use one more substitution homomorphism to send $p$ to specific values in the ring $R$, and we recover the specific matrix in our original problem.  But a homomorphism always sends 0 to 0, so we conclude that \n",
    "$\\det\\big(XY\\big)-\\det\\big(X\\big)\\det\\big(Y\\big)=0$  \n",
    "when $X$ and $Y$ take coefficients in $R$.  \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ex 12.3.1**  \n",
    "The above argument can be applied to show that for matrices $A,B,C$ where well defined product exists,  \n",
    "\n",
    "a) associativity holds-- i.e. consider the polynomial $p\\in\\mathbb Z\\big[\\mathbf w\\big]$ where  \n",
    "$p= \\mathbf e_i^T\\Big(\\big(AB\\big)C- A\\big(BC\\big)\\Big)\\mathbf e_j$  \n",
    "for any / all standard basis vectors $\\mathbf e_i$, $\\mathbf e_j$.  Since the argument holds for arbitrary component of the above matrix, it holds for the entire matrix   \n",
    "again with each matrix coefficient being some formal value in $\\mathbf w$   \n",
    "\n",
    "b) Cayley-Hamilton, with some matrix $A$ having characteristic polynomial $g$    \n",
    "$p= \\mathbf e_i^T g \\big(A\\big)\\mathbf e_j$   \n",
    "\n",
    "c) \"Cramer's Rule\" -- by which Artin actually means the adjugate form of a matrix inverse, given on page 29, though this implies the more familiar form used in the previous exercise  \n",
    "\n",
    "d) the product rule and chain rules for formal differentiation of polynomials.  \n",
    "recall, that ex 10.Misc.2, which is shown for convenience in ('Artin_chp11.ipynb' as it helps for a difficult exercise in that chapter) gives the rule for formal differentiation of polynomials in a single variable ring, and then directly proved the product rule.  In general, even for mutlivariate polynomials, f, g, we compute the same thing two different ways  \n",
    "$\\big(f'\\cdot g + f\\cdot g'\\big) - \\big(f\\cdot g\\big)' = p \\in \\mathbb Z[\\mathbf w]$  \n",
    "and in the multivariate case, we may make use of standard basis vectors here as needed to grab a specific component  \n",
    "\n",
    "Then consider finitely many well chosen substitution homomorphisms with $\\mathbb F:=\\mathbb R$   \n",
    "(we don't have uniform bound on $d$ in this case, but we can argue indirectly -- suppose the rule wasn't true and there is some degree $f\\cdot g$ this does not hold for then we run the above and see that the above is the sum of two polynomials each of which has finite degree, has we can find some $d$ that upper bounds the degree of $p$, run our preceding argument and see $p=0$ after all, a contradiction.) \n",
    "\n",
    "A substantially identical argument proves the result for the chain rule.  \n",
    "\n",
    "The quotient rule in general is implied by the product rule and chain rule.  However a quotient with polynomials involves rational functions, not polynomials in general, so the Principle of Permanence of Identities as we've learned it does not apply.  \n",
    "\n",
    "e) The claim that a polynomial of degree $n$ has at most $n$ roots is not a claim we can model with Principle of Permanence of Identities (and chapter 11 has explicit counterexamples)  \n",
    "\n",
    "f) since polynomials are nilpotent with respect to derivative operator, we should be able to write them as a Taylor Expansion, though the details are somewhat elusive in the case of multivariate polynomials.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*application*  \n",
    "consider and matrix $C\\in \\mathbb F^{n \\times n}$ where $\\text{char}\\big(\\mathbb F\\big) \\gt n$.  \n",
    "Then *Newton's Identities* apply.  \n",
    "\n",
    "recall from Vandermonde Matrix writeup, $C$ has a characteristic polynomial   \n",
    "$g(x) = x^n + a_{n-1}x^{n-1} + a_{n-2}x^{n-2} +... + a_{1}x^{1}+ a_0$   \n",
    "noting that $a_j$ is written as a product of components in $C$  \n",
    "and for $0\\leq r \\lt n$  \n",
    "$a_{n-r}  = -\\frac{1}{r}\\Big( a_{n-r +1}\\cdot \\text{trace}\\big(C^1\\big) + a_{n-r +2}\\cdot \\text{trace}\\big( C^2\\big) + ... + a_{n}\\cdot \\text{trace}\\big(C^r\\big) \\Big)$  \n",
    "*in better form*  \n",
    "$r\\cdot a_{n-r}   +\\Big( a_{n-r +1}\\cdot \\text{trace}\\big( C^1\\big) + a_{n-r +2}\\cdot \\text{trace}\\big( C^2\\big) + ... + a_{n}\\cdot \\text{trace}\\big(C^r\\big) \\Big) = p$   \n",
    "we want to show that $p=0$  \n",
    "note $r\\in\\mathbb Z$ and this a polynomial in components in $C$ with some degree we may crudely upper bound by, say $n^n$   \n",
    "hence our Permanence of Identities argument applies.  \n",
    "\n",
    "since $r\\lt n \\lt \\text{char}\\big(\\mathbb F\\big)$  \n",
    "we know that at the end when we map back to coefficients in $\\mathbb F$, $r\\neq 0$ thus we can recover $a_{n-r}$.  (This may fail when we have small characteristic)  \n",
    "\n",
    "and for $r=n$, we have  \n",
    "\n",
    "$a_0 \\cdot  n = a_0 \\cdot \\text{trace}\\Big( \\mathbf I\\Big) = \\text{trace}\\Big(a_0 \\mathbf I\\Big) =  -\\text{trace}\\Big( a_1 \\mathbf C^{1} +  a_2 \\mathbf C^{2}  + ... + a_n \\mathbf C^{n}\\Big)$  \n",
    "which holds by Cayley Hamilton, and  $n \\lt \\text{char}\\big(\\mathbb F\\big)$, so $n\\neq 0$, which allows us to solve for $a_0$ as well.    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ex 12.4.2**   \n",
    "Let $A$ be a matrix whose entries are in the polynomial ring $\\mathbb F[t]$ and let $A'$ be obtained from $A$ by polynomial row and column operations. Relate $\\det\\big(A\\big)$ and $\\det\\big(A'\\big)$.  \n",
    "\n",
    "since we are in an integral domain, and $\\det\\big(P\\big)$, $\\det\\big(Q\\big)$ are units, they must be values $\\alpha_1,\\alpha_2 \\in \\mathbb F-\\{0\\}$, thus  \n",
    "$\\det\\big(A\\big)=\\det\\big(Q^{-1}A'P\\big)=\\det\\big(Q^{-1}\\big)\\det\\big(A'\\big)\\det\\big(P\\big)=\\alpha_1\\cdot\\alpha_2\\cdot \\det\\big(A'\\big)$  \n",
    "\n",
    "i.e. they are the same except for rescaling by a unit (i.e. an element in $\\mathbb F$)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ex 12.4.4**   \n",
    "Let $d_1, d_2,...$ be the integers referred to in Theorem (4.3) -- i.e. the diagonal elements of $A'$, where $A=Q^{-1}A'P$ for $A\\in \\mathbb Z^{m \\times n}$  --- i.e. rank normal form adapted to the ring of integers, thus $d_i$ are integers and $d_1$ divides $d_2$ which divides $d_3$ which divides $d_4$ and so on.   \n",
    "\n",
    "*prove that for* $r \\times r$ *minors of* $A$, $\\big(\\prod_{k=1}^r d_k\\big)$ *is the GCD of these determinants*   \n",
    "\n",
    "**i.)** To show it divides them, map to a square matrix, then apply Cauchy-Binet.  \n",
    "i.e. where $\\mathbf e_k$ denotes the appropriately sized standard basis vector, consider   \n",
    "\n",
    "$Y := \\bigg[\\begin{array}{c|c|c|c} \\mathbf e_{\\sigma_1} & \\mathbf e_{\\sigma_2} &\\cdots & \\mathbf e_{\\sigma_r}\\end{array}\\bigg]$  \n",
    "and  \n",
    "$X := \\bigg[\\begin{array}{c|c|c|c} \\mathbf e_{\\sigma_1'} & \\mathbf e_{\\sigma_2'} &\\cdots & \\mathbf e_{\\sigma_r'}\\end{array}\\bigg]$  \n",
    "\n",
    "WLOG we assume $A'$ is tall and skinny (if not, re-run the argument on $A^T$)  \n",
    "\n",
    "$\\text{arbitrary r x r minor} $  \n",
    "$= \\det\\Big(Y^T A X\\Big) $  \n",
    "$= \\det\\Big(Y^T\\big(Q^{-1}A'P\\big) X\\Big)$  \n",
    "$= \\det\\Big(\\big(Y^TQ^{-1}A'\\big)\\big(PX\\big)\\Big)$  \n",
    "$=\\sum_{S} \\det\\Big(\\big(Y^TQ^{-1}A'\\big)_{[m],S}\\Big)\\det\\Big(\\big(PX\\big)_{S,[m]}\\Big) $   \n",
    "\n",
    "by Cauchy-Binet  \n",
    "and $\\big(PX\\big)$ is integer valued, while  \n",
    "$\\det\\Big(\\big(Y^TQ^{-1}A'\\big)_{[m],S}\\Big)$, by inspection, each column is scaled by some $d_i$.  So by a pigeon hole argument, the largest of these is divisible by $d_r$ and the next largest must be divisible by $d_{r-1}$ and so on to the smallest being divisible by $d_1$.  \n",
    "$\\sum_{S} \\det\\Big(\\big(Y^TQ^{-1}A'\\big)_{[m],S}\\Big)\\det\\Big(\\big(PX\\big)_{S,[m]}\\Big)$  \n",
    "thus each term in the above is divisible by $\\big(\\prod_{k=1}^r d_k\\big)$ and hence $\\big(\\prod_{k=1}^4 d_k\\big)$ divides  $\\text{arbitrary r x r minor} $  \n",
    "\n",
    "**ii.)** To show that $\\big(\\prod_{k=1}^r d_k\\big)$ is the GCD, to start, we show this result for $1\\leq r\\leq n$ *provided that* $d_r \\neq 0$-- we deal with singularity at the end. We may assume **WLOG** that $A'$ is tall and skinny (if not, run the below argument on the transpose of $A$, the do the factorization.  \n",
    "\n",
    "\n",
    "$\\sum_{S} \\det\\Big(\\big(Y^TQ^{-1}A'\\big)_{[m],S}\\Big)\\det\\Big(\\big(PX\\big)_{S,[m]}\\Big)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Now  $A \\mathbf v= Q^{-1}\\Big(\\big(\\prod_{k=1}^rd_k\\big)\\big(\\mathbf \n",
    "e_k\\big)\\Big)$  \n",
    "for $k\\in\\{1,2,...,r\\}$  \n",
    "is solvable by inspection.  \n",
    "\n",
    "- - - - \n",
    "For avoidance of doubt, we have:  \n",
    "$A \\mathbf v=Q^{-1} A'\\big(P \\mathbf v\\big) =Q^{-1} A'\\mathbf w= Q^{-1}\\Big(\\big(\\prod_{k=1}^rd_k\\big)\\big(\\mathbf \n",
    "e_k\\big)\\Big)$  \n",
    "\n",
    "Suppose we are working over $\\mathbb Q$ and this becomes  \n",
    "$ A'\\mathbf w= \\big(\\prod_{k=1}^rd_k\\big)\\mathbf e_k\\Big)$  then  \n",
    "$ \\mathbf w= \\Big(\\big(\\prod_{k=1}^r d_k\\big)(A')^{-1}\\mathbf e_k\\big)\\Big)$  \n",
    "where $(A')^{-1}$ denotes the pseduo inverse, and by direct calculation the first $r$ components are integer valued so $\\mathbf w$ is integer valued.  \n",
    "- - - - \n",
    "\n",
    "This means there is some $V$ such that   \n",
    "\n",
    "$AV = \\big(\\prod_{k=1}^rd_k\\big) Q^{-1}\\bigg[\\begin{array}{c|c|c|c} \\mathbf e_{1} & \\mathbf e_{2} &\\cdots & \\mathbf e_{r}\\end{array}\\bigg] =\\big(\\prod_{k=1}^rd_k\\big)Q^{-1}S$  \n",
    "\n",
    "and again with \n",
    "$Y := \\bigg[\\begin{array}{c|c|c|c} \\mathbf e_{\\sigma_1} & \\mathbf e_{\\sigma_2} &\\cdots & \\mathbf e_{\\sigma_r}\\end{array}\\bigg]$  \n",
    "for *some* permutation set (which we'll choose later), gives us    \n",
    "\n",
    "Left multiplying each side by  $S^T$, we get  \n",
    "\n",
    "$Y^TAV = \\big(\\prod_{k=1}^rd_k\\big)Y^T Q^{-1}S$  \n",
    "\n",
    "and since $Q^{-1}$ has determinant $\\pm 1$ then $Q^{-1}S$ must have an $r\\times r$ minor of $\\pm 1$ *or else* minors that are, up to a sign, two relatively prime integers (i.e. no non-unit common factor)  -- if not then using expansion by minors, $\\det\\big(Q^{-1}\\big)$ is in some principal ideal that isn't the unit ideal which is a contradiction.  Note: this is very easy to see when $r= m-1$ though it holds for smaller $r$.  So we select $Y_1$ and $Y_2$ to 'grab' these minors   \n",
    "\n",
    "$Y_1^T AVS= \\big(\\prod_{k=1}^r d_k\\big)\\cdot Y_1^TQ^{-1}S$  \n",
    "$Y_2^T AVS= \\big(\\prod_{k=1}^r d_k\\big)\\cdot Y_2^TQ^{-1}S$  \n",
    "and with suitable integers $\\alpha_1, \\alpha_2 \\in \\mathbb Z$ we have $\\alpha_1\\cdot \\det\\big(Y_1^TQ^{-1}S\\big) +   \\alpha_2\\cdot \\det\\big(Y_2^TQ^{-1}S\\big) = 1$ thus taking determinants of each equation, rescaling the first by $\\alpha_1$ and the second by $\\alpha_2$, then adding the two equations gives   \n",
    "\n",
    "$\\alpha_1\\cdot \\det\\Big(T_1^T AVS\\big) +\\alpha_2\\det\\big(Y_2^T AVS\\big) =  \\big(\\prod_{k=1}^r d_k\\big)$  \n",
    "but applying Cauchy Binet to each of the two terms on the LHS, if there is some $\\eta \\gt \\big(\\prod_{k=1}^r d_k\\big)$ that divides each minor of $A$, then we have  $\\eta$ divides the LHS but not the RHS which is a contradiction.  \n",
    "\n",
    "**to deal with the case of**  $d_r = 0$  \n",
    "if $r$ is the maximal index where $d_r \\neq 0$, then if we consider this factorization but enlarge the ring to $\\mathbb Q$, then we see that $\\text{rank}\\big(A\\big) = r\\lt r$.  By problem \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**12.4.7**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**12.5.8**  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**12.5.9**   \n",
    "\n",
    "suppose we have a module $V$ with generators $(w_1,...,w_r)$ with presentation matrix $B$ and also generators $(v_1,...,v_m)$ with presentation matrix $A$  \n",
    "\n",
    "$M:=\\begin{bmatrix}A & -P &I_m & \\mathbf 0 \\\\ \\mathbf 0 & I_r & -Q & B\\end{bmatrix}$   \n",
    "and generators as  \n",
    "$\\bigg[\\begin{array}{c|c|c|c|c}  v_1 & \\cdots & v_{m}& w_1 & \\cdots &  w_r \\end{array}\\bigg]= \\bigg[\\begin{array}{c|c} \\mathbf V & \\mathbf W \\end{array}\\bigg]$  \n",
    "\n",
    "*(a)* is fairly immediate  \n",
    "\n",
    "*(b)*    Show that $M$ can be reduced to $A$ and $B$ by a sequence of operations of the form (5.12)   \n",
    "**note: to streamline the argument, it is convenient to use 5.12(ii) in reverse, so that we may pad w, and/or v as needed with zero generators, such that they each are the same in number, i.e. assume WLOG that r=m**     \n",
    "\n",
    "\n",
    "$\\mathbf 0 = \\bigg[\\begin{array}{c|c} \\mathbf V & \\mathbf W \\end{array}\\bigg]M $   \n",
    "\n",
    "via repeated right multiplication of the above by the transpose of type 1 elementary matrices (allowed implicitly by 5.12(i) or in general these matrices have unit determinants and hence are invertible)   \n",
    "\n",
    "\n",
    "$\\begin{bmatrix}A & -P &I_m & \\mathbf 0 \\\\ \\mathbf 0 & I_r & -Q & B\\end{bmatrix}\\to \\begin{bmatrix}A & -P &-PQ+I_m & \\mathbf 0 \\\\ \\mathbf 0 & I_r & \\mathbf 0& B\\end{bmatrix}\\to \\begin{bmatrix}A & -P &-PQ+I_m & PB \\\\ \\mathbf 0 & I_r & \\mathbf 0& \\mathbf 0\\end{bmatrix}$      \n",
    "\n",
    "which gives the first equality below  \n",
    "$\\mathbf 0 =\\bigg[\\begin{array}{c|c} \\mathbf V & \\mathbf W \\end{array}\\bigg]\\begin{bmatrix}A & -P &-PQ+I_m & PB \\\\ \\mathbf 0 & I_r & \\mathbf 0& \\mathbf 0\\end{bmatrix} = \\bigg[\\begin{array}{c|c} \\mathbf V & \\mathbf W \\end{array}\\bigg]\\begin{bmatrix}A & -P &\\mathbf 0& PB \\\\ \\mathbf 0 & I_r & \\mathbf 0& \\mathbf 0\\end{bmatrix}= \\bigg[\\begin{array}{c|c} \\mathbf V & \\mathbf W \\end{array}\\bigg]\\begin{bmatrix}A & -P & PB \\\\ \\mathbf 0 & I_r & \\mathbf 0\\end{bmatrix} $   \n",
    "\n",
    "where the second equality comes from working through the blocked multiplication, i.e. that the 3rd block 'column' reads  \n",
    "\n",
    "$\\mathbf V\\Big(-PQ+I_m\\Big) = -\\Big(\\big(\\mathbf VP\\big)Q - \\mathbf V\\Big)= -\\Big(\\big(\\mathbf W\\big)Q - \\mathbf V\\Big)= -\\Big(\\big(\\mathbf WQ\\big) - \\mathbf V\\Big)= -\\Big(\\big(\\mathbf V\\big) - \\mathbf V\\Big)=\\mathbf 0 = \\mathbf V \\mathbf 0$   \n",
    "\n",
    "Now more subtly via repeated left multiplication by type 1 matrices we generate a particularly nice upper triangular matrix $R$ (of course with 1s on the diagonal) that has $P$ is its top 'row' and 2nd 'column' block   \n",
    "\n",
    "$\\mathbf 0 =  \\left(\\bigg[\\begin{array}{c|c} \\mathbf V & \\mathbf W \\end{array}\\bigg]R^{-1}\\right)\\left(R\\begin{bmatrix}A & -P & PB \\\\ \\mathbf 0 & I_r & \\mathbf 0\\end{bmatrix}\\right) =  \\bigg[\\begin{array}{c|c} \\mathbf V' & \\mathbf W' \\end{array}\\bigg]\\begin{bmatrix}A & \\mathbf 0 & PB \\\\ \\mathbf 0 & I_r & \\mathbf 0\\end{bmatrix} $   \n",
    "\n",
    "applying 5.12(iii) we see this is equivalent to  \n",
    "\n",
    "$\\mathbf 0 =   \\bigg[\\begin{array}{c|c} \\mathbf V' \\end{array}\\bigg]\\begin{bmatrix}A & PB \\end{bmatrix}$   \n",
    "and application of 5.12(iii) is equivalent to declaring $\\mathbf W' = \\mathbf 0$  so    \n",
    "\n",
    "$\\mathbf 0 =  \\bigg[\\begin{array}{c|c} \\mathbf V' \\end{array}\\bigg]\\begin{bmatrix}A & PB \\end{bmatrix}   =\\bigg[\\begin{array}{c|c} \\mathbf V \\end{array}\\bigg]\\begin{bmatrix}A & PB \\end{bmatrix} $   \n",
    "since the block structure of $R$ implies  $\\mathbf V'=\\mathbf V$   \n",
    "\n",
    "from here there are a couple of ways to finish.  One is to consider   \n",
    "\n",
    "$\\bigg[\\begin{array}{c|c} \\mathbf V \\end{array}\\bigg]\\begin{bmatrix}A & PB \\end{bmatrix} = \\bigg[\\begin{array}{c|c} \\mathbf V \\end{array}\\bigg]\\begin{bmatrix}A & A\\end{bmatrix}\\to \\bigg[\\begin{array}{c|c} \\mathbf V \\end{array}\\bigg]\\begin{bmatrix}A & \\mathbf 0 \\end{bmatrix}= \\bigg[\\begin{array}{c|c} \\mathbf V \\end{array}\\bigg]\\begin{bmatrix}A \\end{bmatrix}$  \n",
    "because  \n",
    "$\\mathbf V \\big(PB\\big) = \\big(\\mathbf V P\\big)B= \\mathbf WB = \\mathbf 0 =\\mathbf V A$  \n",
    "\n",
    "and via more right multiplication by type 1 elementary matrices, we can reduce the 'second' $A$ to zero, then delete it using 5.12(ii)   \n",
    "\n",
    "\n",
    "**open nit:**   \n",
    "$\\mathbf V$ and $\\mathbf W$ have same number, $m=r$ which implies that $A$ and $B$ have the same number of rows... but it doesn't seem to mean they have the same number of columns....? which then causes nominal dimensional issues in the above? though these can be solved by zero padding.   \n",
    "\n",
    "A similar argument can reduce to $B$ and $\\mathbf W$.  Overall the above seems to get bogged down in symbol manipulation and it seems like there should be a better way.  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sympy as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}a & 0 & c\\\\0 & 1 & 0\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "Matrix([\n",
       "[a, 0, c],\n",
       "[0, 1, 0]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = sp.Symbol('p')\n",
    "a = sp.Symbol('a')\n",
    "c = sp.Symbol('c')\n",
    "\n",
    "R = sp.Matrix([[a,-p, c],\n",
    "               [0,1, 0]])\n",
    "R_prime = sp.Matrix([[1,p],[0,1]])\n",
    "R_prime@R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}1 & p\\\\0 & 1\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "Matrix([\n",
       "[1, p],\n",
       "[0, 1]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R_prime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}a & - p & c\\\\0 & 1 & 0\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "Matrix([\n",
       "[a, -p, c],\n",
       "[0,  1, 0]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**uniqueness of the minimal polynomial over a field**  \n",
    "In general for an $n\\times n$ matrix over a field $\\mathbb F$ we know a degree $n$ polynomial annihilates $A$ and hence $\\big\\{I,A,...,A^{n-1},A^n\\big\\}$ is linearly dependent.  Discarding the highest power we consider $\\big\\{I,A,...,A^{n-1}\\big\\}$ and for explicit computational purposes we use the $\\text{vec}$ operator and construct the explicit matrix  \n",
    "\n",
    "$B_{n-1}:=\\bigg[\\begin{array}{c|c|c|c} \\text{vec}\\big(I\\big) & \\text{vec}\\big(A\\big) &\\cdots & \\text{vec}\\big(A^{n-1}\\big) \\end{array}\\bigg]$  and via Gaussian Elimination (or checking of minors) we determine whether $B$ has a non-trivial kernel.  If $\\dim \\ker\\big(B_{n-1}\\big)=0$ then we are done and $\\big\\{I,A,...,A^{n-1}\\big\\}$ is the maximally sized linearly independent set.  If $\\dim \\ker\\big(B_{n-1}\\big)\\geq 1$ then we again discard the highest power and recurse on $\\big\\{I,A,...,A^{n-2}\\big\\}$ and continue until there is a trivial kernel. Note that this recursion must stop -- at the latest at $\\big\\{I\\big\\}$ which occurs *iff* $A=\\mathbf 0$, and of course this relation is a minimal polynomial of degree 1 as the maximally sized linearly independent set had degree 0.  \n",
    "\n",
    "In general the above recursion tells us there is a unique maximally sized linearly independent set for some particular $A$ given by $\\big\\{I,A,...,A^{d-1}\\big\\}$.  As a consequence $A^d$ (or $\\text{vec}\\big(A^d\\big)$ if the reader prefers) may be written *uniquely* as a linearly combination of of elements in that set-- and re-arranging terms, we recover the unique (monic) minimal polynomial of $A$.  No smaller degree polynomial may annihilate $A$ since $A^{d-1}$ cannot be written as a linear combination of lower powers of $A$ -- since these are all linearly independent.  And again the monic degree $d$ polynomial that annihilates $A$ must be unique since $A^d$ may be written uniquely as a linear combination of these lower order powers.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ex 12.7.19**  \n",
    "\n",
    "**claim:**  \n",
    "(a) The minimal polynomial divides the characteristic polynomial, and this holds for any choice of $\\mathbb F$    \n",
    "\n",
    "**proof:**   \n",
    "Let $p$ be the (degree n) characteristic polynomial for $A \\in \\mathbb F^{n\\times n}$  and $q$ be the minimal polynomial.  If the degree of $q$ is $n$, then $q=p$. So assume that the degree of $q$ is $r\\lt n$.  \n",
    "\n",
    "Now $p$ and $q$ must have a nontrivial common factor $f$ -- if they did not then they'd generate the unit ideal (ref chp 11 on principal ideal domains *or* the discussion of resultants in \"Artin_chp12.ipynb\" which if there is no common factor means the $Res(p,q) = \\gamma \\in \\mathbb F-\\{0\\}$ which is a unit, so rescaled by $\\gamma^{-1}$ and we get)    \n",
    "$sp +s^*q= 1$ or $\\mathbf 0 + \\mathbf 0 =s(A)p(A) + s^*(A)q(A) = I$ which is impossible.  \n",
    "\n",
    "if $\\text{degree}(f) =r$ then we are done.   \n",
    "Suppose $\\text{degree}(f) \\lt r$.  \n",
    "so $p=f\\cdot p_1$ and $q=f\\cdot q_1$  where $p_1$ has degree $\\lt n$ and $q_1$ has the same decrease in degree \n",
    "then $0\\lt \\dim\\ker\\Big(f\\big(A\\big)\\Big) \\lt n$  \n",
    "\n",
    "*rationale for lower bound on dim ker:*    \n",
    "if not, then $f\\big(A\\big) $ is invertible and  \n",
    "$\\mathbf 0 = q\\big(A\\big) = f\\big(A\\big) \\cdot q_1\\big(A\\big)$   \n",
    "$\\implies \\mathbf 0 = f\\big(A\\big)^{-1} q\\big(A\\big) =   q_1\\big(A\\big)$   \n",
    "which contradicts the minimality (of degree) of $q$ as a polynomial.  \n",
    "\n",
    "*rationale for upper bound:*    \n",
    "essentially the same -- if the kernel had dimension n then $f$, with degree $\\lt r$, annihilates $A$ but this is strictly less than the degree of the minimal polynomial $q$, a contradiction.  \n",
    "\n",
    "Now repeat the above process  \n",
    "$p_1$ and $q_1$  must have a non-trivial common factor -- if not then they generate the unit ideal and  \n",
    "$s_1p_1 +s_1^*q_1= 1$  or  \n",
    "\n",
    "$=\\mathbf 0 $  \n",
    "$=\\mathbf 0 + \\mathbf 0$  \n",
    "$=s_1\\big(A\\big)p\\big(A\\big)+s_1^*\\big(A\\big)q\\big(A\\big)$  \n",
    "$=s_1\\big(A\\big)\\Big(f\\big(A\\big)p_1\\big(A\\big)\\Big)+s_1^*\\big(A\\big)\\Big(f\\big(A\\big)q_1\\big(A\\big)\\Big)$  \n",
    "$=f\\big(A\\big)s_1\\big(A\\big)p_1\\big(A\\big)+f\\big(A\\big)s_1^*\\big(A\\big)q\\big(A\\big)$  \n",
    "$=f\\big(A\\big)\\Big(s_1\\big(A\\big)p_1\\big(A\\big)+s_1^*\\big(A\\big)q\\big(A\\big)\\Big)$   \n",
    "$= f\\big(A\\big)I$  \n",
    "$=f\\big(A\\big)$  \n",
    "\n",
    "to finish \n",
    "(i)  we can repeatedly apply this process until we've reduced to $q$ to degree 0 (which is a unit since we are working over a field).  \n",
    "(ii) alternatively we can go back to \"Now $p$ and $q$ must have a nontrivial common factor $f$ \" and re-state it as  \"now $p$ and $q$ must have a nontrivial common factor *and let* $f$ be the maximal degree monic polynomial that is of a common factor of each\".  The proof then proceeds in essentially the manner stated above except the finish is by contradiction -- if $\\text{degree}\\big(f\\big)\\lt r$ then we find $q_1$ and $p_1$ must have a common factor (as the end of the above argument shows they can't generate the unit ideal) but this contradicts the maximal degree of $f$.   \n",
    "\n",
    "In either case we reach the same conclusion, $f=q$    \n",
    "\n",
    "$p= f\\cdot p_1 = q\\cdot p_1$  \n",
    "\n",
    "*a much shorter approach*   \n",
    "consider the one variable polynomials of the form  \n",
    "$s(x)$ that annihilate $A$  \n",
    "(i.e. we are looking in effect at the kernel of the homomorphism $x\\mapsto A$)  \n",
    "this forms an ideal, and our coefficients are in a field and hence this is a principal ideal.  That means there is some lowest degree polynomial that generates this ideal.  And once we insist on said polynomial being in monic form it is necessarily the minimal polynomial.  And since the characteristic polynomial lives in this ideal (per Cayley-Hamilton), the characteristic polynomial is necessarily divisible by the sole generator of this ideal, the minimal polynomial.   \n",
    "\n",
    "\n",
    "\n",
    "(b)  \n",
    "For any eigenvector that exists in the field, we have  \n",
    "$A\\mathbf x = \\lambda \\mathbf x$  \n",
    "and  \n",
    "$\\mathbf 0 = q\\big(A\\big)\\mathbf x =  q\\big(\\lambda\\big) \\mathbf x \\implies \\lambda \\text{ is a root of q}$   \n",
    "\n",
    "(c)  \n",
    "prove that linear operator $T$ is diagonalizable *iff* the minimal polynomial $m$ splits into linear factors with no repeated roots.  \n",
    "see Sylvester Rank Inequality section of 'Artin_chp4.ipynb' for an elementary proof of this fact.  \n",
    "\n",
    "*remark:*  \n",
    "we can also get to this result via  \n",
    "$\\text{diagonalizable} \\implies \\text{above criterion}$  \n",
    "is immediate  \n",
    "\n",
    "$\\text{above criterion}\\implies \\text{diagonalizable} $  \n",
    "1.) observing that if char poly = min poly with $n$ distinct eigenvalues then matrix is diagonalizable  (linear independence of eigenspaces / basic chp 4 items)  \n",
    "2.) the below argument that the Companion Matrix's minimal polynomial is equal to its characteristic polynomial  \n",
    "3.) run rational canonical form on a matrix and consider applying $m$ to each block, throwing out factors that are nonsingular... this allows us to apply (1) repeatedly to each Companion block and thus each block is similar to a diagonal matrix hence the original matrix is similar to a diagonal matrix.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(working over $\\mathbb C$-- see remark at the end for arbitrary fields)  \n",
    "**The Companion Matrix's minimal polynomial is the same as its characteristic polynomial.**    \n",
    "\n",
    "Pick some matrix $A$ that is upper triangular and by construction has minimal polynomial $p(x) = x^d + c_{d-1}x^{d-1} +....+ c_1 x + c_0$ with degree $d$.  Now select the associated companion matrix $C$.  \n",
    "\n",
    "Rearranging terms of $p\\big(A\\big)=\\mathbf 0$ we have  \n",
    "$A^d =-\\big(c_{d-1}A^{d-1} +....+ c_1 A + c_0I\\big)$  \n",
    "and being the *minimal polynomial* we know the $d$ terms on the RHS are linearly independent (if not, there would be an even lower degree polynomial to annihilate $A$)  \n",
    "\n",
    "denoting $v_k:=A^k$   \n",
    "**OPEN ITEM:  IS THE BELOW BETTER INTERPRETED IN TERMS OF VECTORS OR MODULES**  \n",
    "$\\bigg[\\begin{array}{c|c|c|c} v_0 & v_1 &\\cdots & v_{d-1} \\end{array}\\bigg]C=\\bigg[\\begin{array}{c|c|c|c} v_1 & v_2 &\\cdots & v_{d} \\end{array}\\bigg] $  \n",
    " \n",
    " $\\bigg[\\begin{array}{c|c|c|c} v_0 & v_1 &\\cdots & v_{d-1} \\end{array}\\bigg]C^2=\\bigg[\\begin{array}{c|c|c|c} v_2 & v_3 &\\cdots & v_{d+1} \\end{array}\\bigg] $   \n",
    "and in general  \n",
    "$\\bigg[\\begin{array}{c|c|c|c} v_0 & v_1 &\\cdots & v_{d-1} \\end{array}\\bigg]C^{k}=\\bigg[\\begin{array}{c|c|c|c} v_{k} & v_{k+1} &\\cdots & v_{k+d-1} \\end{array}\\bigg] $   \n",
    "\n",
    "Now since $p$ is the characteristic polynomial of $C$ we have, by Cayley-Hamilton  \n",
    "$\\bigg[\\begin{array}{c|c|c|c} v_0 & v_1 &\\cdots & v_{d-1} \\end{array}\\bigg]p\\big(C\\big)=\\bigg[\\begin{array}{c|c|c|c} v_1 & v_2 &\\cdots & v_{d} \\end{array}\\bigg]\\mathbf 0 = \\mathbf 0 $   \n",
    " \n",
    "now suppose that there is a polynomial $q$ with degree $\\leq d-1$ that annihilates $C$. Then computing the same thing two different ways (where $C^0$ is understood to be $I$):  \n",
    "\n",
    "$\\mathbf 0 $  \n",
    "$= \\bigg[\\begin{array}{c|c|c|c} v_0 & v_1 &\\cdots & v_{d-1} \\end{array}\\bigg]q\\big(C\\big) $  \n",
    "$=  \\bigg[\\begin{array}{c|c|c|c} v_0 & v_1 &\\cdots & v_{d-1} \\end{array}\\bigg]\\sum_{k=0}^{d-1} \\alpha_k^k C^k $  \n",
    "$= \\sum_{k=0}^{d-1} \\bigg[\\begin{array}{c|c|c|c} v_0 & v_1 &\\cdots & v_{d-1} \\end{array}\\bigg] \\alpha_k^k C^k $  \n",
    "$= \\sum_{k=0}^{d-1} \\bigg[\\begin{array}{c|c|c|c} \\alpha_k ^k v_{k} & \\alpha_k ^k v_{k+1} &\\cdots & \\alpha_k ^k v_{k+d-1} \\end{array}\\bigg] $  \n",
    "\n",
    "hence examining the first 'vector' this reads  \n",
    "$\\sum_{k=0}^{d-1} \\alpha_k^k v_{k}=0$  or  \n",
    "$\\sum_{k=0}^{d-1} \\alpha_k^k\\cdot A^k=\\mathbf 0$  \n",
    "\n",
    "since $\\big\\{I,A,...,A^{d-1}\\big\\}$ are linearly independent $\\implies \\alpha_0=\\alpha_1=....=\\alpha_{d-1}=0$ hence $q$ is identically zero and $p$ is the minimal polynomial of $C$.  \n",
    "\n",
    "**remark:**  \n",
    "when working over $\\mathbb C^{n\\times n}$ any Companion Matrix $C$ has a characteristic polynomial $p$ which must be its minimal polynomial because we can easily construct some upper triangular $\\mathbb A$ that has $p$ as its minimal polynomial (in effect via Jordan blocks though we need not know much about Jordan Forms).  Thus the above argument applies and $C$ has its minimal polynomial equal to its characteristic polynomial. Using the vec operator we may restate this as  \n",
    "\n",
    "$H:=\\bigg[\\begin{array}{c|c|c|c} \\text{vec}\\big(I\\big) & \\text{vec}\\big(C\\big) &\\cdots & \\text{vec}\\big(C^{n-1}\\big) \\end{array}\\bigg]$   \n",
    "\n",
    "and $\\text{rank}\\big(H\\big) = n$  or $H$ is injective or $H$ has a left inverse or $H$ has at least one $n\\times n$ minor that is non-zero (ex 4.Misc.12 -- see 'Artin_chp4.ipynb'). Or the latter criterion could be interpret in terms of ex 12.2.7 and applied to surjective $H^T$ -- its minors generate the unit ideal (but since we are working over a field recall the only ideals are the unit ideal and the zero ideal).  \n",
    "\n",
    "This argument immediately implies the minimal polynomial of a Companion Matrix is its characteristic polynomial for any subfield of $\\mathbb C$.  Now for arbitrary algebraically closed $\\mathbb F$ the same argument works -- as does for any subfield $\\mathbb K$ of said algebraically closed $\\mathbb F$. **Chapter 13 item to be confirmed-- That is for any arbitrary field, we may run the above argument over its algebraic closure F to confirm that the minimal polynomial is the characteristic polynomial for a Companion matrix in an arbitrary field K.**  \n",
    "\n",
    "\n",
    "# a MUCH simpler approach  \n",
    "\n",
    "If the $n\\times n$ Companion Matrix has char poly $p$ and min poly $q$, then we know  \n",
    "$\\text{degree}\\big(q\\big)\\leq n = \\text{degree}\\big(p\\big)$   \n",
    "The simple argument below shows $n\\leq \\text{degree}\\big(q\\big)$ hence $\\text{degree}\\big(q\\big) = n\\implies q=p$ (justification of the implication: (a) monic $p$ is a multiple of monic $q$ as shown preceding ex 12.7.19  or (b) a couple blocks back showed the min poly is unique and since $p=q$ is sufficient, uniqueness of $q$ means $p=q$ is necessary as well).  \n",
    "\n",
    "to prove  \n",
    "$n\\leq \\text{degree}\\big(q\\big)$  \n",
    "with the convention  $C^0:= I$   \n",
    "it suffices to show that the powers  $\\big\\{C^0,C^1,C^2,...,C^{n-1}\\big\\}$ are linearly independent.  I.e. there cannot be an annihilating monic polynomial of degree $\\leq n-1$.  To do this consider,    \n",
    "\n",
    "$I\\mathbf e_1 = \\mathbf e_1$  \n",
    "$C\\mathbf e_1 = \\mathbf e_2$  \n",
    "$C^2\\mathbf e_1 = C\\mathbf e_2 =\\mathbf e_3$   \n",
    "and in general for $k \\in \\{1,2,...,n-1\\}$   \n",
    "$C^k\\mathbf e_1 =C\\big(C^{k-1}\\mathbf e_1\\big) = C\\mathbf e_{k} = \\mathbf e_{k+1}$  \n",
    "\n",
    "that is, the based on just considering the first column of $C^k$ for $k \\in \\big\\{0,1,2,...,n-1\\big\\}$ we see the following set of column vectors    \n",
    "$\\{\\mathbf e_1,\\mathbf e_2,...,\\mathbf e_n\\}$  \n",
    "\n",
    "which are linearly independent hence $\\big\\{C^0,C^1,C^2,...,C^{n-1}\\big\\}$ is a linearly independent set.  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Jordan block for a particularly simple Companion Matrix**  \n",
    "- -  - - \n",
    "*note:*   \n",
    "The below directly derives the Jordan Form for specific Companion Matrix.  In combination with the Rational Canonical Form derived in this chapter, this implies the general Jordan Canonical Form when working over an algebraically closed field.  It involves combining the knowledge of the minimal polynomial of $C$, along with techniques from chapter 1-4 -- i.e. working through rank implications for nilpotence and applying type 1 and type 3 elementary matrices -- that is it.  \n",
    "- -  - - \n",
    "Working over $\\mathbb C$, consider the Companion Matrix $C$ for    \n",
    "$p(x) = (x-\\mu)^m$  \n",
    "\n",
    "Then $C$ is $m \\times m$ has $\\mu$ as an eigenvalue with algebraic multiplicity $m$ and has $p$ as its minimal polynomial.  This implies $\\big(\\mu I -C\\big)$ is nilpotent and $\\big(\\mu I -C\\big)^k\\neq \\mathbf 0$ for $k\\lt n$ (if this was not the case then $C$ would have a minimal polynomial with degree $\\lt m$ which we know by the above is impossible). Combining this with ex 4.3.6 (see 'Artin_chp4.ipynb')   \n",
    "\n",
    "we thus have subspaces   \n",
    "$V_k := \\ker\\Big(\\big(\\mu I -C\\big)^k\\Big)$   \n",
    "$W_k := \\text{image}\\Big(\\big(\\mu I -C\\big)^k\\Big)$   \n",
    "\n",
    "$0=\\text{rank}\\Big(\\big(\\mu I -C\\big)^m\\Big)\\lt \\text{rank}\\Big(\\big(\\mu I -C\\big)^{m-1}\\Big)\\lt ...\\lt  \\text{rank}\\Big(\\big(\\mu I -C\\big)\\Big)\\lt m$  \n",
    "equivalently  \n",
    "$\\mathbf 0 =V_m \\subset V_{m-1}\\subset ... \\subset V_1 \\subset \\mathbb C^m$  \n",
    "where the above should be interpreted as a proper subsetting  \n",
    "\n",
    "applying rank-nullity to the inequalities:  \n",
    "$m=\\text{dim}\\ker\\Big(\\big(\\mu I -C\\big)^m\\Big)\\gt \\text{dim}\\ker\\Big(\\big(\\mu I -C\\big)^{m-1}\\Big)\\gt ...\\gt  \\text{dim}\\ker\\Big(\\big(\\mu I -C\\big)\\Big)=1\\gt 0$  \n",
    "\n",
    "and $V_1$ is uniquely specified by the sole eigenvector (unique up to a scalar)  of $C$ (and up to transposition we know the exact form of this -- it is the moment curve / a row slice of the Vandermonde Matrix).  \n",
    "\n",
    "so we choose a somewhat arbitrary set of generators for these $V_k$   \n",
    "$\\mathbf B = \\bigg[\\begin{array}{c|c|c|c|c} \\mathbf v_1 & \\mathbf v_2 &\\cdots &\\mathbf v_{m-1} & \\mathbf v_{m} \\end{array}\\bigg]$   \n",
    "\n",
    "and combine this with the subsetting of $V_k$.  I.e. in a manner resembling chapter 4 results, consider the image of  $\\big(\\mu I -C\\big)$ applied to the basis $\\mathbf B$.   \n",
    "\n",
    "$\\big(\\mu I -C\\big)\\mathbf B = \\mathbf B A$  \n",
    "we know that the first column of $A$, $\\mathbf a_1 = \\mathbf 0$ since $\\big(\\mu I -C\\big)$ annihilates $\\mathbf v_1$, and *only* vectors $\\propto \\mathbf v_1$.  \n",
    "\n",
    "$\\mathbf a_2 =\\alpha\\cdot \\mathbf e_1$  because $\\big(\\mu I -C\\big)^2\\mathbf v_2=\\mathbf 0$, and up to rescaling $\\mathbf v_1$ is the only vector annihilated by a subsequent application of $\\big(\\mu I -C\\big)$.  By computing the rank of each (m-1), we know that $\\alpha \\neq 0$, so we may multiply each side by an elementary type 3 matrix \n",
    "\n",
    "$\\big(\\mu I -C\\big)\\mathbf BE_3^{(2)} = \\mathbf B\\big( AE_3^{(2)}\\big)= \\mathbf BE_3^{(2)}\\big((E_3^{(2)})^{-1} AE_3^{(2)}\\big)$  \n",
    "# CLEANUP NEEDED AROUND HERE as shown in sympy calcs, more type 3s needed to maintain this under similarity transform  \n",
    "with $-1\\cdot \\alpha^{-1}$ on the 2nd diagonal spot.  The results is our new column $\\mathbf a_2' = -\\mathbf e_1$.  Now $\\mathbf a_3$ may be a linear combination of $\\mathbf e_1$ and $\\mathbf e_2$  (any other standard basis vectors imply nontrivial linear combinations of $\\mathbf v_j$ for $j\\geq 3$ which cannot sum to zero, by linear independence and are not annihilated by $\\big(\\mu I -C\\big)^2$ i.e. they are $\\notin V_2$).  From here we may right multiply by an elementary matrix of type 1 to clear out any non-zero coefficient associated with $\\mathbf e_1$.  And as before, checking rank, this implies a non-zero coefficient $\\beta$ in front of $\\mathbf e_2$ and as before we right multiply by an type 3 elementary matrix with $-1\\cdot \\beta^{-1}$ in the 3rd diagonal spot giving us   \n",
    "$\\big(\\mu I -C\\big)\\mathbf BE_3^{(2)}E_1^{(3)}E_3^{(3)} = \\mathbf B \\big(AE_1^{(3)}E_3^{(3)}\\big)$  \n",
    "as a result the new 3rd column for  \n",
    "$\\big(AE_1^{(3)}E_3^{(3)}\\big)$ is given by $\\mathbf a_3' = -1\\cdot \\mathbf e_2$  \n",
    "\n",
    "and this process continues via iteration or induction -- for arbitrary column $j$ we see that  \n",
    "$\\mathbf a_j = \\sum_{k=1}^{j-1}\\gamma_k \\mathbf e_k$   \n",
    "because $\\mathbf v_i =\\mathbf B\\mathbf e_i$ for $i\\geq j$ are not in $V_i$ i.e. they are linearly independent but not annihilated by $\\big(\\mu I -C\\big)^{j-1}$ but we know that $j-1$ more applications of $\\big(\\mu I -C\\big)$ annihilates the $j$th column.  \n",
    "\n",
    "As before we may eliminate any $\\gamma_k$ for $k\\lt j-1$ by right multiplying by type 1 elementary matrices and finally check rank to confirm $\\gamma_{j-1}\\neq 0$ and then right multiply by a type 3 elementary matrix with $-1\\cdot \\gamma_{j-1}^{-1}$ in its $j$th diagonal component.  The end result is the same and $\\mathbf a_j'=-\\mathbf e_{j-1}$.  \n",
    "\n",
    "After right multiplying by all of these type 1 and type 3 elementary matrices we have  \n",
    "$\\mathbf B \\mapsto \\mathbf B'$  \n",
    "and  \n",
    "\n",
    "$\\big(\\mu I -C\\big)\\mathbf B' = \\mathbf B' A'$  \n",
    "where $A'$ is a nilpotent matrix that is zero everywhere except it has -1s on the super diagonal, i.e. $\\mathbf a_1' =\\mathbf 0$ for $j\\in\\{2,3,...,m\\}$  $\\mathbf a_j' = -\\mathbf e_{j-1}$  \n",
    "\n",
    "(Note: Artin's approach actually derives the transpose of $A'$. This is non-standard as triangular matrices are typically used in the Jordan canonical form.  However we could recover this by simply running the argument on $\\big(\\mu I -C\\big)^T$ and mirroring the closing steps below.)  \n",
    "\n",
    "Now $\\mathbf B'$ is $m \\times m$ with rank m (over a field, in this case $\\mathbb C$), so we right multiply each side by $(\\mathbf B')^{-1}$ to see  \n",
    "\n",
    "$\\big(\\mu I -C\\big) = \\mathbf B'\\mu I(\\mathbf B')^{-1} -C = \\mathbf B' A'(\\mathbf B')^{-1}$  \n",
    "or  \n",
    "$\\mathbf B'\\big(\\mu I + N\\Big)(\\mathbf B')^{-1}=\\mathbf B'\\big(\\mu I -  A'\\Big)(\\mathbf B')^{-1}= \\mathbf B'\\mu I(\\mathbf B')^{-1}  - \\mathbf B' A'(\\mathbf B')^{-1} = C $  \n",
    "\n",
    "where at the end we re-label $N:=-A'$ and $N$ is the standard nilpotent matrix with zeros everywhere except ones on its superdiagonal.  \n",
    "\n",
    "so $P:=\\mathbf B'$   \n",
    "$ C = P\\big(\\mu I_m + N\\Big)P^{-1} $  \n",
    "is the Jordan Canonical Form of our Companion Matrix $C$ associated with $p(x) = (x-\\mu)^m$  \n",
    "\n",
    "via the use of $P$ and blocked multiplication we now have an exact method of showing the Rational Canonical Form implies the Jordan Canonical Form, over an algebraically closed field.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**remark:**  \n",
    "the blocked structure here allows an exact mapping between the Rational Canonical Form (RCF) and the Jordan Canonical Form (JCF).  The blocked structure of the RCF gives a simple proof that the the generalized eigenvectors associated with distinct eigenvalues $\\lambda \\neq \\mu$ are linearly independent.  Other proofs (i) apply Weak Nullstellansatz to $(\\lambda-x)^{m_1}$ and $(\\mu-x)^{m_2}$ -- they generate the unit ideal so substitute linear operator $T$ in for $x$ and consider some $\\mathbf v$ in the kernel of each we'd get $I\\mathbf v= q_1\\cdot (\\lambda-T)^{m_1}\\mathbf v + q_2\\cdot (\\mu-x)^{m_2}\\mathbf v = \\mathbf 0 + \\mathbf 0$, and (ii) this result can also be shown by carefully manipulating Sylvester's Rank Inequality and Cayley Hamilton.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**cautionary note:**  \n",
    "it is sometimes claimed that two matrices are similar *iff* they have the same minimal polynomial  \n",
    "but *this is NOT true*.  E.g.  \n",
    "\n",
    "$A = \\left[\\begin{matrix}0 & 1 & 0 & 0\\\\0 & 0 & 0 & 0\\\\0 & 0 & 0 & 1\\\\0 & 0 & 0 & 0\\end{matrix}\\right]$  \n",
    "\n",
    "$B = \\left[\\begin{matrix}0 & 1 & 0 & 0\\\\0 & 0 & 0 & 0\\\\0 & 0 & 0 & 0\\\\0 & 0 & 0 & 0\\end{matrix}\\right]$  \n",
    "each have the minimal polynomial $p(x) = x^2$  but  \n",
    "$\\text{rank}\\big(A\\big) = 2\\neq 1 =\\text{rank}\\big(B\\big)$  \n",
    "and similarity transforms preserve rank, so we conclude these two matrices are *not* similar  \n",
    "\n",
    "the the if and only if condition, when working over an Algebraically closed field comes down to having the same Jordan Canonical Form.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**claim:**  \n",
    "generalized eigenspaces corresponding to $\\mu\\neq \\lambda$ have zero intersection (i.e. are direct summable) when working over an algebraically closed field.  \n",
    "\n",
    "Proof 1: use rational canonical form derived in chapter -- via similarity we can see this by inspection with standard basis vectors  \n",
    "\n",
    "Proof 2: use weak nullstellansatz (or even more simply refer to the results developed in my 'Artin_chp11.ipynb' if two single variable $p_1,p_2\\in \\mathbb F[x]$ polynomials have no common factors then their resultant is non-zero, i.e. is a unit, $c \\in\\mathbb F-\\{0\\}$ but the resultant may also be written as $q_1(x)p_1(x)+ q_2(x)p_2(x)= c\\neq 0$.  In our particular case we have $p_1(x) = (\\mu-x)^k$ and $p_2(x) = (\\lambda - x)^r$  The kernels cannot have a common non-zero vector -- substitute in $T$ for $x$ and $I$ for $1$.  (We may also recall from chapter 11 that $p(x)$ lives in a PID and get the result from there.)   \n",
    "\n",
    "Proof 3: pushing through results from 'Artin_chp4.ipynb', this can be done with Cayley Hamilton and Sylvester's Rank Inequality.  In general for more details on proofs 2 and 3, see:  \n",
    "\n",
    "https://math.stackexchange.com/questions/3878092/prove-that-the-intersection-of-2-generalised-eigenspaces-is-the-zero-space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**extension:**  \n",
    "Direct derivation of Jordan Form for arbitrary $n\\times n$ Companion Matrix (over an algebraically closed field). This follows almost immediately from the fact that the minimal polynomial is still equal to the characteristic polynomial, the fact that generalized eigenspaces have zero intersection, and then mimicking our earlier proof for the simple case of $C$ having the characteristic polynomial $(x-\\mu)^k$.  \n",
    "\n",
    "There is however, some benefit to be had from working through the mechanics, in particular when there are two distinct factors / spaces, so we do so below.  \n",
    "\n",
    "\n",
    "Suppose we have a matrix in $A\\in \\mathbb F^{n\\times n}$ with characteristic polynomial (which is monic)    \n",
    "$p(x) =q_1(x)\\cdot q_2(x)$  \n",
    "where monic $q_1$ and $q_2$ have no common factors -- i.e. non-zero resultant, and the polynomials have degree $m\\geq $ and $n-m\\geq $ respectively.      \n",
    "\n",
    "Then by Cayley-Hamilton   \n",
    "$p\\big(A\\big) = q_1\\big(A\\big)\\cdot q_2\\big(A\\big) = q_2\\big(A\\big)\\cdot q_1\\big(A\\big) $  \n",
    "(note the role of commutativity in what follows... each is a linear combination of powers of $A$ and necessarily commutes with $A$ and hence any polynomial of $A$, but the result is nevertheless quite useful.)  \n",
    "\n",
    "$\\dim \\ker \\Big(q_1\\big(A\\big)\\Big) = m$  \n",
    "$\\dim \\ker \\Big(q_2\\big(A\\big)\\Big) = n-m$  \n",
    "\n",
    "- - - -  \n",
    "*note:*  as of the time of this writing in chapter 12, your author knows that in an algebraically closed field this holds, e.g. by the above stated/linked result on independent generalized eigenspaces e.g. with Sylvester Rank Inequality.  For arbitrary fields, we may consider the algebraic closure of F (a chapter 13 topic) and note that the rank of $q_i\\big(A\\big)$ is the same in either the original field or its algebraic closure  (ref. ex 4.Misc.10 in 'Artin_chp4.ipynb' noting that the determinants don't change with a field extension) and by rank-nullity the dimensions of the kernels don't change either.  There may be a more direct approach though it isn't known by your author at this time.     \n",
    "- - - -  \n",
    "by our adapting the prior 2nd or 3rd approach to proving linear independence of generalized eigenspaces (either with the unit ideal argument, or re-running with Sylvester's Rank Inequality)  we know  \n",
    "\n",
    "$\\ker \\Big(q_1\\big(A\\big)\\Big)\\bigcap \\ker \\Big(q_2\\big(A\\big)\\Big) = \\big\\{\\mathbf 0\\big\\}$  \n",
    "\n",
    "our vector space $V = \\mathbb F^n$ is a direct sum of these two kernels.  To make the result explicit, we select $\\big\\{\\mathbf v_1,...,\\mathbf v_m\\big\\}$ to be a basis for the first kernel and $\\big\\{\\mathbf v_{m+1},...,\\mathbf v_n\\big\\}$ to be a basis for the second one, and collect these in a hypervector (which is a matrix since we are in a coordinate vector space)   \n",
    "\n",
    "$\\mathbf B = \\bigg[\\begin{array}{c|c|c|c|c|c} \\mathbf v_1 &\\cdots &\\mathbf v_{m} & \\mathbf v_{m+1}&\\cdots &\\mathbf v_{n} \\end{array}\\bigg]$  \n",
    "\n",
    "Then  \n",
    "\n",
    "$q_1\\big(A\\big)\\mathbf B = \\bigg[\\begin{array}{c|c|c|c|c|c} \\mathbf 0 &\\cdots &\\mathbf 0 & *&\\cdots &* \\end{array}\\bigg]= \\left[\\begin{matrix}\\mathbf 0_{m\\times m} & * \\\\\\mathbf 0 & *\\end{matrix}\\right]=\\mathbf B \\left[\\begin{matrix}\\mathbf 0_{m\\times m} & \\mathbf 0 \\\\\\mathbf 0 & Z_{m-n\\times m-n}\\end{matrix}\\right]$  \n",
    "That is we'll show the top right corner of the matrix on the right must be $\\mathbf 0$ which then implies  \n",
    "$m-n = n - \\dim \\ker \\Big(q_1\\big(A\\big)\\Big)= \\text{rank}\\Big( q_1\\big(A\\big)\\Big) =\\text{rank}\\Big( q_1\\big(A\\big)\\mathbf B\\Big)= \\text{rank}\\Big( \\mathbf B\\left[\\begin{matrix}\\mathbf 0_{m\\times m} & \\mathbf 0 \\\\\\mathbf 0 & Z_{m-n\\times m-n}\\end{matrix}\\right]\\Big)=\\text{rank}\\Big(Z_{m-n\\times m-n}\\Big)$  \n",
    "since $\\mathbf B$ is invertible (ref 'guiding inequality' in 'Artin_chp4.ipynb')   \n",
    "\n",
    "Now consider $r\\geq m+1$  \n",
    "$q_1\\big(A\\big)\\mathbf v_r = \\mathbf B\\Big(\\big(\\sum_{k=1}^m \\alpha_k \\mathbf e_k\\big)  +\\big(\\sum_{j=m+1}^n \\alpha_j \\mathbf e_j\\big) \\Big) = \\big(\\sum_{k=1}^m \\alpha_k \\mathbf v_k\\big)  +\\big(\\sum_{j=m+1}^n \\alpha_j \\mathbf v_j\\big) $   \n",
    "suppose some $\\alpha_k \\neq 0$, then   \n",
    "$\\mathbf 0 = q_2\\big(A\\big)q_1\\big(A\\big)\\mathbf v_r= q_2\\big(A\\big)\\big(\\sum_{k=1}^m \\alpha_k \\mathbf v_k\\big)  +q_2\\big(A\\big)\\big(\\sum_{j=m+1}^n \\alpha_j \\mathbf v_j\\big)= q_2\\big(A\\big)\\big(\\sum_{k=1}^m \\alpha_k \\mathbf v_k\\big)  + \\mathbf 0 \\neq 0$  \n",
    "since $\\big(\\sum_{k=1}^m \\alpha_k \\mathbf v_k\\big)\\neq 0$  \n",
    "because the $\\mathbf v_k$ are linearly independent and not all coefficients are zero, and these vectors live in  \n",
    "$\\ker \\Big(q_1\\big(A\\big)\\Big)$ and we know $\\ker \\Big(q_1\\big(A\\big)\\Big)\\bigcap \\ker \\Big(q_2\\big(A\\big)\\Big) = \\big\\{\\mathbf 0\\big\\}$    \n",
    "\n",
    "$\\mathbf 0 \\neq \\mathbf 0$ would be a contradiction -- i.e. this proves   \n",
    "$q_1\\big(A\\big)\\mathbf B = \\mathbf B\\left[\\begin{matrix}\\mathbf 0 & \\mathbf 0 \\\\\\mathbf 0 & Z_{m-n\\times m-n}\\end{matrix}\\right]$  \n",
    "\n",
    "the labeling of $q_1$ and $q_2$ were arbitrary so it immediately follows that  \n",
    "$q_2\\big(A\\big)\\mathbf B = \\mathbf B\\left[\\begin{matrix}Y_{m\\times m} & \\mathbf 0 \\\\\\mathbf 0 & \\mathbf 0\\end{matrix}\\right]$  \n",
    "\n",
    "Now the above *also* implies the nice blocked structure   \n",
    "$A\\mathbf B = \\mathbf B\\left[\\begin{matrix}Y_{m\\times m}^{(1)} & \\mathbf 0 \\\\\\mathbf 0 & Y_{n-m\\times n-m}^{(2)}\\end{matrix}\\right]$  \n",
    "\n",
    "to verify this, again suppose that e.g. the top right block is $X \\neq \\mathbf 0$.  Taking advantage of commutativity  we have  \n",
    "$\\mathbf B\\left[\\begin{matrix}* & \\mathbf 0\\\\* & Y_{n-m\\times n-m}^{(2)}\\end{matrix}\\right] = A\\Big( q_2\\big(A\\big)\\mathbf B\\Big) = q_2\\big(A\\big)\\Big(A\\mathbf B\\Big) = A\\mathbf B\\left[\\begin{matrix}* & X \\\\ * & Y_{n-m\\times n-m}^{(2)}\\end{matrix}\\right]$  \n",
    "but $\\sum_{k=1}^m x_1 \\mathbf v_1\\notin \\ker\\Big(A\\Big)$  \n",
    "a contradiction.  \n",
    "\n",
    "- - - - \n",
    "*technical nit:*  \n",
    "The above obviously holds if $A$ is non-singular.  What if $A$ is singular and $q_2$ has a root of zero -- then the above wouldn't necessarily hold.  However $q_1$ and $q_2$ don't have common nontrivial factors hence the above argument *would* work on $q_1$. The idea is to re-run on $A':= A-\\gamma I$ where $\\gamma $ is not a root of $q_2$.  Technically if $\\big \\vert \\mathbb F\\big \\vert \\leq \\text{degree}(q_2)$ we may need to do field extensions until the resulting field has cardinality $\\gt \\text{degree}(q_2)$, *then* re-run the above argument on $A':= A-\\gamma I$ where $\\gamma$ is not a root of $q_2$. In any case re-running with $A'$ doesn't change $X$ and shows $X=\\mathbf 0$ in the extended field, but $A$ and $\\mathbf B $ exist in our base field, so $X=\\mathbf 0$ there as well.  \n",
    "- - - -  \n",
    "\n",
    "\n",
    "The argument essentially immediately generalizes to e.g.  \n",
    "$p(x) =q_1(x)\\cdot q_2(x)\\cdot q_3(x)\\cdot...\\cdot q_m(x) $  \n",
    "where each monic polynomial has no common factor.  (In fact we should be able to see this without calculation using associativity and commutativity of these products.)   \n",
    "\n",
    "**now suppose F is algebraically closed** \n",
    "then each polynomial is of the form  \n",
    "$q_k(x) = (x-\\lambda_j)^k_j$  \n",
    "and we can factor our characteristic polynomial  \n",
    "$p(x) = \\prod_{k=1}^m q_k(x) = \\prod_{k=1}^m(x-\\lambda_j)^{k_j}$  \n",
    "where $p$ has $m$ distinct roots/ eigenvalues  \n",
    "\n",
    "with little modification, the above argument tells us that  \n",
    "$A\\mathbf B = \\mathbf B \\left[\\begin{matrix}Y_1 & \\mathbf 0 & \\cdots  & \\mathbf 0\\\\\\mathbf 0 & Y_2 & \\cdots & \\mathbf 0\\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ \\mathbf 0 & \\mathbf 0 & \\mathbf 0 & Y_m\\end{matrix}\\right]$  \n",
    "where $\\mathbf B$ has been selected as follows:  \n",
    "with $\\dim \\ker q_1\\big(A\\big) = d_1$, the first $d_1$ vectors in $\\mathbf B$ form a basis for $\\ker q_1\\big(A\\big)$ and the next $d_2=\\dim \\ker q_2\\big(A\\big)$ form a basis for $\\ker q_2\\big(A\\big)$ and so on.  \n",
    "\n",
    "Now suppose $A$ is the Companion matrix.  Then our earlier explicit Jordan Form procedure works verbatim on each block $Y_k$, each of which has a char poly equal to its monic poly equal to $(x-\\lambda_j)^{k_j}$  for distinct $\\lambda_j$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trickier case comes when  \n",
    "$\\dim\\ker\\Big(A-\\lambda_kI\\big) = \\dim\\ker\\Big(\\mathbf B^{-1}\\big(A-\\lambda_kI\\big)\\mathbf B\\big)=\\dim\\ker\\Big(Y_k\\big) \\geq 2$  \n",
    "in such a case the char poly is no longer the minimal polynomial and our jordanization argument for the Companion Matrix needs altered.   \n",
    "\n",
    "**this is an open item**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ex 12.Misc.10**  \n",
    "Prove two $n \\times n$ diagonalizable matrices over a field $\\mathbb F$ are simultaneously diagonalizable *iff* $XY=YX$  \n",
    "\n",
    "*remark:*  \n",
    "This is proven over $\\mathbb C$ in ex 7.5.14 in 'Artin_chp7.ipynb' specifically for Hermitian Matrices, using Hermitian Forms, though that notebook also contains an immediate generalization to arbitrary commuting diagonalizable matrices over $\\mathbb F$ via the use of left and right eigenvectors.  There is also a related problem in Chp 9 on Group Representation Theory (as of yet not tackled).  There is also a direct matrix analysis approach in \"simultaneous_diagonalization.ipynb\" in the Linear Slgebra folder.  All 3 of these proofs are distinct though they have some commonalities.  There are also approaches to doing this via commuting and a minimal polynomial argument though your author has not 'absorbed' these yet.  The bilinear forms approach is your author's favorite.    \n",
    "\n",
    "*proof:*  \n",
    "The first direction is immediate: if both $X$ and $Y$ are similar to diagonal matrices via $P$ then because diagonal matrices commute, so must $Y$ and $X$.  \n",
    "\n",
    "For the second direction, to show that commuting and being diagonalizable implies simultaneously diagonalizability:  \n",
    "recall the main technique in the above 'extension' where we observed \n",
    "$p\\big(A\\big) = q_1\\big(A\\big)\\cdot q_2\\big(A\\big) = q_2\\big(A\\big)\\cdot q_1\\big(A\\big)$ and then used this to prove  \n",
    "$q_1\\big(A\\big)\\mathbf B = \\bigg[\\begin{array}{c|c|c|c|c|c} \\mathbf 0 &\\cdots &\\mathbf 0 & *&\\cdots &* \\end{array}\\bigg]= \\left[\\begin{matrix}\\mathbf 0_{m\\times m} & * \\\\\\mathbf 0 & *\\end{matrix}\\right]=\\mathbf B \\left[\\begin{matrix}\\mathbf 0_{m\\times m} & \\mathbf 0 \\\\\\mathbf 0 & Z_{m-n\\times m-n}\\end{matrix}\\right]$  \n",
    "\n",
    "an essentially identical insight works here:  \n",
    "\n",
    "define $Y_k:=Y- I\\gamma_k$ for $k\\in\\{1,2,...,r\\}$ where $Y$ has $r$ distinct eigenvalues, each labelled $\\gamma_k$, and the associated algebraic = geometric (by diagonalizability of $Y$) multiplicity is $n_j$ for $\\gamma_j$  \n",
    "\n",
    "Mimicking the setup in the above 'extension' construct $\\mathbf B$ to have the eigenvectors of $X$ associated with each of its distinct eigenvalues  \n",
    "\n",
    "Then  \n",
    "$X\\mathbf B = \\mathbf BD$  \n",
    "where $D$ is in fact a block diagonal matrix, having the top left block with being $m_1\\times m_1$ where $m_1$ is the multiplicity of $\\lambda_1$, and so on.  (In fact each block is of the form $\\lambda_j I_{m_j}$ so $D$ is diagonal though we don't need this.)  \n",
    "Now by re-using the extension's argument    \n",
    "$Y_k\\mathbf B = \\mathbf BD_{Y_k}$  \n",
    "where $D_{Y_k}$ is block diagonal with the same block dimensions as $D$.  If e.g. the top left wasn't block diagonal, consider  \n",
    "\n",
    "$Y_k\\big(X-\\lambda_1I\\big)\\mathbf B = \\big(X-\\lambda_1I\\big)Y_k\\mathbf B $  \n",
    "the first $m_1$ columns are zero on the LHS so they must be on the RHS, but $\\big(X-\\lambda_1I\\big)$ only annihilates $\\mathbf v_1,..., \\mathbf v_{m_1}$  (or equivalently when we view this in terms of $\\mathbf B$ as a basis the first $m_1$ standard basis vectors).  That is   \n",
    "$Y_k\\cdot\\ker\\big(X-\\lambda_1 I\\big) \\subseteq \\ker\\big(X-\\lambda_1 I\\big)$  \n",
    "(i.e. if the RHS is a vector subspace specifically chosen to be $\\ker\\big(X-\\lambda_1 I\\big)$ then when we multiply by $Y_k$ on an arbitrary vector in that space, the image is *still* contained in that subspace.  I.e. by design subspace is closed under multiplication by $\\big(X-\\lambda_1 I\\big)$ but by the power of commutativity it is also closed under multiplication by $Y_k$ which is quite powerful)  \n",
    "\n",
    "where the inclusion may be proper or may not.  This should make us recall p.116 and invariant subspaces, i.e. $Y_k$ respects this invariant subspace given by $\\ker\\big(X-\\lambda_1 I\\big)$     \n",
    "\n",
    "Since $X$ is diagonalizable with $s$ distinct eigenvalues, we write our coordinate vector space  \n",
    "$\\mathbb F^n = \\ker\\big(X-\\lambda_1 I\\big)\\oplus \\ker\\big(X-\\lambda_2 I\\big)\\oplus ... \\oplus \\ker\\big(X-\\lambda_s I\\big)$    \n",
    "\n",
    "now consider the homomorphism of $Y_k\\cdot \\mathbb F^n$ and we apply it to each subspace separately (since they span $\\mathbb F^n$ and *are* independent). The kth eigenvalue of $Y$, $\\gamma_k$, has algebraic multiplicity $n_k$ which is equal to its geometric multiplicity since $Y$ is diagonalizable.  By rank-nullity, *and* the fact that $Y_k$ respects the invariant subspaces of $X$ (which means since these different kernels are 'disjoint' --only having the zero vector in common-- before being operated on by $Y_k$ they must be still 'disjoint' after being operated on by $Y_k$, hence the $Y_k$ has an image that is direct summable and the dimension of this image, i.e. rank, splits into independent sums)      \n",
    "\n",
    "$n-n_k=\\text{rank}\\Big(Y_k\\cdot \\mathbb F^n\\Big) = \\text{rank}\\Big(Y_k\\cdot \\ker\\big(X-\\lambda_1 I\\big)\\Big)+\\text{rank}\\Big(Y_k\\cdot \\ker\\big(X-\\lambda_2 I\\big)\\Big)+ ... +\\text{rank}\\Big(Y_k\\cdot \\ker\\big(X-\\lambda_s I\\big)\\Big)$    \n",
    "\n",
    "so for each of the above images when \n",
    "$\\text{rank}\\Big(Y_k\\cdot \\ker\\big(X-\\lambda_j I\\big)\\Big)\\lt \\text{rank}\\Big(\\ker\\big(X-\\lambda_j I\\big)\\Big)$  \n",
    "then the shortage is precisely made up by vectors in the kernel of $Y_k$.  \n",
    "\n",
    "- - - -  \n",
    "To fill in the details in a concrete manner:  \n",
    "Since $Y_k$ is diagonalizable $\\mathbb F^n = \\ker\\Big(Y_k\\Big) \\oplus \\text{image}\\Big(Y_k\\Big)$    \n",
    "(by direct examination or revisiting ex 4.Misc.8)  \n",
    "\n",
    "now make a basis for the image of $Y_k\\cdot \\ker\\big(X-\\lambda_j I\\big)$, then extend it to a basis for $\\ker\\big(X-\\lambda_j I\\big)$, using the basis extension algorithm and those appended linearly independent vectors *must* be in the kernel of $Y_k$ because they are not in the image of $Y_k$ applied to some other subspace $\\ker\\big(X-\\lambda_i I\\big)$, $i\\neq j$, since $Y_k$ respects these invariant subspaces of $X$.  So these vectors are nowhere in the image of $Y_k$ but they are in $\\mathbb F^n$ and by the direct sum this implies they must be $\\in \\ker\\Big(Y_k\\Big)$.  \n",
    "- - - - -  \n",
    "so   \n",
    "$\\ker\\Big(Y_k\\Big) = \\ker\\Big(Y_k\\cdot \\ker\\big(X-\\lambda_1 I\\big)\\Big)\\oplus \\ker\\Big(Y_k\\cdot \\ker\\big(X-\\lambda_2 I\\big)\\Big)\\oplus ... \\oplus \\ker\\Big(Y_k\\cdot \\ker\\big(X-\\lambda_s I\\big)\\Big)$    \n",
    "Thus we select $n_k$ linearly independent eigenvectors associated with $\\gamma_k$, each from a particular subspace $\\ker\\big(X-\\lambda_j I\\big)$.   \n",
    "\n",
    "Finally, the choice of $k$ was arbitrary which means we've found  \n",
    "$n_1+n_2+...+n_r=n$ eigenvectors for $Y$ and we recall the eigenvectors associated with $\\gamma_j\\neq \\gamma_k$ are linearly independent because they are associated with different eigenvalues.  Thus we've found $n$ linearly independent eigenvectors for $Y$ that are also eigenvectors for $X$; this forms a basis in $\\mathbb F^n$.  Collect these $n$ linearly independent eigenvectors in a matrix $P$ and we see that $PXP^{-1}$ and $PYP^{-1}$ are each diagonal, as desired.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**12.misc.11**  Let $G$  be a finite abelian group and let $\\phi:G\\longrightarrow \\mathbb C^x$ be a homomorphism which is not the trivial homomorphism ($\\phi(x)=1$ for all $x$.)  We call the image of $G$ under $\\phi$ the group  $G'$\n",
    "\n",
    "*claim:*   \n",
    "$\\sum_{g\\in G} \\phi(g) = 0$    \n",
    "\n",
    "*note:* Artin uses $A$ to denote the abelian group which is quite strange since we almost exclusively use $A$ for the presentation matrix (or *some* matrix) in this chapter.  I use the more customary $G$ instead.  \n",
    "\n",
    "\n",
    "*gut check*   \n",
    "as a gut check, we know that for any $g\\in G$, since $\\big \\vert G\\big \\vert = n \\lt \\infty$  we know that $(k+1)g = g$ for *some* natural number $k$, which by invertibility of elements in our group means $k\\cdot g = 0$ w.  Under the image of the homomorphism this becomes  \n",
    "\n",
    "$0=\\phi\\big(k\\cdot g\\big)=k\\cdot \\phi\\big(g\\big)$  \n",
    "recalling that homomorphisms map the identity to the identity  \n",
    "\n",
    "'sum' notation is common for Abelian groups, though if we wanted we could write it in product notation of   \n",
    "$1=\\phi\\big( g^k\\big)=\\phi\\big(g\\big)^k$   \n",
    "\n",
    "and when we consider the codomain of $\\phi$ we see it must be that $\\phi\\big(g\\big)$ is a $k$th root of unity.  \n",
    "\n",
    "\n",
    "*proof:*   \n",
    "since $G$ is finite, it has a finite set of generators \n",
    "$\\mathbf B := \\bigg[\\begin{array}{c|c|c|c} \\mathbf v_{1} & \\mathbf v_{2} &\\cdots & \\mathbf v_{m}\\end{array}\\bigg] $   \n",
    "\n",
    "and we may collect all relations in a presentation matrix e.g. as done on p. 464, thus  \n",
    "$\\mathbf B A = \\mathbf 0$   \n",
    "we may then following 5.12(i) (p. 466) to get  \n",
    "\n",
    "$\\mathbf BA=\\mathbf B\\big(Q^{-1}A'P\\big)=\\big(\\mathbf BQ^{-1}\\big)A'P = \\mathbf B' A' P = \\mathbf 0$   \n",
    "where $\\mathbf B'$ is a new set of generators, $A'$ is diagonal (with form given on page 458) and $P$ is invertible thus, e.g. there is some $\\mathbf x$ such that $\\mathbf P\\mathbf x = \\mathbf e_k$ so   \n",
    "$\\mathbf v_k' d_{k} = \\mathbf 0$   \n",
    "\n",
    "or equivalently we can multiply on the right by $P^{-1}$ and we have  \n",
    "$\\mathbf B' A'  = \\mathbf 0$   \n",
    "\n",
    "we may further reduce redundancies in $A'$ by applying $5.12(ii)$ and $5.12(iii)$ though for notational ease we assume WLOG that $\\mathbf B'$ still has $m$ generators.    \n",
    "\n",
    "Note: since $G$ is finite we know that it has no free abelian group, i.e. it decomposes into \n",
    "$r$ disjoint finite cyclic groups.   \n",
    "\n",
    "So each cyclic group has order $d_1$ and up to isomorphism is given by  \n",
    "\n",
    "$r\\cdot \\frac{2\\pi i}{d_k}\\text{ % } (2\\pi_i)$  \n",
    "another way of interpreting this is e.g. with $d_k=6$ \n",
    "$g_1 =g_1$, $2\\cdot g_1 = g_2$, $3\\cdot g_1 = g_3$, $4\\cdot g_1= g_4$, $5\\cdot g_1 = g_5$, $6\\cdot g_1 = 0$    \n",
    "\n",
    "under the image of $\\phi$ each cyclic group is *still* cyclic because \n",
    "$g_r = r\\cdot g_1\\implies  \\phi(g_r) = r\\cdot \\phi(g_1)$ and as before with $r=6$ we still recover $6\\cdot \\phi(g_1)=0$.   However, recalling page 58 and e.g. Lagrange's Theorem,  \n",
    "\n",
    "we have for cyclic group $G_{d_k}$  \n",
    "$d_k=\\big \\vert G_{d_k}\\big \\vert = \\big \\vert \\ker \\phi\\big \\vert \\cdot \\big \\vert \\text{im}\\phi \\big \\vert$  \n",
    "so in the above example the image under the homomorphism could have either 1,2,3 or 6 elements --with 1 occuring iff the homomorphism is trivial on this particular cyclic group.  (Recall that this is because each element in the image exists, with duplication, $s$ times if there are $s$ elements in the kernel.  Or in bigger terms: consider that the kernel of a homomorphism is a normal subgroup and then determine the cosets, each of which necessarily gets mapped to one distinct element in the image of the homomorphism.)      \n",
    "\n",
    "\n",
    "The direct product notation on page 474 is useful.  If our group decomposes into $r$ disjoint cylic subgroups, then we may interpret any element in $G$ as decomposing into  \n",
    "\n",
    "$\\Big(w_1, w_2,...,w_r\\Big)$  \n",
    "\n",
    "and $G'$ decomposes into  \n",
    "$\\Big(\\phi(w_1), \\phi(w_2),...,\\phi(w_{r})\\Big)$  \n",
    "so any element in $G'$ may be written as a sum  \n",
    "$\\phi(w_1)+ \\phi(w_2)+...+ \\phi(w_{r}) $  \n",
    "\n",
    "however, while we have commutativity, the product notation for a group is more suggestive here.  That is, switching to multiplicative notation, any element in $G'$ may be written as   \n",
    "\n",
    "$\\phi(w_1)\\cdot \\phi(w_2)\\cdot ... \\cdot  \\phi(w_{r}) $  \n",
    "\n",
    "since $\\phi$ is not the trivial homomorphism, we know that $\\big \\vert\\text{im} \\phi (W_j)\\big \\vert = n\\gt 1$  for some $j$.  The proof closes by 'attacking' this particular group.  \n",
    "$\\phi\\big(W_j\\big)$  has order $n$ thus each element in its image is given as an eigenvalue of  $C_{x^n-1}$  \n",
    "where this is the Companion matrix associated with the polynomial $x^n -1$.  And $\\ker\\Big(\\phi\\big(W_j\\big)\\Big)$ has cardinality $d$. This allows a very clever way of finishing the problem, via the use of Kronecker Products, i.e. tensor products between well chosen matrices.  \n",
    "\n",
    "Thus if we count all elements, *with duplication* under the $\\phi\\big(W_j\\big)$ (i.e. looking at the domain as a set (or multiset since the group has distinct elements) and considering the image of each element under the homomorphism again in multiset) they are given by the eigenvalues\n",
    "\n",
    "$I_d \\otimes C_{x^n-1}$  \n",
    "further exploiting product notation and the fact that eigenvalues mutliply under the Kronecker Product, *all elements* under the homomorphism, with duplication, are given by the eigenvalues of   \n",
    "\n",
    "$\\phi\\big(C_{x^{d_1}-1}\\big)\\otimes \\phi\\big(C_{x^{d_2}-1}\\big)\\otimes ... \\otimes \\big\\{\\phi\\big(C_{x^{d_j}-1}\\big)\\big\\}\\otimes \\phi\\big(C_{x^{d_{j+1}}-1}\\big)\\otimes ... \\otimes \\phi\\big(C_{x^{d_m}-1}\\big)$  \n",
    "$=\\phi\\big(C_{x^{d_1}-1}\\big)\\otimes \\phi\\big(C_{x^{d_2}-1}\\big)\\otimes ... \\otimes I_d \\otimes C_{x^n-1}\\otimes \\phi\\big(C_{x^{d_{j+1}}-1}\\big)\\otimes ... \\otimes \\phi\\big(C_{x^{d_m}-1}\\big)$  \n",
    "\n",
    "and since we want the sum of all these elements under the image of the homomorphism, that is equivalent to computing the sum of the eigenvalues of the above, i.e. it is given by the trace.  Thus we conclude  \n",
    "\n",
    "$\\sum_{g\\in G} \\phi(g)$   \n",
    "$= \\text{trace}\\Big(      \\phi\\big(C_{x^{d_1}-1}\\big)\\otimes \\phi\\big(C_{x^{d_2}-1}\\big)\\otimes ... \\otimes I_d \\otimes C_{x^n-1}\\otimes \\phi\\big(C_{x^{d_{j+1}}-1}\\big)\\otimes ... \\otimes \\phi\\big(C_{x^{d_m}-1}\\big)\\Big)$  \n",
    "$=\\text{trace}\\Big(      \\phi\\big(C_{x^{d_1}-1}\\big)\\Big)\\cdot  \\text{trace}\\Big(\\phi\\big(C_{x^{d_2}-1}\\big)\\Big)\\cdot ... \\cdot \\text{trace}\\Big(  I_d \\Big)\\cdot \\text{trace}\\Big( C_{x^n-1}\\Big)\\cdot \\text{trace}\\Big(  \\phi\\big(C_{x^{d_{j+1}}-1}\\big)\\Big)\\cdot  ... \\cdot  \\text{trace}\\Big(  \\phi\\big(C_{x^{d_m}-1}\\big)\\Big)$  \n",
    "$=\\text{trace}\\Big(      \\phi\\big(C_{x^{d_1}-1}\\big)\\Big)\\cdot  \\text{trace}\\Big(\\phi\\big(C_{x^{d_2}-1}\\big)\\Big)\\cdot ... \\cdot \\text{trace}\\Big(  I_d \\Big)\\cdot 0\\cdot \\text{trace}\\Big(  \\phi\\big(C_{x^{d_{j+1}}-1}\\big)\\Big)\\cdot  ... \\cdot  \\text{trace}\\Big(  \\phi\\big(C_{x^{d_m}-1}\\big)\\Big)$  \n",
    "$=0$  \n",
    "\n",
    "as desired  \n",
    "\n",
    "note: there are other ways to finish this problem without using Companion matrices, trace and Kronecker Products, (e.g. one could work through this with induction).  However the above approach is a very nice way to finish.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**12.misc.12**  \n",
    "see ex 12.2.7 as this is essentially a duplicate of that exercise  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**12.misc.13**  \n",
    "\n",
    "\n",
    "$\\mathbf B := \\bigg[\\begin{array}{c|c|c|c} \\mathbf v_{1} & \\mathbf v_{2} &\\cdots & \\mathbf v_{m}\\end{array}\\bigg] $\n",
    "where $\\mathbf v_k$ for $k\\in\\big\\{1,2,...,m\\big\\}$ is the set of generators for an R-module $V$.  \n",
    "\n",
    "Let $J$ be an ideal of $R$.  Define $JV$ to be the set of all finite sums of products $a \\mathbf v$ for $a\\in J$, $\\mathbf v \\in V$    \n",
    "\n",
    "*(a)*  \n",
    "show that if $JV=V$ there is an $m\\times m$ matrix $A$ with entries in $J$ such that  \n",
    "$\\mathbf B\\big(I_m - A\\big) = \\mathbf 0$  \n",
    "or  \n",
    "$\\mathbf B A = \\mathbf B$  \n",
    "specializing to arbitrary $\\mathbf v_k$ we want to show  \n",
    "$\\sum_{i=1}^m a_{i,k}\\mathbf v_k = \\mathbf B A\\mathbf e_k = \\mathbf B\\mathbf e_k = \\mathbf v_k$  \n",
    "\n",
    "now $JV = V$ tells us  \n",
    "\n",
    "$\\mathbf v_k = \\sum_{i=1}^n j_{i,k}\\big(\\mathbf v'_i\\big)$  \n",
    "and $\\mathbf B$ being generating means  \n",
    "$\\mathbf v_k = \\sum_{i=1}^n j_{i,k}\\big(\\mathbf v'_i\\big) = \\sum_{i=1}^n j_{i,k}\\big(\\sum_{r=1}^m \\gamma_r \\mathbf v_r\\big)=  \\sum_{r=1}^m \\big(\\sum_{i=1}^n \\gamma_r \\cdot j_{i,k}\\big) \\cdot \\mathbf v_r=  \\sum_{r=1}^m \\big(a_{r,k}\\big) \\cdot \\mathbf v_r $     \n",
    "\n",
    "note:  $a_{r,k}:=\\sum_{i=1}^n \\gamma_r \\cdot j_{i,k} \\in J$  \n",
    "because ideals are defined as being closed under such linear combinations  \n",
    "\n",
    "\n",
    "*remark:*  \n",
    "as a purely formal analogy this should remind us of an eigenvalue equation $\\mathbf B = \\mathbf B A$  \n",
    "which we intuit because $JV = V$ (or perhaps $VJ = V$ would be preferable notation).  \n",
    "\n",
    "\n",
    "*(b)*    \n",
    "i.) With the notation of (a), show that $\\det\\big(I_m-A\\big) = 1 + \\beta$ where $\\beta \\in J$\n",
    "(note: the text uses $\\alpha$ instead of $\\beta$ here but $\\alpha$ looks too much like $a\\in J$ as well as components of $A$)   \n",
    "\n",
    "$\\det\\big(tI_m-A\\big)= t^m - c_{m-1}t^{m-1}+ c_{m-2}t^{m-2}+ +.... + \\text{sign(1)}\\cdot c_1 t + \\text{sign(0)} \\cdot c_0$  \n",
    "where we know from e.g. ex 4.4.11 and the Principle of Permanence of Identities that each $c_j$ is given by sums over $j\\times j$ principal minors of $A$ each $c_j \\in J$ because it is purely the result of sums and products of elements $\\in J$  \n",
    "\n",
    "evaluating at $t:=1$ gives  \n",
    "$\\det\\big(tI_m-A\\big)= 1 - \\Big(c_{m-1}+ c_{m-2}+ +.... + \\text{sign(1)}\\cdot c_1  + \\text{sign(0)} \\cdot c_0\\Big) = 1 - (-\\beta) = 1+ \\beta$  \n",
    "where $(-\\beta) \\in J$  \n",
    "because ideals are closed under linear combinations, and of course $\\beta \\in J$ because this is $-\\beta$ rescaled by $-1$.  \n",
    "\n",
    "ii.)  show that $(1+\\beta)$ annihilates $V$  \n",
    "it is enough to show that $(1+\\beta)$ annihilates the generators of $V$, i.e.  \n",
    "$(1+\\beta)\\cdot \\mathbf B =  \\mathbf B\\big((1+\\beta)I_m\\big) = \\mathbf 0$    \n",
    "\n",
    "referencing the adjugate (aka classical adjoint) and what Artin calls \"Cramer's Rule\" on page 29, which holds over our ring by Principle of Permanence of Identites, we have  \n",
    "\n",
    "$\\mathbf 0 = \\mathbf 0\\text{adj}(A) = \\Big(\\mathbf BA\\Big) \\text{adj}(A)=\\mathbf B\\Big(A\\text{adj}(A)\\Big) = \\mathbf B\\Big( \\det\\big(A\\big)\\cdot I_m\\Big)=\\mathbf B\\Big(\\big(1+\\beta\\big)\\cdot I_m\\Big)$  \n",
    "\n",
    "*note:* if $J\\neq R \\implies 1+\\beta \\notin J$ because $\\beta \\in J$, so if $1+\\beta \\in J$, then $1=\\big((1+\\beta)-\\beta\\big)\\in J$, thus $J$ would have to be the unit ideal, i.e. all over $R$.   \n",
    "\n",
    "*(c)*    \n",
    "An $R-\\text{module}$ $V$ is called *faithful* if $rV = \\mathbf 0$ for some $r\\in R$ implies $r=0$.  Prove the **Nakayama Lemma:**  Let $V$ be a finitely generated, faithful $R-\\text{module}$, and let $J$ be an ideal of $R$.  Given these conditions, the lemma states  \n",
    "$JV=V \\implies J=R$, i.e. that $J$ must be the unit ideal.   \n",
    "\n",
    "$JV=V =\\implies r\\cdot V = \\mathbf 0$, with $r:= 1+\\beta$  \n",
    "per exercise (b)  \n",
    "since our $R-\\text{module}$ is faithful, $\\implies 1+\\beta =0$  \n",
    "thus  \n",
    "$\\beta = -1$  \n",
    "(and of course $\\beta^2 =1$)  \n",
    "so $\\beta \\in J$ and $\\beta $ is a unit, thus $J$ is the unit ideal, i.e. $J=R$ (recall page 357)   \n",
    "\n",
    "*remark:* we saw 'faithful' in chapter 5, on page 182.  Then as here the term, approximately means injective, though we are not looking  at individual elements here, but instead annihilating all of $V$.    \n",
    "\n",
    "*(d)*    \n",
    "Let $V$ be a finitely generated $R-\\text{module}$.  Prove that if $MV =V$ for all maximal ideals $M$, then $V=0$   \n",
    "\n",
    "suppose for contradiction that $V\\neq \\mathbf 0$.  Then WLOG this means $\\mathbf v_1 \\neq \\mathbf 0$ and we may 'attack' $\\mathbf v_1$.  \n",
    "Re-visiting ex 12.1.6 we have the simple module given by, for $r_k\\in R$   \n",
    "\n",
    "$\\sum_{k} r_k \\cdot \\mathbf v_1 = \\big(\\sum_{k} r_k\\big) \\cdot \\mathbf v_1 = r \\cdot \\mathbf v_1$  \n",
    "for arbitrary $r \\in R$  \n",
    "i.e. the set (subspace) including all finite sums where the sums of elements in the ring, or equivalently all scalings of the form $r\\mathbf v_1$. We call this submodule $V_1$.   \n",
    "\n",
    "now recalling  ex 12.1.6a we know that this simple module is isomorphic to $R/M$ for some maximal ideal $M$.  In particular we may construct this via a (module) homomorphism $\\phi: R \\longrightarrow V$, $\\phi\\big(1\\big) = \\mathbf v_1$ and $\\phi\\big(r\\big) =\\phi\\big(r1\\big)=r\\cdot \\phi\\big(1\\big)= r\\cdot \\mathbf v_1$.  The domain may be interpreted as a ring as well as an R-module with $V=1$.   \n",
    "\n",
    "For review we step through (and slightly adapt) the Harvard solution to this problem.      \n",
    "\n",
    "\n",
    "\n",
    "**clean this up**  \n",
    "now $M:=\\ker \\phi$ is a submodule (i.e. closed under linear combinations in image space) but it is surjective and since being a submodule is closed under linear combinations, then we have closure under linear combination in the pre-image space (i.e. it is an ideal in the Ring).  \n",
    "\n",
    "\n",
    "Then $R/M \\cong V_1$ by First Isomorphism Theorem (p. 452).  (While we don't explicitly need it here-- all we need is that the image of the homomorphism is non-zero, hence the kernel is not the unit ideal-- but since the module given by the image is simple, we cannot recurse and quotient out some other proper ideal -- if we could then we'd have another submodule-- that is why must be $M$ is maximal.)  \n",
    "\n",
    "there is some $\\beta \\in M$ which annihilates $\\mathbf v_1$ but we know by (b) that $(1+\\beta)\\notin M$ hence  \n",
    "\n",
    "$(1+\\beta)\\notin M=\\ker \\phi \\implies 0 \\neq \\phi\\big((1+\\beta)\\big)= \\phi\\big((1+\\beta)1\\big) = (1+\\beta)\\cdot \\phi\\big(1\\big) =  (1+\\beta)\\mathbf v_1= \\mathbf 0$   \n",
    "which is a contradiction   \n",
    "\n",
    "**can I finish a different way explicitly using Nakayama Lemma? I think so if using 12.1.6b and Schurs Lemma**    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ex 12.misc.14**  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sympy as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}1 & i\\\\1 & - i\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "Matrix([\n",
       "[1,  I],\n",
       "[1, -I]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = sp.Matrix([[1, sp.I],[1, -sp.I]])\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}\\frac{1}{2} & \\frac{1}{2}\\\\- \\frac{i}{2} & \\frac{i}{2}\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "Matrix([\n",
       "[ 1/2, 1/2],\n",
       "[-I/2, I/2]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.inv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}0 & 1 & 0\\\\0 & 0 & 1\\\\175 & -67 & 13\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "Matrix([\n",
       "[  0,   1,  0],\n",
       "[  0,   0,  1],\n",
       "[175, -67, 13]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = 3\n",
    "C = sp.zeros(m)\n",
    "\n",
    "\n",
    "for i in range(m):\n",
    "    try :\n",
    "        C[i,i+1] =1\n",
    "    except IndexError:\n",
    "        pass\n",
    "C[-1,0] = 175\n",
    "C[-1,1] = -67 \n",
    "C[-1,2] = 13\n",
    "C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3 + 4*I, 3 - 4*I, 7]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eigs = [3+4*sp.I, 3-4*sp.I, 7]\n",
    "eigs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}1\\\\3 + 4 i\\\\\\left(3 + 4 i\\right)^{2}\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "Matrix([\n",
       "[           1],\n",
       "[     3 + 4*I],\n",
       "[(3 + 4*I)**2]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W= sp.zeros(m)\n",
    "for i in range(m):\n",
    "    for j in range(m):\n",
    "        W[i,j] = eigs[j]**i\n",
    "w_0 = W[:,0]\n",
    "w_1 = W[:,1]\n",
    "w_2 = W[:,2]\n",
    "w_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}3 + 4 i & 3 - 4 i & 7\\\\\\left(3 + 4 i\\right)^{2} & \\left(3 - 4 i\\right)^{2} & 49\\\\-26 - 268 i + 13 \\left(3 + 4 i\\right)^{2} & -26 + 13 \\left(3 - 4 i\\right)^{2} + 268 i & 343\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "Matrix([\n",
       "[                      3 + 4*I,                       3 - 4*I,   7],\n",
       "[                 (3 + 4*I)**2,                  (3 - 4*I)**2,  49],\n",
       "[-26 - 268*I + 13*(3 + 4*I)**2, -26 + 13*(3 - 4*I)**2 + 268*I, 343]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C@W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_0 = w_0 + w_1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle 3$"
      ],
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}2\\\\6\\\\-14\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "Matrix([\n",
       "[  2],\n",
       "[  6],\n",
       "[-14]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_0.simplify()\n",
    "v_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}6\\\\-14\\\\-234\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "Matrix([\n",
       "[   6],\n",
       "[ -14],\n",
       "[-234]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_0p = (C@v_0)\n",
    "v_0p.simplify()\n",
    "v_0p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}-6\\\\-36 + \\left(3 - 4 i\\right)^{2} + \\left(3 + 4 i\\right)^{2}\\\\-52 + 7 \\left(3 - 4 i\\right)^{2} + 7 \\left(3 + 4 i\\right)^{2}\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "Matrix([\n",
       "[                                   -6],\n",
       "[    -36 + (3 - 4*I)**2 + (3 + 4*I)**2],\n",
       "[-52 + 7*(3 - 4*I)**2 + 7*(3 + 4*I)**2]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C@v_0 - 2*sp.re(eigs[0])*v_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}0 & 1 & 0 & 0 & 0\\\\0 & 0 & 1 & 0 & 0\\\\0 & 0 & 0 & 1 & 0\\\\0 & 0 & 0 & 0 & 1\\\\0 & 0 & 0 & 0 & 0\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "Matrix([\n",
       "[0, 1, 0, 0, 0],\n",
       "[0, 0, 1, 0, 0],\n",
       "[0, 0, 0, 1, 0],\n",
       "[0, 0, 0, 0, 1],\n",
       "[0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = 5\n",
    "N = sp.zeros(m)\n",
    "\n",
    "c = sp.Symbol('c')\n",
    "for i in range(m):\n",
    "    try :\n",
    "        N[i,i+1] =1\n",
    "    except IndexError:\n",
    "        pass\n",
    "N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}1 & 0 & 0 & 0 & 0\\\\0 & 1 & - c & 0 & 0\\\\0 & 0 & 1 & 0 & 0\\\\0 & 0 & 0 & 1 & 0\\\\0 & 0 & 0 & 0 & 1\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "Matrix([\n",
       "[1, 0,  0, 0, 0],\n",
       "[0, 1, -c, 0, 0],\n",
       "[0, 0,  1, 0, 0],\n",
       "[0, 0,  0, 1, 0],\n",
       "[0, 0,  0, 0, 1]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = 1\n",
    "jdx = 2\n",
    "\n",
    "E_3 = sp.eye(m)\n",
    "E_3[idx,jdx] = c\n",
    "E_3.inv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}0 & 1 & - c & 0 & 0\\\\0 & 0 & 1 & 0 & 0\\\\0 & 0 & 0 & 1 & 0\\\\0 & 0 & 0 & 0 & 1\\\\0 & 0 & 0 & 0 & 0\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "Matrix([\n",
       "[0, 1, -c, 0, 0],\n",
       "[0, 0,  1, 0, 0],\n",
       "[0, 0,  0, 1, 0],\n",
       "[0, 0,  0, 0, 1],\n",
       "[0, 0,  0, 0, 0]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "J = sp.zeros(m) \n",
    "for i in range(m):\n",
    "    try: \n",
    "        J[i,i+1]=1\n",
    "    except IndexError:\n",
    "        pass\n",
    "\n",
    "J[0,2] = -c\n",
    "J\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}0 & 1 & 0 & 0 & 0\\\\0 & 0 & 1 & 0 & 0\\\\0 & 0 & 0 & 1 & 0\\\\0 & 0 & 0 & 0 & 1\\\\0 & 0 & 0 & 0 & 0\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "Matrix([\n",
       "[0, 1, 0, 0, 0],\n",
       "[0, 0, 1, 0, 0],\n",
       "[0, 0, 0, 1, 0],\n",
       "[0, 0, 0, 0, 1],\n",
       "[0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "J@E_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}0 & 1 & 0 & 0 & 0\\\\0 & 0 & 1 & - c & 0\\\\0 & 0 & 0 & 1 & 0\\\\0 & 0 & 0 & 0 & 1\\\\0 & 0 & 0 & 0 & 0\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "Matrix([\n",
       "[0, 1, 0,  0, 0],\n",
       "[0, 0, 1, -c, 0],\n",
       "[0, 0, 0,  1, 0],\n",
       "[0, 0, 0,  0, 1],\n",
       "[0, 0, 0,  0, 0]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Aprime = E_3.inv()@J@E_3\n",
    "Aprime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}1 & 0 & 0 & 0 & 0\\\\0 & 1 & 0 & 0 & 0\\\\0 & 0 & 1 & c & 0\\\\0 & 0 & 0 & 1 & 0\\\\0 & 0 & 0 & 0 & 1\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "Matrix([\n",
       "[1, 0, 0, 0, 0],\n",
       "[0, 1, 0, 0, 0],\n",
       "[0, 0, 1, c, 0],\n",
       "[0, 0, 0, 1, 0],\n",
       "[0, 0, 0, 0, 1]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "E_3b = sp.eye(m)\n",
    "E_3b[idx+1,jdx+1] = c\n",
    "E_3b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}0 & 1 & 0 & 0 & 0\\\\0 & 0 & 1 & 0 & 0\\\\0 & 0 & 0 & 1 & - c\\\\0 & 0 & 0 & 0 & 1\\\\0 & 0 & 0 & 0 & 0\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "Matrix([\n",
       "[0, 1, 0, 0,  0],\n",
       "[0, 0, 1, 0,  0],\n",
       "[0, 0, 0, 1, -c],\n",
       "[0, 0, 0, 0,  1],\n",
       "[0, 0, 0, 0,  0]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Aprime2 = E_3b.inv()@Aprime@E_3b\n",
    "Aprime2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}1 & 0 & 0 & 0 & 0\\\\0 & 1 & 0 & 0 & 0\\\\0 & 0 & 1 & 0 & 0\\\\0 & 0 & 0 & 1 & c\\\\0 & 0 & 0 & 0 & 1\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "Matrix([\n",
       "[1, 0, 0, 0, 0],\n",
       "[0, 1, 0, 0, 0],\n",
       "[0, 0, 1, 0, 0],\n",
       "[0, 0, 0, 1, c],\n",
       "[0, 0, 0, 0, 1]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for m>= 5... won't work when m=4  \n",
    "E_3c = sp.eye(m)\n",
    "E_3c[idx+2,jdx+2] = c\n",
    "E_3c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}1 & 0 & 0 & 0 & 0\\\\0 & 1 & 0 & 0 & 0\\\\0 & 0 & 1 & 0 & 0\\\\0 & 0 & 0 & 1 & - c\\\\0 & 0 & 0 & 0 & 1\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "Matrix([\n",
       "[1, 0, 0, 0,  0],\n",
       "[0, 1, 0, 0,  0],\n",
       "[0, 0, 1, 0,  0],\n",
       "[0, 0, 0, 1, -c],\n",
       "[0, 0, 0, 0,  1]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "E_3c.inv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}0 & 1 & 0 & 0 & 0\\\\0 & 0 & 1 & 0 & 0\\\\0 & 0 & 0 & 1 & 0\\\\0 & 0 & 0 & 0 & 1\\\\0 & 0 & 0 & 0 & 0\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "Matrix([\n",
       "[0, 1, 0, 0, 0],\n",
       "[0, 0, 1, 0, 0],\n",
       "[0, 0, 0, 1, 0],\n",
       "[0, 0, 0, 0, 1],\n",
       "[0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "E_3c.inv()@Aprime2@ E_3c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\left[\\begin{matrix}0 & 1 & 0 & 0\\\\0 & 0 & 1 & 0\\\\0 & 0 & 0 & 1\\\\0 & 0 & 0 & 0\\end{matrix}\\right]\n"
     ]
    }
   ],
   "source": [
    "print(sp.latex(N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\left[\\begin{matrix}0 & 0 & 0 & 1\\\\0 & 0 & 0 & 0\\\\0 & 0 & 0 & 0\\\\0 & 0 & 0 & 0\\end{matrix}\\right]\n"
     ]
    }
   ],
   "source": [
    "print(sp.latex(N@N@N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
