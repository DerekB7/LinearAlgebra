{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ex 12.2.4** \n",
    "\n",
    "Let R be a ring and V be a free R-module of finite rank.  *Prove or disprove*  \n",
    "**(a)** Every set of generators contains a basis  \n",
    "this isn't true, and depending on the exact interpretation of the question we may offer various counter examples.  E.g. consider a submodule with a set of $m$ generators that are nilpotent.  This is not the zero submodule yet has zero linearly independent elements in it so there can be no basis, despite having a set of generators.   \n",
    "\n",
    "remark: reminiscent of the the bilinear forms chapter, the difficult task lies in dealing with things that are self linearly dependent (or self-orthogonal in bilinear forms chapter).  \n",
    "\n",
    "Counterexample:  \n",
    "$\\mathbb C[x,y]$ which can be interpreted as an R-module over itself.  So it has a basis given by $1$.  But if we consider the non-principal ideal $(x,y)$ then this does not have a basis (ex 12.2.1) so despite having generators for this submodule, we cannot find a basis for it.  \n",
    "\n",
    "**(b)** Every linearly independent set can be extended to a basis  \n",
    "for concreteness consider $R:=\\mathbb Z$ and up to isomorphism we are working in $\\mathbb Z^n$.  \n",
    "now the set $\\big(2\\mathbf e_1\\big)$ is linearly independent.  We may extend this to a maximally sized linearly independent set, collected as a hyper-vector, and indeed a matrix,  \n",
    "\n",
    "$\\mathbf B'=\\begin{bmatrix}2 &*\\\\ \\mathbf 0_{n-1} &B_{n-1}\\end{bmatrix}$  \n",
    "but \n",
    "$\\Big\\vert \\det\\big(\\mathbf  B'\\big)\\Big\\vert = 2 \\cdot \\Big\\vert\\det\\big(\\mathbf  B_{n-1}'\\big)\\Big\\vert \\geq 2$  \n",
    "since $\\big(\\mathbf  B_{n-1}\\big) \\in \\mathbb Z -\\{0\\}$  \n",
    "(we can reason over $\\mathbb Q$ if needed for why $\\big(\\mathbf  B_{n-1}\\big) \\neq 0$, and clear denominators as needed for linear dependence arguments)  \n",
    "\n",
    "Thus $\\det\\big(\\mathbf B'\\big)$ is not a unit and hence the matrix, viewed as a homomorphism, would be injective, but not surjective (see ex 12.2.7 for more color)  \n",
    "\n",
    "Equivalently, we know that the standard basis $\\mathbf B$ is a basis for our free module, but if $\\mathbf B'$ were a basis for our space as well, then per page 455, there would be an invertible matrix $A$ such that \n",
    "\n",
    "$\\mathbf B A =\\mathbf B'$  \n",
    "but taking determinants  \n",
    "$\\det\\big(\\mathbf B A\\big) = \\det\\big(\\mathbf B\\big)\\cdot \\det\\big( A\\big)=1\\cdot \\det\\big( A\\big) =\\det\\big(\\mathbf B'\\big)=\\text{not a unit}$   \n",
    "thus $A$ has non-unit determinant which is necessary and sufficient to be invertible (ref. page 453) which contradicts the fact that it is invertible.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ex 12.2.7**  \n",
    "\n",
    "Let $A$ be the matrix of a homomorphism $\\phi: \\mathbb Z^n \\longrightarrow \\mathbb Z^m$  between free modules  \n",
    "*note: this mean A has n columns and m rows*  \n",
    "\n",
    "*(a)* Prove that $\\phi$ is injective *iff* $\\text{rank}\\big(A\\big)= n$  \n",
    "The proof is essentially the same as when working over a field.  \n",
    "$\\text{rank}\\big(A\\big)= n\\implies \\text{all columns independent}\\implies \\ker\\big(A\\big)=\\{0\\} \\implies \\ker\\big(\\phi\\big)=\\{0\\}\\implies \\phi \\text{ injective} $  \n",
    "$\\phi\\text{ injective} \\implies \\ker\\big(\\phi\\big)=\\{0\\} \\implies \\ker\\big(A\\big)=\\{0\\} \\implies \\text{all columns independent}\\implies\\text{rank}\\big(A\\big)= n $   \n",
    "note: we are not using or assuming rank-nullity here.  We are just working with the definition the independence of modules given on page 454.  And no-nontrivial linear combination of $A$'s columns evaluating to zero  is equivalent to a trivial kernel for $A$  \n",
    "\n",
    "*(b)* Prove that $\\phi$ is surjective *iff* the GCD of the determinants of the $m \\times m$ submatrices of $A$ is 1. \n",
    "$\\phi$ is surjective *iff* $A$, so the below develops the problem in terms of $A$   \n",
    "\n",
    "In note that these determinants take values in $\\mathbb Z$ which is a principal ideal domain (chp 11).  Thus the determinants having gcd $\\gt  1$ is equivalent to the minors having a common (non-unit) factor $r$ and being in the principal ideal $(r)$.  And the determinants having a gcd of $1$ is equivalent to them being in the unit ideal.  (We can ignore the trivial case of all determinants being zero and hence living in the zero ideal as this case implies that no matter how we select $r$ columns in $A$ we never have $r$ linearly independent ones -- working over $\\mathbb Q$ if needed for justification-- hence for reason of our guiding rank inequality the map cannot be surjective.)  \n",
    "\n",
    "Thus if we let $A_Z$ be the $m\\times m$ submatrix of $A$ with column set $Z$ our problem is to prove that $A$ is surjective *iff* it $m \\times m$ minors generate the unit ideal in $\\mathbb Z$.   \n",
    "\n",
    "or $A$ is surjective *iff*  \n",
    "$S \\in \\binom{[n]}{m}$  $n_k\\in \\mathbb Z$   \n",
    "$\\sum_{S} n_k\\det\\big(A_{[m],S}\\big)=1$   \n",
    "\n",
    "which reads as the set of all subsets of size $m$  of set $[n]$ --- which is the set $\\{1, 2, ..., n\\}$, so we want to choose subsets of cardinality $m$ from this   \n",
    "(ref 'cauchy_binet.ipynb' or page 232 of *Proofs from THE Book* for discussion of this or comparable notation. These two items also are useful as a reference to for 2nd leg of the proof re: necessity which uses Cauchy-Binet)   \n",
    "\n",
    "**sufficiency:**    \n",
    "$\\text{m x m minors of A generating unit ideal} \\implies \\text{A is surjective}$  \n",
    "to show that $A$ is surjective, it is enough to show that for any standard basis vector $\\mathbf e_j$ in the codomain, there is a non-empty preimage i.e. that for any $\\mathbf e_j$ there is some $\\mathbf x$ such that \n",
    "$A\\mathbf x = \\mathbf e_j$  \n",
    "\n",
    "Take our equation $\\sum_{S} n_k\\det\\big(A_{[m],S}\\big)=1$ and multiply each side by $\\mathbf e_j$ to get  \n",
    "$\\sum_{S} n_k\\det\\big(A_{[m],S}\\big)\\mathbf e_j=\\mathbf e_j$  \n",
    "\n",
    "Thus  \n",
    "$A\\mathbf x = A\\big(\\sum_{S} \\mathbf x^{(k)}\\big) =\\big(\\sum_{S} A\\mathbf x^{(k)}\\big) = \\sum_{S} n_k\\det\\big(A_{[m],S}\\big)\\mathbf e_j = \\mathbf e_j$  \n",
    "\n",
    "and for each item in the summation we may solve  \n",
    "$A\\mathbf x^{(k)}=\\mathbf b = n_k\\det\\big(A_{[m],S}\\big)\\mathbf e_j$   \n",
    "by setting $ x_i^{(k)}:=\\mathbf 0$ if the RHS is zero and otherwise by using Cramer's rule.  I.e. for reasons that amount to graph isomorphism, we need only show this for the first minor of $A$ i.e. the leading m x m submatrix in it.  We set $x_i^{(k)} :=0$ for $i\\gt m$.  Then for $i\\in{1,2,....,m}$  we apply Cramer's Rule (classical form of solving for one component at a time, not the adjugate form that Artin uses)  \n",
    "to get  \n",
    "$x_i^{(k)} := \\frac{\\det\\big(A_{[m],S}[\\mathbf b]\\text{ (swapped in col i) } \\big)}{\\det\\big(A_{[m],S}\\big)} = n_k\\cdot\\det\\big(A_{[m],S}\\big)\\cdot \\frac{\\det\\big(A_{[m],S}[\\mathbf e_j]\\text{ (swapped in col i) } \\big)}{\\det\\big(A_{[m],S}\\big)}=n_k\\cdot \\det\\big(A_{[m],S}[\\mathbf e_j]\\text{ (swapped in col i)}\\big)$  \n",
    "\n",
    "hence we've constructed a solution for  \n",
    "$A\\mathbf x^{(k)}=\\mathbf b = n_k\\det\\big(A_{[m],S}\\big)\\mathbf e_j$   \n",
    "\n",
    "and using linearity, we may sum over this solution set to get  \n",
    "$A\\mathbf x = A\\big(\\sum_{S} \\mathbf x^{(k)}\\big) =\\big(\\sum_{S} A\\mathbf x^{(k)}\\big) = \\sum_{S} n_k\\det\\big(A_{[m],S}\\big)\\mathbf e_j = \\mathbf e_j$  \n",
    "\n",
    "which proves that $A$ is surjective.  \n",
    "\n",
    "*note:* if the nominal division used in Cramer's rule makes the reader uncomfortable, we may temporarily relax/extend the ring from $\\mathbb Z$ to $\\mathbb Q$ and solve for each $x_i$ there, then follow the implications through and finally work backwards.  Alternatively, we could apply Cramer's Rule as  \n",
    "$\\det\\big(A_{[m],S}\\big) \\cdot x_i^{(k)} := \\det\\big(A_{[m],S}[\\mathbf b]\\text{ (swapped in col i) } \\big) = \\det\\big(A_{[m],S}\\big)\\cdot n_k \\cdot \\det\\big(A_{[m],S}[\\mathbf e_j]\\text{ (swapped in col i) } \\big)$   \n",
    "\n",
    "Then noting this allows a solution of $x_i^{(k)} := n_k\\cdot \\det\\big(A_{[m],S}[\\mathbf e_j]\\text{ (swapped in col i) } $   \n",
    "where e.g. we don't even contemplate being in an integral domain.  \n",
    "(The validity of Cramer's Rule for modules is addressed in ex 12.3.1)    \n",
    "\n",
    "**necessity:**    \n",
    "$\\text{A is surjective} \\implies \\text{m x m minors of A generating unit ideal}$   \n",
    "if $A$ is surjective, we may always solve $A\\mathbf x = \\mathbf e_j$, thus if $A$ is surjective, there exists $X \\in \\mathbb Z^{n\\times m}$ such that \n",
    "$I_m = AX$    \n",
    "\n",
    "taking determinants of each side we have   \n",
    "$1 = \\det\\big(I_m\\big) = \\det\\big(AX\\big) = \\sum_{S} \\det\\big(A_{[m],S}\\big)\\det\\big(X_{S,[m]}\\big) = \\sum_{S} \\det\\big(A_{[m],S}\\big)\\cdot n_k$    \n",
    "by Cauchy-Binet.  Hence the determinants of the $m\\times m$ submatrices in $A$ must generate the unit ideal.  \n",
    "\n",
    "(Cauchy-Binet may be justified by considering the matrices as having integer coefficients in $\\mathbb Q$, or likely by applying the below Principle of Permanence of Identities though this requires some careful checking).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**12.3 Principle of Permance of Identities**  \n",
    "This is my notes / re-rendering of pages 456-457, which while short, gives some of the most powerful insights in the book.  The principle goes as follows.  \n",
    "\n",
    "**0.) Background**  \n",
    "*viewpoint 1*  \n",
    "Lemma: For the integer ring $\\mathbb Z$, where $\\mathbb Z\\big[\\mathbf w\\big]$  is a polynomial in $w_i$ for $i\\in \\{1,2,..., m\\}$  \n",
    "$\\phi: \\mathbb Z\\big[\\mathbf w\\big]\\longrightarrow \\Re$ is *injective*   \n",
    "where $\\Re$ is the ring of *polynomial functions* with coefficients in $\\mathbb F$, when $\\mathbb F$ is a field of **characteristic zero** (e.g. $\\mathbb C$ or $\\mathbb Q$).  Note that this implies that $\\mathbb F$ has infinite cardinality).  \n",
    "(this is my adaptation of prop 3.8 on page 355)  \n",
    "\n",
    "To prove injectivity, we check the kernel of $\\phi$.  \n",
    "The zero polynomial is mapped to zero under any ring homomorphism.  \n",
    " \n",
    "i) if $p\\in \\mathbb Z\\big[\\mathbf w\\big]$ where $\\text{degree}(p) = 0$  \n",
    "(i.e. non-zero constant polynomial, which means $p\\in \\mathbb Z$)\n",
    "\n",
    "Then recalling prop 3.9 (p. 355) there is exactly one homomorphism from $\\mathbb Z$ to any ring, so, for $n\\in \\mathbb Z_\\gt0$ \n",
    "$\\phi(n) = \\sum_{k=1}^n \\phi(1) = n\\cdot 1 =n\\neq 0$  and  to deal with negative integers:  \n",
    "$\\phi(-n) = \\sum_{k=1}^n \\phi(-1) = n\\cdot -1 = -n\\neq 0$  \n",
    "\n",
    "since $\\mathbb F$ has characteristic zero.  Thus we've checked the degree 0 case and prove no degree 0 polynomial is mapped to zero.  \n",
    "\n",
    "ii.) if $p\\in \\mathbb Z\\big[\\mathbf w\\big]$ where $\\text{degree}(p)  = d \\geq 1$   \n",
    "then consider a subset $S\\in \\mathbb F$ where $\\big \\vert S\\big \\vert = t$, then by Schwarz-Zippel, the probability that a point selected uniformly at random in $S^{m}$ is zero under the image of $\\phi(p)$, our *polynomial function* , is $\\leq\\frac{d}{t}$.  Hence for any $d$, we may set $t:=d+1$  (since $\\mathbb F$ has infinite cardinality), and $\\phi(p)$ is not zero everywhere on $S$ which proves that $\\phi(p) \\neq 0$.  \n",
    "\n",
    "Thus  $\\phi$ is injective.  \n",
    "\n",
    "*viewpoint 2*  \n",
    "For our purposes, an equivalent way of formulating this is, the degree zero case is obvious.  Now for any polynomial $p\\in \\mathbb Z\\big[\\mathbf w\\big]$ of degree where $1\\leq \\text{degree}(p) \\leq d $, to determine whether or not $d$ is the zero polynomial, it suffices to evaluate under the image of $t$ well chosen \n",
    "*distinct substitution homomorphisms*   \n",
    "\n",
    "$\\Phi_k: \\mathbb Z\\big[\\mathbf w\\big]\\longrightarrow \\mathbb F$  \n",
    "for $1\\leq k \\leq t^m$, where again the $\\Phi_k$ are chosen to be distinct and in particular evaluate at points in $S^{m}$ where $S\\in \\mathbb F$ and $\\big \\vert S\\big \\vert = t \\geq d+1$.  Per Schwartz-Zippel, the only polynomial that will evaluate to zero under every point in $S$ is the zero polynomial (where recalling that the zero polynomial has degree $-\\infty$ under some conventions, though others would say it has undefined degree).  This viewpoint is the essence of Miniature 24 in Matousek's *Thirty-three Miniatures*.  \n",
    "\n",
    "**1.) Main Argument**  \n",
    "When we consider an $r \\times n$ matrix with components some ring $R$, we may wonder whether results that we are familiar with from Linear Algebra still hold in $R$.  The answer supplied via application of the Principle of Permanence of Identities is: if we can map the problem to one involving the polynomial ring  $\\mathbb Z\\big[\\mathbf w\\big]$, then the answer is yes.  The problem explicitly shown in the chapter is to consider two $n\\times n$ matrices $X,Y$ and ask whether  \n",
    "$\\det\\big(XY\\big)=\\det\\big(X\\big)\\det\\big(Y\\big)$  \n",
    "\n",
    "This does still hold and the proof is essentially to re-write it as follows  \n",
    "i.) consider each component in $X$ as a formal variable $x_{i,j} \\in \\mathbb Z\\big[\\mathbf x\\big]$  and for $Y$ $y_{i,j} \\in \\mathbb Z\\big[\\mathbf y\\big]$. For convenience we may further relabel these so they both take values in $\\in \\mathbb Z\\big[\\mathbf w\\big]$, where $\\mathbf w$ has $2\\cdot n$ components, the first n being reserved for $x_{i,j}$ and the remaining reserved for $y_{i,j}$     \n",
    "\n",
    "ii.) Make it a question about the zero polynomial.  So we ask, is    \n",
    "$\\det\\big(XY\\big)-\\det\\big(X\\big)\\det\\big(Y\\big) =0$  \n",
    "where the LHS is the addition of two polynomials $\\in \\mathbb Z\\big[\\mathbf w\\big]$ and hence is some polynomial $p\\in \\mathbb Z\\big[\\mathbf w\\big]$.  \n",
    "\n",
    "Then we can uniformly bound the degree of $p$ as being $\\leq d:= n$.  So using viewpoint 2, it suffices to evaluate this under $t^m$  well chosen substitution homomorphisms in a convenient field of characteristic zero, say $\\mathbb C$ or $\\mathbb Q$ or $\\mathbb R$.  Working with $\\mathbb C$, we know from chapters 1-4 that  \n",
    "$\\det\\big(XY\\big)-\\det\\big(X\\big)\\det\\big(Y\\big) =0$  \n",
    "*everywhere* $in \\mathbb C$ which implies it is zero under all of our substitution homomorphisms, and hence we have confirmed that $p$ is the zero polynomial.  Finally we may may use one more substitution homomorphism to send $p$ to specific values in the ring $R$, and we recover the specific matrix in our original problem.  But a homomorphism always sends 0 to 0, so we conclude that \n",
    "$\\det\\big(XY\\big)-\\det\\big(X\\big)\\det\\big(Y\\big)=0$  \n",
    "when $X$ and $Y$ take coefficients in $R$.  \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ex 12.3.1**  \n",
    "The above argument can be applied to show that for matrices $A,B,C$ where well defined product exists,  \n",
    "\n",
    "a) associativity holds-- i.e. consider the polynomial $p\\in\\mathbb Z\\big[\\mathbf w\\big]$ where  \n",
    "$p= \\mathbf e_i^T\\Big(\\big(AB\\big)C- A\\big(BC\\big)\\Big)\\mathbf e_j$  \n",
    "for any / all standard basis vectors $\\mathbf e_i$, $\\mathbf e_j$.  Since the argument holds for arbitrary component of the above matrix, it holds for the entire matrix   \n",
    "again with each matrix coefficient being some formal value in $\\mathbf w$   \n",
    "\n",
    "b) Cayley-Hamilton, with some matrix $A$ having characteristic polynomial $g$    \n",
    "$p= \\mathbf e_i^T g \\big(A\\big)\\mathbf e_j$   \n",
    "\n",
    "c) \"Cramer's Rule\" -- by which Artin actually means the adjugate form of a matrix inverse, given on page 29, though this implies the more familiar form used in the previous exercise  \n",
    "\n",
    "d) the product rule and chain rules for formal differentiation of polynomials.  \n",
    "recall, that ex 10.Misc.2, which is shown for convenience in ('Artin_chp11.ipynb' as it helps for a difficult exercise in that chapter) gives the rule for formal differentiation of polynomials in a single variable ring, and then directly proved the product rule.  In general, even for mutlivariate polynomials, f, g, we compute the same thing two different ways  \n",
    "$\\big(f'\\cdot g + f\\cdot g'\\big) - \\big(f\\cdot g\\big)' = p \\in \\mathbb Z[\\mathbf w]$  \n",
    "and in the multivariate case, we may make use of standard basis vectors here as needed to grab a specific component  \n",
    "\n",
    "Then consider finitely many well chosen substitution homomorphisms with $\\mathbb F:=\\mathbb R$   \n",
    "(we don't have uniform bound on $d$ in this case, but we can argue indirectly -- suppose the rule wasn't true and there is some degree $f\\cdot g$ this does not hold for then we run the above and see that the above is the sum of two polynomials each of which has finite degree, has we can find some $d$ that upper bounds the degree of $p$, run our preceding argument and see $p=0$ after all, a contradiction.) \n",
    "\n",
    "A substantially identical argument proves the result for the chain rule.  \n",
    "\n",
    "The quotient rule in general is implied by the product rule and chain rule.  However a quotient with polynomials involves rational functions, not polynomials in general, so the Principle of Permanence of Identities as we've learned it does not apply.  \n",
    "\n",
    "e) The claim that a polynomial of degree $n$ has at most $n$ roots is not a claim we can model with Principle of Permanence of Identities (and chapter 11 has explicit counterexamples)  \n",
    "\n",
    "f) since polynomials are nilpotent with respect to derivative operator, we should be able to write them as a Taylor Expansion, though the details are somewhat elusive in the case of multivariate polynomials.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*application*  \n",
    "consider and matrix $C\\in \\mathbb F^{n \\times n}$ where $\\text{char}\\big(\\mathbb F\\big) \\gt n$.  \n",
    "Then *Newton's Identities* apply.  \n",
    "\n",
    "recall from Vandermonde Matrix writeup, $C$ has a characteristic polynomial   \n",
    "$g(x) = x^n + a_{n-1}x^{n-1} + a_{n-2}x^{n-2} +... + a_{1}x^{1}+ a_0$   \n",
    "noting that $a_j$ is written as a product of components in $C$  \n",
    "and for $0\\leq r \\lt n$  \n",
    "$a_{n-r}  = -\\frac{1}{r}\\Big( a_{n-r +1}\\cdot \\text{trace}\\big(C^1\\big) + a_{n-r +2}\\cdot \\text{trace}\\big( C^2\\big) + ... + a_{n}\\cdot \\text{trace}\\big(C^r\\big) \\Big)$  \n",
    "*in better form*  \n",
    "$r\\cdot a_{n-r}   +\\Big( a_{n-r +1}\\cdot \\text{trace}\\big( C^1\\big) + a_{n-r +2}\\cdot \\text{trace}\\big( C^2\\big) + ... + a_{n}\\cdot \\text{trace}\\big(C^r\\big) \\Big) = p$   \n",
    "we want to show that $p=0$  \n",
    "note $r\\in\\mathbb Z$ and this a polynomial in components in $C$ with some degree we may crudely upper bound by, say $n^n$   \n",
    "hence our Permanence of Identities argument applies.  \n",
    "\n",
    "since $r\\lt n \\lt \\text{char}\\big(\\mathbb F\\big)$  \n",
    "we know that at the end when we map back to coefficients in $\\mathbb F$, $r\\neq 0$ thus we can recover $a_{n-r}$.  (This may fail when we have small characteristic)  \n",
    "\n",
    "and for $r=n$, we have  \n",
    "\n",
    "$a_0 \\cdot  n = a_0 \\cdot \\text{trace}\\Big( \\mathbf I\\Big) = \\text{trace}\\Big(a_0 \\mathbf I\\Big) =  -\\text{trace}\\Big( a_1 \\mathbf C^{1} +  a_2 \\mathbf C^{2}  + ... + a_n \\mathbf C^{n}\\Big)$  \n",
    "which holds by Cayley Hamilton, and  $n \\lt \\text{char}\\big(\\mathbb F\\big)$, so $n\\neq 0$, which allows us to solve for $a_0$ as well.    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ex 12.4.2**   \n",
    "Let $A$ be a matrix whose entries are in the polynomial ring $\\mathbb F[t]$ and let $A'$ be obtained from $A$ by polynomial row and column operations. Relate $\\det\\big(A\\big)$ and $\\det\\big(A'\\big)$.  \n",
    "\n",
    "since we are in an integral domain, and $\\det\\big(P\\big)$, $\\det\\big(Q\\big)$ are units, they must be values $\\alpha_1,\\alpha_2 \\in \\mathbb F-\\{0\\}$, thus  \n",
    "$\\det\\big(A\\big)=\\det\\big(Q^{-1}A'P\\big)=\\det\\big(Q^{-1}\\big)\\det\\big(A'\\big)\\det\\big(P\\big)=\\alpha_1\\cdot\\alpha_2\\cdot \\det\\big(A'\\big)$  \n",
    "\n",
    "i.e. they are the same except for rescaling by a unit (i.e. an element in $\\mathbb F$)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ex 12.4.4**   \n",
    "Let $d_1, d_2,...$ be the integers referred to in Theorem (4.3) -- i.e. the diagonal elements of $A'$, where $A=Q^{-1}A'P$ for $A\\in \\mathbb Z^{m \\times n}$  --- i.e. rank normal form adapted to the ring of integers, thus $d_i$ are integers and $d_1$ divides $d_2$ which divides $d_3$ which divides $d_4$ and so on.   \n",
    "\n",
    "*prove that for* $r \\times r$ *minors of* $A$, $\\big(\\prod_{k=1}^r d_k\\big)$ *is the GCD of these determinants*   \n",
    "\n",
    "**i.)** To show it divides them, map to a square matrix, then apply Cauchy-Binet.  \n",
    "i.e. where $\\mathbf e_k$ denotes the appropriately sized standard basis vector, consider   \n",
    "\n",
    "$Y := \\bigg[\\begin{array}{c|c|c|c} \\mathbf e_{\\sigma_1} & \\mathbf e_{\\sigma_2} &\\cdots & \\mathbf e_{\\sigma_r}\\end{array}\\bigg]$  \n",
    "and  \n",
    "$X := \\bigg[\\begin{array}{c|c|c|c} \\mathbf e_{\\sigma_1'} & \\mathbf e_{\\sigma_2'} &\\cdots & \\mathbf e_{\\sigma_r'}\\end{array}\\bigg]$  \n",
    "\n",
    "WLOG we assume $A'$ is tall and skinny (if not, re-run the argument on $A^T$)  \n",
    "\n",
    "$\\text{arbitrary r x r minor} $  \n",
    "$= \\det\\Big(Y^T A X\\Big) $  \n",
    "$= \\det\\Big(Y^T\\big(Q^{-1}A'P\\big) X\\Big)$  \n",
    "$= \\det\\Big(\\big(Y^TQ^{-1}A'\\big)\\big(PX\\big)\\Big)$  \n",
    "$=\\sum_{S} \\det\\Big(\\big(Y^TQ^{-1}A'\\big)_{[m],S}\\Big)\\det\\Big(\\big(PX\\big)_{S,[m]}\\Big) $   \n",
    "\n",
    "by Cauchy-Binet  \n",
    "and $\\big(PX\\big)$ is integer valued, while  \n",
    "$\\det\\Big(\\big(Y^TQ^{-1}A'\\big)_{[m],S}\\Big)$, by inspection, each column is scaled by some $d_i$.  So by a pigeon hole argument, the largest of these is divisible by $d_r$ and the next largest must be divisible by $d_{r-1}$ and so on to the smallest being divisible by $d_1$.  \n",
    "$\\sum_{S} \\det\\Big(\\big(Y^TQ^{-1}A'\\big)_{[m],S}\\Big)\\det\\Big(\\big(PX\\big)_{S,[m]}\\Big)$  \n",
    "thus each term in the above is divisible by $\\big(\\prod_{k=1}^r d_k\\big)$ and hence $\\big(\\prod_{k=1}^4 d_k\\big)$ divides  $\\text{arbitrary r x r minor} $  \n",
    "\n",
    "**ii.)** To show that $\\big(\\prod_{k=1}^r d_k\\big)$ is the GCD, to start, we show this result for $1\\leq r\\leq n$ *provided that* $d_r \\neq 0$-- we deal with singularity at the end. We may assume **WLOG** that $A'$ is tall and skinny (if not, run the below argument on the transpose of $A$, the do the factorization.  \n",
    "\n",
    "\n",
    "$\\sum_{S} \\det\\Big(\\big(Y^TQ^{-1}A'\\big)_{[m],S}\\Big)\\det\\Big(\\big(PX\\big)_{S,[m]}\\Big)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Now  $A \\mathbf v= Q^{-1}\\Big(\\big(\\prod_{k=1}^rd_k\\big)\\big(\\mathbf \n",
    "e_k\\big)\\Big)$  \n",
    "for $k\\in\\{1,2,...,r\\}$  \n",
    "is solvable by inspection.  \n",
    "\n",
    "- - - - \n",
    "For avoidance of doubt, we have:  \n",
    "$A \\mathbf v=Q^{-1} A'\\big(P \\mathbf v\\big) =Q^{-1} A'\\mathbf w= Q^{-1}\\Big(\\big(\\prod_{k=1}^rd_k\\big)\\big(\\mathbf \n",
    "e_k\\big)\\Big)$  \n",
    "\n",
    "Suppose we are working over $\\mathbb Q$ and this becomes  \n",
    "$ A'\\mathbf w= \\big(\\prod_{k=1}^rd_k\\big)\\mathbf e_k\\Big)$  then  \n",
    "$ \\mathbf w= \\Big(\\big(\\prod_{k=1}^r d_k\\big)(A')^{-1}\\mathbf e_k\\big)\\Big)$  \n",
    "where $(A')^{-1}$ denotes the pseduo inverse, and by direct calculation the first $r$ components are integer valued so $\\mathbf w$ is integer valued.  \n",
    "- - - - \n",
    "\n",
    "This means there is some $V$ such that   \n",
    "\n",
    "$AV = \\big(\\prod_{k=1}^rd_k\\big) Q^{-1}\\bigg[\\begin{array}{c|c|c|c} \\mathbf e_{1} & \\mathbf e_{2} &\\cdots & \\mathbf e_{r}\\end{array}\\bigg] =\\big(\\prod_{k=1}^rd_k\\big)Q^{-1}S$  \n",
    "\n",
    "and again with \n",
    "$Y := \\bigg[\\begin{array}{c|c|c|c} \\mathbf e_{\\sigma_1} & \\mathbf e_{\\sigma_2} &\\cdots & \\mathbf e_{\\sigma_r}\\end{array}\\bigg]$  \n",
    "for *some* permutation set (which we'll choose later), gives us    \n",
    "\n",
    "Left multiplying each side by  $S^T$, we get  \n",
    "\n",
    "$Y^TAV = \\big(\\prod_{k=1}^rd_k\\big)Y^T Q^{-1}S$  \n",
    "\n",
    "and since $Q^{-1}$ has determinant $\\pm 1$ then $Q^{-1}S$ must have an $r\\times r$ minor of $\\pm 1$ *or else* minors that are, up to a sign, two relatively prime integers (i.e. no non-unit common factor)  -- if not then using Laplace expansion, $\\det\\big(Q^{-1}\\big)$ is in some principal ideal that isn't the unit ideal which is a contradiction.  Note: this is very easy to see when $r= m-1$ though it holds for smaller $r$.  So we select $Y_1$ and $Y_2$ to 'grab' these minors   \n",
    "\n",
    "$Y_1^T AVS= \\big(\\prod_{k=1}^r d_k\\big)\\cdot Y_1^TQ^{-1}S$  \n",
    "$Y_2^T AVS= \\big(\\prod_{k=1}^r d_k\\big)\\cdot Y_2^TQ^{-1}S$  \n",
    "and with suitable integers $\\alpha_1, \\alpha_2 \\in \\mathbb Z$ we have $\\alpha_1\\cdot \\det\\big(Y_2^TQ^{-1}\\big) +   \\alpha_2\\cdot \\det\\big(Y_1^TQ^{-1}S\\big) = 1$ thus taking determinants of each equation, rescaling the first by $\\alpha_1$ and the second by $\\alpha_2$, then adding the two equations gives   \n",
    "\n",
    "$\\alpha_1\\cdot \\det\\Big(T_1^T AVS\\big) +\\alpha_2\\det\\big(Y_2^T AVS\\big) =  \\big(\\prod_{k=1}^r d_k\\big)$  \n",
    "but applying Cauchy Binet to each of the two terms on the LHS, if there is some $\\eta \\gt \\big(\\prod_{k=1}^r d_k\\big)$ that divides each minor of $A$, then we have  $\\eta$ divides the LHS but not the RHS which is a contradiction.  \n",
    "\n",
    "**to deal with the case of**  $d_r = 0$  \n",
    "if $r$ is the maximal index where $d_r \\neq 0$, then if we consider this factorization but enlarge the ring to $\\mathbb Q$, then we see that $\\text{rank}\\big(A\\big) = r\\lt r$.  By problem \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
