{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Note:  the motivation for this post is with respect to having a lot of data for a multivariate guassian, and then updating the distribution as new data comes in.  Thus the underlying matrix of interest is the covariance matrix which is real valued, and symmetric positive definite.  (In the positive semi-definite case, rank deficiency must first be addressed before the below can be used.)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a walkthrough.  **Warning: if you don't have a deep understanding of eigenvalues and orthogonality, this won't make sense.**\n",
    "\n",
    "*Note: my posting titled \"rank_one_update_spsd_inverse_formula\" proves a more general formula, and then using that, derives this determinant result at the end*\n",
    "\n",
    "For this post, I define the determinant of a matrix as the product of the eigenvalues.  All matrices mentioned in this post are square, **m** x **m** matrices.  For purposes of this post, all indexing begins at one.  Thus these matrices have row indices $= 1, 2, ..., m - 1, m$, and column indices $= 1, 2, ..., m - 1, m$.  \n",
    "\n",
    "Recall that for any triangular matrix, we can simply multiply the diagonal elements to get the determinant. \n",
    "\n",
    "Setup: We have some real valued matrix $\\mathbf A$, where $\\mathbf A$ is symmetric positive definite, and we need to do a rank one update by adding $\\mathbf x \\mathbf x^T$, where $\\mathbf x$ is some arbitrary real valued vector (and note that since we are talking about rank one updates, $\\mathbf x \\neq \\mathbf 0$).   \n",
    "\n",
    "\n",
    "$\\mathbf A = \\mathbf P \\mathbf D \\mathbf P^T = \\mathbf P \\mathbf D \\mathbf P^{-1}$   \n",
    "equivalently, we can say\n",
    "$\\mathbf P^T \\mathbf A \\mathbf P = \\mathbf D $\n",
    "\n",
    "where $\\mathbf D_{k,k} = \\lambda_k \\gt 0$\n",
    "\n",
    "\n",
    "We know det$(\\mathbf A)$, but want to find det($\\mathbf A + \\mathbf x \\mathbf x^T)$.  \n",
    "\n",
    "Preliminary:\n",
    "\n",
    "$\\mathbf B:= \\mathbf A + \\mathbf x \\mathbf x^T$ \n",
    "\n",
    "$\\mathbf y := \\mathbf P^T \\mathbf x$  \n",
    "\n",
    "of course we can say\n",
    "\n",
    "$\\mathbf B = \\mathbf A \\mathbf A^{-1} \\mathbf B$\n",
    "\n",
    "now take determinants \n",
    "\n",
    "$det(\\mathbf B) =  det\\big(\\mathbf A \\mathbf A^{-1} \\mathbf B \\big) = det(\\mathbf A) \\Big( det(\\mathbf A^{-1}) det(\\mathbf B) \\Big)$\n",
    "\n",
    "The above is true via underlying matrix algebra, plus multiplicative properties of determinants.  \n",
    "\n",
    "On the other hand, $\\mathbf B  \\neq \\mathbf A \\mathbf D^{-1} \\mathbf B$\n",
    "\n",
    "yet we can say \n",
    "\n",
    "$det(\\mathbf B) = det(\\mathbf A) \\Big( det(\\mathbf A^{-1}) det(\\mathbf B) \\Big) =  det(\\mathbf A) \\Big( det(\\mathbf D^{-1}) det(\\mathbf B) \\Big)$\n",
    "\n",
    "via multiplicative properties of determinants, and the fact that $\\mathbf A$ and $\\mathbf D$ are similar matrices.  \n",
    "\n",
    "Thus notice, that we already have, $det(\\mathbf A)$, and we want to find $\\Big(det(\\mathbf A^{-1}) det(\\mathbf B) \\Big)$ or perhaps instead $\\Big(det(\\mathbf D^{-1}) det(\\mathbf B) \\Big)$ because once we have this ratio, it is simple scalar multiplication to get $ det(\\mathbf B)$. \n",
    "\n",
    "\n",
    "- - - - \n",
    "we start off interested in $\\mathbf B$, but notice that $\\mathbf P^T \\mathbf B \\mathbf P$ is similar to $\\mathbf B$, and hence has the same eigenvalues, and thus the same determinant as $\\mathbf B$. So we begin by looking at $\\mathbf D^{-1} \\mathbf P^T \\mathbf B \\mathbf P$.  We will take the determinant of this because $det \\big(\\mathbf D^{-1} \\mathbf P^T \\mathbf B \\mathbf P \\big) = det(\\mathbf A^{-1}) det(\\mathbf B)$-- though first we will do a few helpful factorizations. \n",
    "\n",
    "$ \\mathbf D^{-1} \\mathbf P^T \\mathbf B  \\mathbf P = \\mathbf D^{-1} \\Big(\\mathbf P^T \\Big(\\mathbf A + \\mathbf x \\mathbf x^T \\Big) \\mathbf P \\Big) = \\mathbf D^{-1}\\Big( \\mathbf P^T \\mathbf A \\mathbf P + \\mathbf P^T \\mathbf x \\mathbf x^T \\mathbf P \\Big) = \\mathbf D^{-1} \\Big(\\mathbf D + \\mathbf y \\mathbf y^T \\Big) = \\mathbf I + \\mathbf D^{-1} \\mathbf y \\mathbf y^T $  \n",
    "- - - -\n",
    "Now notice:  \n",
    "\n",
    "$rank \\big(\\mathbf D^{-1} \\mathbf y \\mathbf y^T \\big) = 1$  \n",
    "$trace \\big( \\mathbf D^{-1} \\mathbf y \\mathbf y^T \\big) = trace \\big( \\mathbf y^T \\mathbf D^{-1} \\mathbf y \\big) = \\mathbf y^T \\mathbf D^{-1} \\mathbf y = \\sum_{k = 1}^{m} \\frac{y_k^2}{\\lambda_k}$  \n",
    "\n",
    "$\\mathbf D^{-1} \\mathbf y \\mathbf y^T = \\mathbf V \\mathbf R \\mathbf V ^{H} = \\mathbf V \\mathbf R \\mathbf V ^{-1}$\n",
    "\n",
    "$\\mathbf R_{i,j} = \\begin{Bmatrix}\n",
    " & \\sum_{k = 1}^{m} \\frac{y_k^2}{\\lambda_k} &if  &  j = i= 1\\\\ \n",
    " & 0 &if  &  j = 1 , i \\geq 2\\\\ \n",
    " & 0 &if  &  2 \\leq j \\leq  i  \\leq m \\\\ \n",
    " &*& else\\\\\n",
    " \\end{Bmatrix}$\n",
    "\n",
    "Which is to say that $\\mathbf R$ is stricly upper triangular, except it's top left entry is non-negative, and is given by, $\\mathbf y^T \\mathbf D^{-1} \\mathbf y$.  (As convention, an entry given by * indicates we are unconcerned with whatever arbitrary value it has.)\n",
    "\n",
    "- - - -\n",
    "$ \\mathbf D^{-1} \\mathbf P^T \\mathbf B \\mathbf P  = \\mathbf I + \\mathbf D^{-1} \\mathbf y \\mathbf y^T =\\mathbf I + \\Big(\\mathbf V \\mathbf R \\mathbf V ^{-1}\\Big) = \\mathbf V \\mathbf I \\mathbf V ^{-1} + \\mathbf V \\mathbf R \\mathbf V ^{-1} = \\mathbf V \\Big( \\mathbf I + \\mathbf R \\Big) \\mathbf V ^{-1} $  \n",
    "\n",
    "$ det\\Big( \\mathbf D^{-1} \\mathbf P^T \\mathbf B \\mathbf P \\Big) = \n",
    " det\\Big(\\mathbf D^{-1} \\Big) det \\Big(\\mathbf P^T \\Big)  det\\Big(\\mathbf B \\Big) det\\big( \\mathbf P \\Big)  =  det\\Big(\\mathbf D^{-1} \\Big) det\\Big(\\mathbf B \\Big)\n",
    "=  det \\Big(\\mathbf V \\Big( \\mathbf I + \\mathbf R \\Big) \\mathbf V ^{-1} \\Big) $   \n",
    "\n",
    "$ \n",
    " det\\Big(\\mathbf D^{-1} \\Big) det\\Big(\\mathbf B \\Big) = det \\Big(\\mathbf V \\Big) det \\Big( \\mathbf I + \\mathbf R \\Big) det \\Big(\\mathbf V ^{-1} \\Big)  = det \\Big( \\mathbf I + \\mathbf R \\Big)$ \n",
    "\n",
    "recalling that: $det\\Big(\\mathbf D^{-1} \\Big) = det\\Big(\\mathbf A^{-1} \\Big) $  \n",
    "\n",
    "$ det\\Big(\\mathbf A^{-1} \\Big) det\\Big(\\mathbf B \\Big)= det \\Big( \\mathbf I + \\mathbf R \\Big) $  \n",
    "\n",
    "$det\\Big(\\mathbf A^{-1} \\Big) det\\Big(\\mathbf B \\Big)  = det \\Big( \\mathbf I + \\mathbf R \\Big)  = \\Big(1 + \\sum_{k = 1}^{m} \\frac{y_k^2}{\\lambda_k}\\Big) * \\Big(\\prod_{i = 2}^{m} 1 \\Big) = 1 + \\sum_{k = 1}^{m} \\frac{y_k^2}{\\lambda_k} = 1 + \\mathbf y^T \\mathbf D^{-1} \\mathbf y = 1 + \\Big(\\mathbf P^T \\mathbf x\\Big)^T \\mathbf D^{-1} \\Big( \\mathbf P^T \\mathbf x \\Big)$\n",
    "\n",
    "$det\\Big(\\mathbf A^{-1} \\Big) det\\Big(\\mathbf B \\Big)   =  1 + \\Big(\\mathbf x^T \\mathbf P \\Big) \\mathbf D^{-1} \\Big( \\mathbf P^T \\mathbf x \\Big) = 1 + \\mathbf x^T \\Big(\\mathbf P \\mathbf D^{-1} \\mathbf P^T \\Big) \\mathbf x = 1 + \\mathbf x^T \\Big( \\mathbf A^{-1} \\Big) \\mathbf x = 1 + \\mathbf x^T \\mathbf A^{-1} \\mathbf x $\n",
    "\n",
    "\n",
    "To conclude: \n",
    "\n",
    "$ det\\big(\\mathbf A \\big) det\\big(\\mathbf A^{-1} \\big) det\\big(\\mathbf B \\big) = det\\big(\\mathbf B \\big) =   det\\big(\\mathbf A \\big) \\big(1 + \\mathbf x^T \\mathbf A^{-1} \\mathbf x\\big) $\n",
    "\n",
    "QED\n",
    "- - - - \n",
    "**some misc notes:**\n",
    "\n",
    "$\\mathbf A = \\mathbf P \\mathbf D \\mathbf P^T = \\mathbf P \\mathbf D \\mathbf P^{-1}$  \n",
    "\n",
    "is the basic eigenvalue decomposition for any real valued symmetric matrix. In general we can say that $det \\Big(\\mathbf A \\Big) =  det \\Big( \\mathbf P \\mathbf D \\mathbf P^{-1} \\Big) = det \\Big( \\mathbf P \\Big)  det \\Big( \\mathbf D \\Big) det \\Big( \\mathbf P^{-1} \\Big) = det \\Big( \\mathbf D \\Big) det  \\Big( \\mathbf P \\Big)  det \\Big( \\mathbf P^{-1} \\Big) = det \\Big( \\mathbf D \\Big) $  \n",
    "\n",
    "because finite scalar multiplication commutes, and $det\\Big( \\mathbf P \\Big)  det \\Big( \\mathbf P^{-1} \\Big) = 1$\n",
    "\n",
    "Additionally, because $rank \\big(\\mathbf D^{-1} \\mathbf y \\mathbf y^T \\big) = 1$, we know that all but one of its eigenvalues are equal to zero.   And from $trace \\big( \\mathbf D^{-1} \\mathbf y \\mathbf y^T \\big) = \\sum_{k = 1}^{m} \\frac{y_k^2}{\\lambda_k}$, we know that the sum of these eigenvalues is the above trace, and since all eigs are zero except for one of them, then we know the value of the single non-negative eigenvalue is this trace. \n",
    "\n",
    "The matrix $\\mathbf D^{-1} \\mathbf y \\mathbf y^T$ may be diagonalized in the form of $\\mathbf D^{-1} \\mathbf y \\mathbf y^T = \\mathbf S \\mathbf \\Gamma \\mathbf S^{-1}$, where $\\mathbf \\Gamma$ has its eigenvalues, and $ \\mathbf S$ has its eigenvectors, getting the same final result.  \n",
    "\n",
    "However, to stick with othgonality, avoid possible confusion over which diagonal matrix is being referred to, and avoid discussion of linear independence of vectors in null space of $\\mathbf D^{-1} \\mathbf y \\mathbf y^T$, this writeup instead made use of the fact that all square matrices may be triangularized via the Schur Decomposition, $\\mathbf D^{-1} \\mathbf y \\mathbf y^T = \\mathbf V \\mathbf R \\mathbf V ^{H}$, where $\\mathbf R $ is upper triangular and $\\mathbf V $ is unitary, hence $\\mathbf V ^{H} = \\mathbf V ^{-1}$.  Triangular matrices (including the special case of diagonal matrices) are pleasant because you can see the eigenvalues on the diagonal entries.  Thus when we add $\\Big( \\mathbf I + \\mathbf R \\Big)$, we are adding two matrices that are both upper triangular, and we get an upper triangular matrix.  \n",
    "\n",
    "Because $\\mathbf R$ has a zero on all of its diagonal entries, except one, that means the matrix $\\Big( \\mathbf I + \\mathbf R \\Big)$, has a value $= 1$ on all of its diagonal entries, except the top entry has a value of $1 + \\sum_{k = 1}^{m} \\frac{y_k^2}{\\lambda_k}$. The diagonal entries of $\\Big( \\mathbf I + \\mathbf R \\Big)$ are the eigenvalues that matrix, and the product of these eigenvalues gives $det \\Big( \\mathbf I + \\mathbf R \\Big) = 1 + \\sum_{k = 1}^{m} \\frac{y_k^2}{\\lambda_k}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [py35]",
   "language": "python",
   "name": "Python [py35]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
