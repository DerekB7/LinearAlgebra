{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*exercise 2.4.12:*  \n",
    "show that for $\\text{r x r  } \\mathbf A, \\mathbf B$ and $\\text{(m-r) x (m-r)   } \\mathbf Y, \\mathbf Z$  \n",
    "\n",
    "the blocked matrix  \n",
    "$\\begin{bmatrix}\n",
    "\\mathbf A & *\\\\ \n",
    "\\mathbf 0 & \\mathbf Y\n",
    "\\end{bmatrix} \\begin{bmatrix}\n",
    "\\mathbf B & *\\\\ \n",
    "\\mathbf 0 & \\mathbf Z\n",
    "\\end{bmatrix} =  \\begin{bmatrix}\n",
    "\\mathbf {AB} & *\\\\ \n",
    "\\mathbf 0 & \\mathbf {YZ}\n",
    "\\end{bmatrix}$  \n",
    "\n",
    "i.e. blocked matrices of the form   \n",
    "$\\begin{bmatrix}\n",
    "\\mathbf A & *\\\\ \n",
    "\\mathbf 0 & \\mathbf Y\n",
    "\\end{bmatrix}$   \n",
    "\n",
    "are closed under (standard matrix) multiplication  \n",
    "where * refers to entries we are not concerned with  \n",
    "\n",
    "- - - - - \n",
    "*remark:*   \n",
    "based the definition of addition of matrices it is immediate that we have this closure for the binary operation of matrix addition  \n",
    "$\\alpha_1\\begin{bmatrix}\n",
    "\\mathbf A & *\\\\ \n",
    "\\mathbf 0 & \\mathbf Y\n",
    "\\end{bmatrix} +\\alpha_2\\begin{bmatrix}\n",
    "\\mathbf B & *\\\\ \n",
    "\\mathbf 0 & \\mathbf Z\n",
    "\\end{bmatrix} =  \\begin{bmatrix}\n",
    "\\alpha_1\\mathbf A+ \\alpha_2\\mathbf B & *\\\\ \n",
    "\\mathbf 0 & \\alpha_1\\mathbf Y + \\alpha_2\\mathbf Z\n",
    "\\end{bmatrix}$  \n",
    "\n",
    "that blocked upper triangular matrices with an $m$ x $m$ block in the upper left corner and ($m-r$ x $m-r$ block in the lower right corner) for a vector space  \n",
    "\n",
    "After going through the below proof --that we have closure under multiplication-- combined with this, we'll see that we can form a non-trivial linearly dependent combination of powers of blocked matrix which implies its inverse must be blocked of the same form. Hence non-singular blocked matrices of this type form a (sub)group.    \n",
    "\n",
    "\n",
    "\n",
    "- - - - -  \n",
    "*proof of closure under multiplication*    \n",
    "\n",
    "(note these blocks may not render correctly on github but will render correctly locally)  \n",
    "\n",
    "$\\begin{bmatrix}\n",
    "\\mathbf A & *\\\\ \n",
    "\\mathbf 0 & \\mathbf Y\n",
    "\\end{bmatrix}\\begin{bmatrix}\n",
    "\\mathbf B & *\\\\ \n",
    "\\mathbf 0 & \\mathbf Z\n",
    "\\end{bmatrix} = \n",
    "\\bigg[\\begin{array}{c|c|c|c|c|c|c}\n",
    "\\begin{bmatrix} \\mathbf a_1\\\\\\mathbf 0_{m-r} \\end{bmatrix}   &\\cdots &  \\begin{bmatrix} \\mathbf a_r\\\\\\mathbf 0_{m-r}\\end{bmatrix}  & \\begin{bmatrix} *\\\\\\mathbf y_1\\end{bmatrix}&  \\cdots &  \\begin{bmatrix} *\\\\\\mathbf y_{m-r}\\end{bmatrix} \\end{array}\\bigg]\\begin{bmatrix}\n",
    "\\mathbf B & *\\\\ \n",
    "\\mathbf 0 & \\mathbf Z\n",
    "\\end{bmatrix} $  \n",
    "$  = \n",
    "\\bigg[\\begin{array}{c|c|c|c|c|c|c}\n",
    "\\begin{bmatrix} \\sum_{k=1}^r b^{(1)}_k\\mathbf a_k \\\\\\mathbf 0_{m-r} \\end{bmatrix}   &\\cdots &  \\begin{bmatrix} \\sum_{k=1}^r b^{(r)}_k\\mathbf a_k \\\\\\mathbf 0_{m-r}\\end{bmatrix}  & \\begin{bmatrix} *\\\\ \\sum_{k=1}^{m-r} z^{(1)}_k\\mathbf y_k\\end{bmatrix}&  \\cdots &  \\begin{bmatrix} *\\\\ \\sum_{k=1}^{m-r} z^{(m-r)}_k\\mathbf y_k\\end{bmatrix} \\end{array}\\bigg]$  \n",
    "$=  \\begin{bmatrix}\n",
    "\\mathbf {AB} & *\\\\ \n",
    "\\mathbf 0 & \\mathbf {YZ}\n",
    "\\end{bmatrix}$  \n",
    "\n",
    "*corollary*  \n",
    "\n",
    "$\\begin{bmatrix}\n",
    "\\mathbf A & *\\\\ \n",
    "\\mathbf 0 & \\mathbf Y\n",
    "\\end{bmatrix}^k = \\begin{bmatrix}\n",
    "\\mathbf A^k & *\\\\ \n",
    "\\mathbf 0 & \\mathbf Y^k\n",
    "\\end{bmatrix}$  \n",
    "\n",
    "this is immediate by induction, i.e. our above result gives us the $k=2$ case by selecting $\\mathbf B:= \\mathbf A$ and $\\mathbf Z:= \\mathbf Y$ and this is our base case.  For the inductive case using associativity, our inductive hypothesis and our above result with $\\mathbf B:= \\mathbf A^{k-1}$  and $\\mathbf Z:= \\mathbf Y^{k-1}$ gives the result, i.e.  \n",
    "\n",
    "$\\begin{bmatrix}\n",
    "\\mathbf A & *\\\\ \n",
    "\\mathbf 0 & \\mathbf Y\n",
    "\\end{bmatrix}^k = \\begin{bmatrix}\n",
    "\\mathbf A & *\\\\ \n",
    "\\mathbf 0 & \\mathbf Y\n",
    "\\end{bmatrix}\\begin{bmatrix}\n",
    "\\mathbf A & *\\\\ \n",
    "\\mathbf 0 & \\mathbf Y\n",
    "\\end{bmatrix}^{k-1} = \\begin{bmatrix}\n",
    "\\mathbf A & *\\\\ \n",
    "\\mathbf 0 & \\mathbf Y\n",
    "\\end{bmatrix}\\begin{bmatrix}\n",
    "\\mathbf A^{k-1} & *\\\\ \n",
    "\\mathbf 0 & \\mathbf Y^{k-1}\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "\\mathbf A^k & *\\\\ \n",
    "\\mathbf 0 & \\mathbf Y^k\n",
    "\\end{bmatrix}$   \n",
    "\n",
    "\n",
    "*corollary:*  \n",
    "$\\det\\big(\\begin{bmatrix}\n",
    "\\mathbf A & *\\\\ \n",
    "\\mathbf 0 & \\mathbf Y\n",
    "\\end{bmatrix}\\big) = \\det\\big(\\mathbf A\\big) \\det\\big(\\mathbf Y\\big) $  \n",
    "\n",
    "*proof:*    \n",
    "if either $\\mathbf A$ or $\\mathbf Y$ is singular then the determinant is obviously 0   \n",
    "(i.e. consider the first $r$ standard basis vectors for $\\mathbf A$ and the right nullspace or the standard basis vectors $r+1$ through $m$ for the left nullspace and $\\mathbf Y$)  \n",
    "\n",
    "supposing they are both non-singular, then we have   \n",
    "$\\det\\big(\\begin{bmatrix}\n",
    "\\mathbf I_m & *\\\\ \n",
    "\\mathbf 0 & \\mathbf I_r\n",
    "\\end{bmatrix}\\big) = 1$  \n",
    "by immediate application of the recursive definition of determinant (expansion by minors)  \n",
    "\n",
    "and  \n",
    "$\\begin{bmatrix}\n",
    "\\mathbf A & *\\\\ \n",
    "\\mathbf 0 & \\mathbf Y\n",
    "\\end{bmatrix} \\begin{bmatrix}\n",
    "\\mathbf A^{-1} & \\mathbf 0 \\\\ \n",
    "\\mathbf 0 & \\mathbf Y^{-1}\n",
    "\\end{bmatrix} =  \\begin{bmatrix}\n",
    "\\mathbf I_m & *\\\\ \n",
    "\\mathbf 0 & \\mathbf I_r\n",
    "\\end{bmatrix}$  \n",
    "\n",
    "hence  \n",
    "$d \\cdot \\det\\big(\\mathbf A^{-1}\\big)\\det\\big(\\mathbf Y^{-1}\\big)= \\det\\big(\\begin{bmatrix}\n",
    "\\mathbf A & *\\\\ \n",
    "\\mathbf 0 & \\mathbf Y\n",
    "\\end{bmatrix}\\big)\\det \\big( \\begin{bmatrix}\n",
    "\\mathbf A^{-1} & \\mathbf 0 \\\\ \n",
    "\\mathbf 0 & \\mathbf Y^{-1}\n",
    "\\end{bmatrix}\\big) =  \\det\\big(\\begin{bmatrix}\n",
    "\\mathbf I_m & *\\\\ \n",
    "\\mathbf 0 & \\mathbf I_r\n",
    "\\end{bmatrix}\\big)=1$  \n",
    "\n",
    "i.e.  \n",
    "$\\det\\big(\\begin{bmatrix}\n",
    "\\mathbf A & *\\\\ \n",
    "\\mathbf 0 & \\mathbf Y\n",
    "\\end{bmatrix}\\big) = d = \\det\\big(\\mathbf A^{-1}\\big)^{-1}\\det\\big(\\mathbf Y^{-1}\\big)^{-1}= \\det\\big(\\mathbf A\\big)\\det\\big(\\mathbf Y\\big) $  \n",
    "\n",
    "where \n",
    "\n",
    "$\\det\\big(\\begin{bmatrix}\n",
    "\\mathbf A^{-1} & \\mathbf 0 \\\\ \n",
    "\\mathbf 0 & \\mathbf Y^{-1}\n",
    "\\end{bmatrix}\\big) = \\det\\big(\\mathbf A^{-1}\\big)\\det\\big(\\mathbf Y^{-1}\\big) = \\det\\big(\\begin{bmatrix}\n",
    "\\mathbf A^{-1} & \\mathbf 0 \\\\ \n",
    "\\mathbf 0 & \\mathbf I_r\n",
    "\\end{bmatrix}\\big)\\det\\big(\\begin{bmatrix}\n",
    "\\mathbf I_m & \\mathbf 0 \\\\ \n",
    "\\mathbf 0 & \\mathbf Y^{-1}\n",
    "\\end{bmatrix}\\big)$  \n",
    "\n",
    "and the fact that \n",
    "$\\det\\big(\\begin{bmatrix}\n",
    "\\mathbf A^{-1} & \\mathbf 0 \\\\ \n",
    "\\mathbf 0 & \\mathbf I_{m-r}\n",
    "\\end{bmatrix}\\big) = \\det\\big(\\mathbf A^{-1}\\big)$  \n",
    "follows by induction on the dimension of $\\mathbf A^{-1}$ (i.e. r = 1 is immediate because the matrix is diagonal, which gives an inductive hypothesis and apply recursive definition of determinant).  \n",
    "\n",
    "Another approach is to recognize  \n",
    "$\\det\\big(\\begin{bmatrix}\n",
    "\\mathbf A^{-1} & \\mathbf 0 \\\\ \n",
    "\\mathbf 0 & \\mathbf I_{m-r}\n",
    "\\end{bmatrix}\\big) = \\det\\big(\\begin{bmatrix}\n",
    "\\mathbf I_{m-r} & \\mathbf 0 \\\\ \n",
    "\\mathbf 0 & \\mathbf A^{-1}\n",
    "\\end{bmatrix}\\big) = \\det\\big(\\mathbf A^{-1}\\big)$ \n",
    "because the two matrices are related by graph isomorphism, and the latter calculation follows directly from application of recursive definition of the determinant  \n",
    "\n",
    "\n",
    "*corollary*   \n",
    "over $\\mathbb C$ we can easily see that the eigenvalues of the blocked matrix are given by those of \n",
    "$\\mathbf A $ and $\\mathbf Y$ because  \n",
    "\n",
    "$\\begin{bmatrix}\n",
    "\\mathbf A & *\\\\ \n",
    "\\mathbf 0 & \\mathbf Y\n",
    "\\end{bmatrix}^k = \\begin{bmatrix}\n",
    "\\mathbf A^k & *\\\\ \n",
    "\\mathbf 0 & \\mathbf Y^k\n",
    "\\end{bmatrix}$   \n",
    "\n",
    "for all natural numbers of $k$ so  \n",
    "\n",
    "$\\text{trace}\\Big(\\begin{bmatrix}\n",
    "\\mathbf A & *\\\\ \n",
    "\\mathbf 0 & \\mathbf Y\n",
    "\\end{bmatrix}^k\\Big) =  \\text{trace}\\Big(\\mathbf A^k\\Big) + \\text{trace}\\Big(\\mathbf Y^k\\Big)$    \n",
    "\n",
    "which uniquely characterizes the eigenvalues of $\\begin{bmatrix}\n",
    "\\mathbf A & *\\\\ \n",
    "\\mathbf 0 & \\mathbf Y\n",
    "\\end{bmatrix}^k$  \n",
    "\n",
    "as being those of $\\mathbf A$ and $\\mathbf Y$  \n",
    "see e.g. one of 3 different proofs in the Vandermonde matrix notebook.  The most common approach is to apply Newton's Identities and observe that if trace matches for the first $m$ powers then 2 matrices of the same dimension have the same characteristic polynomial (and hence the same eigenvalues)  \n",
    "\n",
    "*corollary*  \n",
    "non-singular blocked matrices are closed under inversion  \n",
    "\n",
    "i.e.  \n",
    "$\\begin{bmatrix}\n",
    "\\mathbf A & *\\\\ \n",
    "\\mathbf 0 & \\mathbf Y\n",
    "\\end{bmatrix}^{-1}= \\begin{bmatrix}\n",
    "\\mathbf A^{-1} & *\\\\ \n",
    "\\mathbf 0 & \\mathbf Y^{-1}\n",
    "\\end{bmatrix}$  \n",
    "\n",
    "for a proof we can apply Cayley Hamilton or if we don't know that, we may instead observe that a linear combination of the first \n",
    "$m^2 + 1$ powers of this matrix must be linearly dependent since these matrices live in a vector space with $m^2$ components (and given the blocked structure we can actually tighten this to $m^2 + 1 - m\\cdot r$ if we'd like)  \n",
    "\n",
    "hence write out this annihilating polynomial  \n",
    "\n",
    "$p\\Big(\\begin{bmatrix}\n",
    "\\mathbf A & *\\\\ \n",
    "\\mathbf 0 & \\mathbf Y\n",
    "\\end{bmatrix}\\Big) = \\alpha_0 \\mathbf I + \\sum_{k=1}^{m^2} \\alpha_k \\begin{bmatrix}\n",
    "\\mathbf A & *\\\\ \n",
    "\\mathbf 0 & \\mathbf Y\n",
    "\\end{bmatrix}^k = \\sum_{k=0}^{m^2} \\alpha_k \\begin{bmatrix}\n",
    "\\mathbf A & *\\\\ \n",
    "\\mathbf 0 & \\mathbf Y\n",
    "\\end{bmatrix}^k = \\mathbf 0$  \n",
    "\n",
    "where by linear dependence we may select not all $\\alpha_k = 0$.  Since the matrix is non-singular, it isn't nilpotent and hence at least two $\\alpha_i$ are non zero.  There are finitely many of these coefficients, so select the lowest index associated with a non-zero scalar, call it $j$, now consider dividing out the negative of that coefficient $ \\alpha_j$ and multiplying the polynomial by \n",
    "\n",
    "$\\begin{bmatrix}\n",
    "\\mathbf A & *\\\\ \n",
    "\\mathbf 0 & \\mathbf Y\n",
    "\\end{bmatrix}^{-j-1}$   \n",
    "\n",
    "(i.e. so that the lowest power is negative one -- the inverse matrix, and it has a coefficient of minus one, so we'll add it to both sides)  \n",
    "\n",
    "after 'moving' the inverse to the other side of the equation we get  \n",
    "\n",
    "$-1(\\alpha_j)^{-1}\\sum_{k \\gt j }\\alpha_k \\begin{bmatrix}\n",
    "\\mathbf A & *\\\\ \n",
    "\\mathbf 0 & \\mathbf Y\n",
    "\\end{bmatrix}^{k-j-1} = \\begin{bmatrix}\n",
    "\\mathbf A & *\\\\ \n",
    "\\mathbf 0 & \\mathbf Y\n",
    "\\end{bmatrix}^{-1}$  \n",
    "\n",
    "where the LHS is a linear combination of non-negative powers of our matrix, i.e. \n",
    "$\\begin{bmatrix}\n",
    "\\mathbf A^k & *\\\\ \n",
    "\\mathbf 0 & \\mathbf Y^k\n",
    "\\end{bmatrix}$ \n",
    "which are all block triangular of the same form, and we know addition of two matrices of this form results in a blocked matrix of the same form, hence the inverse must be of this same form.  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*remark:*  \n",
    "we can take the observation that the inverse is written as a linear combination of powers of  \n",
    "$\\begin{bmatrix}\n",
    "\\mathbf A^k & *\\\\ \n",
    "\\mathbf 0 & \\mathbf Y^k\n",
    "\\end{bmatrix}$ \n",
    "\n",
    "as well as Cayley Hamilton, as well as our earlier corollary that  \n",
    "$\\det\\big(\\begin{bmatrix}\n",
    "\\mathbf A & *\\\\ \n",
    "\\mathbf 0 & \\mathbf Y\n",
    "\\end{bmatrix}\\big) = \\det\\big(\\mathbf A\\big) \\det\\big(\\mathbf Y\\big)$\n",
    "\n",
    "to see that because  \n",
    "$\\begin{bmatrix}\n",
    "\\mathbf A & *\\\\ \n",
    "\\mathbf 0 & \\mathbf Y\n",
    "\\end{bmatrix}$\n",
    "and \n",
    "\n",
    "$\\begin{bmatrix}\n",
    "\\mathbf A & \\mathbf 0\\\\ \n",
    "\\mathbf 0 & \\mathbf Y\n",
    "\\end{bmatrix}$   \n",
    "\n",
    "have the same characteristic polynomial, then  \n",
    "$\\begin{bmatrix}\n",
    "\\mathbf A & *\\\\ \n",
    "\\mathbf 0 & \\mathbf Y\n",
    "\\end{bmatrix}^{-1} = \\begin{bmatrix}\n",
    "\\mathbf A^{-1} & *\\\\ \n",
    "\\mathbf 0 & \\mathbf Y^{-1}\n",
    "\\end{bmatrix}$  \n",
    "\n",
    "because the inverse embedded in the blocked structure is the exact same linear combination of powers of $\\mathbf A$ in our block triangular matrix and in our blocked diagonal matrix and by inspection  \n",
    "\n",
    "$\\begin{bmatrix}\n",
    "\\mathbf A & *\\\\ \n",
    "\\mathbf 0 & \\mathbf Y\n",
    "\\end{bmatrix}\\begin{bmatrix}\n",
    "\\mathbf A^{-1} & *\\\\ \n",
    "\\mathbf 0 & \\mathbf Y^{-1}\n",
    "\\end{bmatrix} = \\mathbf I$   \n",
    "\n",
    "There are more direct ways blocked multiplication approaches to verify the block inverse -- e.g. the standard way would involve taking Schur Complements.  The use of a common invariant (characteristic polynomial) allows us to bypass these granual calulastions and nicely relate the easy case of block diagonal to the more complicated case of block triangular.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**also see**  \n",
    "\n",
    "\"Artin_chp2_SLN_subgroup_generators.ipynb\"  \n",
    "\n",
    "in the linear algebra folder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
