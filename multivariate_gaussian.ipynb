{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracts from my response trying to explain how to interpret the estimation of a covariance matrix for a multivariate gaussian to someone online."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think there are more intuitive derivations than what was presented.  See : https://en.wikipedia.org/wiki/Estimation_of_covariance_matrices  \n",
    " \n",
    "\n",
    "I'm not convinced that any of them will resonate though as you seem to be unfamiliar with some fundamental results from Linear Algebra.\n",
    "\n",
    "  \n",
    "\n",
    "For starters, I'd spend time studying the trace operation, which is a very simple yet surprisingly powerful operation.  Trace is defined only for square matrices.  That also means that the trace of some scalar ( a degnerate, 1x1 matrix) is that same scalar.  So $trace(72) = 72$, $trace(1.4) = 1.4$, and for some arbitrary scalar $\\gamma$, we can say $trace(\\gamma) = \\gamma$, and so forth.\n",
    "\n",
    " \n",
    "At the simplest level, consider two n x 1, real valued vectors: $\\mathbf a$ and $\\mathbf b$. The inner product of these two vectors is $\\mathbf a^T \\mathbf b = \\gamma$.  Thus $\\mathbf a^T \\mathbf b =trace(\\mathbf a^T \\mathbf b) = trace(\\gamma)= \\gamma$.  \n",
    "\n",
    "  \n",
    "\n",
    "With respect to vectors, the trace links the inner product and outer product.  So take a look at the matrix that you get from the outer product, $ \\mathbf b \\mathbf a^T$.  Notice that the diagonal elements are $\\mathbf a_i *\\mathbf b_i$, and the sum of these is called a trace.  \n",
    "\n",
    "\n",
    "But $\\mathbf a_1 *\\mathbf b_1 + \\mathbf a_2*\\mathbf b_2 + ...  +  \\mathbf a_n*\\mathbf b_n$ is also your definition of an inner product in Reals --aka a dot product.  (I'm trying to avoid typical summation notation since $\\Sigma$ is used for the covariance matrix.)  \n",
    "\n",
    "Thus we can say $\\mathbf a^T \\mathbf b = \\gamma =trace(\\mathbf a^T \\mathbf b) = trace( \\mathbf b \\mathbf a^T) $\n",
    "\n",
    "For purposes of one of your immediate questions, the above is enough, but you should read up some more on the cyclic property of traces.  Without understanding the cyclic property, I don't know how you can understand the link between trace and eigenvalues.  \n",
    "\n",
    " \n",
    "In the stack exchange post, you said you had confusion with:\n",
    "\n",
    "$(x_i-\\mu)^T\\Sigma^{-1}(x_i-\\mu)=trace(\\Sigma^{-1}(x_i-\\mu)(x_i-\\mu)^T)$\n",
    "\n",
    "  \n",
    "\n",
    "One simple way to tackle this is: let's call the row vector $(x_i-\\mu)^T = \\mathbf a^T$.  And using associativity, let's call the column vector $\\Big(\\Sigma^{-1}(x_i-\\mu) \\Big)= \\mathbf b$.  We can then say: $(x_i-\\mu)^T\\Sigma^{-1}(x_i-\\mu) = (x_i-\\mu)^T \\Big(\\Sigma^{-1}(x_i-\\mu)\\Big)= \\mathbf a^T \\mathbf b = trace(\\mathbf a^T \\mathbf b) = trace( \\mathbf b \\mathbf a^T) $\n",
    "\n",
    "  \n",
    "\n",
    "and from here use associativity and do substitution $trace(\\mathbf b \\mathbf a^T) = trace(\\Big(\\Sigma^{-1}(x_i-\\mu) \\Big) \\big((x_i-\\mu)^T \\big)) = trace(\\Sigma^{-1}(x_i-\\mu)(x_i-\\mu)^T)  $ \n",
    "\n",
    "- - - -\n",
    "**Post-Script:**\n",
    "\n",
    "while I find the walkthrough in the wikipedia link in the middle under the heading \"Maximum-likelihood estimation for the multivariate normal distribution\" to be most intuitive, here is a sketch regarding the matrix calculus approach.  This section could be called \"guessing derivatives of the trace operation on matrices\"\n",
    "\n",
    "a quick fact about matrix derivatives involving the trace operation:\n",
    "\n",
    "$\\frac{\\partial}{\\partial \\mathbf X} trace(\\mathbf A \\mathbf X \\mathbf B) = \\frac{\\partial}{\\partial \\mathbf X} trace( \\mathbf X \\mathbf B \\mathbf A)  $ \n",
    "\n",
    "via cyclic property of trace.  There are many reference sheets containing matrix derivatives, but you can basically guess most of them.  \n",
    "\n",
    "$\\frac{\\partial}{\\partial \\mathbf X} trace( \\mathbf X \\mathbf B \\mathbf A)  =  \\mathbf A^T \\mathbf B^T $  \n",
    " \n",
    "This seems pretty intuitive from what we know about the product rule that if you differentiate $\\mathbf X$ against itself, it goes away.  You might have guessed a result of $\\mathbf B  \\mathbf A $ instead of the answer $\\big(\\mathbf B  \\mathbf A \\big)^T $.  In fact $\\mathbf B  \\mathbf A $ is not a bad guess, but a quick examination of the dimensions shows you need the transpose of this.  Why?  The partials of $\\mathbf X$ are collected in a matrix with the number of rows equal to the number of rows in $\\mathbf X$, which is equal to the number of columns in $\\mathbf A$ (i.e. rows in $\\mathbf A^T$).  Also the number of columns associated with your partials must be in line with the number of columns in $\\mathbf X$, and hence the rows in $\\mathbf B$ (aka columns in  $\\mathbf B^T$).  Dimensions can be cumbersome at first, but in a manner similar to dimensional analysis in science, they give strong clues to the answers you are looking for. A special case of the above is where all three are square, and $\\mathbf A$ and $\\mathbf B$ are both the identity matrix.  \n",
    "\n",
    "$\\frac{\\partial}{\\partial \\mathbf X} trace( \\mathbf X \\mathbf I \\mathbf I) = \\frac{\\partial}{\\partial \\mathbf X} trace( \\mathbf X)  =  \\mathbf I$\n",
    "\n",
    "which should be pretty intuitive, in the same way the derivative of a scalar variable x is 1.  Also note that since the trace is the sum of diagonal elements in $\\mathbf X$, you can easily verify that the trace of $\\mathbf X$ does not change if you make a small change in a non-diagonal element of $\\mathbf X$, and if you make a small change in value on the diagonal, then all you are doing is: $\\frac{\\partial}{\\partial \\mathbf X_{k,k}} trace( \\mathbf X) = \\frac{\\partial}{\\partial \\mathbf X_{k,k}}\\Big(\\mathbf X_{1,1} + \\mathbf X_{2,2} + \\mathbf X_{3,3} + ... + \\mathbf X_{n,n} \\Big)$ = $   \\frac{\\partial}{\\partial \\mathbf X_{k,k}} \\mathbf X_{k,k} + 0 = 1$  because:\n",
    "\n",
    "$\\begin{matrix}\\frac{\\partial}{\\partial \\mathbf X_{k,k}} \\mathbf X_{i,i} = 0 &if& i\\neq k\\\\       \n",
    "\\frac{\\partial}{\\partial \\mathbf X_{k,k}} \\mathbf X_{i,i} = 1 & if & i = k\\end{matrix}$\n",
    "\n",
    "- - - -\n",
    "Finally, with respect to: $\\frac{\\partial}{\\partial \\mathbf \\Sigma}ln(Det(\\mathbf \\Sigma)) $.    \n",
    "\n",
    "Consider that the determinant is the product of the eigenvalues of $\\mathbf \\Sigma$.  So that expression can be written as $ln(\\lambda_1 * \\lambda_2 * ... * \\lambda_n)$  \n",
    " \n",
    "When someone talks about taking the logarithm of a square matrix, they are talking about taking the logarithm of the eigenvalues -- if the matrix is diagonalizable (and if not diagonalizable they'll likely use a truncated taylor series to approximate this, but that's outside the scope). Because $\\mathbf \\Sigma$ is symmetric positive definite, it is diagonalizable and we know all $\\lambda_i >0$   \n",
    " \n",
    "$ln(\\lambda_1 * \\lambda_2 * ... * \\lambda_n) = ln(\\lambda_1) + ln(\\lambda_2) + ... + ln(\\lambda_n) = trace(ln(\\mathbf \\Sigma)) = ln(Det(\\mathbf \\Sigma))$  \n",
    "\n",
    " \n",
    "\n",
    "\n",
    "Now to basically intuit the derivatives, remember the chain rule.  For some scalar variable x, $\\frac{d}{dx}ln(x) = \\big(\\frac{d}{dx} x \\big) * x^{-1}   = 1 * x^{-1}  = x^{-1}$  \n",
    " \n",
    "\n",
    "With scalar multiplication it does not matter whether you apply chain rule on the left or the right because scalar multiplication commutes.  We'll apply on the left for matrices (due to non-commutative multiplication and, again, if the matrices were not square, the dimensions would hold clues.)  To conclude:  \n",
    " \n",
    "$\\frac{\\partial}{\\partial \\mathbf \\Sigma}ln(Det(\\mathbf \\Sigma)) =  \\frac{\\partial}{\\partial \\mathbf \\Sigma} trace ( ln(\\mathbf\\Sigma))  =\\Big( \\frac{\\partial}{\\partial \\mathbf \\Sigma} trace(\\Sigma) \\Big)  \\mathbf\\Sigma^{-1} = \\Big(\\mathbf I \\Big) \\mathbf\\Sigma^{-1}  = \\mathbf\\Sigma^{-1}$  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
