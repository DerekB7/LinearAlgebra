{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a basis algorithm -- reference  page 288 of Pinter, or page 92 of Artin  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**guiding inequality**  \n",
    "\n",
    "$\\text{rank}\\big(\\mathbf A\\mathbf B\\big) \\leq \\text{rank}\\big(\\mathbf A\\big)$  \n",
    "\n",
    "*This writeup will loosely and somewhat interchangeably use linear map $T$ and a matrix representation using some basis (in particular the standard basis). Unless explicitly stated otherwise, all linear operators and maps are assumed to be finite dimensional.  The equivalent statement is that the composition of linear maps*      \n",
    "\n",
    "$\\text{dim}\\big(\\text{image}(T_1T_2)\\big)\\leq \\text{dim}\\big(\\text{image}(T_1)\\big)$  or  \n",
    "$\\text{rank}\\big(T_1T_2\\big) \\leq \\text{rank}\\big(T_1\\big)$  \n",
    "\n",
    "\n",
    "The result is implied by associativity of matrix multiplication / composition of linear maps.  Working with matrices  \n",
    "\n",
    "$\\mathbf {A}  =\\bigg[\\begin{array}{c|c|c|c|c} \\mathbf a_1 & \\mathbf a_2 &\\cdots & \\mathbf a_{n-1} & \\mathbf a_{n}\\end{array}\\bigg]$   \n",
    "\n",
    "\n",
    "with  \n",
    "$\\mathbf B \\in \\mathbb F^\\text{n x r}$  \n",
    "\n",
    "now  \n",
    "$\\mathbf {AB}  $  \n",
    "$= \\mathbf A \\bigg[\\begin{array}{c|c|c|c|c} \n",
    "\\mathbf b_1 & \\mathbf b_2 &\\cdots & \\mathbf b_{r-1} & \\mathbf b_{r} \n",
    "\\end{array}\\bigg]   $  \n",
    "$=  \\bigg[\\begin{array}{c|c|c|c|c} \\mathbf A \\mathbf b_1 & \\mathbf A\\mathbf b_2 &\\cdots & \\mathbf A\\mathbf b_{r-1} & \\mathbf A\\mathbf b_{r} \n",
    "\\end{array}\\bigg] $  \n",
    "$= \\bigg[\\begin{array}{c|c|c|c|c} \\sum_{k=1}^n b_{k,1} \\mathbf a_j & \\sum_{k=1}^n b_{k,2} \\mathbf a_j &\\cdots & \\sum_{k=1}^n b_{k,r-1} \\mathbf a_j & \\sum_{k=1}^n b_{k,} \\mathbf a_j\n",
    "\\end{array}\\bigg]$  \n",
    "\n",
    "i.e. the column space of $\\mathbf {AB}$ is generated by the columns of $\\mathbf A$  \n",
    "\n",
    "so  \n",
    "$\\text{span}\\big(\\mathbf {AB}\\big) = \\beta_1\\big(\\sum_{k=1}^n b_{k,1} \\mathbf a_j\\big) +\\beta_2\\big(\\sum_{k=1}^n b_{k,2} \\mathbf a_j\\big) + ... \\beta_r\\big(\\sum_{k=1}^n b_{k,r} \\mathbf a_j\\big) \\subset \\sum_{j=1}^n \\alpha_j \\mathbf a_j =\\text{span}\\big(\\mathbf A\\big)   $\n",
    "\n",
    "equivalently for any $\\mathbf x \\in \\mathbb F^r$  \n",
    "\n",
    "the image is given by  \n",
    "$\\big(\\mathbf {ABx}\\big) = \\mathbf A\\big(\\mathbf {Bx}\\big) = \\mathbf A\\mathbf y = \\sum_{j=1}^n y_j \\mathbf a_j \\subset \\text{span}\\big(\\mathbf A\\big)   $\n",
    "where $\\mathbf y$ is in the domain of $\\mathbf A$ \n",
    "\n",
    "if we consider this over all possible $\\mathbf x$ to generate all possible $\\mathbf y$ in the span of $\\mathbf B$ we may or may not span the domain of $\\mathbf A$, though it is possible that any 'gaps' do not diminish the span under the image of $\\mathbf A$ since they *could* be in the nullspace/kernel of $\\mathbf A$    \n",
    " \n",
    "*tightening our claims about the inequality*  \n",
    "since the dimension of the image is given as the number of linearly independent vectors that form a basis for it, we have two cases to consider.  \n",
    "\n",
    "**first**  \n",
    "consider the case where not only  \n",
    "$\\text{span}\\big(\\mathbf {AB}\\big) \\subset \\text{span}\\big(\\mathbf A\\big) $  \n",
    "but also \n",
    "$\\text{span}\\big(\\mathbf A\\big)\\subset \\text{span}\\big(\\mathbf {AB}\\big) $  \n",
    "i.e. \n",
    "$\\text{span}\\big(\\mathbf A\\big)= \\text{span}\\big(\\mathbf {AB}\\big) $  \n",
    "the first inclusion implies that any basis for the image of $\\big(\\mathbf {AB}\\big)$ is included in the image of $\\big(\\mathbf A\\big)$ \n",
    "the second inclusion implies that any basis for the image of $\\big(\\mathbf {A}\\big)$ is included in the image of $\\big(\\mathbf {AB}\\big)$    \n",
    "\n",
    "Using the invariance of dimension size, we can conclude that  \n",
    "$\\text{rank}\\big(\\mathbf A\\big)= \\text{rank}\\big(\\mathbf {AB}\\big) $   \n",
    "\n",
    "*interlude:*  \n",
    "consider the special case of $\\mathbf A\\in \\mathbb F^\\text{n x n}$ and where $\\mathbf B := \\mathbf A$  \n",
    "here we would have (in some vector space $V$)    \n",
    "$\\text{rank}\\big(\\mathbf A\\big)= \\text{rank}\\big(\\mathbf {A}^2\\big) $   \n",
    "which implies every vector in the image of $\\mathbf A$ is in the image of $\\mathbf A^2$.  It *also* implies  \n",
    "$\\text{nullspace}\\big(\\mathbf A\\big) \\cap \\text{image}\\big(\\mathbf A\\big) = \\{\\mathbf 0\\}$  \n",
    "i.e. the intersection is only the trivial one  \n",
    "\n",
    "that is, if (in some basis) $\\mathbf A: V \\longrightarrow W$  \n",
    "$\\text{rank}\\big(\\mathbf A\\big)= r $   \n",
    "\n",
    "then while our our original vector space $V$ had dimension $n$, the one generated by the image of $\\mathbf A$ is $W$ which is an $r$ dimensional vector space.  \n",
    "\n",
    "so in the case of our double inclusion,  \n",
    "$\\mathbf A: W \\longrightarrow W$  \n",
    "\n",
    "(which is equivalent to $\\mathbf A^2: V \\longrightarrow W$)  \n",
    "\n",
    "then by rank-nullity (page 110 of Artin)  \n",
    "$r = \\text{dim } W = \\text{rank: (wrt W) } \\mathbf A + \\text{nullity (wrt W) } \\mathbf A = r + 0$    \n",
    "\n",
    "where \n",
    "$\\text{rank: (wrt W) } \\mathbf A = \\text{rank: (wrt V) } \\mathbf A^2$  \n",
    "and we have assumed for this entire interlude that \n",
    "$\\text{rank}\\big(\\mathbf A\\big)= \\text{rank}\\big(\\mathbf {A}^2\\big) $  in our original vector space $V$  \n",
    "\n",
    "for avoidance of doubt, if the intersection was non-trivial, i.e.  \n",
    "$\\text{nullspace}\\big(\\mathbf A\\big) \\cap \\text{image}\\big(\\mathbf A\\big) = \\{\\mathbf 0, ..., \\mathbf w\\}$  \n",
    "then we may build a basis for the image of $\\mathbf A$ as $\\{\\mathbf w, \\mathbf v_1, ...., \\mathbf v_{r-1}\\big\\}$  \n",
    "and then the image of $\\mathbf A^2$ is given by all linear combinations of  \n",
    "$\\{\\mathbf A\\mathbf w, \\mathbf A\\mathbf v_1, ...., \\mathbf A\\mathbf v_{r-1}\\big\\} = \\{\\mathbf 0, \\mathbf A\\mathbf v_1, ...., \\mathbf A\\mathbf v_{n-r}\\big\\} = \\{ \\mathbf A\\mathbf v_1, ...., \\mathbf A\\mathbf v_{r-1}\\big\\}$  \n",
    "which has at most $r-1$ linearly independent vectors,  so \n",
    "$\\text{rank}\\big(\\mathbf A^2\\big) \\leq r-1 \\lt r= \\text{rank}\\big(\\mathbf A\\big)$  \n",
    "\n",
    "and equivalently, we know that $\\mathbf w \\in \\text{span}\\big(\\mathbf A\\big)$ but  $\\mathbf w \\not\\in \\text{span}\\big(\\mathbf A^2\\big)$ hence we don't have the double inclusion (read: equivalence) of span.  \n",
    "\n",
    "\n",
    "\n",
    "*extension*  \n",
    "the argument works verbatim if we instead use $\\mathbf A^k$ as our left matrix and $\\mathbf A$ as the right matrix. So if, for some natural number $k$ \n",
    "$\\text{rank}\\big(\\mathbf A^{k}\\big)= \\text{rank}\\big(\\mathbf {A}^{k+1}\\big) $   \n",
    "\n",
    "i.e. if (in some basis) $\\mathbf A^k: V \\longrightarrow W$  \n",
    "and \n",
    "$\\text{rank}\\big(\\mathbf A^k\\big)= r $  \n",
    "and $\\mathbf A: W \\longrightarrow W$   (this should probably be linear map $T$ in this basis, though again we use them somewhat interchangeably, for a fixed basis we have an isomorphism...)  \n",
    "then again, by rank nullity   \n",
    "$r $  \n",
    "$= \\text{dim } W $  \n",
    "$= \\text{rank: (wrt W) } \\mathbf A + \\text{nullity (wrt W) } \\mathbf A $  \n",
    "$= \\text{rank: (wrt V) } \\mathbf A^{k+1} + \\text{nullity (wrt W) } \\mathbf A $  \n",
    "$= \\text{rank: (wrt V) } \\mathbf A^{k} + \\text{nullity (wrt W) } \\mathbf A$  \n",
    "$= r + 0$    \n",
    "hence $\\text{nullity (wrt W) } \\mathbf A = 0$  \n",
    "\n",
    "*corollary*  \n",
    "if \n",
    "$\\text{rank}\\big(\\mathbf A^{k}\\big)= \\text{rank}\\big(\\mathbf {A}^{k+1}\\big) $   \n",
    "then \n",
    "$\\text{rank}\\big(\\mathbf A^{k}\\big)= \\text{rank}\\big(\\mathbf {A}^{k+1}\\big)  = \\text{rank}\\big(\\mathbf {A}^{k+2}\\big) = \\text{rank}\\big(\\mathbf {A}^{k+3}\\big) = \\text{rank}\\big(\\mathbf {A}^{k+4}\\big) = ... $   \n",
    "\n",
    "we need only prove \n",
    "$\\text{rank}\\big(\\mathbf {A}^{k+1}\\big)  = \\text{rank}\\big(\\mathbf {A}^{k+2}\\big)$  by some argument as the rest of the sequence follows by induction/iteration    \n",
    "\n",
    "*proof:*  \n",
    "we know $r =\\text{rank}\\big(\\mathbf A^{k}\\big)= \\text{rank}\\big(\\mathbf {A}^{k+1}\\big) $   \n",
    "we had $\\mathbf A: W\\longrightarrow W $, which implies by composition $\\mathbf A^2: W\\longrightarrow W $ \n",
    "and we know by the above 'extension' that the $\\text{nullity (wrt W) } \\mathbf A =0$    \n",
    "so mapping under $\\mathbf A$ is injective, which means an arbitrary set of linearly independent vectors in $W$ stays linearly independent under the image of $\\mathbf A$, which implies that application of $\\mathbf A^2$ (i.e. composition of mapping of linearly independent set to linearly independent set of same size to linearly independent set of same size) is injective and preserves linearly independent set size but since \n",
    "\n",
    "(with respect to the image)  \n",
    "$\\mathbf A^{k+2}:V = \\mathbf A^{2}:W$  \n",
    "(really we should say $T^{k+2}:V = T^{2}:W$ here though it isn't something to dwell on)  \n",
    "\n",
    "we have that \n",
    "$r =\\text{rank}\\big(\\mathbf A^{k}\\big)= \\text{rank}\\big(\\mathbf {A}^{k+1}\\big)\\longrightarrow  \\text{rank}\\big(\\mathbf {A}^{k+2}\\big) = r$   \n",
    "\n",
    "and the equality continues to any number by running a for loop and at each iteration assigning  \n",
    "$k:= k+1$  \n",
    "\n",
    "for a slightly different finish, consider  \n",
    "\"we know by the above 'extension' that the $\\text{nullity (wrt W) } \\mathbf A =0$ so mapping under $\\mathbf A$ is injective\"   \n",
    "and using the fact that the composition of two injective maps gives an injective map, so $\\mathbf {AA} = \\mathbf {A}^2$ is injective with respect to $W$ (and by rank nullity, preserves rank), and thus the composition of $\\mathbf {AA}^2=\\mathbf A^3$ is injective with respect to $W$ and the composition of $\\mathbf {AA}^{k-1} = \\mathbf A^k$ is injective with respect to $W$ for all natural numbers $k$ by induction, which proves our result.  \n",
    "\n",
    "*note: this corollary references rank-nullity which isn't proven until later in our writeup.  However, this placement is the most natural one for this corollary.  A careful review of the the rank-nullity proof will show that it doesn't make use of this corollary -- it in fact uses only the most basic results associated with this guiding inequality.*  \n",
    "- - - - -\n",
    "\n",
    "**second**  \n",
    "consider the case where  \n",
    "$\\text{span}\\big(\\mathbf {AB}\\big) \\subset \\text{span}\\big(\\mathbf A\\big) $   \n",
    "but the subset is proper (i.e. no second inclusion)  \n",
    "\n",
    "This means that there is some non-zero vector $\\mathbf v_1$ in the image of $\\mathbf A$ that is not in the image of $\\mathbf {AB} $    \n",
    "\n",
    "in effect we now march through our basis extension algorithm \n",
    "\n",
    "consider \n",
    "$\\mathbf {AB} : V \\longrightarrow W$  \n",
    "we have $\\text{rank}\\big(\\mathbf {AB}\\big) = r$   \n",
    "and select a basis for $W$, $\\big\\{ \\mathbf z_1, \\mathbf z_2, ..., \\mathbf z_r\\big\\}$  \n",
    "\n",
    "now append $\\mathbf v_1$  to create a basis for $W'$  \n",
    "$\\big\\{ \\mathbf z_1, \\mathbf z_2, ..., \\mathbf z_r, \\mathbf v_1\\big\\}$  \n",
    "this is still a linearly independent set since $\\mathbf v_1$ is not in the span of $\\mathbf {AB}$.  Now re-run our argument and ask  \n",
    "$\\text{is span}\\big(\\mathbf A\\big)  \\subset \\text{span}\\Big(\\big\\{ \\mathbf z_1, \\mathbf z_2, ..., \\mathbf z_r, \\mathbf v_1\\big\\}\\Big) ?$   \n",
    "(where below we refer to $\\text{span}\\big(\\mathbf A\\big)$ as LHS, and the other side, after suitable augmentation, as our RHS)  \n",
    "\n",
    "(i.e. is the image of $\\mathbf A$ generated by set of all linear combination of RHS?)  \n",
    "\n",
    "if not, select some $\\mathbf v_2 \\in \\text{span}\\big(\\mathbf A\\big) $ but not in the span of RHS and append \n",
    "$\\big\\{ \\mathbf z_1, \\mathbf z_2, ..., \\mathbf z_r, \\mathbf v_1, \\mathbf v_2\\big\\}$  \n",
    "\n",
    "(the key invariant (i) is that this is a linearly independent set is maintained at each step of the for loop since $\\mathbf v_2$ not in the span of pre-existing set, which was certified to be linearly independent to begin with and (ii) the inclusion always runs the other way, i.e $RHS \\subset LHS$ because at time zero the RHS was a subset and at each step we augment/ take a union of earlier RHS and one more element LHS, so at each iteration the generators or the RHS are a subset of the generators of the LHS )  \n",
    "\n",
    "Now we repeat until this process stops -- it must stop after at most $n-r$ iterations (since the vector in a space with dimension at most $n$) and in fact it must stop once there are no vectors in the span of $\\mathbf A$ that are not in the span of the RHS -- i.e. once we have a double inclusion.  This occurs after $k$ iterations where $k\\geq 1$.  Thus we have a double inclusion where $LHS \\subset RHS$ (our stopping rule) *and* $RHS \\subset LHS$ by construction (i.e. the RHS was created by taking vectors from the span of the LHS, one by one).  To conclude, we have  \n",
    " $r = \\text{rank}\\big(\\mathbf {AB}\\big) \\lt  \\text{rank}\\big(\\mathbf {A}\\big) = r + k$  \n",
    " \n",
    "qualitatively we can say that starting in some other vector space and then looking at things in the image of $\\mathbf B$ generates a smaller space under the image of $\\mathbf A$ because $\\mathbf B: V \\longrightarrow V'$ did not generate all of $V'$ i.e. it is not surjective.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*corollary:*  \n",
    "right multiplication by an invertible matrix $\\mathbf B$ doesn't change rank  \n",
    "i.e. we have  \n",
    "$\\text{rank}\\big(\\mathbf A\\mathbf B\\big) \\leq \\text{rank}\\big(\\mathbf A\\big)$  \n",
    "\n",
    "then when we apply our guiding inequality to $\\mathbf {CB}^{-1}$  \n",
    "with $\\mathbf C:= \\mathbf {AB}$ \n",
    "\n",
    "$\\text{rank}\\big(\\mathbf A\\big) = \\text{rank}\\big(\\mathbf C\\mathbf B^{-1}\\big) \\leq \\text{rank}\\big(\\mathbf C\\big) = \\text{rank}\\big(\\mathbf {AB}\\big)$  \n",
    "\n",
    "so  \n",
    "$\\text{rank}\\big(\\mathbf A\\mathbf B\\big) \\leq \\text{rank}\\big(\\mathbf A\\big)\\leq \\text{rank}\\big(\\mathbf A\\mathbf B\\big)$   \n",
    "or   \n",
    "$\\text{rank}\\big(\\mathbf A\\mathbf B\\big) = \\text{rank}\\big(\\mathbf A\\big)$  \n",
    "\n",
    "*remark:*  \n",
    "This result is implicit in that our guiding inequality shows monotone non-increasing behavior of rank each time we multiply by another matrix on the right. Since multiplication by an invertible matrix is, well, invertible, whenever we multiply by the inverse on the right, if we had somehow changed rank by originally right multiplying by invertible $\\mathbf B$, we would not be able recover it via multiplication by the inverse on the right due to the monotone behavior of this inequality (i.e. monotone sequences imply any change cannot be 'undone') -- which would contradict the very nature of having an inverse.    \n",
    "\n",
    "*note:*  a careful look at this and and the concluding remarks in the prior cell tell us that we could have just as easily said \n",
    "\n",
    "right multiplication by a matrix $\\mathbf B$ *that is right invertible* (aka acts as a surjective linear map) doesn't change rank    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**remark on injective linear maps**  \n",
    "(where we use matrices with full column rank, and especially invertible matrices, interchangeably with injective linear maps)  \n",
    "\n",
    "it should be immediate (\"obvious\"?) that left multiplication by an injective matrix / application of an injective linear map preserves rank / dimension of the image  \n",
    "\n",
    "\n",
    "i.e. if we know that some collection of vectors is linearly independent, then  \n",
    "$\\mathbf 0 =\\sum_{k=1}^r \\alpha_k \\mathbf v_k \\longrightarrow \\text{each  } \\alpha_i = 0$   \n",
    "\n",
    "but if we look at the image of this under some injective linear map $T$ we have our linear relation  \n",
    "$\\mathbf 0 = \\sum_{k=1}^r \\alpha_k \\big(T\\mathbf v_k\\big) = T\\big(\\sum_{k=1}^r \\alpha_k \\mathbf v_k\\big)\\longrightarrow \\text{each  } \\alpha_i = 0 $  \n",
    "because, being linear $T\\mathbf 0 = \\mathbf 0$ but $T$ is injective so the preimage of zero only includes one vector, the zero vector (i.e. $T$ maps zero to zero but cannot map anything else to zero, by injectivity), which implies $\\sum_{k=1}^r \\alpha_k \\mathbf v_k = \\mathbf 0$ and by linear independence of these vectors we know each coefficient is zero    \n",
    "\n",
    "so if $\\mathbf A$ is injective   \n",
    "$\\text{rank}\\big(\\mathbf {B}\\big) \\leq \\text{rank}\\big(\\mathbf {AB}\\big) $   \n",
    "\n",
    "and for avoidance of doubt, we also know  \n",
    "$\\text{rank}\\big(\\mathbf {AB}\\big) \\leq \\text{rank}\\big(\\mathbf {B}\\big)$   \n",
    "because for any basis of dimension $r$ under the image of $T$  (i.e. $\\text{rank}\\big(\\mathbf {AB}\\big) = r$)  \n",
    "we have our linear relation,  \n",
    "$\\mathbf 0 = \\sum_{k=1}^r \\alpha_k \\big(\\mathbf w_k\\big) = \\sum_{k=1}^r \\alpha_k \\big(T\\mathbf v_k\\big)\\longrightarrow \\text{each  } \\alpha_i = 0 $  \n",
    "implies, via linearity  \n",
    "$\\mathbf 0 = T\\big(\\sum_{k=1}^r \\alpha_k \\mathbf v_k\\big)\\longrightarrow \\text{each  } \\alpha_i = 0 $  \n",
    "so the domain has a basis with at least $r$ elements.  The fact that $T$ is injective is not strictly needed here but it cleans up the argument --- for injective $T$ we know the preimage of $\\mathbf w_k$ has exactly one vector $\\mathbf v_k$ in it, so we may re-write the above as  \n",
    "$\\mathbf 0 = \\sum_{k=1}^r \\alpha_k \\mathbf v_k \\longrightarrow \\text{each  } \\alpha_i = 0 $  \n",
    "which tells us the domain has dimension at least $r$, so \n",
    "\n",
    "$r=\\text{rank}\\big(\\mathbf {AB}\\big) \\leq \\text{rank}\\big(\\mathbf {B}\\big)$      \n",
    "\n",
    "combining all this for injective $\\mathbf A$ we have  \n",
    "$\\text{rank}\\big(\\mathbf {AB}\\big) \\leq \\text{rank}\\big(\\mathbf {B}\\big) \\leq \\text{rank}\\big(\\mathbf {AB}\\big) $   \n",
    "\n",
    "i.e. for injective $\\mathbf A$ we know   \n",
    "$\\text{rank}\\big(\\mathbf {AB}\\big) = \\text{rank}\\big(\\mathbf {B}\\big)$   \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The guiding inequality gives us *many* corollaries almost immediately  \n",
    "\n",
    "**Corollary: a useful decomposition**   \n",
    "for any linear map $T$ we may write it, in terms of the standard basis as \n",
    "$\\mathbf A \\in \\mathbb F^\\text{m x n}$  \n",
    "\n",
    "suppose $\\mathbf A$ has nullspace dimension $n - r$\n",
    "\n",
    "Then \n",
    "\n",
    "$\\mathbf A = \\mathbf A \\mathbf I_n = \\mathbf A \\mathbf Q \\mathbf Q^{-1}$  \n",
    "\n",
    "we should select our $\\mathbf Q$ wisely, by having $\\{\\mathbf q_{r+1}, \\mathbf q_{r+2}, ..., \\mathbf q_{n}\\}$ be a linearly independent set that spans the nullspace of $\\mathbf A$.  Now we apply our basis extension algorithm to select a linearly independent set of $\\{\\mathbf q_{1}, \\mathbf q_{2}, ..., \\mathbf q_{n-r+1}\\}$ and collect these in a matrix  \n",
    "\n",
    "\n",
    "$\\mathbf A $  \n",
    "$= \\mathbf A \\mathbf Q \\mathbf Q^{-1} $  \n",
    "$= \\mathbf A \\bigg[\\begin{array}{c|c|c|c|c|c} \n",
    "\\mathbf q_1 & \\mathbf q_2 &\\cdots & \\mathbf q_{r} & \\mathbf q_{r+1} &\\cdots & \\mathbf q_{n} \n",
    "\\end{array}\\bigg]\\mathbf Q^{-1} $  \n",
    "$=  \\bigg[\\begin{array}{c|c|c|c|c|c} \n",
    "\\mathbf A\\mathbf q_1 & \\mathbf A\\mathbf q_2 &\\cdots & \\mathbf A\\mathbf q_{r} & \\mathbf A\\mathbf q_{r+1} &\\cdots & \\mathbf A\\mathbf q_{n} \n",
    "\\end{array}\\bigg]\\mathbf Q^{-1}   $  \n",
    "$=  \\bigg[\\begin{array}{c|c|c|c|c|c} \n",
    "\\mathbf A\\mathbf q_1 & \\mathbf A\\mathbf q_2 &\\cdots & \\mathbf A\\mathbf q_{r} & \\mathbf 0 &\\cdots & \\mathbf 0 \n",
    "\\end{array}\\bigg]\\mathbf Q^{-1}   $  \n",
    "$=  \\mathbf P\\bigg[\\begin{array}{c|c|c|c|c|c} \n",
    "\\mathbf e_1 & \\mathbf e_2 &\\cdots & \\mathbf e_{r} & \\mathbf 0 &\\cdots & \\mathbf 0 \n",
    "\\end{array}\\bigg]\\mathbf Q^{-1}   $  \n",
    "$=\\mathbf P\\begin{bmatrix}\\mathbf I_r &\\mathbf {00}^T\\\\ \\mathbf {00}^T &\\mathbf {00}^T\\end{bmatrix}\\mathbf Q^{-1}$  \n",
    "\n",
    "i.e. where $\\mathbf P$ collects the image $\\mathbf A$ for the first $r$ vectors, i.e. \n",
    "\n",
    "$\\mathbf p_i := \\mathbf A\\mathbf q_i$ for $1\\leq i\\leq r$, and we then run our basis extension algorithm to append columns $\\mathbf p_j$ $r+1\\leq j \\leq m$ so that $\\mathbf P$ is $m x m$ and invertible.  \n",
    "\n",
    "note: the basis extension algorithm by construction makes the vectors indexed $\\gt r$ linearly independent.  But before we get ahead of ourselves, how do we know the first $r$ vectors are linearly independent?  \n",
    "\n",
    "\n",
    "because, as teased out via our guiding inequality, when dealing with right multiplication and invertible matrices  \n",
    "\n",
    "$r = \\text{rank}\\Big(\\mathbf A\\Big)  =  \\text{rank}\\Big(\\mathbf A \\mathbf Q \\mathbf Q^{-1}\\Big)= \\text{rank}\\Big(\\mathbf A \\mathbf Q \\Big)= \\text{rank}\\Big(\\bigg[\\begin{array}{c|c|c|c|c|c} \n",
    "\\mathbf A\\mathbf q_1 & \\mathbf A\\mathbf q_2 &\\cdots & \\mathbf A\\mathbf q_{r} & \\mathbf 0 &\\cdots & \\mathbf 0 \n",
    "\\end{array}\\bigg]\\Big)$    \n",
    "\n",
    "hence those first $r$ columns, the only non-zero ones, must be linearly independent   \n",
    "\n",
    "**Corollary: Rank-Nullity**     \n",
    "\n",
    "For any $\\mathbf A \\in \\mathbb R^\\text{m x n}$, that has nullspace with dimension $ n-r$ and  \n",
    "\n",
    "$\\text{rank}\\Big(\\mathbf A\\Big) = \\text{rank}\\Big(\\mathbf P\\begin{bmatrix}\\mathbf I_r &\\mathbf {00}^T\\\\ \\mathbf {00}^T &\\mathbf {00}^T\\end{bmatrix}\\mathbf Q^{-1}\\Big)= \\text{rank}\\Big(\\mathbf P\\begin{bmatrix}\\mathbf I_r &\\mathbf {00}^T\\\\ \\mathbf {00}^T &\\mathbf {00}^T\\end{bmatrix}\\Big)= \\text{rank}\\Big(\\begin{bmatrix}\\mathbf I_r &\\mathbf {00}^T\\\\ \\mathbf {00}^T &\\mathbf {00}^T\\end{bmatrix}\\Big) = r $  \n",
    "\n",
    "(where we first make use of the fact that surjective maps don't change rank, and secondly that injective maps don't change rank)  \n",
    "\n",
    "thus we have  \n",
    "\n",
    "$n = \\text{number of columns in }\\mathbf A = r + (n-r) = \\text{rank}\\Big(\\mathbf A\\Big) + \\text{dim null}\\Big(\\mathbf A\\Big)$   \n",
    "\n",
    "for avoidance of doubt, what we've proven, is for a matrix $\\mathbf A$ with some arbitrary fixed $n$ columns we have \n",
    "\n",
    "$\\text{nullspace dim of n-r}  \\longrightarrow \\text{rank r}  $  \n",
    "in many proofs we'd then do the second step of directly proving   \n",
    "$\\text{rank r} \\longrightarrow  \\text{nullspace dim of n-r}  $   \n",
    "however this isn't needed in our case because  \n",
    "$\\text{nullspace dim of n-r}  \\longrightarrow \\text{rank r}  $    \n",
    "is invertible (injective and surjective) here so the second step is not needed.  In particular since there are finitely many values, we can populate a table to represent this mapping  \n",
    "\n",
    "$\\begin{bmatrix}\\text{dim nullspace} &\\text{rank}\\\\\n",
    "0 & n\\\\  \n",
    "1 & n-1\\\\\n",
    "2 & n-2\\\\\n",
    "\\vdots &\\vdots\\\\\n",
    "n-2 & 2\\\\\n",
    "n-1 & 1\\\\\n",
    "n & 0\\\\\n",
    "\\end{bmatrix}$  \n",
    "\n",
    "and by inspection, if we know that there rank is $r$, we see that the dimension of the nullspace must be $n-r$.  \n",
    "\n",
    "\n",
    "Also note:  any finite dimensional linear operator $T$ in turn inherits these results from $\\mathbf A$  \n",
    "\n",
    "\n",
    "**Corollary: Row Rank = Column Rank**   \n",
    "*note:* this is problem (4.2.8) in Artin first edition  \n",
    "\n",
    "with some invertible $\\mathbf P$ and $\\mathbf Q$ any $\\mathbf A$ with rank (aka 'column rank') $=r$ may be written as  \n",
    "\n",
    "\n",
    "$\\mathbf A = \\mathbf P\\begin{bmatrix}\\mathbf I_r &\\mathbf {00}^T\\\\ \\mathbf {00}^T &\\mathbf {00}^T\\end{bmatrix}\\mathbf Q^{-1}$  \n",
    "\n",
    "thus  \n",
    "$\\mathbf A^T = \\big(\\mathbf Q^{-1}\\big)^{T}\\begin{bmatrix}\\mathbf I_r &\\mathbf {00}^T\\\\ \\mathbf {00}^T &\\mathbf {00}^T\\end{bmatrix}\\mathbf P^{T}$  \n",
    "\n",
    "so  \n",
    "\n",
    "$\\text{rank}\\Big(\\mathbf A^T\\Big) = \\text{rank}\\Big(\\big(\\mathbf Q^{-1}\\big)^{T}\\begin{bmatrix}\\mathbf I_r &\\mathbf {00}^T\\\\ \\mathbf {00}^T &\\mathbf {00}^T\\end{bmatrix}\\mathbf P^{T}\\Big) = \\text{rank}\\Big(\\big(\\mathbf Q^{-1}\\big)^{T}\\begin{bmatrix}\\mathbf I_r &\\mathbf {00}^T\\\\ \\mathbf {00}^T &\\mathbf {00}^T\\end{bmatrix}\\Big)=  \\text{rank}\\Big(\\begin{bmatrix}\\mathbf I_r &\\mathbf {00}^T\\\\ \\mathbf {00}^T &\\mathbf {00}^T\\end{bmatrix}\\Big)= r=\\text{rank}\\Big(\\mathbf A\\Big)$   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**ex 4.1.7**   \n",
    "when the field is $\\mathbb F_p$, for a finite dimensional vector space, relate rank-nullity from this chapter with the Lagrange Forumla from chapter 2.  \n",
    "\n",
    "note\n",
    "it is sufficient to consider a vector space over $\\mathbb F_p$ i.e. $\\mathbb F_p^n$, and in particular, we can use standard basis vectors to create an isomorphism with any other finite dimensional vector space (e.g. using the hypervector found on page 96 of Artin, assign $\\mathbf e_i := v_i$.  The invertibility of this correspondence is immediate, and so is the linearity.  In the spirit of the hypervector of chapter 3 we will only need to scale and add these vectors (in accordance with vector space axioms)    \n",
    "\n",
    "so consider a matrix, representing a linear map in the standard basis (where we recall that a linear map is a homomorphism), so $\\mathbf A: \\mathbb F_p^n \\longrightarrow \\mathbb F_p^n$  \n",
    "\n",
    "by the above, we have rank nullity, so  \n",
    "$\\text{dim vector space = n} =  \\text{dim null}\\big(\\mathbf A\\big)+ \\text{rank}\\big(\\mathbf A\\big)  =  \\text{dim kernel}\\big(T\\big)+ \\text{dim image}\\big(T\\big)  $  \n",
    "\n",
    "now exponentiate via $p$, so  \n",
    "\n",
    "$\\Big \\vert G\\Big \\vert$  \n",
    "$=p^n$  \n",
    "$= p^{\\text{dim vector space}} $  \n",
    "$= p^{\\text{dim kernel}\\big(T\\big)+ \\text{dim image}\\big(T\\big) }$  \n",
    "$= p^{\\text{dim kernel}\\big(T\\big)}p^{\\text{dim image}\\big(T\\big) }$  \n",
    "$= p^{\\text{n-r}}\\cdot p^{\\text{r} }$  \n",
    "$=\\Big \\vert \\text{ kernel }\\big(\\phi\\big)\\Big\\vert \\cdot \\Big \\vert\\text{ image }\\big(\\phi\\big)\\Big\\vert $  \n",
    "\n",
    "where a direct counting argument verifies that the group has $p^n$ elements -- i.e. consider any linearly independent set of vectors that form a basis (again emphasizing the standard basis), so *any* vector in our space may be written unique as \n",
    "\n",
    "$\\mathbf x = \\sum_{k=1}^n \\alpha_k\\mathbf e_k $  \n",
    "and for each $\\alpha_k$ there are $p$ different choices, so we have $p\\cdot p \\cdot ... \\cdot p = p^n$ different vectors in the group that is given by our vector space $\\mathbb F_p^n$ (with the binary operation of vector addition)  \n",
    "\n",
    "similarly, the image has dimension $r$ i.e. it has a basis with $r$ linearly independent vectors in it, so any vector in that space is given by $\\mathbf y = \\sum_{k=1}^r \\alpha_k\\mathbf w_k$ with $p$ different choices for each $\\alpha_k$ so $p^r$ choices in total.  \n",
    "\n",
    "A virtually identical argument follows for the kernel -- i.e. the preimage of $\\mathbf A\\mathbf x = \\mathbf 0$ has $n-r$ linearly independent vectors in it and this implies set cardinality of $p^{n-r}$.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ex 4.3.6**  \n",
    "consider a linear operator $T$ that is nilpotent -- or equivalently its associated n x n matrix is $\\mathbf A$ where $\\mathbf A^m = \\mathbf 0$  \n",
    "\n",
    "i.)  we are to prove that\n",
    "$\\mathbf A^k \\neq \\mathbf 0 \\longrightarrow   \\text{rank}\\big(\\mathbf A^{k+1}\\big) \\lt  \\text{rank}\\big(\\mathbf A^{k}\\big)$   \n",
    "*this is dealt with under the 'interlude' and 'extension/corollary' section of the 'guiding inequality' at the top*  \n",
    "\n",
    "\n",
    "it tells to consider the opposite, i.e. suppose for a contradiction that  \n",
    "$\\mathbf A^k \\neq \\mathbf 0 $ and   $\\text{rank}\\big(\\mathbf A^{k+1}\\big) \\geq  \\text{rank}\\big(\\mathbf A^{k}\\big)$  \n",
    "we know by the guiding inequality  \n",
    "$\\text{rank}\\big(\\mathbf A^{k+1}\\big) \\leq  \\text{rank}\\big(\\mathbf A^{k}\\big)$  \n",
    "\n",
    "so this would mean  \n",
    "$\\text{rank}\\big(\\mathbf A^{k+1}\\big) =  \\text{rank}\\big(\\mathbf A^{k}\\big)$  \n",
    "but then the extension + corollary tells us  \n",
    "$\\text{rank}\\big(\\mathbf A^{k+j}\\big) =  \\text{rank}\\big(\\mathbf A^{k}\\big)\\gt 0$  \n",
    "for all natural numbers $j$ and hence our matrix isn't nilpotent after all. (We recall that the zero matrix is the only matrix with rank zero.)  \n",
    "\n",
    "Thus it must be the case that  \n",
    "$\\mathbf A^k \\neq \\mathbf 0 \\longrightarrow   \\text{rank}\\big(\\mathbf A^{k+1}\\big) \\lt  \\text{rank}\\big(\\mathbf A^{k}\\big)$  \n",
    "\n",
    "ii.)  \n",
    "by a counting argument \n",
    "\n",
    "first:  \n",
    "$\\mathbf A^m = \\mathbf 0 $  \n",
    "implies that $\\text{rank}\\big(\\mathbf A\\big) \\lt n$   \n",
    "(by say the equality case of the guiding inequality, or taking determinants)  \n",
    "\n",
    "so if   \n",
    "\n",
    "$a_1 = \\text{rank}\\big(\\mathbf A\\big) \\lt n$   \n",
    "$a_2 = \\text{rank}\\big(\\mathbf A^2\\big) \\lt a_1 \\leq  n-1$   \n",
    "$a_3 = \\text{rank}\\big(\\mathbf A^3\\big) \\lt a_2 \\leq  n-2$   \n",
    "$\\vdots$  \n",
    "$a_j = \\text{rank}\\big(\\mathbf A^j\\big) $  \n",
    "\n",
    "then (since rank can never be less than zero) our sequence is \n",
    "\n",
    "$0 \\leq a_n\\lt a_{n-1} \\lt a_{n-2} \\lt ... \\lt a_3 \\lt a_2 \\lt a_1 \\lt n$  \n",
    "thus via a pidgeon hole argument/ because there can be at most $n$ distinct integers in $\\{0,1,2...,n-2, n-1\\}$  \n",
    "*(there's probably a nicer way to state this)*  \n",
    "we know that $0=a_n =\\text{rank}\\big(\\mathbf A^n\\big)\\longrightarrow \\mathbf A^n = \\mathbf 0$  \n",
    "\n",
    "Thus for a nilpotent matrix $\\mathbf A \\in \\mathbb F^\\text{n x n}$ where \n",
    "$\\mathbf A^m = \\mathbf 0$,  \n",
    "we *know* that $m \\leq n$   \n",
    "(ditto for the linear operator $T$ that $\\mathbf A$ is associated with given some preselected basis that lurks in the background)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.Misc.8**  \n",
    "while chronologically later, this is the natural follow-up to exercise 4.3.6  \n",
    "\n",
    "for an n dimensional linear operator, show that  \n",
    "$\\ker\\big(T^n\\big)\\cap \\text{image}\\big(T^n\\big) =\\big\\{\\mathbf 0\\big\\} $   \n",
    "\n",
    "*proof*  \n",
    "note that the image of an operator and the kernel are both vector spaces so they always have the zero vector as in each, and in  their intersection.  \n",
    "\n",
    "Further the case of a non-singular operator, and the case of nilpotent operator (using the prior problem) trivially holds. For everything else:   \n",
    "\n",
    "using the linear operator formulation, consider \n",
    "$T^n: V \\longrightarrow W$  \n",
    "\n",
    "but based on the preceding exercise and our guiding inequality, we know that for natural number $k$  \n",
    "$\\text{rank}\\big(T^{n+k}\\big) = \\text{rank}\\big(T^{n}\\big)$  \n",
    "\n",
    "by rank nullity, this means that $T^k$ is injective with respect to $W$ and hence selecting $k=n$ nothing, other than the zero vector in $W$ is in the kernel of $T^n$.  *A little tightening up is needed but this is essentially the argument*  \n",
    "\n",
    "*alternative argument*  \n",
    "If we fix a basis and use a matrix $\\mathbf A$ to represent $T$ the argument is a touch messier.  However, based on problem 4.3.6 and our guiding inequality we know that \n",
    "\n",
    "$\\text{rank}\\big(\\mathbf A^{n+k}\\big) = \\text{rank}\\big(\\mathbf A^{n}\\big)$  \n",
    "and again in particular selecting $k=n$ this reads  \n",
    "$\\text{rank}\\big(\\mathbf A^{2n}\\big) = \\text{rank}\\big(\\mathbf A^{n}\\big)$  \n",
    "\n",
    "so consider some collection of basis vector for the range/image/column space of $\\mathbf A^n$ given by \n",
    "\n",
    "$\\big\\{\\mathbf v_1, \\mathbf v_2, ..., \\mathbf v_r\\big\\}$  \n",
    "where  $\\text{rank}\\big(\\mathbf A^n \\big) = r$  \n",
    "\n",
    "now suppose for a contradiction that there is some non-trivial (i.e. at least one non-zero vector in the) intersection between the kernel and image of $\\mathbf A^n$, and select that vector $\\mathbf w_1$.  \n",
    "\n",
    "It isn't needed but we may now apply our basis creation algorithm to create a new, equivalent basis for \n",
    "range/image/column space of $\\mathbf A^n$  \n",
    "\n",
    "$\\big\\{\\mathbf w_1, \\mathbf v_{\\sigma(1)},   \\mathbf v_{\\sigma(2)}, ..., \\mathbf v_{\\sigma(r-1)}\\big\\}$  \n",
    "\n",
    "\n",
    "now applying $\\mathbf A^n$ gives the range / column space / image of $\\mathbf A^{2n}$  \n",
    "$\\big\\{\\mathbf A^{n}\\mathbf w_1, \\mathbf A^{n}\\mathbf v_{\\sigma(1)},   \\mathbf A^{n}\\mathbf v_{\\sigma(2)}, ..., \\mathbf A^{n}\\mathbf v_{\\sigma(r-1)}\\big\\}$  \n",
    "\n",
    "$\\big\\{\\mathbf 0, \\mathbf A^{n}\\mathbf v_{\\sigma(1)},   \\mathbf A^{n}\\mathbf v_{\\sigma(2)}, ..., \\mathbf A^{n}\\mathbf v_{\\sigma(r-1)}\\big\\}$  \n",
    "\n",
    "so $\\text{span}\\big(\\mathbf A^{2n}\\big) \\subset \\text{span}\\big(\\mathbf A^{n}\\big)$   \n",
    "but $\\text{span}\\big(\\mathbf A^{2n}\\big) \\neq \\text{span}\\big(\\mathbf A^{n}\\big)$   \n",
    "\n",
    "which per the two cases in our guiding inequality implies  \n",
    "\n",
    " $\\text{rank}\\big(\\mathbf A^{2n} \\big) \\lt \\text{rank}\\big(\\mathbf A^n \\big) = r = \\text{rank}\\big(\\mathbf A^{2n} \\big)$  \n",
    " \n",
    " which is the contradiction we sought  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.6.17** and **4.6.18**  \n",
    "the writeup is included in the related section (using the vec operato) of \"Kronecker_Product.ipynb\" in the Linear Algebra folder  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.8.7**  \n",
    "note: **4.8.19** is covered under the first \"*remark*\"  \n",
    "\n",
    "$\\det\\big(e^\\mathbf A\\big)= e^{\\text{trace}(\\mathbf A)}$  \n",
    "$\\mathbf A \\in \\mathbb C^\\text{n x n}$  and $t \\in \\mathbb C$   \n",
    "\n",
    "the below is an alternative proof, working through the calculus formulation of the problem in Stillwell's *Naive Lie Theory*    \n",
    "\n",
    "alternative proof that for with \n",
    "the result is nearly immediate using Schur's Triangularization Theorem the closure of triangular matrices as under powers of $k$ and scaling and addition (or jumping straight into use of the Jordan canonical form) and observing that \n",
    "\n",
    "$\\prod_{k=1}^m e^{\\lambda_k} =e^{\\sum_{k=1}^m \\lambda_k}$  \n",
    "\n",
    "and we may interpret this to be a standard result in how the exponential function is the bridge between products and sums  \n",
    "\n",
    "What follows below steps through exercises 5.3.1 - 5.3.5 on page 102 of Stillwell's **Naive Lie Theory**.  This calculus approach involves a bit more work but gives us a different interpretation of the result and in particular since Lie Theory is intimately tied in with differential equations, the form below gives us a different proof *and interpretation* of the result that is perhaps closer to the 'differential equation structure'   \n",
    "\n",
    "where $\\mathbf B_{i,j}$ refers to the submatrix created by deleting row i and column j of $\\mathbf B$ \n",
    "\n",
    "note: we are interested in smooth paths, where each component $b_{i,j}$ of $\\mathbf B$ is a function of a real (or possibly complex) scalar $t$, denoted in general as $\\mathbf B(t)$ and in particular we have smooth path connection to the identity and $\\mathbf B(0) = \\mathbf I$  \n",
    "\n",
    "**5.3.1 and 5.3.2**  \n",
    "Show  \n",
    "$\\det\\big(\\mathbf B(t)\\big) = \\sum_{j=1}^n (-1)^{j+1}b_{1,j}(t)\\cdot \\det\\big(B_{1,j}(t)\\big)$  \n",
    "*remark:* this is the recursive definition of the determinant, e.g. the one stated in Artin  \n",
    "\n",
    "and hence   \n",
    "$\\frac{d}{dt}\\big \\vert_{t=0} \\det\\big(\\mathbf B(t)\\big) $  \n",
    "$= \\frac{d}{dt}\\big \\vert_{t=0} \\sum_{j=1}^n (-1)^{j+1}b_{1,j}(t)\\cdot \\det\\big(\\mathbf B_{1,j}(t)\\big) $  \n",
    "$=  \\sum_{j=1}^n (-1)^{j+1}\\frac{d}{dt}\\big \\vert_{t=0}\\Big\\{ b_{1,j}(t)\\cdot \\det\\big(\\mathbf B_{1,j}(t)\\big)\\Big\\}$  \n",
    "$=  \\sum_{j=1}^n (-1)^{j+1}\\Big\\{ b_{1,j}'(0)\\cdot \\det\\big(\\mathbf B_{1,j}(0)\\big) + b_{1,j}(0)\\cdot\\frac{d}{dt}\\big \\vert_{t=0} \\det\\big(\\mathbf B_{1,j}(0)\\big)\\Big\\}$  (by the product rule)  \n",
    "$=  b_{1,1}'(0) + \\frac{d}{dt}\\big \\vert_{t=0} \\det\\big(\\mathbf B_{1,1}(0)\\big)\\Big\\}$ \n",
    "\n",
    "justification for the final line:  \n",
    "for all $j\\gt 1$ \n",
    "$\\det\\big(B_{1,j}(0) = 0$  \n",
    "because we remove row one of $\\mathbf B(0) = \\mathbf I$ and a column other than 1, hence the resulting submatrix has the 'residual' of column 1 -- i.e. a column of all zeros, and hence has determinant zero.  \n",
    "\n",
    "$ b_{1,j}(0) =0$  \n",
    "by inspection, every component of the first row of the identity matrix is zero except for the j=1 component.  \n",
    "\n",
    "for $j= 1$  \n",
    "$b_{1,1}'(0)\\cdot \\det\\big(B_{1,1}(0)\\big) = b_{1,1}'(0)$   \n",
    "because $B_{1,j}(0) = B_{1,1}(0) = \\mathbf I_{n-1}$  \n",
    "\n",
    "and note \n",
    "$b_{1,1}(0) = 1 $  \n",
    "as it is the top left component of the identity matrix  \n",
    "\n",
    "- - - - -  \n",
    "*remark:*  \n",
    "if we want the determinant in general of $\\mathbf A(t)$, which is problem **4.8.19** in Artin,  \n",
    "\n",
    "we can reconsider the third and fourth lines under \"and hence\" and consider general $t$, not just $t= 0$, we'd have  \n",
    "$\\frac{d}{dt} \\det\\big(\\mathbf B(t)\\big) $  \n",
    "$=  \\sum_{j=1}^n (-1)^{j+1}\\frac{d}{dt}\\Big\\{ b_{1,j}(t)\\cdot \\det\\big(\\mathbf B_{1,j}(t)\\big)\\Big\\}$  \n",
    "$=  \\sum_{j=1}^n (-1)^{j+1}\\Big\\{ b_{1,j}'(t)\\cdot \\det\\big(\\mathbf B_{1,j}(t)\\big) + b_{1,j}(t)\\cdot\\frac{d}{dt}  \\det\\big(\\mathbf B_{1,j}(t)\\big)\\Big\\}$  (by the product rule)  \n",
    "$=  \\Big\\{\\sum_{j=1}^n (-1)^{j+1} b_{1,j}'(t)\\cdot \\det\\big(\\mathbf B_{1,j}(t)\\big)\\Big\\} +\\Big\\{\\sum_{j=1}^n (-1)^{j+1} b_{1,j}(t)\\cdot\\frac{d}{dt}  \\det\\big(\\mathbf B_{1,j}(t)\\big)\\Big\\}$  \n",
    "\n",
    "\n",
    "\n",
    "This sets up an inductive proof for the derivative, in general for  \n",
    "$\\frac{d}{dt} \\det\\big(\\mathbf B(t)\\big) = \\det\\big(\\mathbf D_1\\big) + \\det\\big(\\mathbf D_2\\big) + ... + \\det\\big(\\mathbf D_n\\big) $  \n",
    "\n",
    "where $\\mathbf D_i$ is the same as $\\mathbf B$ except the entries in row $i$ are replaced by their derivatives. (This is a slight change in notation here as we match Meyer where this fact is proven directly on page 471 of Meyer's *Matrix Analysis* using the alternating sign summing over permutations definition of the determinant.)  \n",
    "\n",
    "So consider the case of dimension $n:=2$ and we can immediately see that this holds.  This is a base case.  Now we have the result for $n-1$ and need to show it holds for $n$ for arbitrary natural number $n \\geq 3$. So reconsidering  \n",
    "\n",
    "$\\Big\\{\\sum_{j=1}^n (-1)^{j+1} b_{1,j}'(t)\\cdot \\det\\big(\\mathbf B_{1,j}(t)\\big)\\Big\\} +\\Big\\{\\sum_{j=1}^n (-1)^{j+1} b_{1,j}(t)\\cdot\\frac{d}{dt}  \\det\\big(\\mathbf B_{1,j}(t)\\big)\\Big\\}$  \n",
    "$=\\det\\big(\\mathbf D_1\\big) +\\Big\\{\\sum_{j=1}^n (-1)^{j+1} b_{1,j}(t)\\cdot\\frac{d}{dt}  \\det\\big(\\mathbf B_{1,j}(t)\\big)\\Big\\}$  \n",
    "\n",
    "where by induction hypothesis, each $\\frac{d}{dt}\\det\\big(\\mathbf B_{1,j}(t)\\big)$ is a sum of determinants of $n-1$ matrices each of which is the same as the original except one row has been removed.  Notation can be a bit of a challenge, but basically we show this substitution, take advantage of linearity, and get the result, i.e.  \n",
    "$=\\det\\big(\\mathbf D_1\\big) +\\Big\\{\\sum_{j=1}^n (-1)^{j+1} b_{1,j}(t)\\cdot \\big(\\det\\big(\\mathbf D_1^{1,j}\\big) + \\det\\big(\\mathbf D_2^{1,j}\\big) + ... +\\det\\big(\\mathbf D_{n-1}^{1,j}\\big)\\big)\\Big\\}$  \n",
    "$=\\det\\big(\\mathbf D_1\\big) +\\Big(\\sum_{j=1}^n (-1)^{j+1} b_{1,j}(t)\\cdot \\det\\big(\\mathbf D_1^{1,j}\\big)\\Big) + \\Big(\\sum_{j=1}^n (-1)^{j+1} b_{1,j}(t)\\cdot \\det\\big(\\mathbf D_2^{1,j}\\big)\\Big) + ... \\Big(\\sum_{j=1}^n (-1)^{j+1} b_{1,j}(t)\\cdot \\det\\big(\\mathbf D_{n-1}^{1,j}\\big)\\Big)$   \n",
    "$=\\det\\big(\\mathbf D_1\\big) +\\det\\big(\\mathbf D_2\\big) + \\det\\big(\\mathbf D_3\\big) + .... + \\det\\big(\\mathbf D_n\\big)$   \n",
    "this is a rather general result which immediately implies the below result when we specialize to $t =0$   and know our function is the identity when $t = 0$   \n",
    "\n",
    "- - - -- - \n",
    "\n",
    "**5.3.3**  \n",
    "this sets up a nice induction hypothesis as we now have an $\\text{n-1  } x \\text{  n-1  }$  matrix, given by $\\mathbf B_{1,1}(0)$ that is a function of $t$ and is the identity when $t=0$, so we want to find  \n",
    "$\\frac{d}{dt}\\big \\vert_{t=0} \\det\\big(\\mathbf B_{1,1}(0)\\big)$   \n",
    "\n",
    "note when $n=2$ it is immediate that this evaluates to the derivative of the bottom left corner of the matrix, i.e. $b_{n,n}'(0)$ and this serves as our base case.  Combining this with the prior exercise tells us that \n",
    "\n",
    "$\\frac{d}{dt}\\big \\vert_{t=0} \\det\\big(\\mathbf B(t)\\big) = b_{1,1}'(t) + b_{2,2}'(t) + ... + b_{n,n}'(t) = \\text{trace}\\big(\\mathbf B'(0)\\big)$    \n",
    "\n",
    "- - - - \n",
    "*remark:*  \n",
    "The above is an interesting result in and of itself, relating the derivative of a determinant of matrix with respect to $t$ in the neighborhood of the identity (e.g. since the path is smooth, we know the derivative is continuous) to the trace of the derivative.  \n",
    "\n",
    "**5.3.4**  \n",
    "We now take this machinery and apply it to something very familiar from Lie Theory and differential equations, i.e. we specialize to    \n",
    "$\\mathbf B(t) := \\exp\\big(t\\mathbf A\\big)$, observing $\\mathbf B(0) = \\mathbf I$, and noting that  \n",
    "$\\mathbf B'(t) = \\exp\\big(t\\mathbf A\\big)\\cdot \\frac{d}{dt} t\\mathbf A = \\exp\\big(t\\mathbf A\\big) \\mathbf A $   \n",
    "\n",
    "and in particular  \n",
    "$\\mathbf B'(0) = \\mathbf A $  \n",
    "\n",
    "(which we justify on grounds of the chain rule here, or may explicitly work through that derivative on the power series, or Taylor polynomial, which is what is done in Artin chapter 4)  \n",
    "\n",
    "and since we are interested in the determinant, we can further specialize to  \n",
    "\n",
    "$f(t):= \\det\\Big(\\exp\\big(t\\mathbf A\\big)\\Big)$  \n",
    "\n",
    "direct application of the above tells us that  \n",
    "$f'(t) = \\text{trace}\\Big(\\mathbf B'(t)\\Big) =\\text{trace}\\Big(\\exp\\big(t\\mathbf A\\big) \\mathbf A \\Big) $   \n",
    "and in particular  \n",
    "$f'(0) = \\text{trace}\\Big(\\mathbf B'(0)\\Big) =\\text{trace}\\Big(\\mathbf A\\Big) $   \n",
    "\n",
    "The issue is that while $f'(0)$ is easy to evaluate, since it is simply the trace of $\\mathbf A$, in general $f'(t)$ is not so easy to work with.  The approach forward is to 'hug the identity' and relate the general $t$ case to the $t=0$ case, by making use of the multiplicative property of determinants.  \n",
    "\n",
    "so we start by revisiting the different quotient definition of $f'(t)$  and the fact that if $\\mathbf {AB} = \\mathbf {BA}$ then $e^{\\big({\\mathbf {A} + \\mathbf {B}}\\big)} = e^{\\mathbf {A}} e^{\\mathbf {B}}$  \n",
    "\n",
    "\n",
    " $f'(t) = \\lim_{h\\to 0} \\frac{1}{h}\\Big(\\det\\big(e^{(t+h)\\mathbf A}\\big) - \\det\\big(e^{t\\mathbf A}\\big)\\Big) $  \n",
    " $= \\lim_{h\\to 0} \\frac{1}{h}\\Big(\\det\\big(e^{t\\mathbf A}e^{h\\mathbf A}\\big) - \\det\\big(e^{t\\mathbf A}\\big)\\Big) $  \n",
    " $= \\lim_{h\\to 0} \\frac{1}{h}\\Big(\\det\\big(e^{t\\mathbf A}\\big)\\det\\big(e^{h\\mathbf A}\\big) - \\det\\big(e^{t\\mathbf A}\\big)\\cdot 1\\Big) $  \n",
    " $= \\det\\big(e^{t\\mathbf A}\\big) \\cdot \\lim_{h\\to 0} \\frac{1}{h}\\Big(\\det\\big(e^{h\\mathbf A}\\big)-1\\cdot 1 \\Big) $  \n",
    " $= \\det\\big(e^{t\\mathbf A}\\big) \\cdot \\lim_{h\\to 0} \\frac{1}{h}\\Big(\\det\\big(e^{h\\mathbf A}\\big)\\det\\big(e^{0\\mathbf A}\\big) - \\det\\big(e^{0\\mathbf A}\\big)\\Big) $  \n",
    " $= \\det\\big(e^{t\\mathbf A}\\big) \\cdot \\lim_{h\\to 0} \\frac{1}{h}\\Big(\\det\\big(e^{h\\mathbf A}e^{0\\mathbf A}\\big) - \\det\\big(e^{0\\mathbf A}\\big)\\Big) $  \n",
    " $= \\det\\big(e^{t\\mathbf A}\\big) \\cdot \\lim_{h\\to 0} \\frac{1}{h}\\Big(\\det\\big(e^{h\\mathbf A + 0\\mathbf A}\\big) - \\det\\big(e^{0\\mathbf A}\\big)\\Big) $  \n",
    " $= \\det\\big(e^{t\\mathbf A}\\big) \\cdot \\lim_{h\\to 0} \\frac{1}{h}\\Big(\\det\\big(e^{(h+0)\\mathbf A}\\big) - \n",
    "\\det\\big(e^{0\\mathbf A}\\big)\\Big) $  \n",
    "$= \\det\\big(e^{t\\mathbf A}\\big) \\cdot f'(0)$  \n",
    "$=\\det\\big(e^{t\\mathbf A}\\big) \\cdot\\text{trace}\\Big(\\mathbf A\\Big)$    \n",
    "$=f(t) \\cdot\\text{trace}\\Big(\\mathbf A\\Big)$    \n",
    "$=f(t) \\cdot f'(0)$    \n",
    "\n",
    "**5.3.5**  \n",
    "we now have a differential equation \n",
    "and we may set  \n",
    "\n",
    "$f(t) = g(t)\\cdot e^{t\\cdot \\text{trace}\\big(\\mathbf A\\big)}$  \n",
    "$g$ is some smooth function, $g: \\mathbb C \\longrightarrow \\mathbb C$  \n",
    "\n",
    "\n",
    "so application of the product rule (and then chain rule) tells us  \n",
    "\n",
    "$f'(t) = g'(t)\\cdot e^{t\\cdot \\text{trace}\\big(\\mathbf A\\big)} + g(t)\\cdot e^{t\\cdot \\text{trace}\\big(\\mathbf A\\big)}\\cdot \\text{trace}\\big(\\mathbf A\\big) = g'(t)\\cdot e^{t\\cdot \\text{trace}\\big(\\mathbf A\\big)} + f(t)\\cdot \\text{trace}\\big(\\mathbf A\\big)$  \n",
    "\n",
    "but when we combine this with the prior exercise we have  \n",
    "$g'(t)\\cdot e^{t\\cdot \\text{trace}\\big(\\mathbf A\\big)} + f(t)\\cdot \\text{trace}\\big(\\mathbf A\\big) = f'(t) = f(t) \\cdot\\text{trace}\\big(\\mathbf A\\big)$  \n",
    "\n",
    "hence  \n",
    "\n",
    "$g'(t)\\cdot e^{t\\cdot \\text{trace}\\big(\\mathbf A\\big)} = 0\\longrightarrow g'(t) =0$  \n",
    "because  $e^{t\\cdot \\text{trace}\\big(\\mathbf A\\big)}$ is just the scalar valued complex exponential function which is never zero \n",
    "\n",
    "Now, since $ g'(t) = 0$ everywhere on our path connected domain, application of mean value inequality (see notebook of that name in the 'inequalities' folder) tells us that  $g(t)$ is constant.  (Alternatively, integration along the path, and the fundamental theorem of calculus gets to this result.)  \n",
    "\n",
    "In particular we know \n",
    "$g(t) = g(0)$  \n",
    "for any $t$ in our domain  \n",
    "\n",
    "so re-using the above we have  \n",
    "\n",
    "$f'(t) = g(0)\\cdot e^{t\\cdot \\text{trace}\\big(\\mathbf A\\big)}\\cdot \\text{trace}\\big(\\mathbf A\\big)$  \n",
    "and  \n",
    "$\\text{trace}\\Big(\\mathbf A\\Big) = f'(0) =  g(0)\\cdot e^{0\\cdot \\text{trace}\\big(\\mathbf A\\big)}\\cdot \\text{trace}\\big(\\mathbf A\\big) = g(0)\\cdot e^{0}\\cdot \\text{trace}\\big(\\mathbf A\\big) = g(0)\\cdot \\text{trace}\\big(\\mathbf A\\big)$   \n",
    "hence $g(t) = g(0) = 1$   \n",
    "\n",
    "we may now plug this into our determinant function  \n",
    "$\\det\\Big(e^{\\big(t\\mathbf A\\big)}\\Big) = f(t) = g(t)\\cdot e^{t\\cdot \\text{trace}\\big(\\mathbf A\\big)} = e^{t\\cdot \\text{trace}\\big(\\mathbf A\\big)}$  \n",
    "- - - - - \n",
    "at this point the reader may wonder why we did not have an additive constant in our differential equation like  \n",
    "$f(t) = g(t)\\cdot e^{t\\cdot \\text{trace}\\big(\\mathbf A\\big)} + \\alpha$  \n",
    "but the above work-through at the differential level would *still* show $g(0)=1$ hence we'd have  \n",
    "$\\det\\Big(e^{\\big(t\\mathbf A\\big)}\\Big) = f(t) = e^{t\\cdot \\text{trace}\\big(\\mathbf A\\big)} + \\alpha$  \n",
    "and evaluating at $t=0$ gives  \n",
    "$1 = \\det\\Big(\\mathbf I\\big)=\\det\\Big(e^{\\big(0\\mathbf A\\big)}\\Big) = f(0) =  e^{0\\cdot \\text{trace}\\big(\\mathbf A\\big)} + \\alpha = e^0  + \\alpha = 1+\\alpha \\longrightarrow \\alpha = 0$  \n",
    "\n",
    "- - - - - \n",
    "finally, revisiting  \n",
    "$\\det\\Big(e^{\\big(t\\mathbf A\\big)}\\Big) = e^{t\\cdot \\text{trace}\\big(\\mathbf A\\big)}$  \n",
    "and selecting $t=1$ gives   \n",
    "$\\det\\Big(e^{\\mathbf A}\\Big) = e^{\\text{trace}\\big(\\mathbf A\\big)}$  \n",
    "as desired  \n",
    "\n",
    "**corollary**  \n",
    "while in general \n",
    "$e^{\\mathbf A + \\mathbf B} \\neq e^{\\mathbf A}e^{\\mathbf B}$  \n",
    "the above tells us that they do have equal determinants, because  \n",
    "$\\det\\Big(e^{\\mathbf A + \\mathbf B}\\Big)$  \n",
    "$= e^{\\text{trace}\\big(\\mathbf A + \\mathbf B\\big)}$  \n",
    "$= e^{\\text{trace}\\big(\\mathbf A\\big) + \\text{trace}\\big(\\mathbf B\\big)}$  \n",
    "$= e^{\\text{trace}\\big(\\mathbf A\\big)} e^{\\text{trace}\\big(\\mathbf B\\big)}$  \n",
    "$= \\det\\Big(e^{\\mathbf A}\\Big)\\det\\Big(e^{\\mathbf B}\\Big)$  \n",
    "$= \\det\\Big(e^{\\mathbf A}e^{\\mathbf B}\\Big)$   \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*alternative derivation*  \n",
    "A *much* shorter way of arriving at these results is via a density argument.  Supposing we know that the exponential function (even when applied to n x n matrices) is continuous then we have by composition of continuous functions, and addition/subtraction of continuous functions  \n",
    "\n",
    "$g\\Big( \\mathbf A\\Big) := \\det\\Big(\\exp\\big(\\mathbf A\\big)\\Big) - \\exp\\Big(\\text{trace}\\big(\\mathbf A\\big)\\Big)$  \n",
    "and $g$ is a continuous map   \n",
    "$g:\\mathbb C^\\text{ n x n } \\longrightarrow \\mathbb C$  \n",
    "\n",
    "via a diagonalization argument we should see \n",
    "$g\\Big( \\mathbf A\\Big) = 0$  \n",
    "when $\\mathbf A$ is diagonalizable  \n",
    "\n",
    "now when $\\mathbf A$ is defective, by continuity of $g$, for any $\\epsilon \\gt 0$ we know there is some $\\delta \\gt 0$ neighborhood where any arbitrarily chosen $\\mathbf A'$ gives us $\\Big \\Vert g\\big(\\mathbf A\\big) - g\\big(\\mathbf A'\\big)\\Big \\Vert_F\\lt \\epsilon$  \n",
    "\n",
    "now suppose for a contradiction that for some defective $\\mathbf A$  \n",
    "$g\\big(\\mathbf A\\big) = c \\neq 0$, now select $\\epsilon := \\frac{\\vert c \\vert}{3}$, select $\\mathbf A'$ to be diaonalizable in this $\\delta$ neighborhood and we have  \n",
    "\n",
    "$\\vert c \\vert = \\Big \\Vert g\\big(\\mathbf A\\big) - 0\\Big \\Vert_F = \\Big \\Vert g\\big(\\mathbf A\\big) - g\\big(\\mathbf A'\\big)\\Big \\Vert_F \\lt \\epsilon = \\frac{\\vert c \\vert}{3} $   \n",
    "\n",
    "which is a contradiction -- i.e. it must be the case that $\\vert c \\vert = 0$  \n",
    "\n",
    "*of course the most direct approach, which in some sense is 'the' correct approach is to just work with our matrix in upper triangular form and observe how the power series of the exponential function behaves with respect to the diagonal components of said matrix*  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.4.5**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sympy as sp\n",
    "x = sp.Symbol('x')\n",
    "A = sp.Matrix([[x,-1],[0,-1]])\n",
    "W = sp.Matrix([[1,1],[x, -1]])\n",
    "b = sp.Matrix([x, x**2-1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is a finite alternating geometric series with the degree n-1 term missing and a degree n term included\n",
    "if we select y = -1, this 'just about' gives us the finite geoemetric series in ex 2.5 of *CS Masterclass*  \n",
    "\n",
    "note we don't need to actually solve for the series per se.  We merely need its form and the result follows by induction (see photo on 26 Nov for my scratchwork that is the setup for this as an inductive proof) and once we have the form for degree n-1, the result follows by applying the recurrence  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}\\frac{x^{2} + x - 1}{x + 1}\\\\\\frac{1}{x + 1}\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "Matrix([\n",
       "[(x**2 + x - 1)/(x + 1)],\n",
       "[             1/(x + 1)]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp.simplify(W.inv()*b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x**2 - 1\n",
      "x**2 - 1\n",
      " \n",
      "x**3 - x + 1\n",
      "x**3 - x + 1\n",
      " \n",
      "x**4 - x**2 + x - 1\n",
      "x**4 - x**2 + x - 1\n",
      " \n",
      "x**5 - x**3 + x**2 - x + 1\n",
      "x**5 - x**3 + x**2 - x + 1\n",
      " \n",
      "x**6 - x**4 + x**3 - x**2 + x - 1\n",
      "(x**5*(x**2 + x - 1) - 1)/(x + 1)\n",
      " \n",
      "x**7 - x**5 + x**4 - x**3 + x**2 - x + 1\n",
      "(x**6*(x**2 + x - 1) + 1)/(x + 1)\n",
      " \n",
      "x**8 - x**6 + x**5 - x**4 + x**3 - x**2 + x - 1\n",
      "(x**7*(x**2 + x - 1) - 1)/(x + 1)\n",
      " \n",
      "x**9 - x**7 + x**6 - x**5 + x**4 - x**3 + x**2 - x + 1\n",
      "(x**8*(x**2 + x - 1) + 1)/(x + 1)\n",
      " \n",
      "x**10 - x**8 + x**7 - x**6 + x**5 - x**4 + x**3 - x**2 + x - 1\n",
      "(x**9*(x**2 + x - 1) - 1)/(x + 1)\n",
      " \n"
     ]
    }
   ],
   "source": [
    "y = sp.Matrix([x,1])\n",
    "for k in range(1,10):\n",
    "    b = A*y\n",
    "    print(b[0].expand())\n",
    "    thing = x**k*(x**2+x -1)/(x+1) + (-1)**k*(1/(x+1))\n",
    "    print(sp.simplify(thing))\n",
    "    print(\" \")\n",
    "    y = b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.6.12**  \n",
    "remark: some note on blocked multiplication and determinant in \"Artin_chp2_misc_items.ipynb\" may be of use here  \n",
    "\n",
    "Let $\\mathbf M$ be block diagonal in the form \n",
    "\n",
    "$\\mathbf M: =  \\begin{bmatrix}\n",
    "\\mathbf A & \\mathbf 0\\\\ \n",
    "\\mathbf 0 & \\mathbf B\n",
    "\\end{bmatrix}$  \n",
    "\n",
    "where $\\mathbf A$ and $\\mathbf B$ are both square.  (Note Artin uses D instead of B which is a bit confusing as D frequently represents a diagonal matrix.)  \n",
    "\n",
    "\n",
    "*claim:*\n",
    "$M$ is diagonalizable if and only if $\\mathbf A$ and $\\mathbf B$ are.  \n",
    "\n",
    "\n",
    "First note that the characteristic polynomial of $M$ splits  \n",
    "\n",
    "$\\det\\big(\\lambda \\mathbf I_n -\\mathbf M\\big)\n",
    "=  \\det\\Big(\\begin{bmatrix}\n",
    "\\lambda \\mathbf I_m - \\mathbf A & \\mathbf 0\\\\ \n",
    "\\mathbf 0 & \\lambda \\mathbf I_{n-m} - \\mathbf B\n",
    "\\end{bmatrix}\\Big)= \\det\\big(\\lambda \\mathbf I_m - \\mathbf A\\big)\\det\\big( \\lambda \\mathbf I_{n-m} - \\mathbf B\\big)$   \n",
    "\n",
    "so the **algebraic multiplicities** of the eigenvalues of $\\mathbf M$'s $\\lambda_i$ are equal to  \n",
    "$\\text{alg mult}\\lambda_i^{(M)} =\\text{alg mult}\\lambda_i^{(A)}+\\text{alg mult}\\lambda_i^{(B)}$  \n",
    "\n",
    "regarding **geometric multiplicites**, we can see by inspection that  \n",
    "$\\text{rank}\\big(\\lambda \\mathbf I_n -\\mathbf M\\big) = \\text{rank}\\big(\\lambda \\mathbf I_m - \\mathbf A\\big)+\\text{rank}\\big( \\lambda \\mathbf I_{n-m} - \\mathbf B\\big)$   \n",
    "\n",
    "so by rank-nullity we have  \n",
    "$\\text{dim null}\\big(\\lambda \\mathbf I_n -\\mathbf M\\big) = \\text{dim null}\\big(\\lambda \\mathbf I_m - \\mathbf A\\big)+\\text{dim null}\\big( \\lambda \\mathbf I_{n-m} - \\mathbf B\\big)$   \n",
    "\n",
    "- - - -  \n",
    "if this isn't obvious, we can merely observe two lower bounds  \n",
    "$\\text{rank}\\big(\\lambda \\mathbf I_n -\\mathbf M\\big) \\geq \\text{rank}\\big(\\lambda \\mathbf I_m - \\mathbf A\\big)+\\text{rank}\\big( \\lambda \\mathbf I_{n-m} - \\mathbf B\\big)$   \n",
    "\n",
    "$\\text{dim null}\\big(\\lambda \\mathbf I_n -\\mathbf M\\big) \\geq \\text{dim null}\\big(\\lambda \\mathbf I_m - \\mathbf A\\big)+\\text{dim null}\\big( \\lambda \\mathbf I_{n-m} - \\mathbf B\\big)$   \n",
    "\n",
    "negating and adding $n$ then applying rank-nullity gives  \n",
    "\n",
    "$\\text{rank}\\big(\\lambda \\mathbf I_n -\\mathbf M\\big) \\leq \\text{rank}\\big(\\lambda \\mathbf I_m - \\mathbf A\\big)+\\text{rank}\\big( \\lambda \\mathbf I_{n-m} - \\mathbf B\\big)$  \n",
    "\n",
    "which proves the claim  \n",
    "- - - -   \n",
    "\n",
    "Thus we iterate through all distinct $r$ eigenvalues that $M$ has and check, for $i=1, 2,...,r$  \n",
    "\n",
    "\n",
    "$\\text{geometric mult}\\big(\\lambda_i^{(M)}\\big)$  \n",
    "$=\\text{dim null}\\big(\\lambda_i\\mathbf I_n -\\mathbf M\\big) $  \n",
    "$=\\text{dim null}\\big(\\lambda_i \\mathbf I_m - \\mathbf A\\big)+\\text{dim null}\\big( \\lambda_i \\mathbf I_{n-m} - \\mathbf B\\big) $  \n",
    "$= \\text{geometric mult}\\big(\\lambda_i^{(A)}\\big) + \\text{geometric mult}\\big(\\lambda_i^{(B)}\\big)$  \n",
    "$\\leq \\text{alg mult}\\lambda_i^{(A)}+\\text{alg mult}\\lambda_i^{(B)}$  \n",
    "$= \\text{alg mult}\\lambda_i^{(M)}$ \n",
    "\n",
    "\n",
    "summing over the bound, we have   \n",
    "$\\sum_{i=1}^r \\text{geometric mult}\\big(\\lambda_i^{(M)}\\big)$  \n",
    "$=\\Big(\\sum_{i=1}^r\\text{geometric mult}\\big(\\lambda_i^{(A)}\\big)\\Big) + \\Big(\\text{geometric mult}\\big(\\lambda_i^{(B)}\\big)\\Big)$  \n",
    "$\\leq \\Big(\\sum_{i=1}^r\\text{alg mult}\\big(\\lambda_i^{(A)}\\big)\\Big)+ \\Big(\\sum_{i=1}^r\\text{alg mult}\\big(\\lambda_i^{(B)}\\big)\\Big)$  \n",
    "$= \\text{alg mult}\\big(\\lambda_i^{(M)}\\big)$  \n",
    "$=n$  \n",
    "\n",
    "\n",
    "and since for all $i$    \n",
    "$\\text{geometric mult}\\big(\\lambda_i^{(A)}\\big)\\leq \\text{alg mult}\\lambda_i^{(A)}\\big)$     \n",
    "$\\text{geometric mult}\\big(\\lambda_i^{(B)}\\big)\\leq \\text{alg mult}\\big(\\lambda_i^{(B)}\\big)$   \n",
    "\n",
    "we know  \n",
    "$\\text{total eigenvectors of }\\mathbf M =\\sum_{i=1}^r \\text{geometric mult}\\big(\\lambda_i^{(M)}\\big)\\leq \\sum_{i=1}^r\\text{alg mult}\\big(\\lambda_i^{(M)}\\big) = n$  \n",
    "with equality *iff*  \n",
    "$\\text{geometric mult}\\big(\\lambda_i^{(A)}\\big) = \\text{alg mult}\\big(\\lambda_i^{(A)}\\big)$ and $\\text{geometric mult}\\big(\\lambda_i^{(B)}\\big) = \\text{alg mult}\\big(\\lambda_i^{(B)}\\big)$   \n",
    "for all $i$ \n",
    "\n",
    "but \n",
    "$\\text{geometric mult}\\big(\\lambda_i^{(A)}\\big) = \\text{alg mult}\\big(\\lambda_i^{(A)}\\big)$  \n",
    "implies $\\mathbf A$ is diagonalizable and  \n",
    "$\\text{geometric mult}\\big(\\lambda_i^{(B)}\\big) = \\text{alg mult}\\big(\\lambda_i^{(B)}\\big)$   \n",
    "implies $\\mathbf B$ is diagonalizable  \n",
    "so $\\mathbf M$ is diagonalizable if and only if $\\mathbf A$ and $\\mathbf B$ are diagonalizable   \n",
    "\n",
    "\n",
    "*remark*  \n",
    "A more direct/ immediate proof of the part of the claim:  \n",
    "$\\text{diagonalizable A and B}\\longrightarrow \\text{diagonalizable M}$    \n",
    "can be shown by effecting a similarity transform via blocked multiplication  \n",
    "\n",
    "$\\begin{bmatrix}\n",
    "\\mathbf S_1 & \\mathbf 0\\\\ \n",
    "\\mathbf 0 & \\mathbf S_2\n",
    "\\end{bmatrix}\\begin{bmatrix}\n",
    "\\mathbf A & \\mathbf 0\\\\ \n",
    "\\mathbf 0 & \\mathbf B\n",
    "\\end{bmatrix}\\begin{bmatrix}\n",
    "\\mathbf S_1 & \\mathbf 0\\\\ \n",
    "\\mathbf 0 & \\mathbf S_2\n",
    "\\end{bmatrix}^{-1}=\\begin{bmatrix}\n",
    "\\mathbf S_1 & \\mathbf 0\\\\ \n",
    "\\mathbf 0 & \\mathbf S_2\n",
    "\\end{bmatrix}\\begin{bmatrix}\n",
    "\\mathbf A & \\mathbf 0\\\\ \n",
    "\\mathbf 0 & \\mathbf B\n",
    "\\end{bmatrix}\\begin{bmatrix}\n",
    "\\mathbf S_1^{-1} & \\mathbf 0\\\\ \n",
    "\\mathbf 0 & \\mathbf S_2^{-1}\n",
    "\\end{bmatrix}= \\begin{bmatrix}\n",
    "\\mathbf S_1\\mathbf A\\mathbf S_1^{-1} & \\mathbf 0\\\\ \n",
    "\\mathbf 0 & \\mathbf S_2\\mathbf B \\mathbf S_2^{-1}\n",
    "\\end{bmatrix}=\\begin{bmatrix}\n",
    "\\mathbf D_1 & \\mathbf 0\\\\ \n",
    "\\mathbf 0 & \\mathbf D_2\n",
    "\\end{bmatrix}$ \n",
    "\n",
    "this somewhat obviously implies  \n",
    "$\\text{diagonalizable M}\\longrightarrow \\text{diagonalizable A and B}$    \n",
    "when there are no common eigenvalues between $\\mathbf A$ and $\\mathbf B$. \n",
    "\n",
    "Unfortunately the repeated eigenvalue case, in particular the case where $\\mathbf A$ and $\\mathbf B$ have common eigenvalues becomes a major nuisance, and trying to 'unwind' a blocked structure becomes a mess when dealing with possible linear dependence issues between possibly shared eigenvectors.  The approach taken here using inequalities is best.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.7.7**  \n",
    "\n",
    "suppose we have the linear (differential) relation  \n",
    "$\\frac{d^{n} }{dt^{n}}x(t)+ a_{n-1}\\frac{d^{n-1} }{dt^{n-1}}x(t) + ... a_{1}\\frac{d^{1} }{dt^{1}}x(t)+a_0x(t)=0$  \n",
    "since functions (with certain desired attributes) may form a vector space we may re-label each of these derivatives as new functions:   \n",
    "\n",
    "$x_0(t)=x(t)$  \n",
    "or just $x_0$ for short  \n",
    "\n",
    "$x_1(t) = \\frac{d }{dt}x(t)$  \n",
    "or just $x_1$ for short, then  \n",
    "\n",
    "$x_2 =  \\frac{d^2}{dt^2}x_0 = \\frac{d}{dt}x_1 $   \n",
    "$x_3  = \\frac{d^3}{dt^3}x_0 = \\frac{d^2}{dt^2}x_1= \\frac{d}{dt}x_2  $  \n",
    "$x_4  = ... = \\frac{d}{dt}x_3  $  \n",
    "$x_{n-1}  = ... = \\frac{d}{dt}x_{n-2}  $  \n",
    "$x_{n}  = \\frac{d^{n} }{dt^{n}}x_0 = ... = \\frac{d}{dt}x_{n-1}  $  \n",
    "but using our original linear relation we have  \n",
    "$x_{n}= -\\big(a_{n-1}x_{n-1} + ... +a_{1}x_1 + a_0x_0\\big)$  \n",
    "\n",
    "\n",
    "This system of equations is typically modelled with (transpose of) the Companion Matrix   \n",
    "\n",
    "$\\mathbf C := \\begin{bmatrix}\n",
    "0 & 0& 0&  \\cdots&  0& -a_o\\\\ \n",
    "1 & 0& 0&  \\cdots&  0& -a_1\\\\ \n",
    "0 & 1& 0&  \\cdots&  0& -a_2\\\\ \n",
    "0 & 0& 1&  \\cdots&  0& -a_3\\\\ \n",
    "\\vdots & \\vdots& \\vdots&  \\ddots&  \\vdots& \\vdots\\\\ \n",
    "0 & 0& 0&  \\cdots& 1 & -a_{n-1} \n",
    "\\end{bmatrix}$  \n",
    "\n",
    "or   \n",
    "\n",
    "$\\mathbf C^T := \\begin{bmatrix}\n",
    "0 & 1& 0&  \\cdots&  0& 0\\\\ \n",
    "0 & 0& 1&  \\cdots&  0& 0\\\\ \n",
    "0 & 0& 0&  \\cdots&  0& 0\\\\ \n",
    "0 & 0& 0&  \\cdots&  0& 0\\\\ \n",
    "\\vdots & \\vdots& \\vdots&  \\ddots&  \\vdots& \\vdots\\\\ \n",
    "-a_0 & -a_1& -a_2&  \\cdots& -a_{n-2} & -a_{n-1} \n",
    "\\end{bmatrix}$  \n",
    "\n",
    "so  \n",
    "\n",
    "$\\frac{d}{dt}\\begin{bmatrix}x_0 \\\\ \\vdots \\\\ x_{n-1} \\\\ \\end{bmatrix} = \\begin{bmatrix}x_1 \\\\ \\vdots \\\\ x_{n} \\\\ \\end{bmatrix} = \\mathbf C^T\\begin{bmatrix}x_0 \\\\ \\vdots \\\\ x_{n-1}  \\\\ \\end{bmatrix}$  \n",
    "\n",
    "This problem is one, rather interesting, way of motivating the Companion Matrix.  Other ways are for modeling linear recurrences (difference equations), and for modeling (monic) polynomials.  A lot of the machinery in Linear Algebra was developed specifically with linear differential equations in mind, so the approach here may be closest to the historical roots of the Companion system.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.7.8**  \n",
    "this is a specific example / problem that incorporates the results from the prior exercise  \n",
    "\n",
    "(a) rewrite the second-order linear equation in one variable, in line with the prior exercise  \n",
    "$\\frac{d^2}{dt^2}x(t) + b\\frac{d}{dt}x(t) + cx = 0$  \n",
    "\n",
    "$\\mathbf C^T := \\begin{bmatrix}\n",
    "0 & 1\\\\ \n",
    "-c & -b \n",
    "\\end{bmatrix}$  \n",
    "\n",
    "so  \n",
    "\n",
    "$\\frac{d}{dt}\\begin{bmatrix}x_0 \\\\ x_{1} \\\\ \\end{bmatrix} = \\mathbf C^T\\begin{bmatrix}x_0 \\\\ x_{1}  \\end{bmatrix}$  \n",
    "\n",
    "(b) solve when $b=-4$ and $c=3$  \n",
    "$\\frac{d}{dt}\\begin{bmatrix}x_0 \\\\ x_{1} \\\\ \\end{bmatrix} = \\begin{bmatrix}\n",
    "0 & 1\\\\ \n",
    "-3 & 4 \n",
    "\\end{bmatrix}\\begin{bmatrix}x_0 \\\\ x_{1}  \\end{bmatrix}$  \n",
    "\n",
    "note  \n",
    "$\\text{trace}\\big(\\mathbf C\\big) = \\lambda_1 + \\lambda_2 = 4$  \n",
    "$\\text{det}\\big(\\mathbf C\\big) = \\lambda_1 \\cdot \\lambda_2 = 3$  \n",
    "\n",
    "so if the eigenvalues were not distinct we'd have  \n",
    "$\\lambda_1 + \\lambda_2 = 2\\lambda_1 = 4\\longrightarrow \\lambda_1 = 2$  \n",
    "which implies $4 = \\text{det}\\big(\\mathbf C\\big)  \\neq 3$  \n",
    "hence we know the eigenvalues *are* distinct and our Companion Matrix is diagonalizable  \n",
    "(note we actually have $\\lambda_1 = 3$ and $\\lambda_2 = 1$)  \n",
    "\n",
    "so we know  \n",
    "$\\frac{d}{dt}\\begin{bmatrix}x_0 \\\\ x_{1} \\\\ \\end{bmatrix} = \\mathbf C^T\\begin{bmatrix}x_0 \\\\ x_{1}  \\end{bmatrix} = \\mathbf W \\mathbf D \\mathbf W^{-1}\\begin{bmatrix}x_0 \\\\ x_{1}  \\end{bmatrix} = \\mathbf W \\mathbf D \\mathbf W^{-1}\\begin{bmatrix} x_0 \\\\  x_{1}  \\end{bmatrix}$  \n",
    "\n",
    "since multiplication by some matrix (in this case $\\mathbf W^{-1}$) is linear transformation, we preserve linearity with respect to the derivative operator and recover the classical linear differential equation, i.e.  \n",
    "(where derivative operator is applied component-wise to a vector)  \n",
    "\n",
    "with $\\mathbf y:= \\mathbf W^{-1}\\mathbf x$  \n",
    "\n",
    "$\\frac{d}{dt}\\begin{bmatrix}y_0 \\\\ y_{1} \\\\ \\end{bmatrix} =\\frac{d}{dt}\\mathbf W^{-1}\\begin{bmatrix}x_0 \\\\ x_{1} \\\\ \\end{bmatrix} = \\mathbf W^{-1}\\frac{d}{dt}\\begin{bmatrix}x_0 \\\\ x_{1} \\\\ \\end{bmatrix} =  \\mathbf W^{-1}\\mathbf C^T\\begin{bmatrix}x_0 \\\\ x_{1}  \\end{bmatrix} = \\mathbf D \\mathbf W^{-1}\\begin{bmatrix}x_0 \\\\ x_{1}  \\end{bmatrix} =  \\mathbf D \\begin{bmatrix} y_0 \\\\  y_{1}  \\end{bmatrix}$  \n",
    "\n",
    "so (where $z_i$ and $y_i$ are each functions of $t$)   \n",
    "$\\frac{d}{dt}y_0 = \\lambda_1 y_0$   \n",
    "and  \n",
    "$\\frac{d}{dt}y_1 = \\lambda_2 y_1$  \n",
    "\n",
    "which, per page 133, has its solution of the form   \n",
    "$y_0 = z_0 e^{\\lambda_0t}$  \n",
    "$y_1 = z_1 e^{\\lambda_1t}$  \n",
    "\n",
    "the top of page 137 notes  \n",
    "The coefficients $z_i$ *appearing in these solutions are arbitrary.  They are usually determined by assigning initial conditions, meaning the value of X at some particular* $t_0$.  \n",
    "\n",
    "where $z_i$ are scalars that are constant / do not depend on $t$  \n",
    "\n",
    "Thus  \n",
    "$\\begin{bmatrix}y_0 \\\\ y_{1} \\\\ \\end{bmatrix}= e^{tD} \\mathbf z$  left multiplying by $\\mathbf W$ gives  \n",
    "\n",
    "$\\begin{bmatrix}x_0 \\\\ x_{1} \\\\ \\end{bmatrix}$  \n",
    "$=\\mathbf W\\begin{bmatrix}y_0 \\\\ y_{1} \\\\ \\end{bmatrix}$  \n",
    "$= \\mathbf W e^{tD} \\mathbf z $    \n",
    "$= \\big(\\mathbf W e^{tD} \\mathbf W^{-1}\\big)\\big(\\mathbf W\\mathbf z\\big) $  \n",
    "$= e^{t\\mathbf C}\\mathbf v$  \n",
    "$= \\Big(\\frac{\\lambda_1 e^{t\\lambda_2} - \\lambda_2 e^{t\\lambda_1}}{\\lambda_1-\\lambda_2}\\begin{bmatrix}\n",
    "1 & 0\\\\ \n",
    "0 & 1 \n",
    "\\end{bmatrix} + \\frac{e^{t\\lambda_1} - e^{t\\lambda_2}}{t(\\lambda_1-\\lambda_2)}\\begin{bmatrix}\n",
    "0 & 1\\\\ \n",
    "-3 & 4 \n",
    "\\end{bmatrix}\\Big)\\mathbf v$   \n",
    "$=\\Big(\\frac{3 e^{t} -  e^{3t}}{2}\\begin{bmatrix}\n",
    "1 & 0\\\\ \n",
    "0 & 1 \n",
    "\\end{bmatrix} + \\frac{e^{3t} - e^{t}}{2t}\\begin{bmatrix}\n",
    "0 & 1\\\\ \n",
    "-3 & 4 \n",
    "\\end{bmatrix}\\Big)\\mathbf v$  \n",
    "\n",
    "where the last equality comes from ex 4.Misc.17 (developed further down in this notebook)      \n",
    "\n",
    "*without additional information on the boundary conditions it isn't clear that anything else can be done here*  \n",
    "since  \n",
    "$e^{t\\mathbf C}e^{-t\\mathbf C} = e^{\\mathbf 0} = \\mathbf I$ we know $e^{t\\mathbf C}$ is invertible and in fact  \n",
    "\n",
    "$\\mathbf v=e^{-t\\mathbf C}\\begin{bmatrix}x_0 \\\\ x_{1} \\\\ \\end{bmatrix}$  \n",
    "so if we had initial condition information about function $x_0(t_0)$ and its derivative information at $t_0$ --i.e. $x_1(t_0)$-- we could uniquely solve for $\\mathbf v$  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.8.12**  \n",
    "(a) develop product rule by inspection in prior problem.  Then develop $A(t)^2$ then $A(t)^3 = A(t)^2\\cdot A(t)$ using linearity and product rule  \n",
    "(b) use $A(t)^{-1} \\cdot A(t) = I$ and differentiate each side (RHS is independent of t, so zero) and apply product rule.  This gives \n",
    "\n",
    "$A'(t)^{-1} \\cdot A(t) + A(t)^{-1} \\cdot A'(t) =0$  \n",
    "or  \n",
    "$A'(t)^{-1} \\cdot A(t) = - A(t)^{-1} \\cdot A'(t)\\longrightarrow A'(t)^{-1} = - A(t)^{-1} \\cdot A'(t)A(t)^{-1}$  \n",
    "\n",
    "all of the above should be familiar from single variable calculus, except we have the issue of matrix multiplication not commuting  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.Misc.4**  \n",
    "\n",
    "if $\\mathbf A$ and $\\mathbf B$ are n x n Complex matrices, consider the (commutator) matrix given by   \n",
    "$\\mathbf C:= \\mathbf {AB} - \\mathbf {BA}$  \n",
    "\n",
    "if $\\mathbf A$ commutes with $\\mathbf C$, then $\\mathbf C$ is nilpotent  \n",
    "\n",
    "- - - - -  \n",
    "*remark:*  \n",
    "the prior problem 4.Misc.3 is to show that a complex n x n matrix is nilpotent if and only if   \n",
    "$\\text{trace}\\big(\\mathbf Z^k\\big)= 0$ for $k \\in\\{1,2,3,...\\}$  \n",
    "this is proven mid way through the Vandermonde Matrices writeup under \"Full cycle trace relations and nilpotent matrices\" (and it is sufficient to prove for merely $k \\in\\{1,2,3,...,n\\}$-- i.e. that proves such a matrix has all eigenvalues equal to zero, which means such a matrix is similar to a strictly upper triangular matrix, which necessarily is nilpotent (Cayley Hamilton is but one of many ways of showing this).  The fact that this problem 4.Misc.4 comes immediately after 4.Misc.3 seems like a bit of a hint that we should use the this fact about traces and nilpotence, so that is exactly what we'll do.  Your author subsequently noticed this problem elsewhere on the internet, being referred to as Jacobson's Lemma.  The problem is extremely simple if viewed in the right light, but otherwise can be extremely difficult.    \n",
    "\n",
    "- - - - -   \n",
    "**proof:**  \n",
    "\n",
    "making use of linearity and cyclic property of the trace we have   \n",
    "$\\text{trace}\\Big(\\mathbf C\\Big) = \\text{trace}\\Big(\\mathbf {AB} - \\mathbf {BA}\\Big) = \\text{trace}\\Big(\\mathbf {AB}\\Big) - \\text{trace}\\Big(\\mathbf {BA}\\Big)= \\text{trace}\\Big(\\mathbf {BA}\\Big) - \\text{trace}\\Big(\\mathbf {BA}\\Big)=0$   \n",
    "\n",
    "and for natural numbers $k\\geq 2$ we have  \n",
    "\n",
    "$\\text{trace}\\Big(\\mathbf C^k\\Big)$   \n",
    "$= \\text{trace}\\Big( \\mathbf C^{k-1}\\mathbf C\\Big)$   \n",
    "$= \\text{trace}\\Big(\\mathbf C^{k-1}\\big(\\mathbf {AB} - \\mathbf {BA}\\big)\\Big) $   \n",
    "$= \\text{trace}\\Big(\\mathbf C^{k-1}\\mathbf {AB} - \\mathbf C^{k-1}\\mathbf {BA}\\Big) $   \n",
    "$= \\text{trace}\\Big(\\mathbf C^{k-1}\\mathbf {AB}\\Big) -  \\text{trace}\\Big(\\mathbf C^{k-1}\\mathbf {BA}\\Big) $   \n",
    "$= \\text{trace}\\Big(\\mathbf A\\mathbf C^{k-1}\\mathbf B\\Big) -  \\text{trace}\\Big(\\mathbf C^{k-1}\\mathbf {BA}\\Big) $   \n",
    "$= \\text{trace}\\Big(\\mathbf C^{k-1}\\mathbf B\\mathbf A\\Big) -  \\text{trace}\\Big(\\mathbf C^{k-1}\\mathbf {BA}\\Big) $   \n",
    "$=0$  \n",
    "\n",
    "which proves $\\mathbf C$ is nilpotent if $\\mathbf A$ and $\\mathbf C$ commute (in a field of characteristic zero)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.Misc.10**  \n",
    "with $\\mathbf A \\in \\mathbb F^\\text{m x n}$ and $\\text{rank}\\big(\\mathbf A\\big) = r$, prove  \n",
    "\n",
    "i) there must be some non-zero $\\text{r x r}$ minor (i.e. determinant of a submatrix of $\\mathbf A$)  \n",
    "ii) for $j\\geq 1$, all minors associated with $\\text{(r + j) x (r + j)}$ submatrices  must be zero  \n",
    "\n",
    "*remark: this problem gives us yet one additional way to think about rank, this times as a polynomial in the entries of our matrix, which can be particularly nice because polynomials are highly studied objects that we can evaluate to insights on rank -- e.g. determinants of submatrices are flexible to evaluate if we wanted to change the scalar field, polynomials are continuous (at least in the standard sense in fields of characteristic zero),, etc.*    \n",
    "\n",
    "i.)  \n",
    "since $\\text{rank}\\big(\\mathbf A\\big) = r$, we know that we may select $r$ linearly independent columns in $\\mathbf A$.  (The selection of columns need not be unique-- it is sufficient to make one selection.)  So with suitable permutation matrix $\\mathbf P$, we see that $\\big(\\mathbf A\\mathbf P\\big)$ has the first $r$ columns linearly independent, and any column $j$, for $j\\gt r$ may be written as a linear combination of the first $r$ columns (i.e. is linearly dependent).  So if we (optionally delete columns $j\\gt r$ and) focus on the first $r$ columns of $\\big(\\mathbf A\\mathbf P\\big)$ we see the row rank and column ranks $= r$, which implies that after selecting suitable permutation matrix $\\mathbf P'$, we have  $\\big(\\mathbf P'\\mathbf A\\mathbf P\\big)$ is a matrix with the top $r$ x $r$ principal submatrix which is full rank.  Since this square principal submatrix is full rank , it is invertible, and has a non-zero determinant (per chapter 1 results). But if we 'undo' the effects of $\\mathbf P'$    and $\\mathbf P$ we see this is saying, up to permutation, there is some submatrix in $\\mathbf A$ that has non-zero determinant.  \n",
    "\n",
    "\n",
    "ii.)  \n",
    "Now if we suppose for a contradiction that we find an $\\text{(r + j) x (r + j)}$ submatrix (with $j\\geq 1$) that has non-zero determinant, then after suitable permutation we have a matrix $\\big(\\mathbf P^{(r)}\\mathbf A \\mathbf P^{(c)}\\big)$  which has a non-zero determinant in its (r+j) x (r+j) principal submatrix, which implies the first $r+j$ columns are linearly independent.  Thus \n",
    "\n",
    "$r+j \\leq \\text{rank}\\Big(\\mathbf P^{(r)}\\mathbf A \\mathbf P^{(c)}\\Big) = \\text{rank}\\Big(\\mathbf A\\big)= r $  \n",
    "\n",
    "so $r+j \\leq r$, but $r+ j \\gt r$, which gives the contradiction  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.Misc.11**  \n",
    "Let $\\phi: \\mathbb F^n \\longrightarrow \\mathbb F^m$ be left multiplication by an $\\text{m x n}$ matrix $\\mathbf A$.  Prove the following to be equivalent  \n",
    "\n",
    "(a) $\\mathbf A$ has a right inverse, a matrix $\\mathbf B$ such that $\\mathbf A \\mathbf B = \\mathbf I_m$  \n",
    "(b) $\\phi$ is surjective  \n",
    "(c) There is an $\\text{m x m}$ minor of $\\mathbf A$ that is non-zero  \n",
    "\n",
    "The proof is structured as \n",
    "$\\text{b}\\longrightarrow \\text{c} \\longrightarrow\\text{a} \\longrightarrow\\text{b}$  \n",
    "\n",
    "*remark:*  $\\mathbf A$ is short and fat in this case (i.e. $ m \\leq n$)\n",
    "\n",
    "$(b) \\longrightarrow (c)$  \n",
    "staring with (b), for a surjective map, this implies the columns of $\\mathbf A$ span the codomain $\\mathbb F^m$, so we have $m$ linearly independent columns in $\\mathbf A$.  Thus $\\text{rank}\\big(\\mathbf A\\big) = m$.  Using the results of the prior problem $4.\\text{Misc}.10$ we see $(b)\\longrightarrow (c)$.  Note: since our matrix is $\\text{m x n}$, it is impossble for there to be a 'bigger' minor -- an m x m one is as big as it can be     \n",
    "\n",
    "$(c) \\longrightarrow (a)$  \n",
    "with (c) in hand (i.e. with the equivalence of rank and this maximally sized non-zero minor), we can use our main decomposition  \n",
    "$\\mathbf A= \\mathbf P\\bigg[\\begin{array}{c|c}\\mathbf I_m & \\mathbf {00}^T\\end{array}\\bigg]\\mathbf Q^{-1}$  \n",
    "\n",
    "and we can construct   \n",
    "$\\mathbf B:= \\mathbf Q\\begin{bmatrix}\\mathbf I_m \\\\ \\mathbf {00}^T \\end{bmatrix}\\mathbf P^{-1}$ \n",
    "\n",
    "then  \n",
    "$\\mathbf {AB} = \\mathbf P\\bigg[\\begin{array}{c|c}\\mathbf I_m & \\mathbf {00}^T\\end{array}\\bigg]\\mathbf Q^{-1}\\mathbf Q\\begin{bmatrix}\\mathbf I_m \\\\ \\mathbf {00}^T \\end{bmatrix}\\mathbf P^{-1} = \\mathbf P \\mathbf P\\bigg[\\begin{array}{c|c}\\mathbf I_m & \\mathbf {00}^T\\end{array}\\bigg]\\begin{bmatrix}\\mathbf I_m \\\\ \\mathbf {00}^T \\end{bmatrix}\\mathbf P^{-1} = \\mathbf P \\mathbf I_m \\mathbf P^{-1} = \\mathbf P \\mathbf P^{-1} = \\mathbf I_m  $  \n",
    "\n",
    "where  \n",
    "\n",
    "$\\bigg[\\begin{array}{c|c}\\mathbf I_m & \\mathbf {00}^T\\end{array}\\bigg]\\begin{bmatrix}\\mathbf I_m \\\\ \\mathbf {00}^T \\end{bmatrix} = \\big(\\sum_{k=1}^m\\mathbf e_k\\mathbf e_k^T\\big) + \\big(\\sum_{k=m+1}^n\\mathbf 0\\mathbf 0^T\\big)=\\mathbf I_m $  \n",
    "\n",
    "using the 'rank one update' interpretation of matrix multiplication (with standard basis vectors in $\\mathbb F^m$)  \n",
    "\n",
    "Thus a right inverse for $\\mathbf A$ exists  \n",
    "\n",
    "$(a) \\longrightarrow (b)$  \n",
    "so  \n",
    "\n",
    "$m = \\text{rank}\\big(\\mathbf I_m\\big) = \\text{rank}\\big(\\mathbf A\\mathbf B\\big) \\leq \\text{rank}\\big(\\mathbf A \\big)\\leq m$   \n",
    "(since rank of $\\mathbf A$ equals the rank of $\\mathbf A^T$, and $\\mathbf A^T$ only has $m$ columns, so its rank is at most $m$)  \n",
    "\n",
    "Thus $\\text{rank}\\big(\\mathbf A\\big) = m$   \n",
    "\n",
    "revisiting the 2 cases in the \"Guiding Inequality\" we see that this is the first case, where there is equality in rank, i.e.  \n",
    "\n",
    "$m=\\text{rank}\\big(\\mathbf I_m\\big) = \\text{rank}\\big(\\mathbf I_m\\mathbf A\\big) = \\text{rank}\\big(\\mathbf A\\big)$  \n",
    "or equivalently  \n",
    "$\\text{span}\\big(\\mathbf I_m\\mathbf {A}\\big) \\subset \\text{span}\\big(\\mathbf I_m\\big) $  \n",
    "$\\text{span}\\big(\\mathbf I_m\\big)\\subset \\text{span}\\big(\\mathbf I_m\\mathbf {A}\\big) $  \n",
    "\n",
    "but since $\\mathbf I_m \\mathbf A = \\mathbf A$ this inclusions can be written as  \n",
    "\n",
    "$\\text{span}\\big(\\mathbf {A}\\big) \\subset \\text{span}\\big(\\mathbf I_m\\big) $  \n",
    "$\\text{span}\\big(\\mathbf I_m\\big)\\subset \\text{span}\\big(\\mathbf {A}\\big) $  \n",
    "\n",
    "but this implies the mapping given by $\\phi$ is surjective, i.e. the codomain $\\mathbb F^m$ is generated by the standard basis vectors / columns of $\\mathbf I_m$ but these are a subset of the span of $\\mathbf A$, so the mapping $\\phi$ is surjective   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.Misc.12**  \n",
    "*This is problem is in some sense dual to the prior problem and the results are fairly immediate by mimicking the above argument*  \n",
    "\n",
    "Let $\\phi: \\mathbb F^n \\longrightarrow \\mathbb F^m$ be left multiplication by an $\\text{m x n}$ matrix $\\mathbf A$.  Prove the following to be equivalent  \n",
    "\n",
    "(a) $\\mathbf A$ has a left inverse, a matrix $\\mathbf B$ such that $\\mathbf B\\mathbf A  = \\mathbf I_n$   \n",
    "(b) $\\phi$ is injective    \n",
    "(c) There is an $\\text{n x n}$ minor of $\\mathbf A$ that is non-zero   \n",
    "\n",
    "The proof is structured as \n",
    "$\\text{b}\\longrightarrow \\text{c} \\longrightarrow\\text{a} \\longrightarrow\\text{b}$   \n",
    "\n",
    "*remark:*  $\\mathbf A$ is tall and skinny (i.e. $ m \\geq n$)   \n",
    "\n",
    "$(b) \\longrightarrow (c)$  \n",
    "$\\phi$ is injective implies that $\\mathbf A$ has maximal column rank, so $\\text{rank}\\big(\\mathbf A\\big) =n$  \n",
    "-- if this wasn't true, then there would be some $\\mathbf v \\neq \\mathbf 0$ where $\\mathbf {Av} = \\mathbf 0$ but then the mapping wouldn't be injective since $\\mathbf A\\mathbf 0 = \\mathbf 0$  \n",
    "\n",
    "using 4.Misc.10 we then get (c) \n",
    "\n",
    "for avoidance of doubt, injectivity with finite dimensional linear maps is equivalent to saying the matrix associated with said map (using some selected basis) has full column rank, i.e. $\\mathbf A\\mathbf x =\\mathbf 0 \\longrightarrow \\mathbf x = \\mathbf 0$, because if $\\mathbf A$ is injective *and* $\\mathbf x \\neq \\mathbf y$, then  \n",
    "$\\mathbf 0 \\neq \\mathbf A \\mathbf x- \\mathbf A \\mathbf y = \\mathbf A\\big(\\mathbf x - \\mathbf y\\big)$  \n",
    "where the right hand side follows by linearity  \n",
    "\n",
    "$(c) \\longrightarrow (a)$  \n",
    "so (c) implies that $\\text{rank}\\big(\\mathbf A\\big) = n$, and using our main decomposition we have  \n",
    "$\\mathbf A=\\mathbf P\\begin{bmatrix}\\mathbf I_n \\\\ \\mathbf {00}^T \\end{bmatrix}\\mathbf Q^{-1}$  \n",
    "\n",
    "$\\mathbf B:= \\mathbf Q\\bigg[\\begin{array}{c|c}\\mathbf I_n & \\mathbf {00}^T\\end{array}\\bigg]\\mathbf P^{-1}$   \n",
    "\n",
    "$\\mathbf {BA} = \\mathbf Q\\bigg[\\begin{array}{c|c}\\mathbf I_n & \\mathbf {00}^T\\end{array}\\bigg]\\mathbf P^{-1}\\mathbf P\\begin{bmatrix}\\mathbf I_n \\\\ \\mathbf {00}^T \\end{bmatrix}\\mathbf Q^{-1} = \\mathbf Q\\begin{bmatrix}\\mathbf I_n \\\\ \\mathbf {00}^T \\end{bmatrix}\\bigg[\\begin{array}{c|c}\\mathbf I_n & \\mathbf {00}^T\\end{array}\\bigg]\\mathbf Q^{-1} = \\mathbf Q \\mathbf I_n \\mathbf Q^{-1} = \\mathbf I_n$  \n",
    "\n",
    "$(a) \\longrightarrow (b)$  \n",
    "$n = \\text{rank}\\Big(\\mathbf I_n\\Big) = \\text{rank}\\Big(\\mathbf {BA}\\Big) =  \\text{rank}\\Big(\\big(\\mathbf {BA}\\big)^T\\Big) = \\text{rank}\\Big(\\mathbf {A}^T\\mathbf B^T\\Big)\\leq \\text{rank}\\Big(\\mathbf A^T\\Big) = \\text{rank}\\Big(\\mathbf A\\Big)\\leq n$   \n",
    "using the equivalence of row and column rank, our guiding inequality, and finally the fact that $\\mathbf A$ has $n$ columns  \n",
    "\n",
    "thus  $\\text{rank}\\big(\\mathbf A\\big) = n$  \n",
    "which means all $n$ of its columns are linearly independent, which means the mapping $\\phi$ given by multiplying by $\\mathbf A$ on the left is injective.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.Misc.13**  \n",
    "if $\\mathbf A^r = \\mathbf I_n$, prove that if $\\mathbf A$ has only one eigenvalue $\\lambda_i$ then $\\mathbf A = \\lambda_i\\mathbf I_n$  \n",
    "\n",
    "- - - - \n",
    "*remark:* the field is not explicitly stated in this problem, but since we are dealing with roots of unity it seems natural to assume we are working in $\\mathbb C$.  Note in finite fields the problems statement, as is, may not hold.  E.g. consider involutary $\\mathbf A$, where $r=2$, and working in $\\mathbb F_2$, then the annihilating polynomial is   \n",
    "$0=\\big(\\mathbf A- (-1)\\mathbf I_n\\big) \\big(\\mathbf A- (+1)\\mathbf I_n\\big)= \\big(\\mathbf A- 1\\mathbf I_n\\big)^2$  \n",
    "because $1 = -1$ in $\\mathbb F_2$, thus $\\mathbf A$ may not be diagonalizable in $\\mathbb F_2$.   \n",
    "\n",
    "To be explicit, consider in  $ \\mathbb F_2$  \n",
    "$\\mathbf A = \\left[\\begin{matrix}1 & 1\\\\0 & 1\\end{matrix}\\right]$, then  \n",
    "$\\mathbf A^2 = \\left[\\begin{matrix}1 & 0\\\\0 & 1\\end{matrix}\\right]$  \n",
    "where the only eigenvalue $\\mathbf A$ has is 1 (i.e. the diagonal components)  \n",
    "but $\\mathbf A \\neq \\mathbf I$  \n",
    "\n",
    "- - - -  \n",
    "so supposing we are working in $\\mathbb C$, we have  \n",
    "$\\mathbf A^r = \\mathbf I_n$  \n",
    "\n",
    "where $\\lambda_k$ are the distinct $r$th roots of unity.  We can rewrite the equation as  \n",
    "$\\mathbf 0 = \\mathbf A^r - \\mathbf I_n = \\prod_{k=1}^r\\big(\\mathbf A - \\lambda_k\\mathbf I\\big) = \\big(\\mathbf A - \\lambda_i\\mathbf I\\big)\\Big(\\prod_{k\\neq i}\\big(\\mathbf A - \\lambda_k\\mathbf I\\big)\\Big) $  \n",
    "\n",
    "since $\\lambda_i$ is the only eigenvalue, then  \n",
    "$\\Big(\\prod_{j\\neq i}\\big(\\mathbf A - \\lambda_k\\mathbf I\\big)\\Big)$  \n",
    "is an invertible matrix.  Multiplying on the right by its inverse gives  \n",
    "\n",
    "$\\mathbf 0 = \\big(\\mathbf A - \\lambda_i\\mathbf I\\big)\\longrightarrow \\mathbf A = \\lambda_i \\mathbf I$   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.Misc.17**   \n",
    "\n",
    "**(a)** for $\\mathbf A \\in \\mathbb C^\\text{2 x 2}$ if the eigenvalues of $\\mathbf A$ are $a$ and $b$ but $a \\neq b$, prove  \n",
    "$e^A = \\frac{a e^b - b e^a}{a-b}\\mathbf I + \\frac{e^a - e^b}{a-b}\\mathbf A$  \n",
    "or equivalently, if we orient ourselves to the eigenvalues:   \n",
    "$e^A $  \n",
    "$= \\big(\\frac{a e^b}{a-b}\\mathbf I + \\frac{-e^b}{a-b}\\mathbf A\\big) + \\big(\\frac{-b e^a }{a-b}\\mathbf I + \\frac{e^a}{a-b}\\mathbf A\\big) $  \n",
    "$= \\frac{1}{a-b}\\big(a \\mathbf I -1\\mathbf A\\big)e^b + \\frac{1}{a-b}\\big(-b \\mathbf I + 1\\mathbf A\\big)e^a$  \n",
    "$=  \\frac{1}{a-b}\\big(-b \\mathbf I + 1\\mathbf A\\big)e^a+\\frac{1}{a-b}\\big(a \\mathbf I -1\\mathbf A\\big)e^b $  \n",
    "\n",
    "\n",
    "note: $\\mathbf A^k$ for natural number k (understanding that $\\mathbf A^0:= \\mathbf I$) forms a vector space -- we may take linear combinations of these vectors (compare with page 80 of Artin, except use $\\mathbb C$)  \n",
    "\n",
    "Cayley Hamilton tells us that \n",
    "\n",
    "$\\mathbf A^2 -\\text{trace}\\big(\\mathbf A\\big)\\mathbf A + \\det\\big(\\mathbf A\\big) \\mathbf I= \\mathbf 0$  \n",
    "or  \n",
    "$\\mathbf A^2 = \\text{trace}\\big(\\mathbf A\\big)\\mathbf A - \\det\\big(\\mathbf A\\big) \\mathbf I = \\big(a + b\\big)\\mathbf A - \\big(ab\\big) \\mathbf I$  \n",
    "\n",
    "now using notation from page 96, we let \n",
    "\n",
    "$v_0 := \\mathbf I$   \n",
    "$v_1 := \\mathbf A$  \n",
    "\n",
    "*thus we want to prove*    \n",
    "$e^{v_1} = \\frac{1}{a-b}\\big(-b v_0 + v_1\\big)e^a+\\frac{1}{a-b}\\big(a v_0 - v_1\\big)e^b$  \n",
    "\n",
    "our recurrence from Cayley Hamilton reads as  \n",
    "$v_2=\\big(a + b\\big)v_1 - \\big(ab\\big) v_0$  \n",
    "taking advantage of matrix multiplication we have  \n",
    "$\\mathbf A^3 = \\big(a + b\\big)\\mathbf A^2 - \\big(ab\\big) \\mathbf A$  \n",
    "or  \n",
    "$v_3=\\big(a + b\\big)v_2 - \\big(ab\\big) v_1$  \n",
    "\n",
    "and in general  \n",
    "$v_r=\\big(a + b\\big)v_{r-1} - \\big(ab\\big) v_{r-2}$  \n",
    "for natural number $r\\geq 2$  \n",
    "\n",
    "(we don't 'know' this yet, but for the case of distinct eigenvalues, Cayley Hamilton gives the minimal polynomial and tells us $v_0$ and $v_1$ generate/ form a basis for this vector space.)  \n",
    "\n",
    "\n",
    "\n",
    "Making use of the Companion Matrix, our recurrence is given as  \n",
    "\n",
    "$\\mathbf C := \\begin{bmatrix}\n",
    "0 & -c_o\\\\ \n",
    "1 & -c_{n-1} \n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "0 & -ab\\\\ \n",
    "1 & a+b\n",
    "\\end{bmatrix}$\n",
    "\n",
    "so  \n",
    "\n",
    "$\\bigg[\\begin{array}{c|c}\n",
    "v_0 & v_1\n",
    "\\end{array}\\bigg]\\mathbf C = \\bigg[\\begin{array}{c|c}\n",
    "v_1 & v_2\n",
    "\\end{array}\\bigg]$   \n",
    "and  \n",
    "\n",
    "$\\bigg[\\begin{array}{c|c}\n",
    "v_0 & v_1\n",
    "\\end{array}\\bigg]\\mathbf C^k = \\bigg[\\begin{array}{c|c}\n",
    "v_k & v_{k+1}\n",
    "\\end{array}\\bigg]$   \n",
    "\n",
    "and using the first standard basis vector we have  \n",
    "$\\bigg[\\begin{array}{c|c}\n",
    "v_0 & v_1\n",
    "\\end{array}\\bigg]\\mathbf C^k\\mathbf e_1 = \\bigg[\\begin{array}{c|c}\n",
    "v_k & v_{k+1}\n",
    "\\end{array}\\bigg]\\mathbf e_1 = v_r$\n",
    "\n",
    "the matrix exponential then becomes  \n",
    "\n",
    "$e^{\\mathbf A} $  \n",
    "$= e^{v_1} $  \n",
    "$= \\sum_{k=0}^\\infty \\frac{1}{k!}v_k^k $  \n",
    "$= \\sum_{k=0}^\\infty \\frac{1}{k!}\\Big(\\bigg[\\begin{array}{c|c}\n",
    "v_0 & v_1\n",
    "\\end{array}\\bigg]\\mathbf C^k\\mathbf e_1\\Big)$  \n",
    "$= \\sum_{k=0}^\\infty \\frac{1}{k!}\\Big(a^k z_1 + b^k z_2\\Big)$  \n",
    "$= \\Big(z_1\\sum_{k=0}^\\infty \\frac{1}{k!}a^k \\Big) + \\Big(z_2\\sum_{k=0}^\\infty \\frac{1}{k!}b^k \\Big)$  \n",
    "$= z_1\\cdot e^a  + z_2\\cdot e^b$  \n",
    "\n",
    "where the third to last line made use of the distinctness of the eigenvalues-- see Vandermonde matrix writeup for reference, subsection \"Detour into Solving Linear Recurrences with boundary conditions\".  Again making use of that same section in the Vandermonde Matrix writeup and the distinctness of eigenvalues, we want to check that we have the correct $z_1$ and $z_2$  \n",
    "\n",
    "and it suffices to check  \n",
    "$\\bigg[\\begin{array}{c|c}\n",
    "v_0 & v_1\n",
    "\\end{array}\\bigg]\\mathbf C^k\\mathbf e_1 = a^k z_1 + b^k z_2 = \\frac{1}{a-b}\\big(-b v_0 + v_1\\big)a^k +\\frac{1}{a-b}\\big(a v_0 - v_1\\big)b^k$  \n",
    "for $k \\in \\{0,1\\}$  (where, again it is understood that $a^0 = 1$ and $b^0 = 1$ by convention here).  \n",
    "\n",
    "when k=0 we have  \n",
    "$\\frac{1}{a-b}\\big(-b v_0 + v_1\\big)1 + \\frac{1}{a-b}\\big(a v_0 - v_1\\big)1= \\frac{a-b}{a-b}v_0 + \\frac{1-1}{a-b}v_1 = v_0$  \n",
    "\n",
    "when k=1 we have  \n",
    "$\\frac{1}{a-b}\\big(-b v_0 + v_1\\big)a + \\frac{1}{a-b}\\big(a v_0 - v_1\\big)b= \\frac{-ab+ab}{a-b}v_0 + \\frac{a-b}{a-b}v_1 = v_1$  \n",
    "\n",
    "each as required.  This confirms that  \n",
    "$z_1= \\frac{1}{a-b}\\big(-b v_0 + v_1\\big)$ and  \n",
    "$z_2 = \\frac{1}{a-b}\\big(a v_0 - v_1\\big)$  \n",
    "(again reference that section of the Vandermonde Matrix writeup)  \n",
    "\n",
    "and hence  \n",
    "$e^{\\mathbf A} = e^{v_1} = z_1\\cdot e^a  + z_2\\cdot e^b = \\frac{1}{a-b}\\big(-b v_0 + v_1\\big)e^a + \\frac{1}{a-b}\\big(a v_0 - v_1\\big)e^b$  \n",
    "\n",
    "as was stated in the problem.   \n",
    "\n",
    "**(b)**  find the correct formula for the case that $\\mathbf A$ does not have distinct eigenvalues.  \n",
    "*remark*  \n",
    "since the Companion Matrix is always defective when there is a repeated eigenvalue we can infer that there must be a structurally different format to the solution.  We can also observe that $z_1$ and $z_2$ in the prior problem both involve $\\frac{1}{a-b}$ which isn't well defined when $a=b$  \n",
    "\n",
    "*solution*  \n",
    "revisiting our prior solution, we still have  \n",
    "$e^{\\mathbf A} $  \n",
    "$= e^{v_1} $  \n",
    "$= \\sum_{k=0}^\\infty \\frac{1}{k!}v_k^k $  \n",
    "$= \\sum_{k=0}^\\infty \\frac{1}{k!}\\Big(\\bigg[\\begin{array}{c|c}\n",
    "v_0 & v_1\n",
    "\\end{array}\\bigg]\\mathbf C^k\\mathbf e_1\\Big)$  \n",
    "$= \\bigg[\\begin{array}{c|c}\n",
    "v_0 & v_1\n",
    "\\end{array}\\bigg]\\Big(\\sum_{k=0}^\\infty \\frac{1}{k!}\\mathbf C^k\\Big)\\mathbf e_1$  \n",
    "$= \\bigg[\\begin{array}{c|c}\n",
    "v_0 & v_1\n",
    "\\end{array}\\bigg]\\exp\\big(\\mathbf C\\big)\\mathbf e_1$  \n",
    "\n",
    "which converges absolutely  \n",
    "\n",
    "thus  \n",
    "$e^{v_1} = \\exp\\big(v_1\\big) =\\bigg[\\begin{array}{c|c}\n",
    "v_0 & v_1\n",
    "\\end{array}\\bigg]\\exp\\big(\\mathbf C\\big)\\mathbf e_1$  \n",
    "still exists even when the eigenvalues are repeated.  \n",
    "\n",
    "Further, since the exponential function is continuous, we may get an arbitrarily close approximation to our answer by considering a sequence of diagonalizable Companion matrices that converges to our defective $\\mathbf C$.  In particular,   \n",
    "\n",
    "making use of triangularization we have  \n",
    "$\\mathbf C^{(n)} := \\mathbf S\\Big(\\mathbf R + \\begin{bmatrix}\n",
    "0 & 0\\\\ \n",
    "0 & \\frac{1}{n}\n",
    "\\end{bmatrix}\\Big)\\mathbf S^{-1}$  \n",
    "(in this setup $\\mathbf C^{(n)}$ has eigenvalues of $a$ and $b = a +\\frac{1}{n}$)  \n",
    "\n",
    "so $\\mathbf C^{(n)}$ is diagonalizable for all positive integers $n$ but $\\lim_{n \\to \\infty} \\mathbf C^{(n)} = \\mathbf C$  \n",
    "thus with continuity of the exponential function we have   \n",
    "\n",
    "$\\exp\\big(v_1\\big) $  \n",
    "$= \\bigg[\\begin{array}{c|c}\n",
    "v_0 & v_1\n",
    "\\end{array}\\bigg]\\exp\\big(\\mathbf C\\big)\\mathbf e_1$  \n",
    "$=\\lim_{n\\to \\infty}\\Big\\{\\bigg[\\begin{array}{c|c}\n",
    "v_0 & v_1\n",
    "\\end{array}\\bigg]\\exp\\big(\\mathbf C^{(n)}\\big)\\mathbf e_1\\Big\\}$  \n",
    "$=\\lim_{b \\to a}\\Big\\{\\frac{1}{a-b}\\big(-b v_0 + v_1\\big)e^a + \\frac{1}{a-b}\\big(a v_0 - v_1\\big)e^b\\Big\\}$  \n",
    "$=\\big(v_0 - av_0 +v_1)e^a  $\n",
    "\n",
    "\n",
    "https://www.wolframalpha.com/input/?i=limit+b+to+a+of++%28%5Cfrac%7B1%7D%7Ba-b%7D%28-b+v_0+%2B+v_1%29e%5Ea+%2B+%5Cfrac%7B1%7D%7Ba-b%7D%28a+v_0+-+v_1%29e%5Eb%29\n",
    "\n",
    "or  \n",
    "$e^\\mathbf{A} = \\big((1-a)\\mathbf I + \\mathbf A\\big)e^a$  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**4.Misc.19**  \n",
    "Perron Theory\n",
    "where $\\mathbf x \\geq  \\mathbf 0$  and $\\mathbf A \\gt \\mathbf 0$  \n",
    "(the inequalities are done component-wise, so the vector is real non-negative and the matrix is strictly positive)  \n",
    "\n",
    "(a) Prove that if $\\mathbf x \\geq \\mathbf 0$ and $\\mathbf x \\neq \\mathbf 0$ then $\\mathbf {Ax} \\gt \\mathbf 0$  \n",
    "*solution:*  $ \\mathbf b = \\mathbf A \\mathbf x = \\sum_{j=1}^n x_j \\mathbf a_j \\gt 0$  \n",
    "because at least one $x_j \\gt 0$ and $\\mathbf a_j\\gt 0$, hence we have a non-trivial linear combination of strictly positive vectors with real non-nonnegative coefficients and the result is positive  \n",
    "\n",
    "(b)  Let $C$ denote the set of pairs $(\\mathbf x, t)$  with $t\\in \\mathbb R $ such that $\\mathbf x \\geq 0$ and $\\big \\Vert \\mathbf x \\big \\Vert_2 = 1$ and $\\Big(\\mathbf A - t \\mathbf I\\big)\\mathbf x \\geq \\mathbf 0$. Prove $C$ is a compact set in $\\mathbb R^\\text{n + 1}$  \n",
    "*solution:* using the equivalence of compactness and closed and bounded in $\\mathbb R^d$ we prove that $C$ is closed and bounded.  \n",
    "\n",
    "Note: $\\mathbf A$ is fixed -- i.e. we prove that $C$ is closed and bounded for any feasible choice of $\\mathbf A$.  We already have that $\\mathbf x$ is bounded, with norm = 1.  \n",
    "To see that $t$ is bounded above by a positive constant, consider  \n",
    "$t \\leq \\gamma$  \n",
    "\n",
    "where $\\gamma $ is a some positive constant, perhaps calculated by Schur Test or Gerschgorin discs (/underlying diagonal dominance argument)   \n",
    "\n",
    "*remark*  \n",
    "The problem statement as is does not work for the lower bound. We have an issue in that sufficiently large magnitude negative t always satisfies the inequality here and in fact t is unbounded on the negative side.  If we look ahead to part (c) we see we are interested only in the maximizing value of of $t$ --- so we can remedy the situation by truncation. First observe that $t=0$ always implies $\\Big(\\mathbf A - t \\mathbf I\\big)\\mathbf x = \\mathbf A\\mathbf x \\geq \\mathbf 0$, i.e. is always in $C$ for any valid $\\mathbf x$ .  Since we are interested in the sup (and really the max) of $t$ we can truncate and insist that $0\\leq t$.  Combining this with the above upper bound gives $0 \\leq t \\leq \\gamma$ and hence $t$ is bounded.   \n",
    "\n",
    "since $\\big \\Vert \\mathbf x \\big \\Vert_2 = 1$, then components involving $\\mathbf x$ are bounded  \n",
    "\n",
    "*re: closed*  \n",
    "we may view the complement of $C$ and see that it is open -- for $\\mathbf x$ not on the unit disc this is immediate, and similarly if $\\big(\\mathbf A - t\\mathbf I\\big) \\mathbf x \\lt \\mathbf 0$, then we we may perturb $t$ by a sufficiently small amount and still have this inequality (we can also use linearity and component-wise solve for the perturbation of t that makes it an equality to zero -- there are finitely many components so take the minimum and then set $\\delta $ equal to half this value).  \n",
    "\n",
    "(c) and (d)  \n",
    "by compactness, the function $t$ takes a maximum value on $C$ say a the point $(\\mathbf x_0, t_0)$.  Then \n",
    "$\\big(\\mathbf A - t_0\\mathbf I\\big) \\mathbf x_0 \\geq \\mathbf 0$.  Prove that it is an equality i.e. $\\big(\\mathbf A - t_0\\mathbf I\\big) \\mathbf x_0 = \\mathbf 0$   \n",
    "\n",
    "i.e. prove that $\\mathbf x_0$ is an eigenvector by showing that otherwise the vector $\\mathbf A\\mathbf x_0 = \\mathbf x_1$ would contradict the maximality of $\\mathbf t_0$.  \n",
    "\n",
    "*lemma 1:*  \n",
    "if $\\big(\\mathbf A - t_0\\mathbf I\\big) \\mathbf x_0 = \\mathbf x_1 \\gt 0$ then $t_0$ is not a maximum    \n",
    "(where read this as each component of $\\mathbf x_1$ is positive)  \n",
    "\n",
    "proof:  \n",
    "consider the minimum coordinate on $\\mathbf x_1$ equal to $d \\gt 0$.  Now consider the maximum coordinate on $\\mathbf x_0 = m$ (which is also positive).  \n",
    "Setting $t_0^* := t_0 + \\delta$ where $\\delta = \\frac{d}{m}\\gt 0$,   \n",
    "$\\big(\\mathbf A - t_0^*\\mathbf I\\big) \\mathbf x_0 \\geq \\mathbf 0$, so $t_0$ was not a maximum  \n",
    "\n",
    "note: this lemma in combination with part (a) tells us that $t_0 \\neq 0$ i.e. $t_0 \\gt 0$  \n",
    "\n",
    "*main claim:*  \n",
    "\n",
    "if  \n",
    "$\\big(\\mathbf A - t_0\\mathbf I\\big) \\mathbf x_0 =\\mathbf x_1 \\geq \\mathbf 0$  \n",
    "and $\\mathbf x_1 \\neq \\mathbf 0$, then $t_0$ is not a maximum  \n",
    "\n",
    "proof:  \n",
    "supposing $\\mathbf x_1 \\neq \\mathbf 0$ then we may assume WLOG that the first coordinate given by $x_1^{(1)} = \\gamma \\gt 0$, and attack this. (We could alternatively just focus on coordinate k that is $\\gt 0$, but assuming WLOG that k=1 seems more direct)   \n",
    "\n",
    "In particular consider  \n",
    "$\\mathbf x_0' := \\mathbf x_0 + \\frac{\\gamma}{t_0} \\mathbf e_1 $  \n",
    "(for avoidance of doubt, $\\mathbf x_0'$ has a strictly positive first component and hence we know it isn't the zero vector)  \n",
    "\n",
    "then recalling that $\\mathbf A \\gt 0$ \n",
    "\n",
    "\n",
    "$\\big(\\mathbf A - t_0\\mathbf I\\big) \\mathbf x_0'$  \n",
    "$\\big(\\mathbf A - t_0\\mathbf I\\big) \\big(\\mathbf x_0 + \\frac{\\gamma}{t_0} \\mathbf e_1\\big)$  \n",
    "$=  \\big(\\mathbf A - t_0\\mathbf I\\big) \\mathbf x_0 + \\frac{\\gamma}{t_0}\\big(\\mathbf A - t_0\\mathbf I\\big)   \\mathbf e_1$  \n",
    "$  \\mathbf x_1 +  \\frac{\\gamma}{t_0} \\big(\\mathbf A - t_0\\mathbf I\\big)\\mathbf e_1$  \n",
    "$\\geq  \\gamma \\mathbf e_1 +  \\frac{\\gamma}{t_0} \\big(\\mathbf A - t_0\\mathbf I\\big)\\mathbf e_1$  \n",
    "$=  \\gamma \\mathbf e_1 +  \\frac{\\gamma}{ t_0}\\big(\\mathbf a_1  - t_0 \\mathbf e_1\\big) $  \n",
    "$=  \\frac{\\gamma}{t_0}\\mathbf a_1 + \\big(\\gamma \\mathbf e_1 +   - \\gamma \\mathbf e_1\\big) $  \n",
    "$=  \\frac{\\gamma}{t_0}\\mathbf a_1 $  \n",
    "$\\gt 0$  \n",
    "(where, as a reminder, all inequalities are interpreted component-wise)  \n",
    "since the first column of $\\mathbf A$ (and indeed every column of $\\mathbf A$) is strictly positive  \n",
    "\n",
    "*book-keeping note:*  \n",
    "at this stage we should technically rescale each side by $\\frac{1}{\\big \\Vert \\mathbf x_0' \\big \\Vert_2}$  \n",
    "so that our candidate solution is given by $\\mathbf x_0^*:= \\frac{1}{\\big \\Vert \\mathbf x_0' \\big \\Vert_2}\\mathbf x_0'$   \n",
    "and we have $\\big\\Vert \\mathbf x_0^* \\big \\Vert_2 = 1$ as required \n",
    "\n",
    "now apply lemma 1 and we have contradicted the maximality of $t_0$.  \n",
    "\n",
    "*conclusion*  \n",
    "Now, since we know a maximal value of $t_0$ exists, we know that it must be the case that    \n",
    "$\\big(\\mathbf A - t_0\\mathbf I\\big) \\mathbf x_0 = \\mathbf 0$  \n",
    "$\\mathbf x_0$ is a real non-negative vector that isn't the zero vector, and $t_0 \\gt 0$.  \n",
    "This implies $\\mathbf x_0$ is eigenvector and $t_0$ is an eigenvalue.  \n",
    "\n",
    "*corollary:*  \n",
    "$\\mathbf x_0 \\gt 0$  \n",
    "because $\\mathbf x_0 \\geq 0$ and $t_0 \\mathbf x_0 = \\mathbf A \\mathbf x_0 $  \n",
    "we use (a) and see that  \n",
    "$t_0 \\mathbf x_0 = \\mathbf A \\mathbf x_0 \\gt \\mathbf 0$.  Dividing out our positive $t_0$ gives the claim.   \n",
    "\n",
    "(e)  \n",
    "prove that $t_0$ is the eigenvalue of $\\mathbf A$ with largest absolute value   \n",
    "\n",
    "*proof*  \n",
    "$\\mathbf A$ has $n$ eigenvalues which means there is some maximal magnitude $= \\gamma$.  We know $\\gamma \\geq t_0 \\gt 0$.  Hence it suffices to consider the case of $\\mathbf B:= \\frac{1}{\\gamma} \\mathbf A$, so $\\mathbf B$ is a strictly positive matrix with maximal magnitude eigenvalue equal to one.  \n",
    "\n",
    "**the rest of the post assumes that the maximal magnitude eigenvalue is 1** and uses\n",
    "$\\lambda_1 = \\frac{t_0}{\\gamma}$, i.e. $t_0$ adapted to the matrix $\\mathbf B$.    \n",
    "\n",
    "*Here are two different ways to finish*  \n",
    "The first is accessible to anyone who understands basic analysis and Schur's Triangularization Theorem-- though technically we won't see this until chapter 7.  The second finish is a bit more exotic.  Your author couldn't think of any finish that, with respect to linear algebra, only used items developed up through chapter 4.  \n",
    "\n",
    "*finish 1)*    \n",
    "\n",
    "$ 1$  \n",
    "$=\\max \\big \\vert \\lambda_i\\big \\vert $  \n",
    "$\\leq \\sigma_1 $  \n",
    "$\\leq \\big(\\sum_{k=1}^n \\sigma_1^2\\big)^\\frac{1}{2} $  \n",
    "$=  \\big \\Vert \\mathbf B\\big \\Vert_F $  \n",
    "$= \\big(\\sum_{i=1}^n\\sum_{j=1}^n \\vert b_{i,j}\\vert^2\\big)^\\frac{1}{2}$  \n",
    "$=\\big(\\sum_{i=1}^n\\sum_{j=1}^n b_{i,j}^2\\big)^\\frac{1}{2} $  \n",
    "$\\leq \\big(\\sum_{i=1}^n\\sum_{j=1}^n b_{i,j}\\big)$  \n",
    "$= \\mathbf 1^T \\mathbf B \\mathbf 1$  \n",
    "\n",
    "where  at the end we take advantage of real non-negativity (and indeed positivity) of $\\mathbf B$, and then triangle inequality, and we define a function $g$ that computes the sum of all components of $\\mathbf b$      \n",
    "\n",
    "now notice with $\\alpha := \\Big(\\min\\big(\\mathbf x_0\\big)\\Big)^{-1}$  \n",
    "i.e. the (inverse of the) minimum component of our eigenvector, which we know is positive, so that $\\big(\\alpha \\mathbf x_0\\big)$ has a minimum component of one    \n",
    "\n",
    "$\\mathbf 1 \\leq \\alpha \\mathbf x_0$  \n",
    "this holds component-wise, i.e. $1 \\leq (\\alpha \\cdot x_j)$  \n",
    "\n",
    "So if we rescale by arbitrary positive numbers, and sum over the bound, i.e. for any arbitrary $\\mathbf v \\gt \\mathbf 0$  the above implies   \n",
    "$\\mathbf v^T \\mathbf 1 = \\sum_{j=1}^n  v_j\\cdot 1\\leq  \\sum_{j=1}^n v_j\\cdot (\\alpha \\cdot x_j) = \\mathbf v^T \\big(\\alpha \\mathbf x_0\\big)$  \n",
    "\n",
    "now selecting $\\mathbf v^T:= \\mathbf 1^T\\mathbf B^k $ for any natural number $k$  \n",
    "\n",
    "$\\big(\\mathbf 1^T \\mathbf B^k\\big)\\mathbf 1 \\leq  \\big(\\mathbf 1^T \\mathbf B^k\\big) \\big(\\alpha\\mathbf x_0\\big) =\\alpha \\big(\\mathbf 1^T \\mathbf B^k\\big) \\mathbf x_0 = \\alpha \\mathbf 1^T \\big(\\mathbf B^k \\mathbf x_0\\big) = t_0^k \\big(\\alpha \\mathbf 1^T \\mathbf x_0\\big)$  \n",
    "\n",
    "putting this all together we have  \n",
    "$ 1=\\max \\big \\vert \\lambda_i\\big \\vert^k =\\max \\big \\vert \\lambda_i^k\\big \\vert  \\leq \\big \\Vert \\mathbf B^k\\big \\Vert_F \\leq \\mathbf 1^T \\mathbf B^k \\mathbf 1 \\leq t_0^k \\big(\\alpha \\mathbf 1^T \\mathbf x_0\\big)$   \n",
    "for all natural numbers $k$  \n",
    "\n",
    "or equivalently,  \n",
    "$0\\lt c=\\text{positive constant} = \\frac{1}{\\alpha\\mathbf 1^T \\mathbf x_0}  \\leq t_0^k$  \n",
    "but if $t_0 \\lt 1$ we have a contradiction, because the upper bound may be made arbitrarily small (i.e. $\\lim_{k\\to \\infty} t_0^k =0$, so e.g. select any $k$ large enough such that \n",
    "\n",
    "$\\big \\vert t_0^k - 0\\big \\vert = \\big \\vert t_0^k - L\\big \\vert \\lt \\frac{c}{3}$  \n",
    "and we have a contradiction   \n",
    "\n",
    "Thus it must be the case that $t_0 = 1$ i.e. it is a maximal magnitude eigenvalue\n",
    "\n",
    "*finish 2)*  \n",
    "effect a similarity transform, i.e. consider \n",
    "\n",
    "$\\mathbf P = \\mathbf D^{-1}\\mathbf B \\mathbf D$  \n",
    "with $\\mathbf D:= \\text{Diag}\\big(\\mathbf x_0\\big)$  \n",
    "\n",
    "Then $\\mathbf P$ is a matrix with maximal magnitude eigenvalue of 1 and it has strictly positive components.   Further we know it has the ones vector as an eigenvector, i.e.       \n",
    "$\\mathbf 1 =\\mathbf D^{-1} \\mathbf x_0$ is an eigenvector, as confirmed by  \n",
    "\n",
    "$ \\mathbf P\\mathbf 1 = \\mathbf D^{-1}\\mathbf B \\mathbf D \\mathbf 1 =  \\mathbf D^{-1}\\mathbf B \\mathbf D \\big(\\mathbf D^{-1}\\mathbf x_0\\big) = \\mathbf D^{-1}\\mathbf B \\big(\\mathbf D \\mathbf D^{-1}\\big) \\mathbf x_0 =\\mathbf D^{-1}\\mathbf B \\mathbf x_0 = \\lambda_1 \\mathbf D^{-1}\\mathbf x_0 = \\lambda_1 \\mathbf 1$  \n",
    "\n",
    "*Note:*  \n",
    "This particular case of having the ones vector as an eigenvector tells us that all row sums of $\\mathbf P$ are the same and $\\mathbf P$ has a maximal magnitude eigenvalue equal to 1. We also have two different interpretations /motivations for the select similarity transform.  First it, in essence, allows us to map our positive matrix to the special case of probability and markov chains, for which there is an enormous amount of literature and results at our disposal.  Second, more simplistically: this similarity transform preserves the essential features of our matrix (positivity of all components, and positive eigenvalue and positive eigenvector) but it homogenizes the row sums of our matrix which should make it easier to work with.  \n",
    "\n",
    "By maximality or our positive eigenvalue we know  \n",
    "$\\lambda_1 = \\big \\vert \\lambda_1\\big \\vert \\leq \\max_i \\big \\vert \\lambda_i\\big \\vert = 1$  \n",
    "and application of Gerschgorin's discs (with triangle inequality in the background) tells us that the maximal eigenvalue cannot exceed the maximal row sum of a real-non-negative matrix, hence  \n",
    "$1 = \\max_i \\big \\vert \\lambda_i\\big \\vert \\leq \\text{maximal row sum} = \\text{arbitrary row sum of our matrix} = \\mathbf e_k^T\\mathbf P\\mathbf 1 = \\lambda_1$  \n",
    "hence \n",
    "$1 \\leq \\lambda_1 \\leq 1$  \n",
    "or $\\lambda_1 =1$  \n",
    "\n",
    "*note: for a slightly different finish, we could instead consider* $\\mathbf M :=\\frac{1}{\\lambda_1}\\mathbf P$ *and show that the maximal eigenvalue has to be* $\\lambda_1$.  *If we do this algebraicly then we ultimately are using Gerschgorin discs at least implicitly.  However by construction we have* $\\mathbf M \\mathbf 1 =\\frac{1}{\\lambda_1}\\lambda_1 \\mathbf 1 = \\mathbf 1$ *i.e. we know that the matrix is stochastic -- this construction gives us the option of bypassing algebraic arguments and directly applying stochastic theorems, from renewal theory or countable state markov chains, and the existence of a finite limit (with trace one) then implies a projector with rank one, which implies all eigenvalues other than the Perron root are annihilated and hence have modulus* $\\lt 1$  \n",
    "\n",
    "*extensions associated with finish 2*: this also tells us that $\\mathbf P$ is a stochastic matrix.  Strict positivity in $\\mathbf P$ tells us, with another application of Gerschgorin's discs that the only possible eigenvalue on the unit circle is $=1$.  With some care /use of techniques related to Markov chains we can then prove that the geometric multiplicity of the eigenvalue 1 is 1.  Re-running our original argument on $\\mathbf P^T$ tells us that the left eigenvector $\\mathbf v_1$ associated with eigenvalue 1 for $\\mathbf P$ is strictly positive.  Finally, we can prove that the geometric multiplicity of eigenvalue 1 must equal the algebraic multiplicity, using probability specific tools (e.g. renewal theory or Kolmogorov results on Markov chains) or simply by carefully working through the telescoping identity  \n",
    "\n",
    "$\\big(\\mathbf I - \\mathbf B + \\mathbf E_1\\big)\\mathbf S^{(r)} = \\big(\\mathbf I - \\mathbf B + \\mathbf E_1\\big)\\big(\\mathbf I + \\mathbf B + \\mathbf B^2 + ... + \\mathbf B^{r-1}\\big) = \\mathbf I - \\mathbf A^r + r\\mathbf E_1$  \n",
    "\n",
    "where  \n",
    "$\\mathbf E_1 = \\mathbf 1 \\mathbf v_1^T$ and $\\text{trace}\\big(\\mathbf E_1\\big) = 1$ (i.e. $\\mathbf E_1$ is constructed to be the projector associated with eigenvalue 1) and   \n",
    "$\\mathbf S^{(r)} := \\mathbf I + \\mathbf B + \\mathbf B^2 + ... \\mathbf B^{r-1}$  \n",
    "\n",
    "then proving \n",
    "$\\big(\\mathbf I - \\mathbf B + \\mathbf E_1\\big)^{-1}$ exists, multiplying each side by it, then multiplying each side by $\\frac{1}{r}$  \n",
    "\n",
    "and finally showing \n",
    "$\\lim_{r \\to \\infty} \\frac{\\mathbf S^{(r)}}{r} = \\mathbf E_1$  \n",
    "which has trace 1, which (with some care) implies that the algebraic multiplicity of eigenvalue 1, must in fact be one.  \n",
    "\n",
    "*corollary*  \n",
    "(again sticking with the Perron case of strictly positive $\\mathbf B$ to avoid nuisance cases relating to periodic behavior)  \n",
    "\n",
    "$\\text{min row sum B} \\leq \\lambda_1 = 1 \\leq \\text{max row sum B}$   \n",
    "and the inequalities are strict unless $\\mathbf B$ is stochastic, i.e.  $\\text{max row sum B} = \\text{min row sum B} $    \n",
    "\n",
    "leg one:  \n",
    "for stochastic matrix $\\mathbf B$  \n",
    "$\\mathbf {B1} = \\mathbf {1}$  \n",
    "so we have  \n",
    "$\\text{min row sum B} = \\lambda_1 = 1 = \\text{max row sum B}$   \n",
    "\n",
    "leg two:  \n",
    "for non-stochastic matrix $\\mathbf B\\mathbf 1 \\neq \\mathbf 1$  \n",
    "but we know the Perron vector $\\mathbf x_0 \\gt \\mathbf 0$ exists, it just cannot be the ones vector.  There are finitely many entries in the vector, all of which are positive so we have a minimal and maximal coordinate, which we may call $x_\\text{min}^{(0)}  \\lt x_\\text{max}^{(0)}$ where the inequality is strict because not all values are the same (i.e. the eigenvector isn't proportional to the ones vector).  \n",
    "\n",
    "Via a graph isomorphism:  we can *assume WLOG* that $x_1 = x_\\text{max}^{(0)}$ and $x_n = x_\\text{min}^{(0)}$.  This isn't technically needed  but it cleans up indexing issues, so we can cleanly say $x_1 \\lt x_n$.   \n",
    "\n",
    "when we revisit  \n",
    "$\\mathbf P = \\mathbf D^{-1}\\mathbf B \\mathbf D$   \n",
    "with $\\mathbf D:= \\text{Diag}\\big(\\mathbf x_0\\big)$  \n",
    "\n",
    "with $\\mathbf P\\mathbf 1 = \\mathbf 1$   \n",
    "\n",
    "if we look at row $i$ we have  \n",
    "$\\lambda_1 = 1 = \\sum_{j=1}^n p_{i,j}\\cdot 1 =   \\sum_{j=1}^n \\big(x_i^{-1} \\cdot b_{i,j}x_j\\big)\\cdot 1=x_i^{-1} \\cdot \\sum_{j=1}^n x_j \\cdot b_{i,j}$  \n",
    "\n",
    "specializing to our maximal row $1$ we have  \n",
    "$\\lambda_1  $  \n",
    "$= x_1^{-1} \\cdot \\sum_{j=1}^n x_j \\cdot b_{1,j} $  \n",
    "$= x_1^{-1} \\cdot \\big(\\sum_{j=1}^{n-1} x_j \\cdot b_{1,j}\\big) + x_1^{-1}\\big( x_n \\cdot b_{1,n}\\big) $  \n",
    "$\\leq  x_1^{-1} \\cdot \\big(\\sum_{j=1}^{n-1} x_1 \\cdot b_{1,j}\\big) + x_1^{-1}\\big(x_n \\cdot b_{1,n}\\big) $  \n",
    "$\\lt  x_1^{-1} \\cdot \\big(\\sum_{j=1}^{n-1} x_1 \\cdot b_{1,j}\\big) + x_1^{-1}\\big(x_1 \\cdot b_{1,n}\\big)$  \n",
    "$= \\sum_{j=1}^n  b_{1,j}$  \n",
    "$= \\text{maximal row sum B}$  \n",
    "\n",
    "and specializing to our minimal row $n$ we have  \n",
    "$\\lambda_1  $  \n",
    "$= x_n^{-1} \\cdot \\sum_{j=1}^n x_j \\cdot b_{n,j} $  \n",
    "$= x_n^{-1} \\cdot \\big(\\sum_{j=2}^{n} x_j \\cdot b_{n,j}\\big) + x_n^{-1}\\big( x_1 \\cdot b_{n,1}\\big) $  \n",
    "$\\geq  x_n^{-1} \\cdot \\big(\\sum_{j=2}^{n} x_n \\cdot b_{n,j}\\big) + x_n^{-1}\\big(x_1 \\cdot b_{n,1}\\big) $  \n",
    "$\\gt  x_n^{-1} \\cdot \\big(\\sum_{j=2}^{n} x_n \\cdot b_{n,j}\\big) + x_n^{-1}\\big(x_n \\cdot b_{n,1}\\big)$  \n",
    "$= \\sum_{j=1}^n  b_{n,j}$  \n",
    "$= \\text{minimal row sum B}$  \n",
    "\n",
    "putting this all together, we've proven that \n",
    "$\\text{min row sum B} \\leq \\lambda_1 \\leq \\text{max row sum B}$   \n",
    "and the inequalities are strict unless $\\mathbf B$ is stochastic  \n",
    "\n",
    "This immediately generalizes to any positive matrix $\\mathbf A$ via rescaling by positive constant $\\frac{\\gamma}{t_0}$  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*corollary*  \n",
    "if for two positive matrices $\\mathbf A \\geq \\mathbf Z$ and the inequality is strict in at least one component (i.e. $\\mathbf A \\neq \\mathbf Z$) then   \n",
    "$\\text{perron root of A}\\gt \\text{perron root of Z}$  \n",
    "\n",
    "proof:  \n",
    "for convenience we assume WLOG that $\\text{perron root of A} = 1$  \n",
    "As before construct the similarity transform with $\\mathbf D:= \\text{Diag}\\big(\\mathbf x_0\\big)$   \n",
    "\n",
    "our relation implies  \n",
    "$\\mathbf A \\mathbf D \\geq \\mathbf Z \\mathbf D$  \n",
    "with inequality strict in at least one component, which implies  \n",
    "\n",
    "$\\mathbf P = \\mathbf D^{-1}\\mathbf A \\mathbf D \\geq \\mathbf D^{-1} \\mathbf Z \\mathbf D $  \n",
    "with inequality strict in at least one component\n",
    "\n",
    "but $\\mathbf P$ is stochastic with eigenvalue one, so $\\big(\\mathbf D^{-1} \\mathbf Z \\mathbf D\\big) $ is strictly substochastic in at least one row, i.e.  \n",
    "$\\text{min row sum }\\mathbf D^{-1} \\mathbf Z \\mathbf D \\lt \\text{perron root of Z} \\leq \\text{max row sum of }\\mathbf D^{-1} \\mathbf Z \\mathbf D\\leq 1 = \\text{perron root of A}$   \n",
    "\n",
    "where we make use of the fact that similarity transforms do not change eigenvalues, and our prior corollary which tells us that the upper bound is strict  i.e.  \n",
    "\n",
    "$\\text{perron root of Z} \\lt \\text{perron root of A}$    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*corollary*   \n",
    "for positive matrix $\\mathbf A$ and positive vector $\\mathbf y$ and positive scalar $\\alpha$  \n",
    "\n",
    "(i) if \n",
    "$\\mathbf A\\mathbf y \\geq \\alpha \\mathbf y$   \n",
    "and inequality is strict in one component, then  \n",
    "$\\text{perron root of A}\\gt \\alpha$    \n",
    "\n",
    "(ii) if \n",
    "$\\mathbf A\\mathbf y \\leq \\alpha \\mathbf y$   \n",
    "and inequality is strict in one component, then  \n",
    "$\\text{perron root of A}\\lt \\alpha$    \n",
    "\n",
    "*proof*  \n",
    "we only prove (i) since the proof of (ii) proceeds in a virtually identical manner.   \n",
    "\n",
    "let   \n",
    "$\\mathbf D:= \\text{Diag}\\big(\\mathbf y\\big)$    \n",
    "\n",
    "we can write the claim as   \n",
    "$\\mathbf A\\mathbf y = \\mathbf A \\mathbf D\\mathbf 1 \\geq \\alpha \\mathbf D \\mathbf 1 = \\alpha \\mathbf y$   \n",
    "where, again the inequality is strict in at least one component  \n",
    "\n",
    "$\\mathbf D^{-1}$ is strictly positive (and diagonal), so left multiplication by $\\mathbf D^{-1}$ preserves component-wise inequalities, which gives us  \n",
    "\n",
    "$\\mathbf C \\mathbf 1= \\big(\\mathbf D^{-1} \\mathbf A \\mathbf D\\big) \\mathbf 1 = \\mathbf D^{-1} \\big(\\mathbf A \\mathbf D \\mathbf 1\\big) \\geq  \\mathbf D^{-1}\\big(\\alpha\\mathbf D \\mathbf 1\\big)  =\\alpha \\mathbf D^{-1}\\mathbf D \\mathbf 1 = \\alpha \\mathbf 1$   \n",
    "where the inequality is strict in at least one component  \n",
    "which is to say that positive matrix $\\mathbf C$ has either  \n",
    "$\\alpha \\lt \\text{min row sum C} \\leq \\lambda_1 = \\text{Perron root of C} \\leq  \\text{max row sum C}$    \n",
    "or  \n",
    "$\\alpha = \\text{min row sum C} \\lt \\lambda_1 = \\text{Perron root of C} \\lt  \\text{max row sum C}$    \n",
    "i.e. this expresses that  $\\alpha \\lt \\text{max row sum C}$  in each of the two possible ways, and while making use of our earlier corollary which implies that the Perron root then is strictly greater than $\\alpha$     \n",
    "but since $\\mathbf C$ is similar to $\\mathbf A$ we've proven  \n",
    "$\\alpha \\lt \\text{Perron root of A} $  \n",
    "as desired   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*remark*   \n",
    "This chain of corollaries is PF4, PF5, PF6, and PF7 on page 4 of Brualdi's *The Mutually Beneficial Relationship of Graphs and Matrices*, though the structure of the proofs in Brualdi are very different --  e.g. the proof of our first corollary comes last, and the general structure involves manipulating double sums and more granular operations that your author finds to be rather unpleasant.  So the above serves as a very different route of deriving all of these usual and important Perron Frobenius inequalities.     \n",
    "\n",
    "Brualdi also takes that the Perron root is simple for granted and then proves that the Perron vector, strictly positive, can be the only eigenvector that is real non-negative (via the manipulation of sums); this is PF3.  But there is a more basic, standard approach, which was not taken here-- this involves observing that $\\mathbf A^T$ has a Perron vector $\\mathbf v$ as well with eigenvalue $\\lambda_1$, but for left and right eigenvectors associated with distinct eigenvalues -- they must be orthogonal, i.e. if $\\mathbf A \\mathbf x_i = \\lambda_i \\mathbf x_i$ for $i \\geq 2$ then  \n",
    "$\\lambda_1 \\mathbf v^T \\mathbf x_i =  \\big(\\mathbf v^T \\mathbf A\\big) \\mathbf x_i = \\mathbf v^T \\mathbf A \\mathbf x_i = \\mathbf v^T \\big(\\mathbf A \\mathbf x_i\\big) = \\lambda_i \\mathbf v^T \\mathbf x_i$  \n",
    "but because $\\lambda_1 \\neq \\lambda_i$ this implies $\\mathbf v^T \\mathbf x = 0$ (and since $\\mathbf v$ is real, this is a standard inner product, which implies orthogonality.) But since $\\mathbf v \\gt 0$ and $\\mathbf x \\neq \\mathbf 0$ this implies $\\mathbf x$ cannot be real-nonnegative.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*additional remark: Generalizing from Perron Theory to Perron-Frobenius Theory*  \n",
    "The key step in generalizing from positive matrices, which necessarily have a connected graph, to real non-negative matrices that the moderately more general Perron-Frobenius theory covers, is to confirm whether the real non-negative matrix has one communicating class (/ a connected graph).  If not then the graph is reducible into at least 2 smaller graphs (and we recursively drill down to individual collections of communicating classes, with some special care for analyzing transient states).  The key result, however, is to focus on graph connectivity.  So for our n by n matrices, a connected graph by definition means that each state is reachable from any other state in a finite number of steps, and e.g. by Cayley Hamilton this implies that they are reachable in at most $n$ steps.  So if we look at the Cesaro sum / ensemble average of our real non-negative matrix $\\mathbf A$ associated with a connected graph, then *that* is a positie matrix, i.e.  \n",
    "\n",
    "by convention, *we assume WLOG that the maximal magnitude eigenvalue of* $\\mathbf A$ $=1$ (note $\\mathbf A$ cannot be traceless for all powers and hence cannot be nilpotent if there is a connected graph), and from here consider  \n",
    "\n",
    "$\\mathbf S_n := \\frac{1}{n}\\big(\\mathbf A + \\mathbf A^2 + ... + \\mathbf A^n \\big)$    \n",
    "\n",
    "where   \n",
    "$\\mathbf S_n$ necessarily inherits all of the traits we derived in the above case of Perron Theory since $\\mathbf S_n$ is a positive matrix. E.g. $\\mathbf S_n$ has a single dominant Perron root of 1, with all other eigenvalues strictly less in magnitude and so does $\\mathbf A^k \\mathbf S_n$ for any natural number $k$, which implies $\\mathbf A$ has a single eigenvalue of 1 with the positive eigenvector we developed before. One key difference that may arise is there may be other eigenvalues on the unit circle -- i.e. $\\mathbf A$ being 'only' real non-negative, it underlying graph may exhibit periodic behavior.  \n",
    "\n",
    "The key linkage, then is the graph connectivity embedded in $\\mathbf A$. (Some of this is developed in a separate, somewhat messier manner part way through the Gerschgorin Discs writeup in the Linear Algebra folder.)    \n",
    "\n",
    "*After* we have verified that there is one communicating class in $\\mathbf A$ we may consider perturbing its entries, i.e. consider  \n",
    "\n",
    "$\\mathbf A^{'} = \\mathbf A +\\delta \\mathbf{11}^T$  \n",
    "for some small $\\delta\\gt 0$  \n",
    "\n",
    "We know that $\\mathbf A$ has a Perron root (though possibly others $\\neq 1$ but located elsewhere on the unit circle), and $\\mathbf A'$ has a Perron root as well because it is strictly positive.  Using our earlier corollaries, we can say  \n",
    "\n",
    "$\\text{min row sum }A + n\\cdot \\delta =\\text{min row sum }A' \\leq \\text{Perron root of }A' \\leq \\text{max row sum }A'+n\\cdot \\delta$  \n",
    "\n",
    "the the inequality is strict unless  \n",
    "$\\text{min row sum A} = \\text{max row sum A}$    \n",
    "\n",
    "so supposing $\\text{min row sum A} \\neq \\text{max row sum A}$, then we have      \n",
    "$\\text{min row sum }A + n\\cdot \\delta \\lt \\text{Perron root of }A' \\lt \\text{max row sum }A+n\\cdot \\delta$  \n",
    "for any $\\delta \\gt 0$  \n",
    "(note that amongst positive $\\delta$ one of our corollaries implies that $\\text{Perron root of }A'$ monotone increases with increasing $\\delta$)  \n",
    "\n",
    "This plus topological continuity of the eigenvalues of $\\mathbf A$ gives us many of the results we seek, e.g.     \n",
    "$\\text{min row sum }A  \\leq \\text{Perron root of }A \\leq \\text{max row sum }A$  \n",
    "which we can strengthen to  \n",
    "$\\text{min row sum }A  \\leq \\text{Perron root of }A \\lt \\text{max row sum }A$  \n",
    "where we in fact know the upper bound must be strict, e.g. due to Tausky's refinement of Gerschgorin Discs --  which for our purposes we can get the desired result by embedding $\\mathbf A$ in an absorbing state markov chain by augmenting all rows that have row sum less than max row sum, with paths to an absorbing state -- i.e. if we divide out $\\text{max row sum }A$ this is equivalent to implying  $\\frac{\\text{Perron root of }A }{\\text{max row sum }A}\\lt 1$ which it must be since we have a connected graph and absorbtion happens with probability 1 -- i.e. there cannot be a non-zero fixed point amongst the transient states.      \n",
    "\n",
    "it isn't immediately clear to your author as to whether we may sharpen the lower bound to $\\text{min row sum }A  \\lt \\text{Perron root of }A$.  The strictness seems plausible though proofs remain elusive.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using our earlier corollary we have, if $\\mathbf A$ is positive, then  \n",
    "\n",
    "\n",
    "(i) if \n",
    "$\\mathbf A\\mathbf y \\geq \\alpha \\mathbf y$   \n",
    "and inequality is strict in one component, then  \n",
    "$\\text{perron root of A}\\gt \\alpha$    \n",
    "\n",
    "(ii) if \n",
    "$\\mathbf A\\mathbf y \\leq \\alpha \\mathbf y$   \n",
    "and inequality is strict in one component, then  \n",
    "$\\text{perron root of A}\\lt \\alpha$    \n",
    "\n",
    "but this actually implies the strictness of  \n",
    "$\\text{min row sum }A  \\lt \\text{Perron root of }A \\lt \\text{max row sum }A$  \n",
    "\n",
    "for non-negative $\\mathbf A$ with a connected graph when $\\text{min row sum }A\\neq \\text{max row sum }A$    \n",
    "\n",
    "*for (i)*, suppose for a contradiction that the minimum row sum $\\alpha = 1$, but our perron root $\\lambda_1 = 1$  \n",
    "we will show this implies $1 = \\alpha \\lt \\lambda = 1$ which is the contradiction we seek  \n",
    "\n",
    "we observe  \n",
    "$\\mathbf A \\mathbf 1 \\geq \\alpha \\mathbf 1 = \\mathbf 1$  \n",
    "where the inequality is strict in at least one component.  \n",
    "\n",
    "Then \n",
    "$\\mathbf A\\mathbf 1 = \\mathbf 1 + \\mathbf x^{(1)}$  \n",
    "where $\\mathbf x^{(1)}$ is a real non-negative vector and $\\mathbf x^{(1)} \\neq \\mathbf 0$  \n",
    "\n",
    "then  \n",
    "\n",
    "$\\mathbf A^2\\mathbf 1 = \\mathbf A\\big(\\mathbf A\\mathbf 1\\big) = \\mathbf A\\mathbf 1 + \\mathbf A\\mathbf x^{(1)} = \\mathbf x^{(1)}+\\mathbf x^{(2)}$  \n",
    "\n",
    "and in general  \n",
    "$\\mathbf A^k\\mathbf 1 = \\mathbf x^{(1)}+\\mathbf x^{(2)} + ... + \\mathbf x^{(k)}$  \n",
    "where again $\\mathbf x^{(1)} \\geq \\mathbf 0$ and is strict in at least one component, and for $j\\geq 2$ we know $\\mathbf x^{(j)}\\geq \\mathbf 0$  \n",
    "\n",
    "(the observation is simply that a real non-negative matrix times a real non-negative vector gives a real non-negative vector -- then just repeatedly applying that observation via associativity and linearity)  \n",
    "\n",
    "applying this to considering  \n",
    "$\\mathbf S_n := \\frac{1}{n}\\big(\\mathbf A + \\mathbf A^2 + ... + \\mathbf A^n \\big)\\mathbf 1 \\geq \\mathbf 1 +\\mathbf x^{(1)} \\geq \\mathbf 1$  \n",
    "where the RHS inequality is strict in at least one component because  $\\mathbf 0 \\neq \\mathbf x^{(1)} \\geq \\mathbf 0$  \n",
    "\n",
    "but $\\mathbf S_n$ is a positive matrix with our same Perron root of one, so using our earlier corollary we have  \n",
    "\n",
    "$1 =\\text{Perron root of } \\mathbf A =\\text{Perron root of } \\mathbf S_n \\gt \\alpha = 1$  \n",
    "\n",
    "so it must be the case that $\\alpha \\in (0, \\lambda_1) $  \n",
    "\n",
    "*for (ii)*, suppose for a contradiction that maximal row sum $\\alpha = 1$, but our Perron root $\\lambda_1 = 1$   \n",
    "then  \n",
    "$\\mathbf A\\mathbf 1 = \\mathbf 1 + \\mathbf z^{(1)}$  \n",
    "where $\\mathbf z^{(1)}$ is a real non-positive vector and $\\mathbf z^{(1)} \\neq \\mathbf 0$  \n",
    "the argument, then, proceeds in a nearly identical manner as in (i), where this time we repeatedly make use of the observation that a real non-negative matrix times a real non-positive vector gives a real non-positive vector.  \n",
    "\n",
    "now that we have proven  \n",
    "$\\text{min row sum }A  \\leq \\text{Perron root of }A \\leq \\text{max row sum }A$  \n",
    "for real non-negative $\\mathbf A$ with a connected graph, where the inequalities are strict unless  $\\text{min row sum }A = \\text{max row sum }A$, we inherit our other corollaries for positive matrices and see they too apply to real non-negative ones with a connected graph, by mimicking our earlier arguments, which in effect consisted of using well chosen diagonal matrices to effect a similarity transform and then applying  \n",
    "$\\text{min row sum }A  \\leq \\text{Perron root of }A \\leq \\text{max row sum }A$  \n",
    "and making use of the 'iff' conditions which we know about cases of equality  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Bonus problem (not Artin)*     \n",
    "\n",
    "**Putnam** 2015:  \n",
    "\"Let n be a positive integer. Suppose that A ,B, and M are nn matrices with real entries such that AM=MB, and such that A and B have the same characteristic polynomial. Prove that det(AMX) = det(BXM), for every nn matrix X with real entries  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if $\\mathbf M$ is invertible then $\\mathbf M^{-1}\\mathbf A \\mathbf M = \\mathbf B$  and \n",
    "\n",
    "$\\det\\big(\\mathbf A -\\mathbf M \\mathbf X\\big) = \\det\\big(\\mathbf M^{-1}\\big)\\det\\big(\\mathbf A -\\mathbf M \\mathbf X\\big)\\det\\big(\\mathbf M\\big) = \\det\\big(\\mathbf M^{-1}\\mathbf A\\mathbf M - \\mathbf X\\mathbf M\\big) =  \\det\\big(\\mathbf B -\\mathbf X \\mathbf M\\big) $  \n",
    "working in reals (though using complex number for spectral properties and triangularization as needed), the result should follow by considering a density argument.  I.e. supposing with our fixed $\\mathbf A, \\mathbf B, \\mathbf M$ that we find some nonconforming $\\mathbf X$, i.e. where    \n",
    "$f_X\\big(\\mathbf M\\big) = \\det\\big(\\mathbf A -\\mathbf M \\mathbf X\\big) - \\det\\big(\\mathbf B -\\mathbf X \\mathbf M\\big) = c \\neq 0$  \n",
    "\n",
    "since the determinant is a continuous function of a matrix, which itself has entries as a continuous function of $\\mathbf M$ (via adition and multiplication being continous on matrices), this tells us that for any $\\epsilon \\gt 0$ and in particular $\\epsilon := \\frac{\\vert c\\vert }{10}$ there exists a $\\delta \\gt 0$ neighborhood around $\\mathbf M$  where $f_X\\big(N(\\mathbf M, \\delta)\\big)\\subset N\\big(f_X(\\mathbf M), \\epsilon\\big)$    \n",
    "\n",
    "but for any $\\delta \\gt 0$ we may always find an arbitrarily close $\\mathbf M'$ where $\\big(\\mathbf M^{'}\\big)^{-1}$ exists and hence we have $\\big \\vert f_X\\big(\\mathbf M^{'}\\big) -f_X\\big(\\mathbf M\\big) \\big \\vert = \\big \\vert 0 -c \\big \\vert = \\big \\vert c \\big \\vert \\gt   \\frac{\\vert c\\vert}{10}$  \n",
    "which is a contradiction.  \n",
    "\n",
    "**technical problem** more work is needed to show that that when we perturb $\\mathbf M$ that we still have $\\mathbf {AM}' = \\mathbf {M'B}$, or to bound the error if such a relation does not hold.  This becomes un-workably tedious and is a dead-end.  \n",
    "\n",
    "**a better approach:** focus on the case where $\\mathbf A^{-1}$ exists (and so does $\\mathbf B^{-1}$ since the characterstic polynomials are the same)  \n",
    "\n",
    "and consider where $d$ is the magnitutde of the smalled positive eigenvalue of $\\mathbf A$ (with $d=1$ for convenience if there are no positive eigenvalues)  \n",
    "\n",
    "$\\gamma: = \\min\\big(\\frac{d}{\\sqrt{n}}, \\delta\\big)$  \n",
    "$\\mathbf A' := \\mathbf A - \\gamma \\mathbf I$  \n",
    "and $\\mathbf B' := \\mathbf B - \\gamma \\mathbf I$   \n",
    "\n",
    "so  \n",
    "$ \\mathbf A'\\mathbf M = \\mathbf A\\mathbf M - \\gamma \\mathbf I\\mathbf M = \\mathbf M \\mathbf B-  \\mathbf M \\big(\\gamma \\mathbf I\\big)= \\mathbf M\\big(\\mathbf B - \\gamma \\mathbf I\\big) = \\mathbf M\\mathbf B'$  \n",
    "i.e. our 'pseduo commutativity' relationship is preserved  \n",
    "\n",
    "\n",
    "$\\mathbf A' \\mathbf M =  \\mathbf M \\mathbf B'\\longrightarrow \\big(\\mathbf A'\\big)^{-1}\\mathbf M = \\mathbf M \\big(\\mathbf B'\\big)^{-1}$  \n",
    "\n",
    "so  \n",
    "$\\det\\big(\\mathbf A' -\\mathbf M \\mathbf X\\big) $  \n",
    "$= \\det\\Big(\\mathbf A'\\big(\\mathbf I - \\big(\\mathbf A'\\big)^{-1}\\mathbf M \\mathbf X\\big)\\Big) $  \n",
    "$= \\det\\big(\\mathbf A'\\big)\\det\\big(\\mathbf I - \\big(\\mathbf A'\\big)^{-1}\\mathbf M \\mathbf X\\big) $  \n",
    "$= \\det\\big(\\mathbf A'\\big)\\det\\big(\\mathbf I - \\mathbf M \\big(\\mathbf B'\\big)^{-1}\\mathbf X\\big)$  (by above identity)  \n",
    "$= \\det\\big(\\mathbf I - \\mathbf X\\mathbf M \\big(\\mathbf B'\\big)^{-1}\\big)\\det\\big(\\mathbf A'\\big)$    (see below)   \n",
    "$= \\det\\big(\\mathbf I - \\mathbf X\\mathbf M \\big(\\mathbf B'\\big)^{-1}\\big)\\det\\big(\\mathbf B'\\big)$  \n",
    "$= \\det\\big(\\mathbf B' - \\mathbf X\\mathbf M \\big)$  \n",
    "\n",
    "in the second to last line we used  \n",
    "($\\mathbf A'$ and $\\mathbf B'$ have the same characteristic polynomial since $\\mathbf A$ and $\\mathbf B$ do -- i.e. we know the latter pair have the same characteristic polynomial $p$ and so $p(x) = p(x)$ and $p(x -\\gamma) = p(x -\\gamma)$ which is the characteristic polynomial for the former pair, and then set $x :=0$ to get the determinant for the former pair)    \n",
    "\n",
    "note: \n",
    "and the 3rd to last line ('see below') used  \n",
    "$\\mathbf X\\Big(\\mathbf M \\big(\\mathbf B'\\big)^{-1}\\Big) = \\mathbf Y \\mathbf Z$ \n",
    "has the same characteristic polynomial $g$  as $ \\Big(\\mathbf M \\big(\\mathbf B'\\big)^{-1}\\Big)\\mathbf X =  \\mathbf Z\\mathbf Y$  \n",
    "and in particular they are the same when evaluated at   \n",
    "$h(1) = \\det\\big(\\mathbf I - \\mathbf X\\mathbf M \\big(\\mathbf B'\\big)^{-1}\\big)$   \n",
    "\n",
    "the desired result follows by a simple density argument  \n",
    "e.g. consider the sequence \n",
    "\n",
    "$\\mathbf A(r)' := \\mathbf A + \\frac{d}{r\\sqrt{n}}\\mathbf I$  \n",
    "\n",
    "with  \n",
    "$a_r = \\det\\big(\\mathbf A(r)'-\\mathbf M \\mathbf X\\big) -\\det\\big(\\mathbf B(r)'- \\mathbf X\\mathbf M \\big) = 0$  \n",
    "for all $r$ and \n",
    "\n",
    "$\\lim_{r \\to \\infty}a_r =   \\det\\big(\\mathbf A-\\mathbf M \\mathbf X\\big)-\\det\\big(\\mathbf B - \\mathbf X\\mathbf M \\big) = 0$  \n",
    "by continuity of the determinant (and continuity of addition of two continuous functions)  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**remark**  \n",
    "using Schur Complements, and e.g. page 475 of Meyer's *Matrix Analysis* we can see that the claim is equivalent to proving   \n",
    "\n",
    "\n",
    "$ \\det\\Big(\\begin{bmatrix}\n",
    "\\mathbf A & \\mathbf M\\\\ \n",
    "\\mathbf X & \\mathbf I\n",
    "\\end{bmatrix}\\Big) = \\det\\Big(\\begin{bmatrix}\n",
    "\\mathbf B & \\mathbf X\\\\ \n",
    "\\mathbf M & \\mathbf I\n",
    "\\end{bmatrix}\\Big) $   \n",
    "\n",
    "because  \n",
    "$\\det\\big(\\mathbf A-\\mathbf M \\mathbf X\\big) = \\det\\Big(\\begin{bmatrix}\n",
    "\\mathbf A & \\mathbf M\\\\ \n",
    "\\mathbf X & \\mathbf I\n",
    "\\end{bmatrix}\\Big)$  \n",
    "\n",
    "and  \n",
    "$\\det\\Big(\\begin{bmatrix}\n",
    "\\mathbf B & \\mathbf X\\\\ \n",
    "\\mathbf M & \\mathbf I\n",
    "\\end{bmatrix}\\Big)= \\det\\big(\\mathbf B - \\mathbf X\\mathbf M \\big) $   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "an alternative approach would be to look at  \n",
    "$\\text{trace}\\Big(\\big(\\mathbf A -\\mathbf M \\mathbf X\\big)^k\\Big) =\\text{trace}\\Big(\\big(\\mathbf B -\\mathbf X \\mathbf M\\big)^k\\Big)$   \n",
    "\n",
    "and to try and match their traces for all powers of k.\n",
    "\n",
    "for k =1 the result is immediate since A and B have the same characteristic polynomial and hence the same trace  \n",
    "for k = 2 we have  \n",
    "$\\text{trace}\\Big(\\big(\\mathbf A -\\mathbf M \\mathbf X\\big)^2\\Big) $  \n",
    "$= \\text{trace}\\Big(\\mathbf A^2 -\\mathbf A\\mathbf M \\mathbf X + \\mathbf M \\mathbf X\\mathbf A + \\big(\\mathbf M \\mathbf X\\big)^2\\Big) $  \n",
    "$= \\text{trace}\\Big(\\mathbf A^2\\big) -\\text{trace}\\big(\\mathbf A\\mathbf M \\mathbf X\\big) - \\text{trace}\\big(\\mathbf M \\mathbf X\\mathbf A\\big) + \\text{trace}\\big(\\big(\\mathbf M \\mathbf X\\big)^2\\big)$  \n",
    "$= \\text{trace}\\Big(\\mathbf B^2\\big) -2\\cdot \\text{trace}\\big(\\mathbf M \\mathbf B\\mathbf X\\big) + \\text{trace}\\big(\\big(\\mathbf X \\mathbf M\\big)^2\\big)$  \n",
    "$= \\text{trace}\\Big(\\mathbf B^2\\big) -\\text{trace}\\big( \\mathbf B\\mathbf X\\mathbf M\\big) -\\text{trace}\\big( \\mathbf X\\mathbf M\\mathbf B\\big) + \\text{trace}\\big(\\big(\\mathbf X \\mathbf M\\big)^2\\big)$  \n",
    "$=\\text{trace}\\Big(\\big(\\mathbf B -\\mathbf X \\mathbf M\\big)^2\\Big)$   \n",
    "\n",
    "\n",
    "However, direct estimates at this point become difficult for higher powers of k, with respect to cross terms.  Note: the site on Putnam problems maintained by Kedlaya does in fact have a solution that follows this form, though it isn't particularly easy for your author to follow.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
