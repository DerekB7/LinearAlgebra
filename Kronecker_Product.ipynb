{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To motivate this a little, $^{-1}$ naturally corresponds with the matrix group operation of $\\text{x}$ not the group operation of $+$, so its desirable to 'push' the multiplicative inverse operation onto multiplication not addition of matrices.  Combining this with the fact that a matrix is real sym PD iff its inverse is, it suffices to prove $\\big(A(A+B)^{-1}B\\big)^{-1} = \\big(B^{-1}(A+B)A^{-1}\\big) \\succ 0$  but $\\big(B^{-1}(A+B)A^{-1}\\big) = B^{-1} + A^{-1}$ which gives the result.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This posting is my notes on Kronecker Products.  It is mostly a re-cut of the materials found here: \n",
    "\n",
    "http://www.siam.org/books/textbooks/OT91sample.pdf \n",
    "\n",
    "with some interesting extensions that I added (e.g. the relationship between Kronecker and Hadamard Products.)  \n",
    "\n",
    "In general, the field for this posting is $\\mathbb C$\n",
    "\n",
    "all matrices and vectors in this post are finite dimensional\n",
    "\n",
    "note that the conjugate transpose of a matrix $\\mathbf B$ is given by $\\mathbf B^H$, though in some other postings it is written as $\\mathbf B^*$.  \n",
    "\n",
    "Since the Kronecker product is deeply tied in with coordinates, there are some cases where $\\mathbf B^T$ is used which means transpose without conjugation -- this is not an issue if the matrix has only real-valued scalars, but it requires special care if the matrix has scalars in $\\mathbb C$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "definition of Kronecker product: \n",
    "\n",
    "$\\mathbf B \\otimes  \\mathbf C = \\begin{bmatrix}\n",
    "b_{1,1}\\mathbf C & \\cdots & b_{1,n}\\mathbf C\\\\ \n",
    "\\vdots & \\ddots & \\vdots \\\\ \n",
    "b_{m,1}\\mathbf C & \\cdots & b_{m,n}\\mathbf C\n",
    "\\end{bmatrix}$\n",
    "\n",
    "where $\\mathbf B$ is  $m$ x $n$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**claim: The Kronecker Product is Associative**\n",
    "\n",
    "**proof:**\n",
    "\n",
    "where $\\mathbf B$ is  $m$ x $n$ and $\\mathbf C$ is $p$ x $q$\n",
    "\n",
    "let $\\mathbf W: = \\big(\\mathbf C \\otimes  \\mathbf E\\big) = \\begin{bmatrix}\n",
    "c_{1,1}\\mathbf E & \\cdots & c_{1,q}\\mathbf E\\\\ \n",
    "\\vdots & \\ddots & \\vdots \\\\ \n",
    "c_{p,1}\\mathbf E & \\cdots & c_{p,q}\\mathbf E\n",
    "\\end{bmatrix}$   \n",
    "\n",
    "- - - - \n",
    "\n",
    "by inspection, we see: \n",
    "\n",
    "\n",
    "$\\big(\\mathbf B \\otimes  \\mathbf C \\big) \\otimes \\mathbf E = \\begin{bmatrix}\n",
    "b_{1,1}\\mathbf C & \\cdots & b_{1,n}\\mathbf C\\\\ \n",
    "\\vdots & \\ddots & \\vdots \\\\ \n",
    "b_{m,1}\\mathbf C & \\cdots & b_{m,n}\\mathbf C\n",
    "\\end{bmatrix} \\otimes \\mathbf E = \\begin{bmatrix}\n",
    "b_{1,1}\\mathbf W & \\cdots & b_{1,n}\\mathbf W\\\\ \n",
    "\\vdots & \\ddots & \\vdots \\\\ \n",
    "b_{m,1}\\mathbf W & \\cdots & b_{m,n}\\mathbf W\n",
    "\\end{bmatrix} = \\mathbf B \\otimes  \\mathbf W = \\mathbf B \\otimes  \\big(\\mathbf C  \\otimes \\mathbf E\\big)$\n",
    "\n",
    "*commentary: The other (high brow) approach is to recognize the Kronecker product as a special case of the tensor product and then remember the associativity of the tensor product.*  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*some technical nits:*   \n",
    "    \n",
    "The article points out that, if $\\mathbf x \\in \\mathbb C^m$ and $\\mathbf y \\in \\mathbb C^n$\n",
    "\n",
    "$\\mathbf {xy}^H = \\mathbf x  \\otimes \\mathbf y^H $\n",
    "\n",
    "I.e. that we get and $m$ x $n$, rank one matrix\n",
    "\n",
    "also note\n",
    "\n",
    "$\\mathbf x^H \\otimes \\mathbf y = \\mathbf y  \\mathbf x^H $\n",
    "\n",
    "but \n",
    "\n",
    "$\\mathbf x \\otimes \\mathbf y$\n",
    "\n",
    "is a vector with $mn$ entries.  \n",
    "\n",
    "I think other sources may write \n",
    "\n",
    "$\\mathbf x  \\otimes \\bar{\\mathbf y} $ when they actually mean $\\mathbf x  \\otimes \\mathbf y^H $\n",
    "\n",
    "*conclusion: take extra care when considering Kronecker Products of vectors*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "where $\\mathbf B \\in \\mathbb C^{m x n}$ and $\\mathbf X \\in \\mathbb C^{m x n}$, $\\mathbf C \\in \\mathbb C^{r x s}$\n",
    "\n",
    "*Kronecker products distribute across addition*\n",
    "\n",
    "**claim:**  \n",
    "$\\big(\\mathbf B \\otimes \\mathbf C \\big) + \\big(\\mathbf X \\otimes \\mathbf C \\big) = \\big(\\mathbf B  + \\mathbf X \\big) \\otimes \\mathbf C  $\n",
    "\n",
    "**proof**\n",
    "\n",
    "$\\big(\\mathbf B \\otimes \\mathbf C \\big) + \\big(\\mathbf X \\otimes \\mathbf C \\big)$  \n",
    "$ =   \n",
    "\\begin{bmatrix} b_{1,1}\\mathbf C & \\cdots & b_{1,n}\\mathbf C\\\\ \n",
    "\\vdots & \\ddots & \\vdots \\\\ \n",
    "b_{m,1}\\mathbf C & \\cdots & b_{m,n}\\mathbf C\n",
    "\\end{bmatrix}  + \\begin{bmatrix}\n",
    "x_{1,1}\\mathbf C & \\cdots & x_{1,n}\\mathbf C\\\\ \n",
    "\\vdots & \\ddots & \\vdots \\\\ \n",
    "x_{m,1}\\mathbf C & \\cdots & x_{m,n}\\mathbf C\n",
    "\\end{bmatrix} $  \n",
    "$= \\begin{bmatrix}\n",
    "(b_{1,1} + x_{1,1})\\mathbf C & \\cdots & (b_{1,n} + x_{1,n})\\mathbf C\\\\ \n",
    "\\vdots & \\ddots & \\vdots \\\\ \n",
    "(b_{m,1} + x_{m,1})\\mathbf C & \\cdots & (b_{m,n} + x_{m,n} )\\mathbf C\\end{bmatrix} $  \n",
    "$  = \\big(\\mathbf B  + \\mathbf X \\big) \\otimes \\mathbf C $\n",
    "\n",
    "- - - - -\n",
    "\n",
    "**claim:**  \n",
    "$\\big(\\mathbf C \\otimes \\mathbf B \\big) + \\big(\\mathbf C \\otimes \\mathbf X \\big) = \\mathbf C  \\otimes  \\big( \\mathbf B + \\mathbf X \\big)  $\n",
    "\n",
    "\n",
    "**proof**\n",
    "\n",
    "$\\big(\\mathbf C \\otimes \\mathbf B \\big) + \\big(\\mathbf C \\otimes \\mathbf X \\big)$  \n",
    "$ =   \n",
    "\\begin{bmatrix} c_{1,1}\\mathbf B & \\cdots & c_{1,s}\\mathbf B\\\\ \n",
    "\\vdots & \\ddots & \\vdots \\\\ \n",
    "c_{r,1}\\mathbf B & \\cdots & c_{r,s}\\mathbf B\n",
    "\\end{bmatrix}  + \\begin{bmatrix} c_{1,1}\\mathbf X & \\cdots & c_{1,s}\\mathbf X\\\\ \n",
    "\\vdots & \\ddots & \\vdots \\\\ \n",
    "c_{r,1}\\mathbf X & \\cdots & c_{r,s}\\mathbf X\n",
    "\\end{bmatrix}$  \n",
    "$= \\begin{bmatrix} c_{1,1}\\big(\\mathbf B + \\mathbf X\\big) & \\cdots & c_{1,s}\\big(\\mathbf B + \\mathbf X\\big)\\\\ \n",
    "\\vdots & \\ddots & \\vdots \\\\ \n",
    "c_{r,1}\\big(\\mathbf B + \\mathbf X\\big) & \\cdots & c_{r,s}\\big(\\mathbf B + \\mathbf X\\big)\n",
    "\\end{bmatrix} $  \n",
    "$= \\mathbf C  \\otimes  \\big( \\mathbf B + \\mathbf X \\big) $\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**key property:** where $\\mathbf B \\in \\mathbb C^{m x n}$, $\\mathbf C \\in \\mathbb C^{r x s}$, $\\mathbf X \\in \\mathbb C^{n x p}$, and $\\mathbf Y \\in \\mathbb C^{s x t}$\n",
    "\n",
    "$\\big(\\mathbf B \\otimes \\mathbf C \\big)\\big(\\mathbf X \\otimes \\mathbf Y\\big) = \\big(\\mathbf{BX} \\big)\\otimes \\big(\\mathbf{CY}\\big)$\n",
    "\n",
    "**proof**  \n",
    "We verify this via inspection.  Note this one attribute alone is immensely useful and is key to unlocking most other attributes.\n",
    "\n",
    "\n",
    "$\\mathbf B = \\begin{bmatrix}\n",
    "\\mathbf b_1^{\\sim{T}}\\\\ \n",
    "\\vdots \\\\ \n",
    "\\mathbf b_m^{\\sim{T}}\\\\ \n",
    "\\end{bmatrix}$\n",
    "\n",
    "Technical note: $\\mathbf b_k^{\\sim{T}}$ denotes the kth row vector in $\\mathbf B$.  There is no complex conjugation being suggested here.  \n",
    "\n",
    "$\\mathbf X = \n",
    "\\bigg[\\begin{array}{c|c|c|c}\n",
    "\\mathbf x_1 & \\mathbf x_2 &\\cdots & \\mathbf x_{p}\n",
    "\\end{array}\\bigg]\n",
    "$   \n",
    "\n",
    "$\\big(\\mathbf B \\otimes \\mathbf C \\big)\\big(\\mathbf X \\otimes \\mathbf Y\\big)  = \\begin{bmatrix}\n",
    "b_{1,1}\\mathbf C & \\cdots & b_{1,n}\\mathbf C\\\\ \n",
    "\\vdots & \\ddots & \\vdots \\\\ \n",
    "b_{m,1}\\mathbf C & \\cdots & b_{m,n}\\mathbf C\n",
    "\\end{bmatrix}  \\begin{bmatrix}\n",
    "x_{1,1}\\mathbf Y & \\cdots & x_{1,p}\\mathbf Y\\\\ \n",
    "\\vdots & \\ddots & \\vdots \\\\ \n",
    "x_{n,1}\\mathbf Y & \\cdots & x_{n,p}\\mathbf Y\n",
    "\\end{bmatrix} $\n",
    "\n",
    "And we then see:\n",
    "\n",
    "$\\begin{bmatrix}\n",
    "b_{1,1}\\mathbf C & \\cdots & b_{1,n}\\mathbf C\\\\ \n",
    "\\vdots & \\ddots & \\vdots \\\\ \n",
    "b_{m,1}\\mathbf C & \\cdots & b_{m,n}\\mathbf C\n",
    "\\end{bmatrix}  \\begin{bmatrix}\n",
    "x_{1,1}\\mathbf Y & \\cdots & x_{1,p}\\mathbf Y\\\\ \n",
    "\\vdots & \\ddots & \\vdots \\\\ \n",
    "x_{n,1}\\mathbf Y & \\cdots & x_{n,p}\\mathbf Y\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "\\text{dot}\\big(\\mathbf b_1^{\\sim{T}},\\mathbf x_1\\big) \\mathbf {CY} & \\cdots & \\text{dot}\\big(\\mathbf b_1^{\\sim{T}},\\mathbf x_p\\big) \\mathbf {CY}\\\\ \n",
    "\\vdots & \\ddots & \\vdots \\\\ \n",
    "\\text{dot}\\big(\\mathbf b_m^{\\sim{T}},\\mathbf x_1 \\big) \\mathbf {CY} & \\cdots & \\text{dot}\\big(\\mathbf b_m^{\\sim{T}},\\mathbf x_p\\big) \\mathbf {CY}\n",
    "\\end{bmatrix} = \\big(\\mathbf{BX}\\big) \\otimes \\big(\\mathbf{CY}\\big)$\n",
    "\n",
    "Thus  \n",
    "\n",
    "$\\big(\\mathbf B \\otimes \\mathbf C \\big)\\big(\\mathbf X \\otimes \\mathbf Y\\big)  = \\big(\\mathbf{BX}\\big) \\otimes \\big(\\mathbf{CY}\\big)$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**remark on singularity**   \n",
    "This immediately implies that if $\\mathbf B$ or $\\mathbf C$ is singular then so is    $\\big(\\mathbf B \\otimes \\mathbf C \\big)$   \n",
    "\n",
    "i.e. revisiting the dimensions we can select $\\mathbf X$ and $\\mathbf Y$ to be column vectors \n",
    "\n",
    "so \n",
    "$\\big(\\mathbf B \\otimes \\mathbf C \\big) \\big(\\mathbf x \\otimes \\mathbf y\\big) = \\big(\\mathbf B\\mathbf x \\otimes \\mathbf C \\mathbf y\\big) $  \n",
    "\n",
    "where if $\\mathbf x$ is in $\\mathbf B$'s nullspace, we have  \n",
    "\n",
    "$\\big(\\mathbf B \\otimes \\mathbf C \\big) \\big(\\mathbf x \\otimes \\mathbf y\\big)= \\big(\\mathbf 0 \\otimes \\mathbf C \\mathbf y\\big)= \\mathbf 0$  \n",
    "\n",
    "and if $\\mathbf y$ is in $\\mathbf C$'s nullspace, we have  \n",
    "$\\big(\\mathbf B \\otimes \\mathbf C \\big) \\big(\\mathbf x \\otimes \\mathbf y\\big)= \\big(\\mathbf {Bx} \\otimes  \\mathbf 0\\big)= \\mathbf 0$  \n",
    "\n",
    "(with a little thought, we can run this argument the other way to show that if $\\mathbf B$ and $\\mathbf C$ both have trivial nullspaces, then so does there Kronecker product)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\big(\\mathbf B \\otimes  \\mathbf C\\big)^H = \\begin{bmatrix}\n",
    "b_{1,1}\\mathbf C & \\cdots & b_{1,n}\\mathbf C\\\\ \n",
    "\\vdots & \\ddots & \\vdots \\\\ \n",
    "b_{m,1}\\mathbf C & \\cdots & b_{m,n}\\mathbf C\n",
    "\\end{bmatrix}^H = \\big(\\mathbf B^H \\otimes  \\mathbf C^H\\big)$\n",
    "\n",
    "verify by inspection\n",
    "\n",
    "follow- up note: with respect to Hermitian operators: \n",
    "\n",
    "this means that if $\\mathbf B = \\mathbf B^H$ and $\\mathbf C = \\mathbf C^H$ then we see\n",
    "\n",
    "$\\big(\\mathbf B \\otimes  \\mathbf C\\big) = \\big(\\mathbf B^H \\otimes  \\mathbf C^H \\big) = \\big(\\mathbf B \\otimes  \\mathbf C\\big)^H$\n",
    "\n",
    "hence the matrix resulting from their Kronecker product is Hermitian as well\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "suppose that $\\mathbf B \\in \\mathbb C^{m x m}$ and $\\mathbf C \\in \\mathbb C^{n x n}$, and that each matrix is non-singular:  \n",
    "\n",
    "then  \n",
    "$\\big(\\mathbf B \\otimes  \\mathbf C\\big)\\big(\\mathbf B \\otimes  \\mathbf C\\big)^{-1} = \\big(\\mathbf B \\otimes  \\mathbf C\\big)\\big(\\mathbf B^{-1} \\otimes  \\mathbf C^{-1}\\big)=  \\big(\\mathbf{BB}^{-1} \\otimes   \\mathbf{CC}^{-1}\\big) = \\big(\\mathbf I_m \\otimes \\mathbf I_n\\big) = \\mathbf I_{mn}$\n",
    "\n",
    "thus:  \n",
    "$\\big(\\mathbf B \\otimes  \\mathbf C\\big)^{-1} = \\big(\\mathbf B^{-1} \\otimes  \\mathbf C^{-1}\\big)$ \n",
    "\n",
    "\n",
    "**extension:** \n",
    "\n",
    "suppose both $\\mathbf B$ and $\\mathbf C$ are non-singular $m$ x $m$ matrices.  \n",
    "\n",
    "Since the Hadamard product $\\big(\\mathbf B \\circ \\mathbf C\\big)$ is a principal submatrix of $\\big(\\mathbf B \\otimes  \\mathbf C\\big)^{-1}$, this tells us that $\\big(\\mathbf B \\circ \\mathbf C\\big)^{-1}$ exists if $\\mathbf B$ and $\\mathbf C$ are non-singular.  (It may also exist in other cases as well, e.g. $\\Big(\\mathbf B \\circ \\big(\\mathbf {11}^T\\big)\\Big)$ is invertible if $\\mathbf B$ is, even though $\\big(\\mathbf {11}^T\\big)$ is a rank-one matrix; this statement is an 'if' but not an 'iff'.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we again suppose that $\\mathbf B \\in \\mathbb C^{m x m}$ and $\\mathbf C \\in \\mathbb C^{n x n}$\n",
    "\n",
    "$\\big(\\mathbf B \\otimes  \\mathbf C\\big) = \\begin{bmatrix}\n",
    "b_{1,1}\\mathbf C & \\cdots & b_{1,m}\\mathbf C\\\\ \n",
    "\\vdots & \\ddots & \\vdots \\\\ \n",
    "b_{m,1}\\mathbf C & \\cdots & b_{m,m}\\mathbf C\n",
    "\\end{bmatrix}$\n",
    "\n",
    "and by inspection we see that \n",
    "\n",
    "$\\text{trace}\\big(\\mathbf B \\otimes  \\mathbf C\\big) = b_{1,1}(c_{1,1} + c_{2,2} + ... + c_{n,n}) + b_{2,2}(c_{1,1} + c_{2,2} + ... + c_{n,n}) + ... + b_{m,m}(c_{1,1} + c_{2,2} + ... + c_{n,n})$\n",
    "\n",
    "$\\text{trace}\\big(\\mathbf B \\otimes  \\mathbf C\\big) = b_{1,1}\\text{trace}\\big(\\mathbf C\\big) + b_{2,2}\\text{trace}\\big(\\mathbf C\\big) + ... + b_{m,m}\\text{trace}\\big(\\mathbf C\\big) = \\big(b_{1,1} + b_{2,2} + ... + b_{m,m}\\big)\\text{trace}\\big(\\mathbf C\\big) = \\text{trace}\\big(\\mathbf B\\big)\\text{trace}\\big(\\mathbf C\\big)$\n",
    "\n",
    "we then apply the same argument  \n",
    "$\\text{trace}\\Big(\\big(\\mathbf B \\otimes  \\mathbf C\\big)\\big(\\mathbf B \\otimes  \\mathbf C\\big)\\Big) = \\text{trace}\\Big(\\big(\\mathbf B^2 \\otimes  \\mathbf C ^2 \\big)\\Big) = \\text{trace}\\big(\\mathbf B^2\\big) \\text{trace}\\big(\\mathbf C ^2 \\big)$  \n",
    "\n",
    "and more generally for any natural number $k = \\{1, 2, 3,... \\}$, we see that \n",
    "\n",
    "$\\text{trace}\\Big(\\big(\\mathbf B \\otimes  \\mathbf C\\big)^k \\Big) = \\text{trace}\\Big(\\big(\\mathbf B^k \\otimes  \\mathbf C^k \\big)\\Big) = \\text{trace}\\big(\\mathbf B^k\\big) \\text{trace}\\big(\\mathbf C^k \\big)$\n",
    "\n",
    "The many proofs contained in the Vandermonde matrices posting tells us that if this holds for enough $k$ (and in this case it holds for all k that are natural numbers which is *plenty* large...)\n",
    "\n",
    "where $\\lambda_i$ is the ith eigenvalue of $\\mathbf B$ and $\\mu_i$ is the ith eigenvalue of $\\mathbf C$, then we know for certain that \n",
    "\n",
    "$\\text{trace}\\Big(\\big(\\mathbf B \\otimes  \\mathbf C\\big)^k \\Big) = \\text{trace}\\big(\\mathbf B^k\\big) \\text{trace}\\big(\\mathbf C^k \\big) = \\big(\\lambda_{1}^k + \\lambda_{2}^k + ... + \\lambda_{m}^k\\big)\\big(\\mu_{1}^k + \\mu_{2}^k + ... + \\mu_{n}^k\\big)$    \n",
    "$\\text{trace}\\Big(\\big(\\mathbf B \\otimes  \\mathbf C\\big)^k \\Big) = \\big((\\lambda_{1}\\mu_1)^k + (\\lambda_1 \\mu_2)^k +... (\\lambda_1 \\mu_n)^k + (\\lambda_{2} \\mu_1)^k + (\\lambda_{2} \\mu_2)^k +...+ (\\lambda_{2} \\mu_n)^k + ... + (\\lambda_{m} \\mu_1)^k + (\\lambda_{m} \\mu_2)^k + ... +(\\lambda_{m} \\mu_n)^k\\big)$\n",
    "\n",
    "And again we can recall the same proof at the end of the Vandermonde matrices posting, and recognize that since this relationship holds for all natural numbers $k$, then after observing that it holds for $\\geq 2(mn) -1 $ iterations, we can be certain that the above eigenvalues are uniquely specified.  That is:  \n",
    "\n",
    "$eig\\Big(\\mathbf B \\otimes  \\mathbf C\\Big) = \\{(\\lambda_{1}\\mu_1), (\\lambda_1 \\mu_2), ..., (\\lambda_1 \\mu_n), (\\lambda_{2} \\mu_1),  (\\lambda_{2} \\mu_2), ..., (\\lambda_{2} \\mu_n),  ... , (\\lambda_{m} \\mu_1), (\\lambda_{m} \\mu_2), ...,(\\lambda_{m} \\mu_n)\\}$\n",
    "\n",
    "(where the above set is technically a multi-set)\n",
    "\n",
    "That is, we know $\\Big(\\mathbf B \\otimes  \\mathbf C\\Big)$ has $mn$ eigenvalues, and we could also think of $eig\\Big(\\mathbf B \\otimes  \\mathbf C\\Big)$ as being the Cartesian product of the multiset of eigenvalues for $\\mathbf B$ and the multiset of eigenvlaues for $\\mathbf C$.\n",
    "\n",
    "- - - -\n",
    "another way to show this, borrowing from some notation shown under Kronecker Sums, is that the multiset of eigenvalues is contained in the below square 'box', where $\\mathbf P$ is the full cylce permtuation matrix / cylic shift matrix \n",
    "\n",
    "\n",
    "$\\Big(\\bigg[\\begin{array}{c|c|c|c|c}\n",
    "\\mathbf {\\mu} & \\mathbf {\\mu} & \\mathbf {\\mu} &\\cdots & \\mathbf {\\mu}\n",
    "\\end{array}\\bigg] \\circ \\bigg[\\begin{array}{c|c|c|c|c}\n",
    "\\mathbf P^0 \\mathbf {\\Lambda 1} & \\mathbf P^1\\mathbf {\\Lambda1} & \\mathbf P^2\\mathbf {\\Lambda 1} &\\cdots & \\mathbf P^{n-1}\\mathbf {\\Lambda1}\n",
    "\\end{array}\\bigg]\\Big) $\n",
    "\n",
    "$=  \\begin{bmatrix}\n",
    "\\mu_1 & \\mu_{1} & \\mu_{1} & \\dots & \\mu_1 & \\mu_1 \\\\ \n",
    "\\mu_2 & \\mu_2 & \\mu_{2} & \\dots & \\mu_2 & \\mu_2 \\\\ \n",
    "\\mu_3 & \\mu_3 & \\mu_3 & \\dots & \\mu_3 & \\mu_3 \\\\\n",
    "\\vdots & \\vdots  & \\vdots &\\ddots & \\vdots & \\vdots\\\\ \n",
    "\\mu_{n-1} & \\mu_{n-1} & \\mu_{n-1} & \\dots & s_{n-1}  & \\mu_{n-1} \\\\ \n",
    "\\mu_{n} & \\mu_{n}  & \\mu_{n} & \\dots & \\mu_n &  \\mu_n\n",
    "\\end{bmatrix}\\circ\\begin{bmatrix}\n",
    "\\lambda_1 & \\lambda_{n} & \\lambda_{n-1} & \\dots & \\lambda_3 & \\lambda_2 \\\\ \n",
    "\\lambda_2 & \\lambda_1 & \\lambda_{n} & \\dots & \\lambda_4 & \\lambda_3 \\\\ \n",
    "\\lambda_3 & \\lambda_2 & \\lambda_1 & \\dots & \\lambda_5 & \\lambda_4 \\\\\n",
    "\\vdots & \\vdots  & \\vdots &\\ddots & \\vdots & \\vdots\\\\ \n",
    "\\lambda_{n-1} & \\lambda_{n-2} & \\lambda_{n-3} & \\dots & \\lambda_1  & \\lambda_{n} \\\\ \n",
    "\\lambda_{n} & \\lambda_{n-1}  & \\lambda_{n-2} & \\dots & \\lambda_2 &  \\lambda_1\n",
    "\\end{bmatrix}$\n",
    "\n",
    "\n",
    "$= \\begin{bmatrix}\n",
    "\\mu_1  \\lambda_1 & \\mu_1 \\lambda_{n} & \\mu_1 \\lambda_{n-1} & \\dots & \\mu_1  \\lambda_3 & \\mu_1  \\lambda_2 \\\\ \n",
    "\\mu_2 \\lambda_2 & \\mu_2 \\lambda_1 & \\mu_2 \\lambda_{n} & \\dots & \\mu_2  \\lambda_4 & \\mu_2 \\lambda_3\\\\ \n",
    "\\mu_3  \\lambda_3 & \\mu_3 \\lambda_2 & \\mu_3 \\lambda_1 & \\dots & \\mu_3  \\lambda_5 & \\mu_3  \\lambda_4 \\\\\n",
    "\\vdots & \\vdots  & \\vdots &\\ddots & \\vdots & \\vdots\\\\ \n",
    "\\mu_{n-1} \\lambda_{n-1} & \\mu_{n-1} \\lambda_{n-2} & \\mu_{n-1} \\lambda_{n-3} & \\dots & \\mu_{n-1}\\lambda_1  & \\mu_{n-1}\\lambda_{n} \\\\ \n",
    "\\mu_n \\lambda_{n} & \\mu_n \\lambda_{n-1}  & \\mu_n \\lambda_{n-2} & \\dots &\\mu_{n} \\lambda_2 & \\mu_{n}  \\lambda_1\n",
    "\\end{bmatrix}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many other results follow simply from the multiplication rule.  \n",
    "\n",
    "\n",
    "**claim:**  \n",
    "If $\\mathbf Q$ and $\\mathbf V$ are both unitary matrices, then the matrix given by\n",
    "\n",
    "$\\big(\\mathbf Q \\otimes \\mathbf V \\big)$\n",
    "\n",
    "is also unitary.\n",
    "\n",
    "**proof:** \n",
    "\n",
    "$\\big(\\mathbf Q \\otimes \\mathbf V \\big)^H \\big(\\mathbf Q \\otimes \\mathbf V \\big) = \\big(\\mathbf Q^H \\otimes \\mathbf V^H \\big)\\big(\\mathbf Q \\otimes \\mathbf V \\big)= \\big(\\mathbf Q^H \\mathbf Q \\otimes \\mathbf V^H \\mathbf V \\big) = \\big(\\mathbf I_m \\otimes \\mathbf I_n \\big)= \\mathbf I_{mn}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may do SVD on $\\mathbf B = \\mathbf U_B \\mathbf \\Sigma_B \\mathbf V_B^H$ and $\\mathbf C = \\mathbf U_C \\mathbf \\Sigma_C \\mathbf V_C^H$.  Note: in this setup $\\mathbf U$ and and $\\mathbf V$ are both full rank unitary matrices while the $\\mathbf \\Sigma$ is diagonal, but generally not square.  \n",
    "\n",
    "We then have \n",
    "\n",
    "$\\Big(\\mathbf B \\otimes \\mathbf C\\Big) = \\Big(\\big(\\mathbf U_B \\mathbf \\Sigma_B \\mathbf V_B^H\\big) \\otimes \\big(\\mathbf U_C \\mathbf \\Sigma_C \\mathbf V_C^H\\big)\\Big) = \\Big(\\big(\\mathbf U_B \\otimes \\mathbf U_C\\big)\\big(\\mathbf \\Sigma_B  \\otimes \\mathbf \\Sigma_C \\big)\\big( \\mathbf V_B^H \\otimes \\mathbf V_C^H\\big)\\Big) $\n",
    "\n",
    "\n",
    "Observe that if $\\mathbf B$ is square, and $\\mathbf C$ is square, then $\\mathbf \\Sigma_B$ is square and so is $\\mathbf \\Sigma_C$.  Thus when we take the Kronecker Product of two *square diagonal matrices*\n",
    "\n",
    "$\\big(\\mathbf \\Sigma_B  \\otimes \\mathbf \\Sigma_C \\big)$\n",
    "\n",
    "the result is a square diagonal matrix.  \n",
    "\n",
    "However, in the more general non-square $\\mathbf B$ and $\\mathbf C$ case, our $\\mathbf \\Sigma_B$ and $\\mathbf \\Sigma_C$ are not square, and hence the matrix given by $\\big(\\mathbf \\Sigma_B  \\otimes \\mathbf \\Sigma_C \\big)$ is \"almost\" diagonal.  I.e. it has at most one non-zero entry in every row and at most one non-zero entry in every column, but these entries are not on the diagonal of the matrix.  Multiplication by suitable permutation matrices can fix this, and give us an actual, non-square matrix.\n",
    "\n",
    "Revisiting our factorization, we can say \n",
    "\n",
    "$\\big(\\mathbf U_B \\otimes \\mathbf U_C\\big)^H\\big(\\mathbf B  \\otimes \\mathbf C \\big)\\big( \\mathbf V_B \\otimes \\mathbf V_C\\big) = \\big(\\mathbf \\Sigma_B  \\otimes \\mathbf \\Sigma_C \\big) $\n",
    "\n",
    "All we need need now are suitably sized permutation matrices to multiply on the left and right by (recalling that permutation matrices are a very special case of a unitary matrix).  This gives us\n",
    "\n",
    "\n",
    "$\\mathbf P^{(l)}\\big(\\mathbf U_B \\otimes \\mathbf U_C\\big)^H\\big(\\mathbf B  \\otimes \\mathbf C \\big)\\big( \\mathbf V_B \\otimes \\mathbf V_C\\big)\\mathbf P^{(r)} = \\mathbf P^{(l)}\\big(\\mathbf \\Sigma_B  \\otimes \\mathbf \\Sigma_C \\big)\\mathbf P^{(r)} $\n",
    "\n",
    "And now the right hand side is diagonal.  However to recover the SVD, we do the following: \n",
    "\n",
    "$\\Big(\\mathbf P^{(l)}\\big(\\mathbf U_B \\otimes \\mathbf U_C\\big)^H\\Big)^{H}\\Big(\\mathbf P^{(l)}\\big(\\mathbf U_B \\otimes \\mathbf U_C\\big)^H\\Big)\\big(\\mathbf B  \\otimes \\mathbf C \\big)\\big( \\mathbf V_B \\otimes \\mathbf V_C\\big)\\mathbf P^{(r)} = \\Big(\\mathbf P^{(l)}\\big(\\mathbf U_B \\otimes \\mathbf U_C\\big)^H\\Big)^{H}\\mathbf P^{(l)}\\big(\\mathbf \\Sigma_B  \\otimes \\mathbf \\Sigma_C \\big)\\mathbf P^{(r)} $\n",
    "\n",
    "$\\big(\\mathbf B  \\otimes \\mathbf C \\big)\\big( \\mathbf V_B \\otimes \\mathbf V_C\\big)\\mathbf P^{(r)} = \\big(\\mathbf U_B \\otimes \\mathbf U_C\\big)\\big(\\mathbf P^{(l)}\\big)^H\\mathbf P^{(l)}\\big(\\mathbf \\Sigma_B  \\otimes \\mathbf \\Sigma_C \\big)\\mathbf P^{(r)} $\n",
    "\n",
    "\n",
    "$\\big(\\mathbf B  \\otimes \\mathbf C \\big) = \\big(\\mathbf B  \\otimes \\mathbf C \\big)\\Big(\\big( \\mathbf V_B \\otimes \\mathbf V_C\\big)\\mathbf P^{(r)}\\Big)\\Big(\\big( \\mathbf V_B \\otimes \\mathbf V_C\\big)\\mathbf P^{(r)}\\Big)^H = \\big(\\mathbf U_B \\otimes \\mathbf U_C\\big)\\big(\\mathbf P^{(l)}\\big)^H\\mathbf P^{(l)} \\big(\\mathbf \\Sigma_B  \\otimes \\mathbf \\Sigma_C \\big)\\mathbf P^{(r)}\\Big(\\big( \\mathbf V_B \\otimes \\mathbf V_C\\big)\\mathbf P^{(r)}\\Big)^H  $\n",
    "\n",
    "\n",
    "$\\big(\\mathbf B  \\otimes \\mathbf C \\big)= \\big(\\mathbf U_B \\otimes \\mathbf U_C\\big)\\big(\\mathbf P^{(l)}\\big)^H\\mathbf P^{(l)} \\big(\\mathbf \\Sigma_B  \\otimes \\mathbf \\Sigma_C \\big)\\mathbf P^{(r)}\\big(\\mathbf P^{(r)}\\big)^H \\big( \\mathbf V_B^H \\otimes \\mathbf V_C^H\\big)   $\n",
    "\n",
    "From here we make use of associativity and decide on how we want to interpret the result:\n",
    "\n",
    "$\\big(\\mathbf B  \\otimes \\mathbf C \\big)= \\Big(\\big(\\mathbf U_B \\otimes \\mathbf U_C\\big)\\big(\\mathbf P^{(l)}\\big)^H\\Big) \\Big(\\mathbf P^{(l)} \\big(\\mathbf \\Sigma_B  \\otimes \\mathbf \\Sigma_C \\big)\\mathbf P^{(r)}\\Big)\\Big(\\big(\\mathbf P^{(r)}\\big)^H \\big( \\mathbf V_B^H \\otimes \\mathbf V_C^H\\big) \\Big) = \\big(\\mathbf U_B \\otimes \\mathbf U_C\\big) \\big(\\mathbf \\Sigma_B  \\otimes \\mathbf \\Sigma_C \\big)\\big( \\mathbf V_B^H \\otimes \\mathbf V_C^H\\big)    $\n",
    "\n",
    "where the middle part of the equality shows the proper form of the Singular Value Decomposition (i.e. where the diagonal matrix is in fact diagonal), but the right hand side of the equality is visually most easy to interpret.  But the point is that they are equivalent.  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**remark:**  \n",
    "*a much easier derivation*  \n",
    "\n",
    "a much easier way to derive the above results is to use the multiplicative property of Kronecker products, so observe the squared singular values of $\\big(\\mathbf B  \\otimes \\mathbf C \\big)$ are given by the eigenvalues of   \n",
    "\n",
    "$\\big(\\mathbf B  \\otimes \\mathbf C \\big)\\big(\\mathbf B  \\otimes \\mathbf C \\big)^H =\\big(\\mathbf B  \\otimes \\mathbf C \\big)\\big(\\mathbf B^H  \\otimes \\mathbf C^H \\big)= \\big(\\mathbf B\\mathbf B^H  \\otimes \\mathbf C\\mathbf C^H \\big)$  \n",
    "\n",
    "and we can use our early results on eigenvalues to infer what the singular values must look like.  \n",
    "\n",
    "Then we can use our results about eigenvectors (which are derived just below this section -- do a search on \"diagonalizable\"), to infer what the vectors $\\mathbf u_k$ and $\\mathbf v_j$ associated with the singular values must look like in particular by examining $\\big(\\mathbf B\\mathbf B^H  \\otimes \\mathbf C\\mathbf C^H \\big)$ and $\\big(\\mathbf B^H \\mathbf B  \\otimes \\mathbf C^H\\mathbf C \\big)$\n",
    "\n",
    "\n",
    "- - - - -- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The writeup cleverly then notes that \n",
    "\n",
    "$rank\\Big( \\big(\\mathbf B  \\otimes \\mathbf C \\big) \\Big) = rank\\big(\\mathbf B \\big) rank\\big(\\mathbf C\\big) = rank\\Big( \\big(\\mathbf C  \\otimes \\mathbf B  \\big) \\Big)$\n",
    "\n",
    "This follows directly from the above walk through of the impact of Kronecker Products on singular value decomposition.  \n",
    "\n",
    "*remark:*  \n",
    "we can get this result over arbitrary fields by using rank normal form i.e.   \n",
    "$\\mathbf B = \\mathbf P\\begin{bmatrix}\\mathbf I_r &\\mathbf {00}^T\\\\ \\mathbf {00}^T &\\mathbf {00}^T\\end{bmatrix}\\mathbf Q^{-1}$  \n",
    "see \"Corollary: a useful decomposition\" in Artin chp4 ntoes  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for operators $\\mathbf B$ and $\\mathbf C$, using Schur Decomposition, we have\n",
    "\n",
    "$\\mathbf B = \\mathbf{QRQ}^H$   \n",
    "$\\mathbf C = \\mathbf{VTV}^H$\n",
    "\n",
    "$\\big(\\mathbf B  \\otimes \\mathbf C \\big)  = \\big(\\mathbf{QRQ}^H \\otimes \\mathbf{VTV}^H \\big) = \\big(\\mathbf Q \\otimes \\mathbf{V}\\big)\\big(\\mathbf{R} \\otimes \\mathbf{T} \\big)\\big(\\mathbf{Q}^H \\otimes \\mathbf{V}^H \\big) = \\big(\\mathbf Q \\otimes \\mathbf{V}\\big)\\big(\\mathbf{R} \\otimes \\mathbf{T} \\big)\\big(\\mathbf{Q} \\otimes \\mathbf{V} \\big)^H  $\n",
    "\n",
    "where of course $\\big(\\mathbf Q \\otimes \\mathbf{V}\\big)$ is a unitary matrix, and because we have two square, upper triangular matrices in $\\mathbf R$ and $\\mathbf T$, then\n",
    "\n",
    "$\\big(\\mathbf{R} \\otimes \\mathbf{T} \\big)$\n",
    "\n",
    "is upper triangular as well.  For avoidance of doubt, consider the below, matrix,\n",
    "\n",
    "$\\big(\\mathbf R \\otimes  \\mathbf T\\big) = \\begin{bmatrix}\n",
    "r_{1,1}\\mathbf T & \\cdots & r_{1,m}\\mathbf T\\\\ \n",
    "\\vdots & \\ddots & \\vdots \\\\ \n",
    "r_{m,1}\\mathbf T & \\cdots & r_{m,m}\\mathbf T\n",
    "\\end{bmatrix}$\n",
    "\n",
    "by inspection we see that it is block upper triangular, because each $r_{i,j} = 0$ if $i \\gt j$, by definition of $\\mathbf R$ being upper triangular.  From there we consider the diagonal elements given by $r_{i,i}\\mathbf T = \\lambda_i \\mathbf T$, and because $\\mathbf T$ is upper triangular we observe that each and every block has no non-zero entries below the diagonal of $\\big(\\mathbf R \\otimes  \\mathbf T\\big)$, thus we conclude that $\\big(\\mathbf R \\otimes  \\mathbf T\\big)$ is upper triangular.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, note that if $\\mathbf B$ and $\\mathbf C$ are diagonalizable operators, then we may say:\n",
    "\n",
    "$\\big(\\mathbf B  \\otimes \\mathbf C \\big)  = \\Big(\\big(\\mathbf P \\mathbf \\Lambda \\mathbf P^{-1}\\big) \\otimes \\big( \\mathbf S \\mathbf D \\mathbf S^{-1} \\big)\\Big) = \\big(\\mathbf P \\otimes \\mathbf S\\big)\\big(\\mathbf \\Lambda \\otimes \\mathbf {D} \\big)\\big(\\mathbf{P}^{-1} \\otimes \\mathbf{S}^{-1} \\big) = \\big(\\mathbf P \\otimes \\mathbf S\\big)\\big(\\mathbf \\Lambda \\otimes \\mathbf {D} \\big)\\big(\\mathbf{P} \\otimes \\mathbf{S} \\big)^{-1}  $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is immediate that if we take Kronecker Products of $k$ diagonalizable operators, then \n",
    "\n",
    "$\\big(\\mathbf B^{(1)}  \\otimes \\mathbf B^{(2)}\\otimes ... \\otimes \\mathbf B^{(k-1)} \\otimes \\mathbf B^{(k)}\\big)$    \n",
    "= $\\big(\\mathbf B^{(1)}  \\otimes \\mathbf B^{(2)}\\otimes ... \\otimes \\mathbf B^{(k-1)}\\big) \\otimes \\big(\\mathbf B^{(k)}\\big)$    \n",
    "= $\\big(\\mathbf S \\mathbf D \\mathbf S^{-1}\\big) \\otimes \\big(\\mathbf X \\mathbf \\Lambda \\mathbf X^{-1}\\big)$    \n",
    "= $\\big(\\mathbf S \\otimes \\mathbf X \\big) \\big(\\mathbf D \\otimes \\mathbf \\Lambda  \\big)\\big(\\mathbf S^{-1}\\otimes \\mathbf X^{-1}\\big)$    \n",
    "= $\\big(\\mathbf S \\otimes \\mathbf X \\big) \\big(\\mathbf D \\otimes \\mathbf \\Lambda  \\big)\\big(\\mathbf S\\otimes \\mathbf X\\big)^{-1}$    \n",
    "\n",
    "\n",
    "where we apply i) associativity, ii) induction hypothesis, and iii) prior result, and finally the property of how inverses work with Kronecker Products.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Determinant Formula over Arbitrary Fields** \n",
    "\n",
    "we again suppose that $\\mathbf B \\in \\mathbb C^{m x m}$ and $\\mathbf C \\in \\mathbb C^{n x n}$  \n",
    "\n",
    "over fields of characteristic zero our results on trace (or with schur's triangularization) prove that  \n",
    "\n",
    "$\\det\\big(\\mathbf B \\otimes \\mathbf C\\big) = \\det\\big(\\mathbf B\\big)^m \\det\\big(\\mathbf C\\big)^n$  \n",
    "we can also derive this by observing that  \n",
    "$\\det\\Big(\\mathbf B \\otimes \\mathbf C\\Big) = \\det\\Big(\\big(\\mathbf B \\otimes \\mathbf I_n\\big)\\big(\\mathbf I_m \\otimes \\mathbf C\\big)\\Big)= \\det\\Big(\\mathbf B \\otimes \\mathbf I_n\\Big)\\det\\Big(\\mathbf I_m \\otimes \\mathbf C\\Big) = \\det\\Big(\\mathbf B\\Big)^m   \\det\\Big(\\mathbf C\\Big)^n$  \n",
    "\n",
    "so the claim reduces to proving  \n",
    "$\\det\\Big(\\mathbf B \\otimes \\mathbf I_n\\Big)= \\det\\Big(\\mathbf B\\Big)^m $    \n",
    "$\\det\\Big(\\mathbf I_m \\otimes \\mathbf C\\Big) = \\det\\Big(\\mathbf C\\Big)^n$   \n",
    "\n",
    "if we examine the blocked structure, the second claim would be relatively easy to prove directly, while the first claim would seem difficult.  However our above result that the Kronecker product of an upper triangular matrix and an upper triangular matrix is an upper triangular matrix, combined with a basic observation regarding the generators of $\\mathbf A$ and $\\mathbf B$ gives the result in both cases  \n",
    "\n",
    "We have the desired determinant identities in the case that $\\mathbf A$ and/or $\\mathbf B$ is singular under the section called \"remark on singularity\".  So we proceed by considering the jointly non-singular case, i.e.  \n",
    "$\\mathbf B  \\in GL_m(\\mathbb F)$ and $\\mathbf C  \\in GL_n(\\mathbb F)$  -- we show the 'more difficult' argument for $\\mathbf B$ since the argument for $\\mathbf C$ proceeds almost identically.  \n",
    "\n",
    "*case 1:*  \n",
    "$\\mathbf B$ has determinant of 1, so $\\mathbf B  \\in SL_m(\\mathbb F)$, a subgroup of $GL_m(\\mathbb F)$.  \n",
    "A standard result see \"Artin_chp2_SLN_subgroup_generators.ipynb\"   is to show that $\\mathbf B$ is generated solely by elementary matrices of the first type -- i.e. multiplication of $r$ matrices, each of which is triangular, with ones on the diagonal, and (at most) one non-zero component off the diagonal  \n",
    "\n",
    "so  \n",
    "$\\mathbf B = \\mathbf E^{(1)} \\mathbf E^{(2)} ... \\mathbf E^{(r)}\\longrightarrow \\big(\\mathbf B \\otimes \\mathbf I_n\\big) = \\big(\\mathbf E^{(1)} \\mathbf E^{(2)} ... \\mathbf E^{(r)} \\otimes \\mathbf I_n\\big) = \\big(\\mathbf E^{(1)} \\otimes \\mathbf I_n\\big)\\big(\\mathbf E^{(2)} \\otimes \\mathbf I_n\\big)...\\big(\\mathbf E^{(r)} \\otimes \\mathbf I_n\\big)$  \n",
    "so  \n",
    "$\\det\\Big(\\mathbf B \\otimes \\mathbf I_n\\Big) = \\det\\Big(\\mathbf E^{(1)}\\otimes \\mathbf I_n\\Big)\\det\\Big(\\mathbf E^{(2)} \\otimes \\mathbf I_n)\\Big)...\\det\\Big(\\mathbf E^{(r)} \\otimes \\mathbf I_n\\Big)= 1\\cdot 1 \\cdot ... \\cdot 1 = 1$  \n",
    "\n",
    "where each  \n",
    "$\\det\\Big(\\mathbf E^{(i)}\\otimes \\mathbf I_n\\Big)=1$  \n",
    "because if $\\mathbf E^{(i)}$ is upper triangular, then we have the Kronecker product of two upper triangular matrices, each with all ones on the diagonal, and the result is an upper triangular matrix with all ones on the diagonal and hence the determinant is one. If $\\mathbf E^{(i)}$ is lower triangular, repeat the above sentence with \"lower\" used instead of \"upper\".   \n",
    "\n",
    "thus \n",
    "$\\det\\Big(\\mathbf B \\otimes \\mathbf I_n\\Big)= \\det\\Big(\\mathbf B\\Big)^n = 1^n = 1$    \n",
    "\n",
    "*case 2:*  \n",
    "$0\\neq \\det\\big(\\mathbf B\\big) = c \\neq 0$,  \n",
    "so $\\mathbf B  \\in GL_m(\\mathbb F)$ and in fact $\\mathbf B$ is a coset to  $SL_m(\\mathbb F)$  \n",
    "\n",
    "proof:  \n",
    "using an elementary matrix of the third type given by   \n",
    "$\\begin{bmatrix}\n",
    "c^{-1} & \\mathbf 0\\\\ \n",
    " \\mathbf 0 & \\mathbf I_m\n",
    "\\end{bmatrix}$\n",
    "\n",
    "we have  \n",
    "$\\mathbf B' := \\begin{bmatrix}\n",
    "c^{-1} & \\mathbf 0\\\\ \n",
    " \\mathbf 0 & \\mathbf I_{m-1}\n",
    "\\end{bmatrix} \\mathbf B = \\mathbf D^{-1}\\mathbf B$  \n",
    "\n",
    "where  \n",
    "$\\mathbf B'  \\in SL_m(\\mathbb F)$  \n",
    "so  \n",
    "$\\mathbf B = \\begin{bmatrix}\n",
    "c & \\mathbf 0\\\\ \n",
    " \\mathbf 0 & \\mathbf I_{m-1}\n",
    "\\end{bmatrix}  \\mathbf B' = \\mathbf D \\mathbf B'$  \n",
    "\n",
    "and thus  \n",
    "$\\det\\Big(\\mathbf B\\otimes \\mathbf I_n \\Big) = \\det\\Big(\\mathbf D \\mathbf B'\\otimes \\mathbf I_n \\Big) = \\det\\Big(\\big(\\mathbf D\\otimes \\mathbf I_n\\big) \\big(\\mathbf B'\\otimes \\mathbf I_n\\big) \\Big) = \\det\\Big(\\mathbf D\\otimes \\mathbf I_n\\Big)\\cdot \\det\\Big(\\mathbf B'\\otimes \\mathbf I_n \\Big)= c^n \\cdot 1 = \\det\\Big(\\mathbf B\\Big)^n  $    \n",
    "\n",
    "where  \n",
    "$\\Big(\\mathbf D\\otimes \\mathbf I_n\\Big)$  \n",
    "is a diagonal matrix and if we look at our original kronecker product definition, we see the element $c$ is in the top left corner scaling $\\mathbf I_n$, so $c$ shows up $n$ times  along this diagonal, and all other diagonal components of $\\Big(\\mathbf D\\otimes \\mathbf I_n\\Big)$ are 1, thus its determinant is $c^n$      \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remark on Kronecker Product vs  Hadamard Product\n",
    "\n",
    "\n",
    "an alternative way to view this (that I haven't seen elsewhere) is to view the operation in two steps:\n",
    "\n",
    "First 'blow up' $\\mathbf B$ and $\\mathbf C$ into the appropriate block matrices, then do the Hadamard product between them.  I.e.:\n",
    "\n",
    "where $\\mathbf {11}^H$ i.e. the appropriately sized ones matrix (i.e. in this case it has the same dimensions as $\\mathbf C$ ), and $\\circ$ denotes the Hadamard product\n",
    "\n",
    "$\\mathbf B \\otimes  \\mathbf C = \\begin{bmatrix}\n",
    "b_{1,1}\\mathbf {11}^H & \\cdots & b_{1,n}\\mathbf {11}^H\\\\ \n",
    "\\vdots & \\ddots & \\vdots \\\\ \n",
    "b_{m,1}\\mathbf {11}^H & \\cdots & b_{m,n}\\mathbf {11}^H\n",
    "\\end{bmatrix} \\circ \\begin{bmatrix}\n",
    "\\mathbf C & \\cdots & \\mathbf C\\\\ \n",
    "\\vdots & \\ddots & \\vdots \\\\ \n",
    "\\mathbf C & \\cdots & \\mathbf C\n",
    "\\end{bmatrix}= \\begin{bmatrix}\n",
    "b_{1,1}\\mathbf C & \\cdots & b_{1,n}\\mathbf C\\\\ \n",
    "\\vdots & \\ddots & \\vdots \\\\ \n",
    "b_{m,1}\\mathbf C & \\cdots & b_{m,n}\\mathbf C\n",
    "\\end{bmatrix}$\n",
    "\n",
    "Assuming that $\\big(\\mathbf B \\circ \\mathbf C\\big)$ is well defined, this viewpoint may help us visualize that the elements of $\\big(\\mathbf B \\circ \\mathbf C\\big)$ are contained in $\\big(\\mathbf B \\otimes \\mathbf C\\big)$.\n",
    "- - - - \n",
    "Of particular interest is the case where $\\mathbf B$ is $m$ x $m$ and $\\mathbf C$ is $m$ x $m$ as well.  Taking a closer look at this case, we see:\n",
    "\n",
    "$\\mathbf B \\otimes  \\mathbf C = \\begin{bmatrix}\n",
    "b_{1,1}\\mathbf {11}^H & b_{1,2}\\mathbf {11}^H & \\cdots & b_{1,m-1}\\mathbf {11}^H & b_{1,m}\\mathbf {11}^H\\\\ \n",
    "b_{2,1}\\mathbf {11}^H & b_{2,2}\\mathbf {11}^H & \\cdots & b_{2,m-1}\\mathbf {11}^H & b_{2,m}\\mathbf {11}^H\\\\ \n",
    "\\vdots & \\vdots & \\ddots & \\vdots & \\vdots   \\\\ \n",
    "b_{m-1,1}\\mathbf {11}^H & b_{m-1,2}\\mathbf {11}^H & \\cdots & b_{m-1,m-1}\\mathbf {11}^H & b_{m-1,m}\\mathbf {11}^H\\\\ \n",
    "b_{m,1}\\mathbf {11}^H & b_{m,2}\\mathbf {11}^H & \\cdots & b_{m,m-1}\\mathbf {11}^H & b_{m,m}\\mathbf {11}^H\\\\ \n",
    "\\end{bmatrix} \\circ \\begin{bmatrix}\n",
    "\\mathbf C & \\mathbf C & \\cdots & \\mathbf C & \\mathbf C \\\\ \n",
    "\\mathbf C & \\mathbf C & \\cdots & \\mathbf C & \\mathbf C \\\\ \n",
    "\\vdots & \\vdots & \\ddots & \\vdots & \\vdots   \\\\ \n",
    "\\mathbf C & \\mathbf C & \\cdots & \\mathbf C & \\mathbf C \\\\ \n",
    "\\mathbf C & \\mathbf C & \\cdots & \\mathbf C & \\mathbf C \\\\ \n",
    "\\end{bmatrix}$\n",
    "\n",
    "\n",
    "The above matrix is $m^2$ x $m^2$.  Note that we can 'see' that the Hadamard Product is contained in the above.  Specifically,  \n",
    "\n",
    "$\\big(\\mathbf B \\circ \\mathbf C\\big)_{i,j} = \\mathbf e_i^H \\big(\\mathbf B \\otimes  \\mathbf C \\big)\\mathbf e_j  = \\big(\\mathbf B \\otimes  \\mathbf C \\big)_{i,j} $\n",
    "\n",
    "for $i,j \\in \\{0(m+1) + 1, 1(m+1)+1,  2(m+1)+1, ..., (m-2)(m+1)+1 , (m-1)(m+1)+1\\}$\n",
    "\n",
    "where $\\mathbf e_k$ is the standard basis vector, shown below.\n",
    "\n",
    "\n",
    "$\\mathbf I_{m^2} = \n",
    "\\bigg[\\begin{array}{c|c|c|c|c}\n",
    "\\mathbf e_1 & \\mathbf e_2 &\\cdots & \\mathbf e_{m^2 - 1} & \\mathbf e_{m^2}\n",
    "\\end{array}\\bigg]\n",
    "$   \n",
    "- - - -  \n",
    "**example:**  if $\\mathbf B$ and $\\mathbf C$ were both $3$ x $ 3$ matrices, then we would have \n",
    "for $i,j = \\{1, 5, 9\\}$\n",
    "- - - -  \n",
    "\n",
    "\n",
    "For avoidance of doubt, in the below, $\\mathbf x$ is a vector with $m^2$ entries, whereas $\\mathbf y$ is a vector with $m$ entries.  \n",
    "\n",
    "\n",
    "In the case where $\\mathbf B$ and $\\mathbf C$ are Hermitian, then we know \n",
    "\n",
    "$\\big(\\mathbf B \\otimes  \\mathbf C \\big)$ is Hermitian.  We may also verify by inspection that $\\big(\\mathbf B \\circ \\mathbf C\\big)$ is Hermitian (or by recognizing it is a principal submatrix of the Hermitian matrix $\\big(\\mathbf B \\otimes  \\mathbf C \\big)$  -- either way is ok).   \n",
    "\n",
    "We now use the useful fact that maximizing and minimizing quadratic forms, subject to a length constraint, gives the maximal and minimal eigenvalues of a Hermitian matrix.  \n",
    "\n",
    "**note: the LaTeX blocks stating the underlying optimization problem do not render properly on Github.  Viewing the notebook locally should fix this problem.** \n",
    "\n",
    "- - - - \n",
    "*The Kronecker Product optimization case (max)*  \n",
    "\n",
    "$\\begin{equation*}\n",
    "\\begin{aligned}\n",
    "& {\\text{maximize}}\n",
    "& & \\mathrm{\\mathbf x^H \\big(\\mathbf B \\otimes  \\mathbf C \\big)\\mathbf x  }\\\\\n",
    "& \\text{subject to}\n",
    "& &\\big \\Vert \\mathbf x\\big \\Vert_2^2 = 1 \\\\\n",
    "&{\\text{result}}\n",
    "&& \\mathrm{\\lambda_{max}\\big(\\mathbf B\\big)\\lambda_{max}\\big(\\mathbf C\\big)= \\lambda_{max}\\big(\\mathbf B \\otimes  \\mathbf C \\big) }\n",
    "\\end{aligned}\n",
    "\\end{equation*}$\n",
    "- - - - \n",
    "\n",
    "*and in the minimization case*  \n",
    "\n",
    "$\\begin{equation*}\n",
    "\\begin{aligned}\n",
    "& {\\text{minimize}}\n",
    "& & \\mathrm{\\mathbf x^H \\big(\\mathbf B \\otimes  \\mathbf C \\big)\\mathbf x} \\\\\n",
    "& \\text{subject to}\n",
    "& &\\big \\Vert \\mathbf x\\big \\Vert_2^2 = 1 \\\\\n",
    "&{\\text{result}}\n",
    "&& \\mathrm{\\lambda_{min}\\big(\\mathbf B\\big)\\lambda_{min}\\big(\\mathbf C\\big) = \\lambda_{min}\\big(\\mathbf B \\otimes  \\mathbf C \\big)}\n",
    "\\end{aligned}\n",
    "\\end{equation*}$\n",
    "- - - - \n",
    "\n",
    "*The Hadamard Product optimization case (max)*  \n",
    "\n",
    "$\\begin{equation*}\n",
    "\\begin{aligned}\n",
    "& {\\text{maximize}}\n",
    "& & \\mathrm{ \\mathbf y^H \\big(\\mathbf B \\circ \\mathbf C\\big) \\mathbf y = \\mathbf x^H \\big(\\mathbf B \\otimes  \\mathbf C \\big)\\mathbf x}\\\\\n",
    "& \\text{subject to}\n",
    "& &\\big \\Vert \\mathbf y\\big \\Vert_2^2 = 1 \\\\\n",
    "&&& \\big \\Vert \\mathbf x\\big \\Vert_2^2 = 1 \\\\\n",
    "&&& \\mathbf x = \\sum_{r=0}^{m-1} \\gamma_r \\mathbf e_{r(m+1)+1}\\\\\n",
    "&{\\text{result}}\n",
    "&& \\mathrm{\\lambda_{max}\\big(\\mathbf B \\circ  \\mathbf C \\big)}\n",
    "\\end{aligned}\n",
    "\\end{equation*}$\n",
    "- - - - \n",
    "\n",
    "Thus we see, \n",
    "\n",
    "$\\lambda_{max}\\big(\\mathbf B\\big)\\lambda_{max}\\big(\\mathbf C\\big) = \\lambda_{max}\\big(\\mathbf B \\otimes  \\mathbf C \\big)  \\geq   \\lambda_{max}\\big(\\mathbf B \\circ \\mathbf C\\big) $\n",
    "\n",
    "because the Kronecker Product maximization case is the same as the Hadamard Product Optimization, except the former has less constraints than the latter, and hence the 'payoff' from the Kronecker Product maximization must be at least as big as that from the Hadamard Product Optimization.  (Put differently, the Optimization from the Hadamard Product quadratic form is always a 'backup plan' for the Kronecker Product quadratic form optimization.)  \n",
    "\n",
    "- - - -  \n",
    "*A similar phenomenon exists in the minimization case:*   \n",
    "\n",
    "$\\begin{equation*}\n",
    "\\begin{aligned}\n",
    "& {\\text{minimize}}\n",
    "& & \\mathrm{\\mathbf y^H \\big(\\mathbf B \\circ \\mathbf C\\big) \\mathbf y = \\mathbf x^H \\big(\\mathbf B \\otimes  \\mathbf C \\big)\\mathbf x}\\\\\n",
    "& \\text{subject to}\n",
    "& &\\big \\Vert \\mathbf y\\big \\Vert_2^2 = 1 \\\\\n",
    "&&& \\big \\Vert \\mathbf x\\big \\Vert_2^2 = 1 \\\\\n",
    "&&& \\mathbf x = \\sum_{r=0}^{m-1} \\gamma_r \\mathbf e_{r(m+1)+1}\\\\\n",
    "&{\\text{result}}\n",
    "&& \\mathrm{\\lambda_{min}\\big(\\mathbf B \\circ  \\mathbf C \\big)}\n",
    "\\end{aligned}\n",
    "\\end{equation*}$\n",
    "- - - -  \n",
    "\n",
    "and we conclude that: \n",
    "\n",
    "$\\lambda_{min}\\big(\\mathbf B\\big)\\lambda_{min}\\big(\\mathbf C\\big) = \\lambda_{min}\\big(\\mathbf B \\otimes  \\mathbf C \\big)  \\leq \\lambda_{min}\\big(\\mathbf B \\circ \\mathbf C\\big) $\n",
    "\n",
    "again, because the optimization (i.e. minimization) case for the quadratic form given by the Kronecker Product is the same as that for the Hadamard Product quadratic form, except there are less constraints on the former.  Thus the optimization for the former can always be the the same as the latter, though it could also be \"better\".  (Crucially, the Hadamard case *cannot* be better than the Kronecker, since the Hadamard quadratic form optimization set of possibilities  is a proper subset of the configurations available to the Kronecker case.)  \n",
    "\n",
    "Note: if we want a more granular conclusion, we may use the fact that the Hadamard product is a principal submatrix of the Hermitian matrix created by the Kronecker product, and then recursively apply Cauchy eigenvalue interlacing. \n",
    "\n",
    "**extension:** \n",
    "\n",
    "If $\\mathbf B$ and $\\mathbf C$ are Hermitian Positive (semi)definite, then so must be $\\big(\\mathbf B \\circ \\mathbf C\\big)$\n",
    "\n",
    "We already know that $\\big(\\mathbf B \\circ \\mathbf C\\big)$ is Hermitian. In the positive definite case, all eigenvalues are real, and we have $0 \\lt \\lambda_{min}\\big(\\mathbf B\\big)$  and $0 \\lt \\lambda_{min}\\big(\\mathbf C\\big)$ thus   \n",
    "\n",
    "$0 \\lt \\lambda_{min}\\big(\\mathbf B\\big)\\lambda_{min}\\big(\\mathbf C\\big)  \\leq \\lambda_{min}\\big(\\mathbf B \\circ \\mathbf C\\big) $  \n",
    "\n",
    "In the positive semi-definite case we have $0 \\leq \\lambda_{min}\\big(\\mathbf B\\big)$ and $0 \\leq \\lambda_{min}\\big(\\mathbf C\\big)$ thus  \n",
    "\n",
    "$0 \\leq \\lambda_{min}\\big(\\mathbf B\\big)\\lambda_{min}\\big(\\mathbf C\\big)  \\leq \\lambda_{min}\\big(\\mathbf B \\circ \\mathbf C\\big) $\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "note: I suspect I could derive the Cauchy Eigenvalues Interlacing from a forward, then backward pass of optimizations here.   \n",
    "This is currently an open item.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kronecker sum\n",
    "\n",
    "for two square matrices $\\mathbf B \\in \\mathbb C^{m x m}$ and $\\mathbf C \\in \\mathbb C^{n x n}$, the Kronecker Sum is defined as\n",
    "\n",
    "\n",
    "$\\mathbf B \\oplus \\mathbf C = \\big(\\mathbf I_n \\otimes \\mathbf B\\big) + \\big(\\mathbf C \\otimes \\mathbf I_m\\big) $\n",
    "\n",
    "**remarks:**  \n",
    "$\\mathbf B \\oplus \\mathbf C \\neq \\mathbf C \\oplus \\mathbf B$ in general.  \n",
    "\n",
    "Also it seems that some other sources may 'flip around' the definition to be something like \n",
    "\n",
    "$\\big(\\mathbf B \\otimes \\mathbf I_n\\big) + \\big(\\mathbf I_m \\otimes \\mathbf C\\big)$\n",
    "\n",
    "which confuses things a bit. The takeaway is: be extra careful and check in on notation / definitions.  My understanding is the definition at the top of this cell is the most common one, for better or for worse.\n",
    "\n",
    "- - - - \n",
    "**Link to the Kronecker Product via the exponential map** \n",
    "\n",
    "note: reference pages 76 and 96 of Stillwell's 'Naive Lie Theory' for a derivation of the fact that \n",
    "\n",
    "$e^{\\mathbf X}e^{\\mathbf Y} = e^{\\mathbf X + \\mathbf Y}$, if $\\mathbf X \\mathbf Y= \\mathbf Y\\mathbf X$, i.e. if the two matrices commute. (Alternatively see the Lie Product Formula and its derivation under the majorization section in the \"fun with trace\" notebook.)    \n",
    "\n",
    "key idea:\n",
    "\n",
    "$\\big(\\mathbf C \\otimes \\mathbf I\\big) \\big(\\mathbf I \\otimes \\mathbf B\\big) = \\big(\\mathbf C \\otimes \\mathbf B \\big)  = \\big(\\mathbf I \\otimes \\mathbf B\\big)\\big(\\mathbf C \\otimes \\mathbf I\\big) $ \n",
    "\n",
    "i.e. we have commutativity where $\\mathbf X:= \\big(\\mathbf C \\otimes \\mathbf I\\big)$ and $\\mathbf Y:= \\big(\\mathbf I \\otimes \\mathbf B\\big)$, \n",
    "\n",
    "from here observe that \n",
    "\n",
    "$\\big(\\mathbf C\\otimes \\mathbf I\\big)^k = \\mathbf C^k \\otimes \\mathbf I^k = \\mathbf C^k \\otimes \\mathbf I$\n",
    "\n",
    "and the same phenomenon follows for $\\big(\\mathbf I\\otimes \\mathbf B\\big)$\n",
    "\n",
    "- - - -\n",
    "then from here use distributivity across addition\n",
    "\n",
    "i.e. at the very top of this posting we noted:\n",
    "\n",
    "$\\big(\\mathbf C \\otimes \\mathbf B \\big) + \\big(\\mathbf C \\otimes \\mathbf X \\big) = \\mathbf C  \\otimes  \\big( \\mathbf B + \\mathbf X \\big)$\n",
    "\n",
    "now  apply this with the power series underlying the exponential function\n",
    "- - - -\n",
    "\n",
    "$e^{\\big(\\mathbf C\\otimes \\mathbf I\\big)} = \\sum_{k=0}^{\\infty} \\frac{1}{k!}\\big(\\mathbf C\\otimes \\mathbf I\\big)^k = \\sum_{k=0}^{\\infty}  \\frac{1}{k!}\\big(\\mathbf C^k\\otimes \\mathbf I\\big)=\\sum_{k=0}^{\\infty}  \\big(\\frac{1}{k!}\\mathbf C^k\\otimes \\mathbf I\\big) = \\big(\\sum_{k=0}^{\\infty} \\frac{1}{k!}\\mathbf C^k\\big)\\otimes \\mathbf I= e^{\\mathbf C} \\otimes \\mathbf I $\n",
    "\n",
    "and the same phenomenon with $\\big(\\mathbf I\\otimes \\mathbf B\\big)$\n",
    "\n",
    "hence we have \n",
    "\n",
    "\n",
    "$e^{\\mathbf C\\otimes \\mathbf I}e^{\\mathbf I \\otimes \\mathbf B } = \\big(e^{\\mathbf C} \\otimes \\mathbf I\\big) \\big(\\mathbf I \\otimes  e^{\\mathbf B}\\big) = e^{\\mathbf C}  \\otimes e^{\\mathbf B }$\n",
    "\n",
    "\n",
    "\n",
    "so we may re-write this as:  \n",
    "\n",
    "$e^{\\mathbf C}  \\otimes e^{\\mathbf B }  = e^{(\\mathbf C \\otimes \\mathbf I)} e^{(\\mathbf I \\otimes \\mathbf B)} = e^{ (\\mathbf C \\otimes \\mathbf I)+(\\mathbf I \\otimes \\mathbf B)}=e^{(\\mathbf I \\otimes \\mathbf B) + (\\mathbf C \\otimes \\mathbf I)}  = e^{\\mathbf B \\oplus \\mathbf C}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*remark:*\n",
    "The above generalizes easily to higher dimensional Kronecker sums. E.g.  \n",
    "\n",
    "$\\big(\\mathbf C\\otimes \\mathbf I\\otimes \\mathbf I\\big)^k = \\mathbf C^k \\otimes \\mathbf I^k \\otimes \\mathbf I^k = \\mathbf C^k \\otimes \\mathbf I \\otimes \\mathbf I$  \n",
    "\n",
    "and the obvious holds for the 'flipped arrangement', we should consider the below intermediate cyclic permutation though:  \n",
    "\n",
    "$\\Big(\\mathbf I\\otimes \\mathbf C\\otimes  \\mathbf I\\Big)^2 $  \n",
    "$= \\Big(\\mathbf I\\otimes \\mathbf C\\otimes  \\mathbf I\\Big)\\Big(\\mathbf I\\otimes \\mathbf C\\otimes  \\mathbf I\\big) $   \n",
    "$= \\Big(\\mathbf I\\otimes \\big(\\mathbf C\\otimes  \\mathbf I\\big)\\Big) \\Big(\\mathbf I\\otimes \\big(\\mathbf C\\otimes  \\mathbf I\\big)\\Big) $   \n",
    "$= \\mathbf I\\otimes \\big(\\mathbf C\\otimes  \\mathbf I\\big)^2$   \n",
    "$= \\mathbf I\\otimes \\mathbf C^2\\otimes  \\mathbf I$   \n",
    "and the result for natural number $k$ follows by induction.  \n",
    "\n",
    "This tells us that \n",
    "\n",
    "$e^{\\big(\\mathbf C\\otimes \\mathbf I\\otimes \\mathbf I\\big)} = e^{\\mathbf C} \\otimes \\mathbf I\\otimes \\mathbf I$\n",
    "and the result holds for any cyclic permutation of the above Kronecker products  \n",
    "\n",
    "and so  \n",
    "\n",
    "$e^{\\mathbf C\\otimes \\mathbf I \\otimes \\mathbf I}e^{\\mathbf I \\otimes \\mathbf B \\otimes \\mathbf I}e^{\\mathbf I \\otimes \\mathbf I\\otimes \\mathbf A} = \\big(e^{\\mathbf C} \\otimes \\mathbf I \\otimes \\mathbf I \\big) \\big(\\mathbf I \\otimes  e^{\\mathbf B}\\otimes \\mathbf I\\big)\\big(\\mathbf I \\otimes \\mathbf I \\otimes  e^{\\mathbf A}\\big) = e^{\\mathbf C}  \\otimes e^{\\mathbf B }\\otimes e^{\\mathbf A }$  \n",
    "\n",
    "and this holds for any permutation, e.g.   \n",
    "$\\big(\\mathbf I \\otimes  e^{\\mathbf B}\\otimes \\mathbf I\\big)\\big(\\mathbf I \\otimes \\mathbf I \\otimes  e^{\\mathbf A}\\big)\\big(e^{\\mathbf C} \\otimes \\mathbf I \\otimes \\mathbf I \\big) = e^{\\mathbf C}  \\otimes e^{\\mathbf B }\\otimes e^{\\mathbf A }$  \n",
    "\n",
    "hence  \n",
    "\n",
    "$ e^{\\mathbf C}  \\otimes e^{\\mathbf B }\\otimes e^{\\mathbf A }= e^{\\mathbf C\\otimes \\mathbf I \\otimes \\mathbf I}e^{\\mathbf I \\otimes \\mathbf B \\otimes \\mathbf I}e^{\\mathbf A \\otimes \\mathbf I \\otimes \\mathbf I} = e^{\\mathbf C\\otimes \\mathbf I \\otimes \\mathbf I+ \\mathbf I \\otimes \\mathbf B \\otimes \\mathbf I + \\mathbf I \\otimes \\mathbf I\\otimes \\mathbf A}$  \n",
    "\n",
    "In a very special case of interest, where we know the eigenvalues of $\\mathbf A$, $\\mathbf B$ and $\\mathbf C$ are real (i.e. specialized to Hermitian matrices), the exponential map is real and invertible.  The Left hand side shows that the eigenvalues of all three exponentiated matrices multiply which corresponds to addition in the exponential domain on the right hand side.  This immediately tells us that after applying the real logarithm to the RHS, the eigenvalues must add when doing 3 degree (or higher, though a little work is needed) Kronecker sums.  Note: this sort of result is quite common, e.g. in Laplacians for discretized differential equations.  And because the exponential map (or its inverse) applied to a diagonalizable matrix (with real eigenvalues) does not change the eigenvectors, just the eigenvalues, we know based on earlier work on Kronecker products, exactly how the eigenvectors will look -- they multiply component-wise.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- - - -\n",
    "\n",
    "**important special case: suppose** $\\mathbf B$ **and** $\\mathbf C$ **are both upper triangular**\n",
    "\n",
    "$\\mathbf I \\otimes  \\mathbf B = \\begin{bmatrix}\n",
    "\\mathbf {11}^H & \\mathbf {00}^H & \\cdots & \\mathbf {00}^H & \\mathbf {00}^H\\\\ \n",
    "\\mathbf {00}^H & \\mathbf {11}^H & \\cdots & \\mathbf {00}^H & \\mathbf {00}^H\\\\ \n",
    "\\vdots & \\vdots & \\ddots & \\vdots & \\vdots   \\\\ \n",
    "\\mathbf {00}^H & \\mathbf {00}^H & \\cdots & \\mathbf {11}^H & \\mathbf {00}^H\\\\ \n",
    "\\mathbf {00}^H & \\mathbf {00}^H & \\cdots & \\mathbf {00}^H & \\mathbf {11}^H\\\\ \n",
    "\\end{bmatrix} \\circ \\begin{bmatrix}\n",
    "\\mathbf B & \\mathbf B & \\cdots & \\mathbf B & \\mathbf B \\\\ \n",
    "\\mathbf B & \\mathbf B & \\cdots & \\mathbf B & \\mathbf B \\\\ \n",
    "\\vdots & \\vdots & \\ddots & \\vdots & \\vdots   \\\\ \n",
    "\\mathbf B & \\mathbf B & \\cdots & \\mathbf B & \\mathbf B \\\\ \n",
    "\\mathbf B & \\mathbf B & \\cdots & \\mathbf B & \\mathbf B \\\\ \n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "\\mathbf {B} & \\mathbf {00}^H & \\cdots & \\mathbf {00}^H & \\mathbf {00}^H\\\\ \n",
    "\\mathbf {00}^H & \\mathbf {B} & \\cdots & \\mathbf {00}^H & \\mathbf {00}^H\\\\ \n",
    "\\vdots & \\vdots & \\ddots & \\vdots & \\vdots   \\\\ \n",
    "\\mathbf {00}^H & \\mathbf {00}^H & \\cdots & \\mathbf {B} & \\mathbf {00}^H\\\\ \n",
    "\\mathbf {00}^H & \\mathbf {00}^H & \\cdots & \\mathbf {00}^H & \\mathbf {B}\\\\ \n",
    "\\end{bmatrix} $\n",
    "\n",
    "\n",
    "\n",
    "$\\mathbf C \\otimes  \\mathbf I = \\begin{bmatrix}\n",
    "c_{1,1}\\mathbf {11}^H & c_{1,2}\\mathbf {11}^H & \\cdots & c_{1,n-1}\\mathbf {11}^H & c_{1,n}\\mathbf {11}^H\\\\ \n",
    "\\mathbf {00}^H & c_{2,2}\\mathbf {11}^H & \\cdots & c_{2,n-1}\\mathbf {11}^H & c_{2,n}\\mathbf {11}^H\\\\ \n",
    "\\vdots & \\vdots & \\ddots & \\vdots & \\vdots   \\\\ \n",
    "\\mathbf {00}^H & \\mathbf {00}^H & \\cdots & c_{n-1,n-1}\\mathbf {11}^H & c_{n-1,n}\\mathbf {11}^H\\\\ \n",
    "\\mathbf {00}^H & \\mathbf {00}^H & \\cdots & \\mathbf {00}^H & c_{n,n}\\mathbf {11}^H\\\\ \n",
    "\\end{bmatrix} \\circ \\begin{bmatrix}\n",
    "\\mathbf I & \\mathbf I & \\cdots & \\mathbf I & \\mathbf I \\\\ \n",
    "\\mathbf I & \\mathbf I & \\cdots & \\mathbf I & \\mathbf I \\\\ \n",
    "\\vdots & \\vdots & \\ddots & \\vdots & \\vdots   \\\\ \n",
    "\\mathbf I & \\mathbf I & \\cdots & \\mathbf I & \\mathbf I \\\\ \n",
    "\\mathbf I & \\mathbf I & \\cdots & \\mathbf I & \\mathbf I \\\\ \n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "c_{1,1}\\mathbf {I} & c_{1,2}\\mathbf {I} & \\cdots & c_{1,n-1}\\mathbf {I} & c_{1,n}\\mathbf {I}\\\\ \n",
    "\\mathbf {00}^H & c_{2,2}\\mathbf {I} & \\cdots & c_{2,n-1}\\mathbf {I} & c_{2,n}\\mathbf {I}\\\\ \n",
    "\\vdots & \\vdots & \\ddots & \\vdots & \\vdots   \\\\ \n",
    "\\mathbf {00}^H & \\mathbf {00}^H & \\cdots & c_{n-1,n-1}\\mathbf {I} & c_{n-1,n}\\mathbf {I}\\\\ \n",
    "\\mathbf {00}^H & \\mathbf {00}^H & \\cdots & \\mathbf {00}^H & c_{n,n}\\mathbf {I}\\\\ \n",
    "\\end{bmatrix} $\n",
    "\n",
    "and we verify that both $\\big(\\mathbf I \\otimes  \\mathbf B \\big)$ and  $\\big(\\mathbf C \\otimes  \\mathbf I \\big)$ are upper triangular. \n",
    "\n",
    "Hence we conclude $\\big(\\mathbf B \\oplus \\mathbf C \\big)$ is upper triangular if both $\\mathbf B$ and $\\mathbf C$ are upper triangular, because that the sum of two upper triangular matrices (with legal dimensions) is an upper triangular matrix.  \n",
    "\n",
    "**extension** if $\\mathbf B$ and $\\mathbf C$ are upper triangular, their eigenvalues are on their diagonal.  We examine the diagonal of $\\big(\\mathbf B \\oplus \\mathbf C \\big)$  and see that it has \n",
    "\n",
    "$c_{1,1} + b_{1,1}, c_{1,1} + b_{2,2}, ..., c_{1,1} + b_{m,m}, c_{2,2} + b_{1,1}, c_{2,2} + b_{2,2}, ..., c_{2,2} + b_{m,m}, ..., c_{n,n} + b_{1,1}, c_{n,n} + b_{2,2}, ..., c_{n,n} + b_{m,m}$\n",
    "\n",
    "or equivalently, where $s_i$ is an eigenvalue for $\\mathbf B$ and $\\lambda_i$ is an eigenvalue for $\\mathbf C$\n",
    "\n",
    "$\\lambda_{1} + s_{1}, \\lambda_{1} + s_{2}, ..., \\lambda_{1} + s_{m}, \\lambda_{2} + s_{1}, \\lambda_{2} + s_{2}, ..., \\lambda_{2} + s_{m}, ..., \\lambda_{n} + s_{1}, \\lambda_{n} + s_{2}, ..., \\lambda_{n} + s_m$\n",
    "\n",
    "The $mn$ eigenvalues above, should feel similar to the regular Kronecker Product setup, except they are being added, not multiplied here.  \n",
    "\n",
    "note if both matrices are the same dimension, $n$ x $n$, we may be able to more easily visualize these eigenvalues by collecting them in two matrices.  The $n$ eigenvalues of $\\mathbf B$ are contained in a vector $\\mathbf {s}$ and the eigenvalues of $\\mathbf C$ are contained in the vector $\\mathbf {\\Lambda 1}$, and $\\mathbf P$ is the full cycle permutation matrix (alternatively the Companion matrix associated with $x^n - 1$), then all $m^2$ eigenvalues are contained in cells of the matrix below.\n",
    "\n",
    "(Thus the eigenvalues are shown somewhat differently, below -- hopefully more clearly.)\n",
    "\n",
    "$\\Big(\\bigg[\\begin{array}{c|c|c|c|c}\n",
    "\\mathbf {s} & \\mathbf {s} & \\mathbf {s} &\\cdots & \\mathbf {s}\n",
    "\\end{array}\\bigg] + \\bigg[\\begin{array}{c|c|c|c|c}\n",
    "\\mathbf P^0 \\mathbf {\\Lambda 1} & \\mathbf P^1\\mathbf {\\Lambda1} & \\mathbf P^2\\mathbf {\\Lambda 1} &\\cdots & \\mathbf P^{n-1}\\mathbf {\\Lambda1}\n",
    "\\end{array}\\bigg]\\Big) $\n",
    "\n",
    "$=  \\begin{bmatrix}\n",
    "s_1 & s_{1} & s_{1} & \\dots & s_1 & s_1 \\\\ \n",
    "s_2 & s_2 & s_{2} & \\dots & s_2 & s_2 \\\\ \n",
    "s_3 & s_3 & s_3 & \\dots & s_3 & s_3 \\\\\n",
    "\\vdots & \\vdots  & \\vdots &\\ddots & \\vdots & \\vdots\\\\ \n",
    "s_{n-1} & s_{n-1} & s_{n-1} & \\dots & s_{n-1}  & s_{n-1} \\\\ \n",
    "s_{n} & s_{n}  & s_{n} & \\dots & s_n &  s_n\n",
    "\\end{bmatrix} + \\begin{bmatrix}\n",
    "\\lambda_1 & \\lambda_{n} & \\lambda_{n-1} & \\dots & \\lambda_3 & \\lambda_2 \\\\ \n",
    "\\lambda_2 & \\lambda_1 & \\lambda_{n} & \\dots & \\lambda_4 & \\lambda_3 \\\\ \n",
    "\\lambda_3 & \\lambda_2 & \\lambda_1 & \\dots & \\lambda_5 & \\lambda_4 \\\\\n",
    "\\vdots & \\vdots  & \\vdots &\\ddots & \\vdots & \\vdots\\\\ \n",
    "\\lambda_{n-1} & \\lambda_{n-2} & \\lambda_{n-3} & \\dots & \\lambda_1  & \\lambda_{n} \\\\ \n",
    "\\lambda_{n} & \\lambda_{n-1}  & \\lambda_{n-2} & \\dots & \\lambda_2 &  \\lambda_1\n",
    "\\end{bmatrix}$\n",
    "\n",
    "\n",
    "$= \\begin{bmatrix}\n",
    "s_1 + \\lambda_1 & s_1 + \\lambda_{n} & s_1 + \\lambda_{n-1} & \\dots & s_1 + \\lambda_3 & s_1 + \\lambda_2 \\\\ \n",
    "s_2 + \\lambda_2 & s_2 + \\lambda_1 & s_2 + \\lambda_{n} & \\dots & s_2 + \\lambda_4 & s_2 +\\lambda_3\\\\ \n",
    "s_3 + \\lambda_3 & s_3 + \\lambda_2 & s_3 + \\lambda_1 & \\dots & s_3 + \\lambda_5 & s_3 + \\lambda_4 \\\\\n",
    "\\vdots & \\vdots  & \\vdots &\\ddots & \\vdots & \\vdots\\\\ \n",
    "s_{n-1} + \\lambda_{n-1} & s_{n-1}+ \\lambda_{n-2} & s_{n-1} + \\lambda_{n-3} & \\dots & s_{n-1} +\\lambda_1  & s_{n-1} + \\lambda_{n} \\\\ \n",
    "s_n + \\lambda_{n} & s_{n} + \\lambda_{n-1}  & s_{n} + \\lambda_{n-2} & \\dots & s_{n} + \\lambda_2 &  s_{n} + \\lambda_1\n",
    "\\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kronecker Sum - Schur form Implications\n",
    "\n",
    "This one has important and powerful results, but was in fact quite tricky / subtle.  I had to consult the official result and then 'unpack' it into many different lines.  \n",
    "\n",
    "\n",
    "for two square matrices $\\mathbf B \\in \\mathbb C^{m x m}$ and $\\mathbf C \\in \\mathbb C^{n x n}$\n",
    "\n",
    "where $\\mathbf B = \\mathbf{VRV}^H$ and $\\mathbf C = \\mathbf {QTQ}^H$\n",
    "\n",
    "\n",
    "$\\mathbf B \\oplus \\mathbf C = \\big(\\mathbf I_n \\otimes \\mathbf B\\big) + \\big(\\mathbf C \\otimes \\mathbf I_m\\big) = \\big(\\mathbf I_n \\otimes \\mathbf{VRV}^H \\big) + \\big(\\mathbf {QTQ}^H \\otimes \\mathbf I_m \\big)    = \\big(\\mathbf I_n\\mathbf I_n\\mathbf I_n \\otimes \\mathbf{VRV}^H \\big) + \\big(\\mathbf {QTQ}^H \\otimes \\mathbf I_m\\mathbf I_m\\mathbf I_m \\big) $\n",
    "\n",
    "$\\mathbf B \\oplus \\mathbf C = \\big(\\mathbf I_n \\otimes \\mathbf{V} \\big)\\big(\\mathbf I_n \\otimes \\mathbf{R} \\big)\\big(\\mathbf I_n \\otimes \\mathbf{V}^H \\big) + \\big(\\mathbf {Q} \\otimes \\mathbf I_m \\big)\\big(\\mathbf {T} \\otimes \\mathbf I_m \\big)\\big(\\mathbf {Q}^H \\otimes \\mathbf I_m \\big)$\n",
    "\n",
    "$\\mathbf B \\oplus \\mathbf C = \\big(\\mathbf Q \\otimes \\mathbf{V} \\big)\\big(\\mathbf I_n \\otimes \\mathbf{R} \\big)\\big(\\mathbf Q^H \\otimes \\mathbf{V}^H \\big) + \\big(\\mathbf {Q} \\otimes \\mathbf V \\big)\\big(\\mathbf {T} \\otimes \\mathbf I_m \\big)\\big(\\mathbf {Q}^H \\otimes \\mathbf V^H \\big)$\n",
    "\n",
    "\n",
    "$\\mathbf B \\oplus \\mathbf C = \\big(\\mathbf Q \\otimes \\mathbf{V} \\big)\\Big(\\big(\\mathbf I_n \\otimes \\mathbf{R} \\big) +  \\big(\\mathbf {T} \\otimes \\mathbf I_m \\big)\\Big)\\big(\\mathbf {Q}^H \\otimes \\mathbf V^H \\big)$\n",
    "\n",
    "$\\mathbf B \\oplus \\mathbf C = \\big(\\mathbf Q \\otimes \\mathbf{V} \\big)\\big( \\mathbf{R} \\oplus \\mathbf {T}\\big)\\big(\\mathbf {Q} \\otimes \\mathbf V \\big)^H$\n",
    "\n",
    "\n",
    "where we reference earlier, (just under the line \"Many other results follow simply from the multiplication rule\")  which noted that $\\big(\\mathbf Q \\otimes \\mathbf{V} \\big)$ is a unitary matrix.  \n",
    "\n",
    "Further, per the special case worked through in the above cell, $\\big( \\mathbf{R} \\oplus \\mathbf {T}\\big)$ is itself upper triangular, and there are $mn$ eigenvalues that comes about by adding each eigenvalue from $\\mathbf B$ to each eigenvalue of $\\mathbf C$.  \n",
    "\n",
    "Thus we have the Schur Form of $\\big(\\mathbf B \\oplus \\mathbf C\\big)$ which nicely gives us the eigenvalues associated with such a matrix.\n",
    "\n",
    "**remark:**  \n",
    "for diagonalizable matrices \n",
    "$\\mathbf B = \\mathbf{VDV}^{-1}$ and $\\mathbf C = \\mathbf {Q\\Lambda Q}^{-1}$  \n",
    "\n",
    "we re-run the above argument essentially verbatim and get  \n",
    "$\\mathbf B \\oplus \\mathbf C = \\big(\\mathbf Q \\otimes \\mathbf{V} \\big)\\big( \\mathbf{D} \\oplus \\mathbf {\\Lambda}\\big)\\big(\\mathbf {Q} \\otimes \\mathbf V \\big)^{-1}$  \n",
    "\n",
    "recalling that  \n",
    "$\\big(\\mathbf {Q}^{-1} \\otimes \\mathbf V^{-1} \\big) = \\big(\\mathbf {Q} \\otimes \\mathbf V \\big)^{-1}$   \n",
    "and we know that $\\big( \\mathbf{D} \\oplus \\mathbf {\\Lambda}\\big)$ is a diagonal matrix \n",
    "(i.e. our earlier work tells us that the kronecker sum of two upper triangular matrices is upper triangular, and by transposing the argument, the kronecker sum of two lower triangular matrices is lower triangular.  Since a (square) diagonal matrix is defined as one that is both upper triangular and lower triangular, we have that the kronecker sum of two diagonal matrices results in a matrix that is both upper triangular and lower triangular, and hence diagonal.)    \n",
    "\n",
    "*important special case:*  \n",
    "(note this is a problem in Artin chapter 4)  \n",
    "when $\\mathbf B$ is diagonalizable and  \n",
    "if $\\mathbf C:= -\\mathbf B$ or even better (for Sylvester equation, which is discussed in the box below) let us consider $\\mathbf C:= -\\mathbf B^T$    \n",
    "\n",
    "this means that the resulting Kronecker sum has rank $\\leq n^2 - n$, because said matrix is $\\text{n}^2 \\text{ x n}^2$  and its rank is given by the number of non-zero eigenvalues (i.e. the rank of the diagonal matrix) and the diagonal matrix has at least $\\text{n}$ zeros (and this minimum is achieved when all eigenvalues are distinct so each one has exactly one 'match').  We may also recall that the rank of the $\\text{n}^2 \\text{ x n}^2$ matrix is given by the largest non-zero minor.  A density argument tells us that this rank upper bound holds even if $\\mathbf B$ is defective.  I.e. suppose for contradiction that we find a maximally sized non-zero minor that is $\\text{d x d}$ with $d \\gt n^2 - n$.  We start by unitarily triangularizing the matrix-- since similarity transformations do not change rank some $\\text{d x d}$ minor that is non-zero still exists by assumption -- so we proceed by **assuming WLOG that** $\\mathbf B$ **is upper triangular** and the maximal sized non-zero minor of $\\mathbf B \\oplus \\mathbf B$ is $\\text{d x d}$ with value $c \\neq 0$.  Minors are determinants which are polynomials in the entries of matrices, and hence vary continuously with their entries.  This implies there is some $\\delta \\gt 0$ neighborhood where we may perturb any of the entries of the matrix $\\mathbf B \\oplus \\mathbf B^T$ and change this minor by an arbitrarily small amount, say $\\epsilon \\lt \\frac{\\vert c \\vert}{3}$.  This means that minor does not become zero anywhere in this neighborhood, and as a result the rank of $\\mathbf B$ cannot decrease in this neighborhood (note: we are actually silent on whether or not rank may *increase* in this neighborhood as larger dimension minors that were zero, could become non-zero on perturbation).  \n",
    "\n",
    "so select  \n",
    "$\\mathbf Z:= \\big(\\mathbf B + \\mathbf \\Lambda\\big)$  \n",
    "\n",
    "\n",
    "$\\Big \\Vert \\mathbf B \\oplus \\mathbf B^T - \\mathbf Z \\oplus \\mathbf Z^T \\Big \\Vert_F = \\Big \\Vert \\mathbf \\Lambda \\oplus \\mathbf \\Lambda \\Big \\Vert_F \\lt \\delta$  \n",
    "\n",
    "where $\\mathbf \\Lambda$ is a diagonal matrix with rescaled nth roots of unity constructed as in \"Cayley_Hamilton_density_argument.ipynb\" (i.e. rescale by positive number $\\gamma$ and ensure it is both less than the minimum distinct eigenvalue distance as in that writeup, and that $\\gamma \\lt \\frac{\\delta}{n^2}$)  \n",
    "\n",
    "but then $\\mathbf Z$ is diagonalizable (all of its eigenvalues are distinct), so by the above argument  \n",
    "$\\text{rank}\\big(\\mathbf Z \\oplus \\mathbf Z^T\\big) \\leq n^2 - n$   \n",
    "and yet it has a $\\text{d x d}$ minor whose magnitude is  \n",
    "$\\gt \\frac{2 \\vert c \\vert}{3} \\gt 0$ which implies  \n",
    "$\\text{rank}\\big(\\mathbf Z \\oplus \\mathbf Z^T\\big) \\gt n^2 - n$ which is a contradiction.  \n",
    "\n",
    "Thus it must be the case that  \n",
    "$\\text{rank}\\big(\\mathbf B \\oplus \\mathbf B^T\\big) \\leq n^2 - n$ even for defective matrices.   \n",
    "\n",
    "*note:*  \n",
    "another way of estimating the upper bound for the rank in the below Sylvester equation when we are considering this special case, is to consider  \n",
    "\n",
    "$T\\big(\\mathbf X\\big) = \\mathbf B \\mathbf X - \\mathbf X \\mathbf B$  \n",
    "where $T$ is a (finite dimensional) linear transformation \n",
    "\n",
    "our Kronecker sum formulation is direct and gets to the heart of the matter.  However for the case of diagonalizable $\\mathbf B$ we can also get to the heart of the matter by using rank-nullity and showing that the nullspace of this linear transformation has dimension $\\geq n$. The approach is to directly make use of diagonalizability  \n",
    "\n",
    "if $\\mathbf B = \\mathbf S\\mathbf D \\mathbf S^{-1}$  \n",
    "\n",
    "then when we consider   \n",
    "\n",
    "$\\mathbf X^{(k)} = \\mathbf S\\mathbf e_k \\mathbf e_k^T \\mathbf S^{-1}$  \n",
    "(with the kth standard basis vector in the above)\n",
    "\n",
    "we have  \n",
    "$T\\big(\\mathbf X^{(k)}\\big) = \\mathbf B \\mathbf X^{(k)} - \\mathbf X^{(k)} \\mathbf B = \\mathbf S\\mathbf D \\mathbf S^{-1}\\mathbf S\\mathbf e_k \\mathbf e_k^T \\mathbf S^{-1} -  \\mathbf S\\mathbf e_k \\mathbf e_k^T \\mathbf S^{-1}\\mathbf S\\mathbf D \\mathbf S^{-1} $  $=\\mathbf S\\big(\\mathbf e_k \\mathbf e_k^T \\mathbf D - \\mathbf e_k \\mathbf e_k^T \\mathbf D\\big)\\mathbf S^{-1}= \\mathbf S\\big(\\mathbf 0\\big)\\mathbf S^{-1}=\\mathbf 0$  \n",
    "where we observe that $\\mathbf e_k \\mathbf e_k^T$ and $\\mathbf D$ commute because they are both diagonal matrices  \n",
    "\n",
    "we can also verify the linear independence of $\\mathbf X^{(k)}$ by setting up a linear relation  \n",
    "\n",
    "$\\sum_{k=1}^n \\gamma_k \\mathbf X^{(k)}=\\mathbf 0$  \n",
    "and determine that it must the case that each $\\gamma_k =0$, by left multiplying each side by $\\mathbf S^{-1}$ and right multiplying each side by $\\mathbf S$ gives us \n",
    "\n",
    "$\\sum_{k=1}^n \\gamma_k \\mathbf S^{-1}\\mathbf X^{(k)}\\mathbf S =\\sum_{k=1}^n \\gamma_k \\mathbf e_k\\mathbf e_k^T = \\begin{bmatrix}\n",
    "\\gamma_1 & 0& \\cdots& 0 \\\\ \n",
    "0 & \\gamma_2&  \\cdots& 0\\\\ \n",
    "\\vdots & \\vdots& \\ddots&  \\vdots\\\\ \n",
    "0& 0&  \\cdots& \\gamma_n \n",
    "\\end{bmatrix}=  \\mathbf 0$\n",
    "\n",
    "and hence we see that each $\\gamma_k=0$, confirming that each $\\mathbf X^{(k)}$ is linearly independent and our linear operator $T$ has nullspace dimension $\\geq \\text{n}$.  \n",
    "\n",
    "To lift this result from the diagonalizable $\\mathbf B$ to the defective case via a density argument would require something akin to the above argument, either either explicitly using the above machinery of kronecker sums, or perhaps abstractly noting that $T$ is a linear operator in a vector space that is dimension $n^2$  (justification: consider matrices of the form $\\mathbf e_i \\mathbf e_j^H$ for $i,j \\in\\{1,2,..., n\\}$, where these generate, via matrix addition, any $\\mathbf X$ -- i.e. $T\\big(\\mathbf X\\big) = T\\big(\\sum_{i=1}^n\\sum_{j=1}^n \\mathbf e_i \\mathbf e_j^H\\big) =  \\sum_{i=1}^n\\sum_{j=1}^n T\\big(\\mathbf e_i \\mathbf e_j^H\\big)$ via linearity, and by inspection the $\\mathbf e_i \\mathbf e_j^H$ are mutually independent), and based on Theorem ___ in Artin, we know that after choosing an arbitrary basis we may represent this finite dimensional linear operator with matrix vector multiplication, and recall that rank is invariant to choice of basis, and hence our earlier minor + continuity argument may be applied abstractly.  \n",
    "\n",
    "*in what amounts to essentially the same thing*  \n",
    "when considering  \n",
    "$T\\big(\\mathbf X\\big) = \\mathbf B \\mathbf X - \\mathbf X \\mathbf B$  \n",
    "\n",
    "let the right eigenvectors of $\\mathbf B$ be $\\mathbf v_k$ and the left eigenvectors of $\\mathbf N$ be given by $\\mathbf z_k^*$ i.e. $\\mathbf z_k^* \\mathbf N = \\lambda_k \\mathbf z_k^*$.  In the case of $\\mathbf B$ being diagonalizable we can see   \n",
    "\n",
    "$T\\big(\\mathbf X\\big) = T\\big(\\mathbf v_j\\mathbf z_i^*\\big) = \\mathbf B \\mathbf v_j\\mathbf z_i^* - \\mathbf v_j\\mathbf z_i^* \\mathbf B = \\lambda_j \\mathbf v_j\\mathbf z_i^* - \\lambda_i\\mathbf v_j\\mathbf z_i^* = (\\lambda_i - \\lambda_j)\\mathbf v_j\\mathbf z_i^* $  \n",
    "\n",
    "for $i,j \\in \\{1,2,...,n\\}$  \n",
    "\n",
    "which gives us the spectrum in the case of non-defective $\\mathbf B$, without the use of kronecker products or sums.  \n",
    "\n",
    "note that when we consider \n",
    "$T\\big(\\mathbf X\\big) := \\mathbf A \\mathbf X \\mathbf B$  with all matrices square, and right eigenvectors of $\\mathbf A$ as $\\mathbf v_j$ and left eigenvectors of $\\mathbf B$ as $\\mathbf z_k^*$  \n",
    "\n",
    "then  \n",
    "$T\\big(\\mathbf X\\big) = T\\big(\\mathbf v_j\\mathbf z_i^*\\big)= \\mathbf A \\mathbf v_j\\mathbf z_i^* \\mathbf B = \\big(\\mathbf A \\mathbf v_j\\big)\\big(\\mathbf z_i^* \\mathbf B\\big) = \\lambda_j\\lambda_i \\mathbf v_j \\mathbf z_i^*$  \n",
    "\n",
    "which gives us the spectral insights here with out the use of kronecker products or vec operator, at least in the case of non-defective matrices.  (note: this particular instance extension is treated below with kronecker machinery in the section \"more on the vec operator\".)  And again, making use of minors, this time to relate to coefficients of the characteristic polynomial we should be able to abstractly relate this to the defective case via a density argument.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sylvester equation\n",
    "\n",
    "where for again $\\mathbf B \\in \\mathbb C^{m x m}$ and $\\mathbf C \\in \\mathbb C^{n x n}$,\n",
    "\n",
    "$\\mathbf B \\mathbf X + \\mathbf X \\mathbf C = \\mathbf Y$\n",
    "\n",
    "can be re-written as\n",
    "\n",
    "$\\big(\\mathbf B \\oplus \\mathbf C^T\\big)\\text{vec}\\big(\\mathbf X\\big)  =\\Big( \\big(\\mathbf I \\otimes \\mathbf B\\big) + \\big(\\mathbf C^T \\otimes \\mathbf I\\big)\\Big)\\text{vec}\\big(\\mathbf X\\big) = \\text{vec}\\big(\\mathbf Y\\big)$\n",
    "- - - - \n",
    "note the appearance of $\\mathbf C^T$ **not** $\\mathbf C^H$, as we are re-organizing the arrangement of entries of a matrix -- not explicitly dealing with inner products, and hence complex conjugation has nothing to do with the manipulations.\n",
    "\n",
    "also recall how the $vec$ operator works, for example on $\\mathbf X$  \n",
    "$\\mathbf X = \\bigg[\\begin{array}{c|c|c|c|c}\n",
    "\\mathbf x_1 & \\mathbf x_2 &\\cdots & \\mathbf x_{n-1} & \\mathbf x_n\\end{array}\\bigg]$\n",
    "\n",
    "$\\text{vec}\\big(\\mathbf X\\big)  = \\begin{bmatrix}\n",
    "\\mathbf x_1 \\\\ \n",
    "\\mathbf x_2\\\\ \n",
    "\\vdots \\\\ \n",
    "\\mathbf x_{n-1}\\\\ \n",
    "\\mathbf x_n\n",
    "\\end{bmatrix}$\n",
    "\n",
    "i.e. it flattens the matrix $\\mathbf X$ into a vector by stacking the columns of $\\mathbf X$ on top of each other.\n",
    "- - - - \n",
    "\n",
    "revisiting our equation, $\\mathbf B \\mathbf X + \\mathbf X \\mathbf C = \\mathbf Y$,  and **looking at column** $k$ **of both sides**, we have \n",
    "\n",
    "$\\big(\\mathbf B \\mathbf x_k\\big) + \\big(c_{1,k}\\mathbf x_1  + c_{2,k} \\mathbf x_2 + ... +  c_{n-1,k}\\mathbf x_{n-1} +c_{n,k}\\mathbf x_n \\big)  = \\big(\\mathbf B \\mathbf x_k\\big) + \\big(c_{1,k}\\mathbf I\\mathbf x_1  + c_{2,k} \\mathbf I \\mathbf x_2 + ...+  c_{n-1,k}\\mathbf I \\mathbf x_{n-1} + c_{n,k}\\mathbf I \\mathbf x_n \\big)  = \\mathbf y_k$\n",
    "\n",
    "thus, across all columns, we have \n",
    "\n",
    "\n",
    "\n",
    "$  \\begin{bmatrix}\n",
    "\\mathbf B \\mathbf x_1 \\\\ \n",
    "\\mathbf B\\mathbf x_2\\\\ \n",
    "\\vdots \\\\ \n",
    "\\mathbf B\\mathbf x_{n-1}\\\\ \n",
    "\\mathbf B \\mathbf x_n\n",
    "\\end{bmatrix}+ \\begin{bmatrix}\n",
    "c_{1,1}\\mathbf {I} & c_{2,1}\\mathbf {I} & \\cdots & c_{n-1,1}\\mathbf {I} & c_{n,1}\\mathbf {I}\\\\ \n",
    "c_{1,2} \\mathbf I & c_{2,2}\\mathbf {I} & \\cdots & c_{n-1,2}\\mathbf {I} & c_{n,2}\\mathbf {I}\\\\ \n",
    "\\vdots & \\vdots & \\ddots & \\vdots & \\vdots   \\\\ \n",
    "c_{1,n-1} \\mathbf I & c_{2,n-1}\\mathbf {I} & \\cdots & c_{n-1,n-1}\\mathbf {I} & c_{n,n-1}\\mathbf {I}\\\\\n",
    "c_{1,n} \\mathbf I & c_{2,n}\\mathbf {I} & \\cdots & c_{n-1,n}\\mathbf {I} & c_{n,n}\\mathbf {I}\\\\\n",
    "\\end{bmatrix}\\begin{bmatrix}\n",
    "\\mathbf x_1 \\\\ \n",
    "\\mathbf x_2\\\\ \n",
    "\\vdots \\\\ \n",
    "\\mathbf x_{n-1}\\\\ \n",
    "\\mathbf x_n\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "\\mathbf y_1 \\\\ \n",
    "\\mathbf y_2\\\\ \n",
    "\\vdots \\\\ \n",
    "\\mathbf y_{n-1}\\\\ \n",
    "\\mathbf y_n\n",
    "\\end{bmatrix} = \\text{vec}\\big(\\mathbf Y\\big)$\n",
    "\n",
    "or equivalently:  \n",
    "$\\Big(\\begin{bmatrix}\n",
    "\\mathbf {B} & \\mathbf {00}^H & \\cdots & \\mathbf {00}^H & \\mathbf {00}^H\\\\ \n",
    "\\mathbf {00}^H & \\mathbf {B} & \\cdots & \\mathbf {00}^H & \\mathbf {00}^H\\\\ \n",
    "\\vdots & \\vdots & \\ddots & \\vdots & \\vdots   \\\\ \n",
    "\\mathbf {00}^H & \\mathbf {00}^H & \\cdots & \\mathbf {B} & \\mathbf {00}^H\\\\ \n",
    "\\mathbf {00}^H & \\mathbf {00}^H & \\cdots & \\mathbf {00}^H & \\mathbf {B}\\\\ \n",
    "\\end{bmatrix} +  \\begin{bmatrix}\n",
    "c_{1,1}\\mathbf {I} & c_{2,1}\\mathbf {I} & \\cdots & c_{n-1,1}\\mathbf {I} & c_{n,1}\\mathbf {I}\\\\ \n",
    "c_{1,2} \\mathbf I & c_{2,2}\\mathbf {I} & \\cdots & c_{n-1,2}\\mathbf {I} & c_{n,2}\\mathbf {I}\\\\ \n",
    "\\vdots & \\vdots & \\ddots & \\vdots & \\vdots   \\\\ \n",
    "c_{1,n-1} \\mathbf I & c_{2,n-1}\\mathbf {I} & \\cdots & c_{n-1,n-1}\\mathbf {I} & c_{n,n-1}\\mathbf {I}\\\\\n",
    "c_{1,n} \\mathbf I & c_{2,n}\\mathbf {I} & \\cdots & c_{n-1,n}\\mathbf {I} & c_{n,n}\\mathbf {I}\\\\\n",
    "\\end{bmatrix}\\Big)\\begin{bmatrix}\n",
    "\\mathbf x_1 \\\\ \n",
    "\\mathbf x_2\\\\ \n",
    "\\vdots \\\\ \n",
    "\\mathbf x_{n-1}\\\\ \n",
    "\\mathbf x_n\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "\\mathbf y_1 \\\\ \n",
    "\\mathbf y_2\\\\ \n",
    "\\vdots \\\\ \n",
    "\\mathbf y_{n-1}\\\\ \n",
    "\\mathbf y_n\n",
    "\\end{bmatrix} $\n",
    "\n",
    "or equivalently,\n",
    "\n",
    "$\\Big(\\mathbf B \\oplus \\mathbf C^T\\Big)\\text{vec}\\Big(\\mathbf X\\Big) = \\Big(\\big(\\mathbf I_n \\otimes \\mathbf B\\big) + \\big(\\mathbf C^T \\otimes \\mathbf I_m\\big)\\Big) \\text{vec}\\Big(\\mathbf X\\Big)= \\text{vec}\\Big(\\mathbf Y\\Big)$\n",
    "\n",
    "In order for this equation to have a unique solution, we know that the square matrix given by \n",
    "\n",
    "$\\Big(\\mathbf B \\oplus \\mathbf C^T\\Big)$\n",
    "\n",
    "cannot have any eigenvalues equal to zero.  This means that $\\mathbf B$ cannot have any eigenvalues equal to those of $-\\mathbf C$ (or equivalently, any eigenvalues equal to those of $-\\mathbf C^T$ as transposition does not change eigenvalues -- no conjugation is involved).  An easy special case worth mentioning, is if both $\\mathbf B$ and $\\mathbf C$ are Hermitian Positive Definite, then both have strictly real valued, positive eigenvalues-- thus neither can have eigenvalues equal to the negative of the other, and hence the equation must be solvable in such a case. (The same conclusion follows if both matrices are Hermitian negative definite.)\n",
    "\n",
    "An important special case of the above is the **Lyapunov Equation**, which is given below:   \n",
    "$\\mathbf B \\mathbf X + \\mathbf X \\mathbf B = \\mathbf Y$\n",
    "\n",
    "Hence if $\\mathbf B$ has any real eigenvalues with same magnitude and opposite signs, then the equation is singular (cannot be solved uniquely). (This also applies to complex eigenvalues but is a bit more verbose.)  Perhaps even more interesting, is the common special case where all scalars are in $\\mathbb R$.  In this case, even if $\\mathbf B$ is non-singular and if $\\mathbf B$ has *any* purely imaginary eigenvalues, then the equation does not have a solution.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More on the Vec() Operator \n",
    "\n",
    "**claim:** \n",
    "\n",
    "$\\text{vec}\\big(\\mathbf {ABC}\\big) = \\big(\\mathbf C^T \\otimes \\mathbf A\\big)\\text{vec}\\big(\\mathbf {B}\\big) $\n",
    "\n",
    "\n",
    "where for simplicity all matrices are $n$ x $n$ (though in fact any legal product can be used here)\n",
    "\n",
    "**proof:**  \n",
    "$\\mathbf B = \n",
    "\\bigg[\\begin{array}{c|c|c|c|c}\n",
    "\\mathbf b_1 & \\mathbf b_2 &\\cdots & \\mathbf b_{n-1} & \\mathbf b_{n}\n",
    "\\end{array}\\bigg]\n",
    "$   \n",
    "$\\mathbf C= \n",
    "\\begin{bmatrix}\n",
    "\\tilde{ \\mathbf c_1}^T \\\\\n",
    "\\tilde{ \\mathbf c_2}^T \\\\ \n",
    "\\vdots\\\\ \n",
    "\\tilde{ \\mathbf c}_{n-1}^T \\\\ \n",
    "\\tilde{ \\mathbf c_n}^T\n",
    "\\end{bmatrix}\n",
    "$   \n",
    "\n",
    "- - - -    \n",
    "Playing around with associativity of matrix multiplication, we can see:  \n",
    "\n",
    "$\\mathbf Y = \\mathbf {AB}\\mathbf C  = \\big(\\mathbf {AB}\\big)\\mathbf C = \\bigg[\\begin{array}{c|c|c|c|c} \\mathbf A \\mathbf b_1 & \\mathbf A\\mathbf b_2 &\\cdots & \\mathbf A\\mathbf b_{n-1} & \\mathbf A\\mathbf b_{n}\n",
    "\\end{array}\\bigg]\\begin{bmatrix}\n",
    "\\tilde{ \\mathbf c_1}^T \\\\\n",
    "\\tilde{ \\mathbf c_2}^T \\\\ \n",
    "\\vdots\\\\ \n",
    "\\tilde{ \\mathbf c}_{n-1}^T \\\\ \n",
    "\\tilde{ \\mathbf c_n}^T\n",
    "\\end{bmatrix}\n",
    "=  \\sum_{k=1}^n \\mathbf A \\mathbf b_k\\tilde{ \\mathbf c_k}^T  = \\mathbf A\\Big(\\sum_{k=1}^n  \\mathbf b_k\\tilde{ \\mathbf c_k}^T \\Big) = \\mathbf {A}\\big(\\mathbf {BC}\\big) $  \n",
    "\n",
    "remark: with minor adjustment the above gives a proof of the associativity of matrix multiplication, which can be proven by other means, though typically said proofs are tedious.  The sum of rank one updates / outer-products formulation of matrix multiplication also gives an easy proof of other rather typically tedious operations.  At its core it has a visual component, and breaks matrices down into their simplest components (i.e. a sum of rank one matrices) where we may then verify simple claims and exploit linearity.  \n",
    "\n",
    "e.g:  \n",
    "\n",
    "$\\big(\\mathbf {BC}\\big)^T =\\mathbf {C}^T \\mathbf B^T $   \n",
    "*proof:*  \n",
    "by direct inspection, verify that $\\big(\\mathbf x\\mathbf y^T\\big)^T = \\mathbf y \\mathbf x^T$   \n",
    "\n",
    "$\\big(\\mathbf {BC}\\big)^T = \\Big(\\sum_{k=1}^n  \\mathbf b_k\\tilde{ \\mathbf c_k}^T \\Big)^T =  \\sum_{k=1}^n  \\tilde{ \\mathbf c_k}\\mathbf b_k^T =\\mathbf C^T \\mathbf B^T$  \n",
    "\n",
    "\n",
    "- - - -  \n",
    "*returning to the matter at hand*  \n",
    "to figure out what $\\text{vec}\\big(\\mathbf Y \\big)$ looks like, as before, **we look at the kth column** of the above series:\n",
    "\n",
    "$c_{1,k}\\mathbf A\\mathbf b_{1} + c_{2,k}\\mathbf A\\mathbf b_{2} + ... + c_{n-1,k}\\mathbf A \\mathbf b_{n-1} + c_{n,k}\\mathbf A \\mathbf b_{n}  = \\mathbf y_k$\n",
    "\n",
    "equivalently written as \n",
    "\n",
    " $\\bigg[\\begin{array}{c|c|c|c|c} c_{1,k}\\mathbf A &c_{2,k}\\mathbf A &\\cdots & c_{n-1,k}\\mathbf A & c_{n,k}\\mathbf A\n",
    "\\end{array}\\bigg]\\begin{bmatrix}\n",
    "\\mathbf b_{1} \\\\\n",
    "\\mathbf b_{2} \\\\ \n",
    "\\vdots\\\\ \n",
    "\\mathbf b_{n-1} \\\\ \n",
    "\\mathbf b_{n}\n",
    "\\end{bmatrix} = \\mathbf y_k$\n",
    "\n",
    "thus taking a step back and looking at this over *all* columns in $\\mathbf Y$\n",
    "\n",
    "\n",
    "$\\begin{bmatrix}\n",
    "c_{1,1}\\mathbf {A} & c_{2,1}\\mathbf {A} & \\cdots & c_{n-1,1}\\mathbf {A} & c_{n,1}\\mathbf {A}\\\\ \n",
    "c_{1,2} \\mathbf A & c_{2,2}\\mathbf {A} & \\cdots & c_{n-1,2}\\mathbf {A} & c_{n,2}\\mathbf {A}\\\\ \n",
    "\\vdots & \\vdots & \\ddots & \\vdots & \\vdots   \\\\ \n",
    "c_{1,n-1} \\mathbf A & c_{2,n-1}\\mathbf {A} & \\cdots & c_{n-1,n-1}\\mathbf {A} & c_{n,n-1}\\mathbf {A}\\\\\n",
    "c_{1,n} \\mathbf A & c_{2,n}\\mathbf {A} & \\cdots & c_{n-1,n}\\mathbf {A} & c_{n,n}\\mathbf {A}\\\\\n",
    "\\end{bmatrix}\\begin{bmatrix}\n",
    "\\mathbf b_1 \\\\ \n",
    "\\mathbf b_2\\\\ \n",
    "\\vdots \\\\ \n",
    "\\mathbf b_{n-1}\\\\ \n",
    "\\mathbf b_n\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "\\mathbf y_1 \\\\ \n",
    "\\mathbf y_2\\\\ \n",
    "\\vdots \\\\ \n",
    "\\mathbf y_{n-1}\\\\ \n",
    "\\mathbf y_n\n",
    "\\end{bmatrix} = \\text{vec}\\big(\\mathbf Y\\big)$\n",
    "\n",
    "Thus\n",
    "\n",
    "\n",
    "$\\big(\\mathbf C^T \\otimes \\mathbf A\\big)\\text{vec}\\big(\\mathbf {B}\\big) = \\text{vec}\\big(\\mathbf {ABC}\\big) = \\text{vec}\\big(\\mathbf {Y}\\big)$\n",
    "\n",
    "\n",
    "- - - -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**extension:** Problems 4.111 and 4.112 in Zhang's *Linear Algebra: Challenging Problems for Students*  \n",
    "\n",
    "4.111\n",
    "(a) \n",
    "*claim:*  \n",
    "if $\\mathbf A\\mathbf B + \\mathbf B\\mathbf A = \\mathbf {00}^H$, and if $\\mathbf B$ is not nilpotent, then the matrix equation $\\mathbf {AX} + \\mathbf {XA} = \\mathbf B$ has no solution\n",
    "\n",
    "**subsequent note: It may be preferable to say that there is no unique solution.  An open item is the wishy-washy singular case where sometimes there is no solution at all but sometimes there are multiple solutions.  The main focus as always with square matrices is getting one unique correct answer.  However sometimes that can be too narrow a viewpoint.**  \n",
    "\n",
    "\n",
    "*answer:*  \n",
    "based on the above analysis, we know that this equation is not solvable if some eigenvalue of $\\mathbf A$, $\\lambda_k$, is equal to the negative of some eigenvalue of $\\mathbf A$ (or $\\mathbf A^T$) -- i.e. $\\lambda_k = -\\lambda_r$.  An important special case, that this problem tries to address, is when $\\lambda_k = 0 = -0$, i.e. when $\\mathbf A$ is singular.  That is, we know that when $\\mathbf A$ is singular, the equation $\\mathbf {AX} + \\mathbf {XA} = \\mathbf B$ has no solution. \n",
    "\n",
    "in the above problem $\\mathbf {AB} = -\\mathbf {BA}$ (i.e. they anti-commute).  If $\\mathbf A$ is non-singular, then we can use it to effect a similarity transformation, i.e.\n",
    "\n",
    "$\\mathbf B = - \\mathbf A^{-1}\\mathbf {BA}$, hence we have \n",
    "\n",
    "$\\text{trace}\\big(\\mathbf B^{2k-1}\\big) = - \\text{trace}\\big(\\mathbf A^{-1}\\mathbf B^{2k-1} \\mathbf A\\big) = -\\text{trace}\\big(\\mathbf B^{2k-1}\\big)$\n",
    "\n",
    "which means that for all odd positive integer exponents (r), we have: \n",
    "$\\text{trace}\\big(\\mathbf B^r\\big) = -\\text{trace}\\big(\\mathbf B^r\\big) = 0$ -- i.e. the trace is zero.\n",
    "\n",
    "now we examine the case of odd positive integers in context of our original equation:  \n",
    "\n",
    "$\\big(\\mathbf B\\big)\\mathbf B^{2k-1} = \\mathbf B^{2k} = \\big(\\mathbf {AX} + \\mathbf {XA}\\big)\\mathbf B^{2k-1} = \\mathbf {AX}\\mathbf B^{2k-1} + \\mathbf {XA}\\mathbf B^{2k-1} $ \n",
    "\n",
    "but recall that $\\mathbf {AB} = -\\mathbf {BA}$ which means\n",
    "\n",
    "\n",
    "$\\mathbf {AX}\\mathbf B^{2k-1} + \\mathbf {XA}\\mathbf B^{2k-1} = \\mathbf {AX}\\mathbf B^{2k-1} - \\mathbf {X}\\mathbf B \\mathbf A \\mathbf B^{2k-2}= \\mathbf {AX}\\mathbf B^{2k-1} + \\mathbf {X}\\mathbf B^3 \\mathbf A \\mathbf B^{2k-3} $\n",
    "\n",
    "and so on (for large enough k, as needed).  The key point is that the the negative sign exists after an odd amount of these pairwise swaps and doesn't after an even amount.  However after $2k - 1$ pairwise swaps, i.e. an odd amount, we can move all  the $\\mathbf B$'s to the left of the $\\mathbf A$'s.  \n",
    "\n",
    "$\\mathbf B^{2k} = \\mathbf {AX}\\mathbf B^{2k-1} + \\mathbf {XA}\\mathbf B^{2k-1} = \\mathbf {AX}\\mathbf B^{2k-1} - \\mathbf {X}\\mathbf B^{2k-1}\\mathbf A $\n",
    "\n",
    "hence \n",
    "\n",
    "$\\text{trace}\\big(\\mathbf B^{2k}\\big) =  \\text{trace}\\big(\\mathbf A \\mathbf {X}\\mathbf B^{2k-1} - \\mathbf {X}\\mathbf B^{2k-1} \\mathbf A\\big) =  \\text{trace}\\big(\\mathbf A \\mathbf {X}\\mathbf B^{2k-1}\\big) - \\text{trace}\\big(\\mathbf {X}\\mathbf B^{2k-1} \\mathbf A\\big) = \\text{trace}\\big(\\mathbf A \\mathbf {X}\\mathbf B^{2k-1}\\big) - \\text{trace}\\big(\\mathbf A\\mathbf {X}\\mathbf B^{2k-1}\\big) = 0 $\n",
    "\n",
    "Thus the trace of $\\big(\\mathbf B^r\\big) = 0$ when $r$ is an even positive integer and (as before) when $r$ is an odd positive integer, thus $\\mathbf B$ must be nilpotent.  (See 'Vandermonde_Matrices_Permutations_and_Discrete_Fourier_Transform.ipynb' for multiple proofs of the relation of zero trace and nilpotence -- when the underlying scalar field is not finite, as is the case in $\\mathbb R$ and $\\mathbb C$, of course.)  \n",
    "\n",
    "Hence in this case, if $\\mathbf B$ is not nilpotent, then $\\mathbf A$ must be singular, and the equation cannot be solved.  (Note: the second leg of this -- to find the hidden zero matrix when $\\mathbf B$ is raised to an even power, is quite tricky, and required your author to consult the official solutions.) \n",
    "\n",
    "(b) \n",
    "(i) if $\\mathbf A$ is Hermitian positive definite, then $\\mathbf {AX} + \\mathbf {XA} = \\mathbf B$ can be solved.  (This was addressed at the end of the above cell.) \n",
    "\n",
    "(ii )Moreover if $\\mathbf B$ is (Hermitian) positive semi-definite, then so is $\\mathbf X$ \n",
    "\n",
    "if $\\mathbf B$ is positive semi-definite, then this means\n",
    "\n",
    "$\\mathbf y^H \\mathbf {AX}\\mathbf y + \\mathbf y^H \\mathbf {XA}\\mathbf y = \\mathbf y^H \\mathbf B \\mathbf y \\geq 0$ \n",
    "\n",
    "for any $\\mathbf y$\n",
    "\n",
    "now, we recognize that if $\\mathbf A$ is Hermitian positive definite, then its eigenvectors form a partition, and we use associativity.\n",
    "\n",
    "$\\big(\\mathbf y^H \\mathbf A\\big)\\mathbf X\\mathbf y + \\mathbf y^H \\mathbf X \\big(\\mathbf A \\mathbf y\\big) = \\Big(\\sum_{k=1}^{m} \\big(\\alpha_k \\lambda_k \\mathbf y^H \\big)\\mathbf X\\mathbf y + \\mathbf y^H \\mathbf X \\big(\\alpha_k \\lambda_k  \\mathbf y\\big) \\Big)  = \\mathbf y^H \\mathbf B \\mathbf y \\geq 0$\n",
    "\n",
    "where each $\\alpha_k \\geq 0$ and of course each eigenvalue $\\lambda_k \\geq 0$\n",
    "\n",
    "we can collect all of these real valued non-negative scalars and call that sum $\\gamma$, where $\\gamma \\geq 0$\n",
    "\n",
    "$\\gamma \\mathbf y^H \\mathbf X\\mathbf y = \\mathbf y^H \\mathbf B \\mathbf y \\geq 0$\n",
    "\n",
    "which is zero if $\\gamma = 0$, otherwise we divide it out and see the familiar inequality for positive semidefiniteness\n",
    "\n",
    "$\\mathbf y^H \\mathbf X\\mathbf y\\geq 0$\n",
    "\n",
    "for any $\\mathbf y$, which means that $\\mathbf X$ is positive semi definite.\n",
    "\n",
    "\n",
    "*4.112*\n",
    "\n",
    "The matrix equation $\\mathbf{BX} + \\mathbf{XC} = \\mathbf Y$ has a unique solution **iff**\n",
    "\n",
    "there is some $\\mathbf S$ where\n",
    "\n",
    "\n",
    "$\\mathbf S\\begin{bmatrix}\n",
    "\\mathbf B & \\mathbf Y\\\\ \n",
    " \\mathbf {00}^H& -\\mathbf C\n",
    "\\end{bmatrix} \\mathbf S^{-1} = \\begin{bmatrix}\n",
    "\\mathbf B & \\mathbf {00}^H \\\\ \n",
    " \\mathbf {00}^H& -\\mathbf C\n",
    "\\end{bmatrix}$\n",
    "\n",
    "(note that $\\mathbf S$ is involutive)  \n",
    "\n",
    "The answer is actually given as remark 13.20 in the sample chapter 13 on Kronecker products (i.e. what this posting is based off on -- recall the link at the very top of this posting). \n",
    "\n",
    "$\\mathbf S = \\mathbf S^{-1} = \\begin{bmatrix}\n",
    "\\mathbf I & \\mathbf X\\\\ \n",
    " \\mathbf {00}^H& -\\mathbf I\n",
    "\\end{bmatrix} $\n",
    "\n",
    "hence \n",
    "\n",
    "$\\mathbf S\\begin{bmatrix}\n",
    "\\mathbf B & \\mathbf Y\\\\ \n",
    " \\mathbf {00}^H& -\\mathbf C\n",
    "\\end{bmatrix} \\mathbf S^{-1} = \n",
    "\\begin{bmatrix}\n",
    "\\mathbf B & \\mathbf {BX} + \\mathbf{XC} -\\mathbf Y \\\\ \n",
    " \\mathbf {00}^H& -\\mathbf C\n",
    "\\end{bmatrix}= \\begin{bmatrix}\n",
    "\\mathbf B & \\mathbf {00}^H \\\\ \n",
    " \\mathbf {00}^H& -\\mathbf C\n",
    "\\end{bmatrix}$\n",
    "\n",
    "which of course, in the upper right corner we see our equation again:\n",
    "\n",
    "$\\mathbf {BX} + \\mathbf{XC} -\\mathbf Y = \\mathbf{00}^H$\n",
    "\n",
    "or equivalently\n",
    "\n",
    "$\\mathbf {BX} + \\mathbf{XC} = \\mathbf Y$\n",
    "\n",
    "This blocked triangular formulation would seem to indicate that there is some tie-in or useful analogy with repeated eigenvalues in an upper triangular matrix -- i.e. that such a matrix is defective and requires Jordan blocks with super diagonal elements -- i.e. that that the desired similarity transform does not exist.  This analogy is perhaps worth thinking on some more.  However it is worth noting that since we've solved using the more general Kronecker Sum structure, we don't need said analogy for solving Sylvester or Lyapunov equations.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# extension:  \n",
    "\n",
    "note: below, we confine our selves to **Reals** for now -- there is a fairly recent and explicit problem that this addresses. \n",
    "\n",
    "by way of reminder, we have \n",
    "\n",
    "$\\mathbf X = \\bigg[\\begin{array}{c|c|c|c|c}\n",
    "\\mathbf x_1 & \\mathbf x_2 &\\cdots & \\mathbf x_{n-1} & \\mathbf x_n\\end{array}\\bigg]$\n",
    "\n",
    "$\\text{vec}\\big(\\mathbf X\\big)  = \\begin{bmatrix}\n",
    "\\mathbf x_1 \\\\ \n",
    "\\mathbf x_2\\\\ \n",
    "\\vdots \\\\ \n",
    "\\mathbf x_{n-1}\\\\ \n",
    "\\mathbf x_n\n",
    "\\end{bmatrix}$\n",
    "\n",
    "\n",
    "$\\mathbf Y = \\bigg[\\begin{array}{c|c|c|c|c}\n",
    "\\mathbf y_1 & \\mathbf y_2 &\\cdots & \\mathbf y_{n-1} & \\mathbf y_n\\end{array}\\bigg]$\n",
    "\n",
    "$\\text{vec}\\big(\\mathbf Y\\big)  = \\begin{bmatrix}\n",
    "\\mathbf y_1 \\\\ \n",
    "\\mathbf y_2\\\\ \n",
    "\\vdots \\\\ \n",
    "\\mathbf y_{n-1}\\\\ \n",
    "\\mathbf y_n\n",
    "\\end{bmatrix}$\n",
    "\n",
    "\n",
    "$\\text{trace}\\big(\\mathbf X^T \\mathbf Y\\big) = \\text{vec}\\big(\\mathbf X\\big)^T \\text{vec} \\big(\\mathbf Y \\big) = \\mathbf x_1^T \\mathbf y_1 + \\mathbf x_2^T \\mathbf y_2 + ... + \\mathbf x_n^T \\mathbf y_n  $  \n",
    "\n",
    "- - - -\n",
    "This suggests a way to convert the Sylvester Equation -- designed to solve for $\\text{vec}\\big(\\mathbf X\\big)$, into a quadratic form.  \n",
    "\n",
    "i.e. suppose we had this equation, \n",
    "\n",
    "$\\mathbf X^T \\mathbf B \\mathbf X + \\mathbf X^T \\mathbf X \\mathbf C = \\mathbf X^T \\mathbf Y$\n",
    "\n",
    "or equivalently\n",
    "\n",
    "$\\mathbf X^T \\big(\\mathbf B \\mathbf X +  \\mathbf X \\mathbf C\\big) = \\mathbf X^T \\big(\\mathbf Y\\big)$\n",
    "\n",
    "and we were interested in how its trace varied with different $\\mathbf X$'s.  Then we could look at \n",
    "$\\text{trace}\\big(\\mathbf X^T \\mathbf B \\mathbf X + \\mathbf X^T \\mathbf X \\mathbf C\\big) = \\text{trace}\\big(\\mathbf X^T \\mathbf Y\\big)$\n",
    "\n",
    "and we know that \n",
    "\n",
    "$\\text{trace}\\big(\\mathbf X^T \\mathbf Y\\big) =  \\text{vec}\\big(\\mathbf X\\big)^T \\text{vec}\\big(\\mathbf Y\\big)  =  \\text{vec}\\big(\\mathbf X\\big)^T\\big(\\mathbf B \\oplus \\mathbf C^T\\big)\\text{vec}\\big(\\mathbf X\\big)  = \\text{vec}\\big(\\mathbf X\\big)^T \\Big( \\big(\\mathbf I \\otimes \\mathbf B\\big) + \\big(\\mathbf C^T \\otimes \\mathbf I\\big)\\Big)\\text{vec}\\big(\\mathbf X\\big) $\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling of exercise 1.13 in \"Cauchy Schwarz Masterclass\"\n",
    "\n",
    "**claim:**\n",
    "\n",
    "where $\\mathbf X$ is a real valued $m$ x $n$ matrix\n",
    "\n",
    "$\\Big(\\sum_{i=1}^m \\sum_{j=1}^n  x_{i,j}\\Big)^2 + mn\\big( \\sum_{i=1}^m \\sum_{j=1}^n  x_{i,j}^2\\big) \\geq m\\sum_{i=1}^m \\Big(\\sum_{j=1}^n x_{i,j}\\Big)^2 + n\\sum_{j=1}^n \\Big(\\sum_{i=1}^m x_{i,j}\\Big)^2$\n",
    "\n",
    "\n",
    "This can be re-written as \n",
    "\n",
    "$\\Big(\\mathbf 1_m^T \\mathbf X \\mathbf 1_n\\Big)^2 + mn \\big \\Vert \\mathbf X \\big \\Vert_F^2 \\geq m\\big \\Vert \\mathbf X \\mathbf 1_n\\big \\Vert_2^2 + n\\big \\Vert \\mathbf X^T \\mathbf 1_m\\big \\Vert_2^2$\n",
    "\n",
    "or equivalently\n",
    "\n",
    "$\\Big(\\mathbf 1_m^T \\mathbf X \\mathbf 1_n\\Big)^2 + mn \\big \\Vert \\mathbf X \\big \\Vert_F^2 - m\\big \\Vert \\mathbf X \\mathbf 1_n\\big \\Vert_2^2 - n\\big \\Vert \\mathbf X^T \\mathbf 1_m\\big \\Vert_2^2 \\geq 0$\n",
    "\n",
    "for all $\\mathbf X$\n",
    "\n",
    "**remark:** \n",
    "\n",
    "from \"SPSD_Trace_Inequality.ipynb\", we know that \n",
    "\n",
    "$\\big \\Vert \\mathbf X^T \\mathbf 1_m\\big \\Vert_2^2 \\leq n \\big \\Vert \\mathbf X \\big \\Vert_F^2    $  \n",
    "\n",
    "*tbc: it seems that this should read*    \n",
    "$\\big \\Vert \\mathbf X^T \\mathbf 1_m\\big \\Vert_2^2 \\leq m \\big \\Vert \\mathbf X \\big \\Vert_F^2    $  \n",
    "\n",
    "\n",
    "**commentary:**   \n",
    "\n",
    "This problem is exercise 1.13 in \"Cauchy Schwarz Masterclass\" -- the actual problem solution is much shorter and does not require knowledge of matrices let alone Kronecker Products -- just application of Cauchy Schwarz to very specially chosen variables.  However the official solution looks like uninspired symbol manipulation that uses several opaque tricks -- i.e. the steps can be followed and the result verified but no real intuition or understanding was developed by your author while doing so.  \n",
    "\n",
    "That said, the official solution, mentioned that the result is due to van Dam (\"A Cauchy-Khinchin Matrix Inequality\") from 1998, using matrix theory.  This writeup's solution uses the framework that van Dam uses, however it uses a considerably different starting point to begin / model the problem -- van Dam's justification to get it in Kronecker product form did not quite make sense to your author.  Note that van Dam then makes an argument around commutativity when looking at the spectrum of the Kronecker Product matrix (which we see as a quadratic form).  This may be the preferred high brow approach.  However, this writeup looks at two other Approaches -- one: a Kronecker Product decomposition (inspired by a recent follow-up article read elsewhere) and two: just following the Schur Decomposition implications (noting that real symmetric matrices are orthogonally diagonalizable) and looking at the eigenvector that is proportional to the ones vector (nothing else turns out to matter).  \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**proof:** \n",
    "\n",
    "looking at the last expression, our goal is to now re-write each term instead as part of a quadratic form that makes use of $\\text{vec}\\big(\\mathbf X\\big)$\n",
    "\n",
    "$\\Big(\\mathbf 1_m^T \\mathbf X \\mathbf 1_n\\Big)^2 + mn \\big \\Vert \\mathbf X \\big \\Vert_F^2 - \\Big(m\\big \\Vert \\mathbf X \\mathbf 1_n\\big \\Vert_2^2 + n\\big \\Vert \\mathbf X^T \\mathbf 1_m\\big \\Vert_2^2 \\Big) \\geq 0$\n",
    "\n",
    "\n",
    "where $\\mathbf J_r = \\mathbf 1_r \\mathbf 1_r^T $ i.e. it indicates an $r$ x $r$ ones matrix\n",
    "\n",
    "(a)\n",
    "$\\Big(\\mathbf 1_m^T \\mathbf X \\mathbf 1_n\\Big)^2 = sum\\Big(\\text{vec}\\big(\\mathbf X\\big) \\text{vec}\\big(\\mathbf X\\big)^T\\Big) = sum\\Big(\\mathbf J_{mn} \\circ \\big(\\text{vec}\\big(\\mathbf X\\big) \\text{vec}\\big(\\mathbf X\\big)^T\\big)\\Big) = \\text{vec}\\big(\\mathbf X\\big)^T \\big( \\mathbf J_{mn}\\big) \\text{vec}\\big(\\mathbf X\\big)$\n",
    "\n",
    "where $\\circ$ indicates the Hadamard Product.  (Hopefully the fonts render properly so that the reader can see clearly the difference between $\\circ$, the Hadamard Product, and $\\cdot$ which is a dot product / regular multiplication!)   \n",
    "\n",
    "(Note this decomposition was originally used in Markov Optimization folder when dealing with (julia_hmm_viterbi_as_qp_and_lp_upload, and more recently it was used in the somewhat messy \"Fun_with_Trace_and_Quadratic_Forms_and_CauchySchwarz.ipynb\" at the very end, under the heading \"Hermitian Positive Semi Definite Inequalities with Hadamard Product\")\n",
    "\n",
    "(b) $mn \\big \\Vert \\mathbf X \\big \\Vert_F^2 =  mn \\cdot \\text{vec}\\big(\\mathbf X\\big)^T \\text{vec}\\big(\\mathbf X\\big) = \\text{vec}\\big(\\mathbf X\\big)^T \\big( mn\\mathbf I_{mn}\\big) \\text{vec}\\big(\\mathbf X\\big)$ \n",
    "\n",
    "\n",
    "(c)  $\\Big(m\\big \\Vert \\mathbf X \\mathbf 1_n\\big \\Vert_2^2 + n\\big \\Vert \\mathbf X^T \\mathbf 1_m\\big \\Vert_2^2 \\Big) = m\\mathbf 1_n^T  \\mathbf X^T \\mathbf X \\mathbf 1_n +  n\\mathbf 1_m^T \\mathbf X \\mathbf X^T \\mathbf 1_m = m \\cdot \\text{trace}\\big(\\mathbf 1_n^T  \\mathbf X^T \\mathbf X \\mathbf 1_n\\big) +  n\\cdot\\text{trace}\\big(\\mathbf 1_m^T \\mathbf X \\mathbf X^T \\mathbf 1_m \\big)$\n",
    "\n",
    "$\\Big(m\\big \\Vert \\mathbf X \\mathbf 1_n\\big \\Vert_2^2 + n\\big \\Vert \\mathbf X^T \\mathbf 1_m\\big \\Vert_2^2 \\Big) = m \\cdot\\text{trace}\\big(  \\mathbf X^T \\mathbf X \\mathbf 1_n \\mathbf 1_n^T \\big) +  n\\cdot\\text{trace}\\big( \\mathbf X \\mathbf X^T \\mathbf 1_m \\mathbf 1_m^T \\big) = m \\cdot \\text{trace}\\big(  \\mathbf X^T \\mathbf X \\mathbf J_n \\big) +  n\\cdot\\text{trace}\\big( \\mathbf X^T \\mathbf J_m\\mathbf X\\big)$\n",
    "\n",
    "\n",
    "$\\Big(m\\big \\Vert \\mathbf X \\mathbf 1_n\\big \\Vert_2^2 + n\\big \\Vert \\mathbf X^T \\mathbf 1_m\\big \\Vert_2^2 \\Big) =  n\\cdot\\text{trace}\\big( \\mathbf X^T \\mathbf J_m\\mathbf X\\big) + m \\cdot \\text{trace}\\big(  \\mathbf X^T \\mathbf X \\mathbf J_n \\big)  = \\text{trace}\\Big(  \\mathbf X^T \\big( \\big(n\\mathbf J_m\\big)\\mathbf X + \\mathbf X \\big(m\\mathbf J_n\\big) \\big)\\Big)$  \n",
    "\n",
    "and we recognize the familiar Sylvester Equation: \n",
    "\n",
    "$\\mathbf Y: = \\Big( \\big(n\\mathbf J_m\\big) \\mathbf X + \\mathbf X \\big(m\\mathbf J_n \\big)\\Big) $ \n",
    "\n",
    "is equivalent to \n",
    "\n",
    "$\\text{vec}\\big(\\mathbf Y\\big) = \\big(n \\mathbf J_m \\oplus m\\mathbf J_n^T\\big)\\text{vec}\\big(\\mathbf X\\big) = \\big(n \\mathbf J_m \\oplus m\\mathbf J_n\\big)\\text{vec}\\big(\\mathbf X\\big)$\n",
    "\n",
    "recalling that $\\mathbf J_n$ is symmetric, and we combine this with the above mentioned fact that \n",
    "\n",
    "$\\text{trace}\\big(\\mathbf X^T \\mathbf Y\\big) = \\text{vec}\\big(\\mathbf X\\big)^T \\text{vec}\\big(\\mathbf Y\\big)$ \n",
    "\n",
    "we get \n",
    "\n",
    "$\\Big(m\\big \\Vert \\mathbf X \\mathbf 1_n\\big \\Vert_2^2 + n\\big \\Vert \\mathbf X^T \\mathbf 1_m\\big \\Vert_2^2 \\Big) =  \\text{vec}\\big(\\mathbf X\\big)^T\\text{vec}\\big(\\mathbf Y\\big)=  \\text{vec}\\big(\\mathbf X\\big)^T\\big(n \\mathbf J_m \\oplus m\\mathbf J_n\\big)\\text{vec}\\big(\\mathbf X\\big) = \\text{vec}\\big(\\mathbf X\\big)^T \\Big(n \\mathbf J_m \\otimes \\mathbf I_n +  \\mathbf I_m \\otimes m\\mathbf J_n\\Big)\\text{vec}\\big(\\mathbf X\\big)$\n",
    "\n",
    "\n",
    "\n",
    "*Thus we rewrite our original expression as follows:*  \n",
    "\n",
    "original equation:  \n",
    "$\\Big(\\mathbf 1_m^T \\mathbf X \\mathbf 1_n\\Big)^2 + mn \\big \\Vert \\mathbf X \\big \\Vert_F^2 - \\Big(m\\big \\Vert \\mathbf X \\mathbf 1_n\\big \\Vert_2^2 + n\\big \\Vert \\mathbf X^T \\mathbf 1_m\\big \\Vert_2^2 \\Big) \\geq 0$\n",
    "\n",
    "becomes \n",
    "\n",
    "$ \\text{vec}\\big(\\mathbf X\\big)^T\\big( \\mathbf J_{mn}\\big) \\text{vec}\\big(\\mathbf X\\big) +  \\text{vec}\\big(\\mathbf X\\big)^T \\big( mn\\mathbf I_{mn}\\big) \\text{vec}\\big(\\mathbf X\\big)   - \\Big(\\text{vec}\\big(\\mathbf X\\big)^T \\Big(n \\mathbf J_m \\otimes \\mathbf I_n +  \\mathbf I_m \\otimes m\\mathbf J_n\\Big)\\text{vec}\\big(\\mathbf X\\big)\\Big) \\geq 0 $\n",
    "\n",
    "$ \\text{vec}\\big(\\mathbf X\\big)^T \\Big(  \\mathbf J_{mn} +  mn\\mathbf I_{mn}  - n \\mathbf J_m \\otimes \\mathbf I_n -  \\mathbf I_m \\otimes m\\mathbf J_n\\Big)\\text{vec}\\big(\\mathbf X\\big) \\geq 0 $\n",
    "\n",
    "which is a quadratic form for any (real valued) $\\text{vec}\\big(\\mathbf X\\big)$\n",
    "- - - -\n",
    "**remark:**  \n",
    "This is a considerably longer setup than what was shown in van Dam, however it is hopefully quite intuitive and makes use of many tools developed earlier in this writeup.  Put differently, this long-form approach was taken because it is useful from an intuition and (future) modeling standpoint.  As previously mentioned, at this point van Dam makes a commuting matrices argument for the matrix inside the quadratic form.  However, we'll first deploy a decomposition similar to what your author saw elsewhere and then also do a very simplistic eigenvalue analysis as an alternative route.  \n",
    "- - - -\n",
    "\n",
    "$ \\text{vec}\\big(\\mathbf X\\big)^T \\Big(  \\mathbf J_{mn} +  mn\\mathbf I_{mn}  - n \\mathbf J_m \\otimes \\mathbf I_n -  \\mathbf I_m \\otimes m\\mathbf J_n\\Big)\\text{vec}\\big(\\mathbf X\\big) \\geq 0 $\n",
    "\n",
    "we may verify by inspection that the real matrix given by   \n",
    "$\\mathbf Z := \\Big(  \\mathbf J_{mn} +  mn\\mathbf I_{mn}  - n \\mathbf J_m \\otimes \\mathbf I_n -  \\mathbf I_m \\otimes m\\mathbf J_n\\Big)$  \n",
    "\n",
    "is symmetric. Thus, it is enough to verify that $\\mathbf Z$ has no negative eigenvalues -- that confirms it is positive semi-definite. \n",
    "\n",
    "now consider the following factorization: \n",
    "\n",
    "$\\mathbf Z = \\Big(  \\mathbf J_{m} \\otimes \\mathbf J_n +  n\\mathbf I_{m} \\otimes m\\mathbf I_{m}  -  \\mathbf J_m \\otimes n\\mathbf I_n -  m\\mathbf I_m \\otimes \\mathbf J_n\\Big) = \\Big(\\mathbf J_m -m\\mathbf I_{m}\\Big) \\otimes \\Big( \\mathbf J_n -n\\mathbf I_{n}\\Big)  $\n",
    "\n",
    "The matrix given by $\\Big(\\mathbf J_m -m\\mathbf I_{m}\\Big)$ has a single eigenvalue equal to $m - m =0$ and all other eigenvalues are equal to $-m$. The matrix given by $\\Big( \\mathbf J_n -n\\mathbf I_{n}\\Big)  $ has a single eigenvalue equal to $ n - n = 0$ and all other eigenvalues equal to $-n$.  \n",
    "\n",
    "Using our original analysis for eigenvalues and Kronecker Products, we can say that the eigenvalues for $\\mathbf Z$ are given by the Cartesian Product of the multiset of eigenvalues of $\\Big(\\mathbf J_m -n\\mathbf I_{m}\\Big)$ and $\\Big( \\mathbf J_n -n\\mathbf I_{n}\\Big)$.  That is, each eigenvalue for $\\mathbf Z$ is either the product of two negative eigenvalues, two zeros, or a negative eigenvalue and a zero.  The first product results in a positive eigenvalue and all other products are equal to zero.  This proves that $\\mathbf Z$ is positive semi-definite. \n",
    "\n",
    "Thus we have confirmed that \n",
    "\n",
    "$ \\text{vec}\\big(\\mathbf X\\big)^T \\Big(  \\mathbf J_{mn} +  mn\\mathbf I_{mn}  - n \\mathbf J_m \\otimes \\mathbf I_n -  \\mathbf I_m \\otimes m\\mathbf J_n\\Big)\\text{vec}\\big(\\mathbf X\\big) \\geq 0 $\n",
    "\n",
    "or equivalently \n",
    "\n",
    "$ \\text{vec}\\big(\\mathbf X\\big)^T \\Big(  \\mathbf Z\\Big)\\text{vec}\\big(\\mathbf X\\big) \\geq 0 $\n",
    "\n",
    "or equivalently \n",
    "\n",
    "$\\Big(\\mathbf 1_m^T \\mathbf X \\mathbf 1_n\\Big)^2 + mn \\big \\Vert \\mathbf X \\big \\Vert_F^2 - \\Big(m\\big \\Vert \\mathbf X \\mathbf 1_n\\big \\Vert_2^2 + n\\big \\Vert \\mathbf X^T \\mathbf 1_m\\big \\Vert_2^2 \\Big) \\geq 0$\n",
    "\n",
    "for *any* real valued $\\mathbf X$ \n",
    "\n",
    "- - - -\n",
    "*begin alternative way to prove positive semi-definiteness of (real symmetric matrix)* $\\mathbf Z$\n",
    "\n",
    "$\\mathbf Z = \\Big(  \\mathbf J_{mn} +  mn\\mathbf I_{mn}  -  \\mathbf J_m \\otimes n\\mathbf I_n -  m\\mathbf I_m \\otimes \\mathbf J_n\\Big)   = \\Big( \\mathbf J_{mn} +  mn\\mathbf I_{mn} - \\big(n \\mathbf J_m \\oplus m\\mathbf J_n\\big) \\Big)$\n",
    "\n",
    "\n",
    "Start by looking at the matrix given by \n",
    "\n",
    "$-\\big(n \\mathbf J_m \\oplus m\\mathbf J_n\\big)$\n",
    "\n",
    "reviewing what we know about Kronecker sums, we know that this matrix is real symmetric, and only has eigenvalues given in the set:   \n",
    "\n",
    "$\\{-2nm, -nm, 0\\}$\n",
    "\n",
    "crucially, $\\lambda_1 = -2nm$  and this eigenvalue occurs only once (i.e. algebraic and geometric multiplicity of $-2nm$ is one).  Its eigenvector is $\\propto \\mathbf 1_{mn}$ (review Kronecker Sum's impact on Schur Decomposition for further details)\n",
    "\n",
    "when we then look at the matrix\n",
    "\n",
    "$-\\big(n \\mathbf J_m \\oplus m\\mathbf J_n\\big) + \\mathbf J_{mn} $ \n",
    "\n",
    "we see that they are both orthogonally diagonalizable in the same way -- i.e. each symmetric matrix has a dominant eigenvalue with eigenvector $\\propto \\mathbf 1_{mn}$, and $\\mathbf J_{mn} $ has all other eigenvectors in its nullspace, so they may be chosen to be mutually orthonormal, and indeed they may be chosen to be the same as the the other $mn-1$ eigenvectors in $-\\big(n \\mathbf J_m \\oplus m\\mathbf J_n\\big)$ \n",
    "\n",
    "Thus the set of eigenvalues associated with \n",
    "\n",
    "$-\\big(n \\mathbf J_m \\oplus m\\mathbf J_n\\big) + \\mathbf J_{mn} $, \n",
    "\n",
    "are given by \n",
    "\n",
    "$\\{-2nm + \\text{trace}\\big(\\mathbf J_{mn}\\big), -nm, 0\\} = \\{-2nm + nm, -nm, 0\\} = \\{-nm, 0\\}$\n",
    "\n",
    "finally, to this we add  $mn\\mathbf I_{mn} $ which shifts *all* eigenvalues by $nm$\n",
    "\n",
    "$\\mathbf Z = -\\big(n \\mathbf J_m \\oplus m\\mathbf J_n\\big) + \\mathbf J_{mn}  + mn\\mathbf I_{mn} $\n",
    "\n",
    "so $\\mathbf Z$ has eigenvalues given in the set  \n",
    "\n",
    "$\\{-nm + nm, 0 + nm\\} = \\{0, nm\\}$\n",
    "\n",
    "since $\\mathbf Z$ is real symmetric and all of its eigenvalues are non-negative, we confirm that it is positive semi-definite.  \n",
    "\n",
    "*end alternative way to prove positive semi-definiteness of (real symmetric matrix)* $\\mathbf Z$\n",
    "\n",
    "- - - - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
