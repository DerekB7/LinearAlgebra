{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this was written to expand on the simple proof in the wikipedia article (which is quite good, though notational issues may present challenges for many).  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as a **lemma** (reminder?) we need that fact that \n",
    "\n",
    "$e_k(\\lambda) = e_k(\\lambda_1, \\lambda_2, ..., \\lambda_n) = \\sum_{1 \\leq i_1 \\lt i_2 \\lt ... \\lt i_k \\leq n} \\lambda_{i,1}\\lambda_{i,2}...\\lambda_{i,k} $\n",
    "\n",
    "note that there are $\\binom{n}{k}$ terms in the above summation\n",
    "\n",
    "i.e. this is the kth elementary symmetric function (and if we scale by $(-1)^k$ this is given in the coefficient associated with $x^{n-k}$ in a degree $n$ polynomial with the above roots) \n",
    "\n",
    "referencing e.g. page 494 of Meyer's *Matrix Analysis* we recall that a principal submtratix of some $n$ x $n$ matrix $\\mathbf A$ is is one that can be selected by $\\{\\mathbf e_{i_1}, \\mathbf e_{i_2}, ..., \\mathbf e_{i_k}\\}$ where (note the bold!) $\\mathbf e_{1}$ refers to the first standard basis vector -- that is suppose we restrict $\\mathbf x$ to be a a linear combination of the above set of standard basis vectors -- our principal submatrix is the one that can be selected over all possible $\\mathbf x$ via the quadratic form  $\\mathbf x^* \\mathbf A \\mathbf x$.  \n",
    "\n",
    "Perhaps more simply we could also just write $\\mathbf x := \\mathbf e_{i_1}+ \\mathbf e_{i_2}+ ... + \\mathbf e_{i_k}$ and say the $k $ x $k$ principal submatrix is given by $\\mathbf A \\circ \\mathbf {xx}^*$ *and then deleting all rows and columns that have been zero'd out by this application of Hadamard product*.  Finally, the conventional defintion of a principal submatrix is more succinct: the submatrix that lies on the same set of $k$ rows and columns.  \n",
    "\n",
    "A *principal minor* is the determinant of said submatrix.  \n",
    "\n",
    "what is then proven (admittedly by the author as tediously) on page 495 of Meyer is that \n",
    "\n",
    "$e_k(\\lambda) = \\sum\\big(\\text{all k x k principal minors}\\big)$  \n",
    "\n",
    "note that the proof technique uses calculus and hence (like everything in that book) is designed for a field of characteristic zero.  \n",
    "\n",
    "It is worth noting that two very special cases of the above are $k=1$ and $k=n$, because in those cases we recover \n",
    "\n",
    "$e_1(\\lambda) = \\text{trace}\\big(\\mathbf A\\big)$    \n",
    "$e_n(\\lambda) = \\text{det}\\big(\\mathbf A\\big)$    \n",
    "\n",
    "While the specifics of principal minors can be a bit tedious, they are important to learn because they're a matrix invariant (i.e. they give terms of the characteristic polynomial).  \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Main Theorem**  \n",
    "\n",
    "consider an $m$ x $n$ matrix $\\mathbf A$ and an $n$ x $m$ matrix $\\mathbf B$  \n",
    "\n",
    "Thus $\\mathbf {AB}$ and $\\mathbf {BA}$ are both square matrices though their dimensions are different -- \n",
    "\n",
    "$\\big(\\mathbf {AB}\\big) $ is $m$ x $m$   \n",
    "while  \n",
    "$\\big(\\mathbf {BA}\\big)$ is $n$ x $n$  \n",
    "\n",
    "- - - -\n",
    "*in the familiar case of* $n = m$ there is a well known property that determinants multiply and hence \n",
    "\n",
    "$\\det\\big(\\mathbf {AB}\\big) = \\det\\big(\\mathbf A\\big) \\det\\big(\\mathbf B\\big)= \\det\\big(\\mathbf B\\big)\\det\\big(\\mathbf A\\big)=\\det\\big(\\mathbf {BA}\\big)$  \n",
    "\n",
    "- - - -\n",
    "*Cauchy Binet* generalizes this for the case where $m \\neq n$.  \n",
    "\n",
    "Interpretting the formula may be about as challenging as proving it (at least if we use the very elegant proof by Tao).  \n",
    "\n",
    "The formula states in terms of sets of principal submatrices \n",
    "\n",
    "$S \\in \\binom{[n]}{m}$  \n",
    "\n",
    "\n",
    "$\\det\\big(\\mathbf {AB}\\big) = \\sum_{S} \\det\\big(\\mathbf A_{[m],S}\\big)\\det\\big(\\mathbf B_{S,[m]}\\big) $  \n",
    "\n",
    "which reads as the set of all subsets of size $m$  of set $[n]$ --- which is the set $\\{1, 2, ..., n\\}$, so we want to choose subsets of cardinality $m$ from this   \n",
    "\n",
    "Since $\\big(\\mathbf {AB}\\big)$ is $m$ x $m$ this should seem somewhat intuitive.  It says that $\\det\\big(\\mathbf {AB}\\big)$ is given by summing over all principal minors of $\\mathbf A$ (times the 'related' principal minor in $\\mathbf B$).  And what size of principal minor would we be interested in? Well size $m$ of course because that is the dimension of $\\big(\\mathbf {AB}\\big)$ (which means that the underlying characteristic polynomial of such a product is degree $m$).  \n",
    "\n",
    "\n",
    "Note that if $m \\gt n$ then an immediate consequence is that our subset is the empty set, and hence $\\det\\big(\\mathbf {AB}\\big) = 0$.  And if $m =n$ we recover the familiar formula that determinants multiply.  \n",
    "\n",
    "the main case of interest, then, is when $m \\lt n$.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**lemma** \n",
    "\n",
    "$\\det\\big( z \\mathbf I_n + \\mathbf{BA}\\big)  = z^{n-m}\\det\\big( z \\mathbf I_n + \\mathbf{AB}\\big) $  \n",
    "\n",
    "the above formula holds in general (though it is cleanest for any $z \\neq 0$ or alternatively if we insist $m\\lt n$ )  \n",
    "\n",
    "the way this reads, is consider the characteristic polynomial of $\\big( \\mathbf{AB}\\big) $  \n",
    "\n",
    "$p(x) = c_0 x^m + c_1 x^{m-1} + c_2 x^{m-2} + ... + c_{m-2} x^{2} + c_{m-1} x^{1} + c_{m} $   \n",
    "\n",
    "and the characteristic polynomial of $\\big(\\mathbf{BA}\\big)$ which is given by \n",
    "\n",
    "$q(x) = a_0 x^n + a_1 x^{n-1} + a_2 x^{n-2} + ... + a_{n-2} x^{2} + a_{n-1} x^{1} + a_{n} $   \n",
    "\n",
    "then\n",
    "\n",
    "$ z_1^{n-m} \\cdot p(z_1)= q(z_1) = \\det\\big( z_1 \\mathbf I_n + \\mathbf{BA}\\big)$  \n",
    "\n",
    "and  \n",
    "\n",
    "$ z_2^{n-m} \\cdot p(z_2)= q(z_2) = \\det\\big( z_2 \\mathbf I_n + \\mathbf{BA}\\big)$  \n",
    "\n",
    "$ z_r^{n-m} \\cdot p(z_r)= q(z_r) = \\det\\big( z_r \\mathbf I_n + \\mathbf{BA}\\big)$   \n",
    "\n",
    "and of course we need only verify this at $\\text{max}(n,m)+1$ distinct points to completely characterize the polynomial and confirm that \n",
    "\n",
    "$q(x) = x^{n-m} \\cdot p(x) $   \n",
    "\n",
    "As is common this is most easily viewed and proven in fields with characteristic zero -- the eigenvalues existing when the scalar fields is $\\mathbb C$ and the elementary symmetric functions of those roots exist in general for a field of characteristic zero.  \n",
    "\n",
    "As proven in the associated Vandermonde Matrix writeup $\\det\\big(\\mathbf{BA}\\big)$ and $\\det\\big(\\mathbf{AB}\\big)$ have the same non-zero eigenvalues with same algebraic multiplicities.  (The number of zeros then is completely determined by dimension.)  If we apply the elementary symmetric function $e_k(\\lambda)$ to the eigenvalues of $\\det\\big(\\mathbf{BA}\\big)$ and $\\det\\big(\\mathbf{AB}\\big)$ then that is *exactly* the result we get -- i.e. the characteristic polynomials are identical -- and even have the same number of non-zero terms-- except if $m\\neq n$ then one polynomial is the \"inflated\" one of the other.  \n",
    "\n",
    "Note that the above identity is closely related to the sylvester determinant identity.    \n",
    "https://en.wikipedia.org/wiki/Sylvester%27s_determinant_identity\n",
    "\n",
    "\n",
    "Two other points are worth making though we won't dwell on them: \n",
    "\n",
    "1.) $\\det\\big(\\mathbf{BA}\\big)$ and $\\det\\big(\\mathbf{AB}\\big)$ are 'almost similar' which has topological implications\n",
    "\n",
    "2.) With respect to the sign function -- the above actually tells us we have nothing to worry about.  In particular it says that if we list the characteristic polynomials as we have -- and again with the case of $m\\leq n$ in mind-- what it tells us is if we index at zero and look at that the $(m)$th coefficient from the left of $p(x)$ (which is $c_{m}$ the constant term / determinant of $\\big( \\mathbf{AB}\\big) $ multiplied by $(-1)^m$ ) is the same as the $(m)$th coefficient from the left of $q(x)$ which is $a_m$ which is $(-1)^m$ times the sum over size m principal minors for  $\\big( \\mathbf{BA}\\big)$.  That is-- the impact on the characteristic polynomials for each is a scaling by $-1$ for both terms or by $+1$ for both terms -- and hence it is a wash when comparing the terms. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** proof of the main theorem: Cauchy Binet**  \n",
    "\n",
    "Making use the prior two lemmas what we have is, supposing $m\\leq  n$, then the sum of the $mth$ principal minors of $\\big(\\mathbf{BA}\\big)$  \n",
    "\n",
    "is equal to the constant term (read: determinant, setting aside the sign function for now) of the characteristic polynomial of $\\big(\\mathbf{AB}\\big)$ .  \n",
    "\n",
    "That is for $S \\in \\binom{[n]}{m}  $  \n",
    "\n",
    "we have \n",
    "\n",
    "$e_m(\\lambda_1, \\lambda_2, ..., \\lambda_m, 0, 0, ..., 0) = \\sum_{S} \\det\\big(\\mathbf {BA}\\big)_{S,S} = \\sum_{S} \\det\\big(\\mathbf B_{S,[m]}\\mathbf A_{[m],S}\\big) = \\sum_{S} \\det\\big(\\mathbf B_{S,[m]}\\big)\\det\\big(\\mathbf A_{[m],S}\\big)$  \n",
    "\n",
    "where we first make use of relating the elementary symmetric function to the sum over principal minors, then note that $\\det\\big(\\mathbf {BA}\\big)_{S,S}$ involves a square matrix and hence the determinants multiply.  \n",
    "\n",
    "\n",
    "$= \\sum_{S} \\det\\big(\\mathbf A_{[m],S}\\big)\\det\\big(\\mathbf B_{S,[m]}\\big) =  e_m(\\lambda_1, \\lambda_2, ..., \\lambda_m)  = \\det\\big(\\mathbf {AB}\\big) $\n",
    "\n",
    "and then because the determinants are scalars which commute, we can interchange the way they mutliply and because the relevant terms in the characteristic polynomial $\\big(\\mathbf{BA}\\big)$ and $\\big(\\mathbf{AB}\\big)$ are the same, then this summation must be equal to the determinant.   \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Now a closer look at the formula, using blocked matrices*  \n",
    "\n",
    "If we first partition $\\mathbf A$ and then $\\mathbf B$, we have \n",
    "\n",
    "\n",
    "$\\mathbf A = \n",
    "\\bigg[\\begin{array}{c|c|c|c|c}\n",
    "\\mathbf a_1 & \\mathbf a_2 &\\cdots & \\mathbf a_{n-1} & \\mathbf a_{n}\n",
    "\\end{array}\\bigg]\n",
    "$   \n",
    "\n",
    "$\\mathbf B= \n",
    "\\begin{bmatrix}\n",
    "\\tilde{ \\mathbf b_1}^T \\\\\n",
    "\\tilde{ \\mathbf b_2}^T \\\\ \n",
    "\\vdots\\\\ \n",
    "\\tilde{ \\mathbf b}_{n-1}^T \\\\ \n",
    "\\tilde{ \\mathbf b_n}^T\n",
    "\\end{bmatrix}\n",
    "$   \n",
    "\n",
    "and the multiplication looks like:  \n",
    "\n",
    "$\\mathbf{BA} = \n",
    "\\begin{bmatrix}\n",
    "\\tilde{ \\mathbf b_1}^T\\mathbf{A} \\\\\n",
    "\\tilde{ \\mathbf b_2}^T \\mathbf{A} \\\\ \n",
    "\\vdots\\\\ \n",
    "\\tilde{ \\mathbf b}_{m}^T \\mathbf{A} \\\\ \n",
    "\\vdots\\\\ \n",
    "\\tilde{ \\mathbf b_n}^T \\mathbf{A}\n",
    "\\end{bmatrix} = \\bigg[\\begin{array}{c|c|c|c|c|c}\n",
    "\\mathbf B \\mathbf a_1 & \\mathbf B \\mathbf a_2 &\\cdots &\\mathbf B \\mathbf a_m &\\cdots & \\mathbf B \\mathbf a_{n}\n",
    "\\end{array}\\bigg]$  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence if we want the mth term of the characteristic polynomial of $\\mathbf{BA}$ -- again setting aside the sign function-- then we want to sum over the $\\binom{n}{m}$ principal minors of $\\mathbf{BA}$ hence we want to sum over all n choose m selections of principal submatrices (and in particular their determinants) \n",
    "\n",
    "Now using permutation matrices / graph isomorphism arguments it is enough to consider the case WLOG that we are interested in the top left $m$ x $m$ principal submatrix and its determinant  \n",
    "\n",
    "*It is generally advantageous to model what we want to \"grab\" via matrix multiplication (which behaves in line with well known Linear Algebra rules) so we introduce:  *  \n",
    "\n",
    "\n",
    "To borrow from the (in general) non-square diagonal matrix in SVD, consider the $m$ x $n$ matrix  \n",
    "\n",
    "$\\mathbf \\Sigma := \\begin{bmatrix}\n",
    "\\mathbf I_m &\n",
    " \\mathbf {00}^T \n",
    "\\end{bmatrix}$\n",
    "\n",
    "and of course its transpose is given by \n",
    "\n",
    "$\\mathbf \\Sigma^T := \\begin{bmatrix}\n",
    "\\mathbf I_m \\\\\n",
    " \\mathbf {00}^T \n",
    "\\end{bmatrix}$\n",
    "\n",
    "note that $\\mathbf \\Sigma \\mathbf \\Sigma^T = \\mathbf I_m$ and $\\mathbf S = \\mathbf \\Sigma^T \\mathbf \\Sigma$ is a projector with rank m (so we have the identity that $\\mathbf S^2 = \\mathbf S$ among other things).   \n",
    "\n",
    "\n",
    "now our top left principal submatrix is given by \n",
    "\n",
    "\n",
    "$\\big(\\mathbf B\\mathbf A\\big)_{1:m,1:m} = \\mathbf \\Sigma\\big(\\mathbf{BA}\\big)\\mathbf \\Sigma^T = \\big(\\mathbf \\Sigma\\mathbf{BA}\\big)\\mathbf \\Sigma^T = \n",
    "\\begin{bmatrix}\n",
    "\\tilde{ \\mathbf b_1}^T\\mathbf{A}\\\\\n",
    "\\tilde{ \\mathbf b_2}^T \\mathbf{A} \\\\ \n",
    "\\vdots\\\\ \n",
    "\\tilde{ \\mathbf b}_{m}^T \\mathbf{A} \\\\ \n",
    "\\end{bmatrix}\\mathbf\\Sigma^T = \\mathbf \\Sigma\\big(\\mathbf{BA}\\mathbf \\Sigma^T\\big) = \\mathbf \\Sigma \\bigg[\\begin{array}{c|c|c|c|c|c}\n",
    "\\mathbf B \\mathbf a_1 & \\mathbf B \\mathbf a_2 &\\cdots & \\mathbf B \\mathbf a_m \n",
    "\\end{array}\\bigg] $  \n",
    "\n",
    "$= \\big(\\mathbf \\Sigma\\mathbf B\\big)\\big(\\mathbf A\\mathbf \\Sigma^T\\big) =  \\big(\\mathbf B_{1:m,1:m}\\big)\\big(\\mathbf A_{1:m,1:m}\\big) $\n",
    "\n",
    "\n",
    "taking determinants \n",
    "\n",
    "$ \\det\\big(\\mathbf \\Sigma\\mathbf B\\big)\\det\\big(\\mathbf A\\mathbf \\Sigma^T\\big) =  \\det\\big(\\mathbf A\\mathbf \\Sigma^T\\big)\\det\\big(\\mathbf \\Sigma\\mathbf B\\big) = \\det\\big(\\mathbf A\\mathbf \\Sigma^T \\mathbf \\Sigma\\mathbf B\\big) = \\det\\big(\\mathbf A \\mathbf S \\mathbf B\\big) $\n",
    "\n",
    "we may have overloaded S here as we're using it for both the projector, and for $S \\in \\binom{[n]}{m}$.  However, we hope that this (almost) overloading hits the point home.  What Cauchy Binet tells us is precisely that \n",
    "\n",
    "$\\det\\big(\\mathbf {AB}\\big) = \\sum_{\\mathbf \\Sigma} \\det\\big(\\mathbf A \\mathbf \\Sigma^T\\big) \\det\\big( \\mathbf \\Sigma \\mathbf B\\big) = \\sum_{\\mathbf \\Sigma} \\det\\big(\\mathbf A \\mathbf \\Sigma^T \\mathbf \\Sigma \\mathbf B\\big) = \\sum_{\\mathbf \\Sigma} \\det\\big(\\mathbf A \\mathbf S \\mathbf B\\big) = \\sum_{\\mathbf \\Sigma} \\det\\big(\\mathbf A \\mathbf S^2 \\mathbf B\\big)$  \n",
    "\n",
    "\n",
    "note: we cannot actually directly make the argument with $\\det\\big(\\mathbf A \\mathbf S\\big)$ or $\\det\\big( \\mathbf S \\mathbf B\\big)$ here because they are not square matrices in general.  We need to work directly with $\\mathbf \\Sigma$ which is 'almost' a projector.  \n",
    "\n",
    "\n",
    "in words: the determinant of the matrix $\\big(\\mathbf {AB}\\big)$ is equal to the summation over all determinants computed on the projections of $\\mathbf A$ in the relevant $m$ dimensional subspace times the determinant of $\\mathbf B$ in that m dimensional subspace.  The summation has $\\binom{n}{m}$ terms in it because an $n$ x $m$ matrix $\\mathbf A$ (and $m$ x $n$ matrix for $\\mathbf B$) has precisely $\\binom{n}{m}$ different $m$ dimensional subspaces to be projected down onto.  \n",
    "\n",
    "This projector interpretation is an extremely nice result and very close to the \"geometric interpretation\" section in the wikipedia writeup here: https://en.wikipedia.org/wiki/Cauchy%E2%80%93Binet_formula  \n",
    "\n",
    "in general, seeing Cauchy Binet as a kind of bridge between two different but closely related characteristic polynomials (and their elementary symmetric functions) *and* this geometric interpretation in terms of m dimensional subspaces, is a very nice result.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [py35]",
   "language": "python",
   "name": "Python [py35]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
