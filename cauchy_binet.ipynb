{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this was written to expand on the simple proof in the wikipedia article (which is quite good, though notational issues may present challenges for many).  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as a **lemma** (reminder?) we need that fact that \n",
    "\n",
    "$e_k(\\lambda) = e_k(\\lambda_1, \\lambda_2, ..., \\lambda_n) = \\sum_{1 \\leq i_1 \\lt i_2 \\lt ... \\lt i_k \\leq n} \\lambda_{i,1}\\lambda_{i,2}...\\lambda_{i,k} $\n",
    "\n",
    "note that there are $\\binom{n}{k}$ terms in the above summation\n",
    "\n",
    "i.e. this is the kth elementary symmetric function (and if we scale by $(-1)^k$ this is given in the coefficient associated with $x^{n-k}$ in a degree $n$ polynomial with the above roots) \n",
    "\n",
    "referencing e.g. page 494 of Meyer's *Matrix Analysis* we recall that a principal submtratix of some $n$ x $n$ matrix $\\mathbf A$ is is one that can be selected by $\\{\\mathbf e_{i_1}, \\mathbf e_{i_2}, ..., \\mathbf e_{i_k}\\}$ where (note the bold!) $\\mathbf e_{1}$ refers to the first standard basis vector -- that is suppose we restrict $\\mathbf x$ to be a a linear combination of the above set of standard basis vectors -- our principal submatrix is the one that can be selected over all possible $\\mathbf x$ via the quadratic form  $\\mathbf x^* \\mathbf A \\mathbf x$.  \n",
    "\n",
    "Perhaps more simply we could also just write $\\mathbf x := \\mathbf e_{i_1}+ \\mathbf e_{i_2}+ ... + \\mathbf e_{i_k}$ and say the $k $ x $k$ principal submatrix is given by $\\mathbf A \\circ \\mathbf {xx}^*$ *and then deleting all rows and columns that have been zero'd out by this application of Hadamard product*.  Finally, the conventional defintion of a principal submatrix is more succinct: the submatrix that lies on the same set of $k$ rows and columns.  \n",
    "\n",
    "A *principal minor* is the determinant of said submatrix.  \n",
    "\n",
    "what is then proven (admittedly by the author as tediously) on page 495 of Meyer is that \n",
    "\n",
    "$e_k(\\lambda) = \\sum\\big(\\text{all k x k principal minors}\\big)$  \n",
    "\n",
    "note that the proof technique uses calculus and hence (like everything in that book) is designed for a field of characteristic zero.  \n",
    "\n",
    "It is worth noting that two very special cases of the above are $k=1$ and $k=n$, because in those cases we recover \n",
    "\n",
    "$e_1(\\lambda) = \\text{trace}\\big(\\mathbf A\\big)$    \n",
    "$e_n(\\lambda) = \\text{det}\\big(\\mathbf A\\big)$    \n",
    "\n",
    "While the specifics of principal minors can be a bit tedious, they are important to learn because they're a matrix invariant (i.e. they give terms of the characteristic polynomial).  \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Main Theorem**  \n",
    "\n",
    "consider an $m$ x $n$ matrix $\\mathbf A$ and an $n$ x $m$ matrix $\\mathbf B$  \n",
    "\n",
    "Thus $\\mathbf {AB}$ and $\\mathbf {BA}$ are both square matrices though their dimensions are different -- \n",
    "\n",
    "$\\big(\\mathbf {AB}\\big) $ is $m$ x $m$   \n",
    "while  \n",
    "$\\big(\\mathbf {BA}\\big)$ is $n$ x $n$  \n",
    "\n",
    "- - - -\n",
    "*in the familiar case of* $n = m$ there is a well known property that determinants multiply and hence \n",
    "\n",
    "$\\det\\big(\\mathbf {AB}\\big) = \\det\\big(\\mathbf A\\big) \\det\\big(\\mathbf B\\big)= \\det\\big(\\mathbf B\\big)\\det\\big(\\mathbf A\\big)=\\det\\big(\\mathbf {BA}\\big)$  \n",
    "\n",
    "- - - -\n",
    "*Cauchy Binet* generalizes this for the case where $m \\neq n$.  \n",
    "\n",
    "Interpretting the formula may be about as challenging as proving it (at least if we use the very elegant proof by Tao).  \n",
    "\n",
    "The formula states in terms of sets of principal submatrices \n",
    "\n",
    "$S \\in \\binom{[n]}{m}$  \n",
    "\n",
    "\n",
    "$\\det\\big(\\mathbf {AB}\\big) = \\sum_{S} \\det\\big(\\mathbf A_{[m],S}\\big)\\det\\big(\\mathbf B_{S,[m]}\\big) $  \n",
    "\n",
    "which reads as the set of all subsets of size $m$  of set $[n]$ --- which is the set $\\{1, 2, ..., n\\}$, so we want to choose subsets of cardinality $m$ from this   \n",
    "\n",
    "Since $\\big(\\mathbf {AB}\\big)$ is $m$ x $m$ this should seem somewhat intuitive.  It says that $\\det\\big(\\mathbf {AB}\\big)$ is given by summing over all principal minors of $\\mathbf A$ (times the 'related' principal minor in $\\mathbf B$).  And what size of principal minor would we be interested in? Well size $m$ of course because that is the dimension of $\\big(\\mathbf {AB}\\big)$ (which means that the underlying characteristic polynomial of such a product is degree $m$).  \n",
    "\n",
    "\n",
    "Note that if $m \\gt n$ then an immediate consequence is that our subset is the empty set, and hence $\\det\\big(\\mathbf {AB}\\big) = 0$.  And if $m =n$ we recover the familiar formula that determinants multiply.  \n",
    "\n",
    "the main case of interest, then, is when $m \\lt n$.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**lemma** \n",
    "\n",
    "$\\det\\big( z \\mathbf I_n + \\mathbf{BA}\\big)  = z^{n-m}\\det\\big( z \\mathbf I_m + \\mathbf{AB}\\big) $  \n",
    "\n",
    "\n",
    "**commentary:**  \n",
    "note 1.) this holds over arbitrary fields (as developed subsequently using blocked multiplication)  \n",
    "note 2.) more conventionally we'd see this relation, for the characteristic polynomial, as \n",
    "$\\det\\big( z \\mathbf I_n - \\mathbf{BA}\\big)  = z^{n-m}\\det\\big( z \\mathbf I_m - \\mathbf{AB}\\big) $  \n",
    "\n",
    "though this equality is, in effect, the same as the lemma   \n",
    "i.e. we can select $\\mathbf C:= - \\mathbf A$   \n",
    "and re-run the exact same argument in the lemma to get \n",
    "\n",
    "$\\det\\big( z \\mathbf I_n + \\mathbf{BC}\\big)  = z^{n-m}\\det\\big( z \\mathbf I_m + \\mathbf{CB}\\big) $  \n",
    "but this is equivalent to saying   \n",
    "$\\det\\big( z \\mathbf I_n - \\mathbf{BA}\\big)  = z^{n-m}\\det\\big( z \\mathbf I_m - \\mathbf{AB}\\big) $  \n",
    "which is equivalent to saying that the characteristic polynomials of $\\big(\\mathbf{BA}\\big)$ and $\\big(\\mathbf{AB}\\big)$ are the same, except one needs to be rescaled by $z^{n-m}$ (and this is precisely zero inflating).  \n",
    "\n",
    "the statement of the lemma is, as said, ultimately the same thing, however it gets rid of the effect of the sign function and says that the sums of principal minors of degree $r \\in \\{1, 2, ..., min(m,n)\\}$  are the same.  It is a small (and ultimately irrelevant) point about the role of the sign function that the reader may safely ignore.  \n",
    "\n",
    "\n",
    "- - - - - \n",
    "\n",
    "\n",
    "the above formula holds in general (though it is cleanest for any $z \\neq 0$ or alternatively if we insist $m\\lt n$ )  \n",
    "\n",
    "the underlying idea is: consider the characteristic polynomial of $\\big( \\mathbf{AB}\\big) $  \n",
    "\n",
    "$p(x) = c_0 x^m + c_1 x^{m-1} + c_2 x^{m-2} + ... + c_{m-2} x^{2} + c_{m-1} x^{1} + c_{m} $   \n",
    "\n",
    "and the characteristic polynomial of $\\big(\\mathbf{BA}\\big)$ which is given by \n",
    "\n",
    "$q(x) = a_0 x^n + a_1 x^{n-1} + a_2 x^{n-2} + ... + a_{n-2} x^{2} + a_{n-1} x^{1} + a_{n} $   \n",
    "\n",
    "then\n",
    "\n",
    "$ z_1^{n-m} \\cdot p(z_1)= q(z_1) = \\det\\big( z_1 \\mathbf I_n + \\mathbf{BA}\\big)$  \n",
    "\n",
    "and  \n",
    "\n",
    "$ z_2^{n-m} \\cdot p(z_2)= q(z_2) = \\det\\big( z_2 \\mathbf I_n + \\mathbf{BA}\\big)$  \n",
    "\n",
    "$ z_r^{n-m} \\cdot p(z_r)= q(z_r) = \\det\\big( z_r \\mathbf I_n + \\mathbf{BA}\\big)$   \n",
    "\n",
    "and of course we need only verify this at $\\text{max}(n,m)+1$ distinct points to completely characterize the polynomial and confirm that \n",
    "\n",
    "$q(x) = x^{n-m} \\cdot p(x) $   \n",
    "\n",
    "As is common this is most easily viewed and proven in fields with characteristic zero -- the eigenvalues existing when the scalar fields is $\\mathbb C$ and the elementary symmetric functions of those roots exist in general for a field of characteristic zero.  \n",
    "\n",
    "There are several ways to proceed.  Supposing $m\\leq n$, noting that \n",
    "\n",
    "$\\text{trace}\\Big(\\big(\\mathbf{BA}\\big)^k\\Big) = \\text{trace}\\Big(\\big(\\mathbf{AB}\\big)^k\\Big)$ for all natural numbers $k$ which obviously covers $k \\in \\{1, 2, ..., m, m+1\\}$, and then combining this with Newton's Identities tells us, starting from the largest / monic term and working down to lower order terms. that the first $m+1$ coefficients of $p(x)$ and $q(x)$ must be the same.  In fact the only difference is the degree associated with those terms which occurs when $m\\lt n$ and the 'correction' is given by multiplying the lower order polynomial by $z^{n-m}$.  \n",
    "\n",
    "\n",
    "\n",
    "As proven in the associated Vandermonde Matrix writeup $\\det\\big(\\mathbf{BA}\\big)$ and $\\det\\big(\\mathbf{AB}\\big)$ have the same non-zero eigenvalues with same algebraic multiplicities.  (The number of zeros then is completely determined by dimension.)  If we apply the elementary symmetric function $e_k(\\lambda)$ to the eigenvalues of $\\det\\big(\\mathbf{BA}\\big)$ and $\\det\\big(\\mathbf{AB}\\big)$ then that is *exactly* the result we get -- i.e. the characteristic polynomials are identical -- and even have the same number of non-zero terms-- except if $m\\neq n$ then one polynomial is the \"inflated\" one of the other.  \n",
    "\n",
    "Note that the above identity is closely related to the sylvester determinant identity.    \n",
    "https://en.wikipedia.org/wiki/Sylvester%27s_determinant_identity\n",
    "\n",
    "\n",
    "Two other points are worth making though we won't dwell on them: \n",
    "\n",
    "1.) $\\det\\big(\\mathbf{BA}\\big)$ and $\\det\\big(\\mathbf{AB}\\big)$ are 'almost similar' which has topological implications\n",
    "\n",
    "2.) With respect to the sign function -- the above actually tells us we have nothing to worry about.  In particular it says that if we list the characteristic polynomials as we have -- and again with the case of $m\\leq n$ in mind-- what it tells us is if we index at zero and look at that the $(m)$th coefficient from the left of $p(x)$ (which is $c_{m}$ the constant term / determinant of $\\big( \\mathbf{AB}\\big) $ multiplied by $(-1)^m$ ) is the same as the $(m)$th coefficient from the left of $q(x)$ which is $a_m$ which is $(-1)^m$ times the sum over size m principal minors for  $\\big( \\mathbf{BA}\\big)$.  That is-- the impact on the characteristic polynomials for each is a scaling by $-1$ for both terms or by $+1$ for both terms -- and hence it is a wash when comparing the terms. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**detour: blocked mutliplication proof of the above result over some field** $\\mathbb K$  \n",
    "this introduces Schur Complements and is exercise 6.2.16 in Meyer's *Matrix Analysis*, albeit with somewhat different notation  \n",
    "\n",
    "to get a more general result, consider the $(m+n)$  x $ (m+n)$ matrix \n",
    "\n",
    "$\\begin{bmatrix}\n",
    "z \\mathbf I_n & z\\mathbf B\\\\ \n",
    "-\\mathbf A & z \\mathbf I_m\n",
    "\\end{bmatrix}$ \n",
    "\n",
    "where as before, we have \n",
    "\n",
    "an $m$ x $n$ matrix $\\mathbf A$ and an $n$ x $m$ matrix $\\mathbf B$  \n",
    "\n",
    "the text then mentions (6.2.1) on page 475 which states that if $\\mathbf W$ and $\\mathbf Z$ are square then we have \n",
    "\n",
    "(in each case if the relevent inverse exists)  \n",
    "\n",
    "$\\begin{bmatrix}\n",
    "\\mathbf W & \\mathbf X\\\\ \n",
    "\\mathbf Y & \\mathbf Z\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "\\mathbf I & \\mathbf 0\\\\ \n",
    "\\mathbf {YW}^{-1} & \\mathbf I\n",
    "\\end{bmatrix}\\begin{bmatrix}\n",
    "\\mathbf W & \\mathbf X\\\\ \n",
    "\\mathbf 0 & \\mathbf Z - \\mathbf {YW}^{-1}\\mathbf X\n",
    "\\end{bmatrix}$ \n",
    "\n",
    "The result may be verified by direct blocked multiplication.  The components in the bottom right corner $\\big(\\mathbf Z - \\mathbf {YW}^{-1}\\mathbf X\\big)$ are called the Schur Complement.  \n",
    "\n",
    "\n",
    "it follows that \n",
    "\n",
    "$\\det\\Big(\\begin{bmatrix}\n",
    "\\mathbf W & \\mathbf X\\\\ \n",
    "\\mathbf Y & \\mathbf Z\n",
    "\\end{bmatrix} \\Big) = 1 \\cdot \\det\\big(\\mathbf W\\big)\\det\\big(\\mathbf Z - \\mathbf {YW}^{-1}\\mathbf X\\big)$ \n",
    "\n",
    "by making use of determinants over blocked triangular matrices and the fact that determinants of matrices multiply.  \n",
    "\n",
    "we can re-run the entire argument over a slightly different structure, or more cleverly notice that with suitable permutation matrix $\\mathbf P$, we have a graph isomorphism whereby \n",
    "\n",
    "\n",
    "$\\mathbf P \\begin{bmatrix}\n",
    "\\mathbf W & \\mathbf X\\\\ \n",
    "\\mathbf Y & \\mathbf Z\n",
    "\\end{bmatrix}\\mathbf P^T = \\mathbf P\\Big( \\begin{bmatrix}\n",
    "\\mathbf W & \\mathbf 0\\\\ \n",
    "\\mathbf 0 & \\mathbf Z\n",
    "\\end{bmatrix} + \\begin{bmatrix}\n",
    "\\mathbf 0 & \\mathbf X\\\\ \n",
    "\\mathbf Y & \\mathbf 0\n",
    "\\end{bmatrix}\\Big) \\mathbf P^T =\\mathbf P \\begin{bmatrix}\n",
    "\\mathbf W & \\mathbf 0\\\\ \n",
    "\\mathbf 0 & \\mathbf Z\n",
    "\\end{bmatrix}\\mathbf P^T  + \\mathbf P \\begin{bmatrix}\n",
    "\\mathbf 0 & \\mathbf X\\\\ \n",
    "\\mathbf Y & \\mathbf 0\n",
    "\\end{bmatrix}\\mathbf P^T =\\begin{bmatrix}\n",
    "\\mathbf Z & \\mathbf 0\\\\ \n",
    "\\mathbf 0 & \\mathbf W\n",
    "\\end{bmatrix} +\\begin{bmatrix}\n",
    "\\mathbf 0 & \\mathbf Y\\\\ \n",
    "\\mathbf X & \\mathbf 0\n",
    "\\end{bmatrix} =  \\begin{bmatrix}\n",
    "\\mathbf Z & \\mathbf Y\\\\ \n",
    "\\mathbf X & \\mathbf W\n",
    "\\end{bmatrix}$\n",
    "\n",
    "\n",
    "where the intermediate steps of decomposing the matrix / graph into two separate graphs -- one that is a reducible graph and the other is bipartite-- hopefully makes the graph isomorphism more obvious.  \n",
    "\n",
    "But since graph isomorpohism are a (very special) kind of similarity transform, the invariants, including determinants, do not change.  Thus we immediately get (by 'eyeballing' the underlying Schur Complement for the latter case which again can be interpretted from Graph Isomorphism perspective as re-labelling the underlying nodes )  \n",
    "\n",
    "$\\det\\big(\\mathbf W\\big)\\det\\big(\\mathbf Z - \\mathbf {YW}^{-1}\\mathbf X\\big) = \\det\\Big(\\begin{bmatrix}\n",
    "\\mathbf W & \\mathbf X\\\\ \n",
    "\\mathbf Y & \\mathbf Z\n",
    "\\end{bmatrix}\\Big) = \\det\\Big(\\begin{bmatrix}\n",
    "\\mathbf Z & \\mathbf Y\\\\ \n",
    "\\mathbf X & \\mathbf W\n",
    "\\end{bmatrix}\\Big) = \\det\\big(\\mathbf Z\\big)\\det\\big(\\mathbf W - \\mathbf {XZ}^{-1}\\mathbf Y\\big)$\n",
    "\n",
    "again, supposing that $\\mathbf Z^{-1}$ and $\\mathbf W^{-1}$ exist -- if they do not then the determinant may still be non-zero, but these blocked arguments using Schur complements are not directly usable.  \n",
    "\n",
    "- - - -  -\n",
    "now we return to our original problem here, for any $z \\neq 0$ \n",
    "\n",
    "$  \\det\\Big(z \\mathbf I_m\\Big) \\det\\Big(z\\mathbf I_n - z\\mathbf B \\big(z \\mathbf I_m \\big)^{-1} \\big(-\\mathbf A\\big) \\Big)= \\det\\Big(\\begin{bmatrix}\n",
    "z \\mathbf I_n & z\\mathbf B\\\\ \n",
    "-\\mathbf A & z \\mathbf I_m\n",
    "\\end{bmatrix}\\Big)  = \\det\\Big(z \\mathbf I_n\\Big) \\det\\Big(z\\mathbf I_m - \\big(-\\mathbf A\\big)\\big(z \\mathbf I_n\\big)^{-1}z\\mathbf B\\Big)  $ \n",
    "\n",
    "which simplifies to \n",
    "\n",
    "$z^m \\det\\Big(z\\mathbf I_n + \\mathbf B  \\mathbf A \\Big)=z^n \\det\\Big(z\\mathbf I_m + \\mathbf A\\mathbf B\\Big)  $ \n",
    "\n",
    "$\\det\\Big(z\\mathbf I_n + \\mathbf B  \\mathbf A \\Big) =z^{n-m} \\det\\Big(z\\mathbf I_m + \\mathbf A\\mathbf B\\Big)$ \n",
    "\n",
    "**remark:** \n",
    "This is a more general result that works over arbitrary fields.  It introduces the Schur Complement, but aside from that it is elementary, relying only on blocked multiplication and the fact that determinants multiply.  However, to your author, the final result feels almost like a trick or some kind of coincidence.  Interpretting the result directly over scalar fields of characteristic zero -- and interparticular in terms of characteristic polynomials being the same (except for 'zero inflating') requires more advanced machinery and would seem to be a less general result *but* the result seems somewhat obvious in this setting -- the traces match and apply Newton's Identities.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# proof of the main theorem: Cauchy Binet\n",
    "\n",
    "Making use the prior two lemmas what we have is, supposing $m\\leq  n$, then the sum of the $mth$ principal minors of $\\big(\\mathbf{BA}\\big)$  \n",
    "\n",
    "is equal to the constant term (read: determinant, setting aside the sign function for now) of the characteristic polynomial of $\\big(\\mathbf{AB}\\big)$ .  \n",
    "\n",
    "That is for $S \\in \\binom{[n]}{m}  $  \n",
    "\n",
    "we have \n",
    "\n",
    "$e_m(\\lambda_1, \\lambda_2, ..., \\lambda_m, 0, 0, ..., 0) = \\sum_{S} \\det\\big(\\mathbf {BA}\\big)_{S,S} = \\sum_{S} \\det\\big(\\mathbf B_{S,[m]}\\mathbf A_{[m],S}\\big) = \\sum_{S} \\det\\big(\\mathbf B_{S,[m]}\\big)\\det\\big(\\mathbf A_{[m],S}\\big)$  \n",
    "\n",
    "where we first make use of relating the elementary symmetric function to the sum over principal minors, then note that $\\det\\big(\\mathbf {BA}\\big)_{S,S}$ involves a square matrix and hence the determinants multiply.  \n",
    "\n",
    "\n",
    "$= \\sum_{S} \\det\\big(\\mathbf A_{[m],S}\\big)\\det\\big(\\mathbf B_{S,[m]}\\big) =  e_m(\\lambda_1, \\lambda_2, ..., \\lambda_m)  = \\det\\big(\\mathbf {AB}\\big) $\n",
    "\n",
    "and then because the determinants are scalars which commute, we can interchange the way they mutliply and because the relevant terms in the characteristic polynomial $\\big(\\mathbf{BA}\\big)$ and $\\big(\\mathbf{AB}\\big)$ are the same, then this summation must be equal to the determinant.   \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Now a closer look at the formula, using blocked matrices*  \n",
    "\n",
    "If we first partition $\\mathbf A$ and then $\\mathbf B$, we have \n",
    "\n",
    "\n",
    "$\\mathbf A = \n",
    "\\bigg[\\begin{array}{c|c|c|c|c}\n",
    "\\mathbf a_1 & \\mathbf a_2 &\\cdots & \\mathbf a_{n-1} & \\mathbf a_{n}\n",
    "\\end{array}\\bigg]\n",
    "$   \n",
    "\n",
    "$\\mathbf B= \n",
    "\\begin{bmatrix}\n",
    "\\tilde{ \\mathbf b_1}^T \\\\\n",
    "\\tilde{ \\mathbf b_2}^T \\\\ \n",
    "\\vdots\\\\ \n",
    "\\tilde{ \\mathbf b}_{n-1}^T \\\\ \n",
    "\\tilde{ \\mathbf b_n}^T\n",
    "\\end{bmatrix}\n",
    "$   \n",
    "\n",
    "and the multiplication looks like:  \n",
    "\n",
    "$\\mathbf{BA} = \n",
    "\\begin{bmatrix}\n",
    "\\tilde{ \\mathbf b_1}^T\\mathbf{A} \\\\\n",
    "\\tilde{ \\mathbf b_2}^T \\mathbf{A} \\\\ \n",
    "\\vdots\\\\ \n",
    "\\tilde{ \\mathbf b}_{m}^T \\mathbf{A} \\\\ \n",
    "\\vdots\\\\ \n",
    "\\tilde{ \\mathbf b_n}^T \\mathbf{A}\n",
    "\\end{bmatrix} = \\bigg[\\begin{array}{c|c|c|c|c|c}\n",
    "\\mathbf B \\mathbf a_1 & \\mathbf B \\mathbf a_2 &\\cdots &\\mathbf B \\mathbf a_m &\\cdots & \\mathbf B \\mathbf a_{n}\n",
    "\\end{array}\\bigg]$  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence if we want the mth term of the characteristic polynomial of $\\mathbf{BA}$ -- again setting aside the sign function-- then we want to sum over the $\\binom{n}{m}$ principal minors of $\\mathbf{BA}$ hence we want to sum over all n choose m selections of principal submatrices (and in particular their determinants) \n",
    "\n",
    "Now using permutation matrices / graph isomorphism arguments it is enough to consider the case WLOG that we are interested in the top left $m$ x $m$ principal submatrix and its determinant  \n",
    "\n",
    "*It is generally advantageous to model what we want to \"grab\" via matrix multiplication (which behaves in line with well known Linear Algebra rules) so we introduce:  *  \n",
    "\n",
    "\n",
    "To borrow from the (in general) non-square diagonal matrix in SVD, consider the $m$ x $n$ matrix  \n",
    "\n",
    "$\\mathbf \\Sigma := \\begin{bmatrix}\n",
    "\\mathbf I_m &\n",
    " \\mathbf {00}^T \n",
    "\\end{bmatrix}$\n",
    "\n",
    "and of course its transpose is given by \n",
    "\n",
    "$\\mathbf \\Sigma^T := \\begin{bmatrix}\n",
    "\\mathbf I_m \\\\\n",
    " \\mathbf {00}^T \n",
    "\\end{bmatrix}$\n",
    "\n",
    "note that $\\mathbf \\Sigma \\mathbf \\Sigma^T = \\mathbf I_m$ and $\\mathbf S = \\mathbf \\Sigma^T \\mathbf \\Sigma$ is a projector with rank m (so we have the identity that $\\mathbf S^2 = \\mathbf S$ among other things).   \n",
    "\n",
    "\n",
    "now our top left principal submatrix is given by \n",
    "\n",
    "\n",
    "$\\big(\\mathbf B\\mathbf A\\big)_{1:m,1:m} = \\mathbf \\Sigma\\big(\\mathbf{BA}\\big)\\mathbf \\Sigma^T = \\big(\\mathbf \\Sigma\\mathbf{BA}\\big)\\mathbf \\Sigma^T = \n",
    "\\begin{bmatrix}\n",
    "\\tilde{ \\mathbf b_1}^T\\mathbf{A}\\\\\n",
    "\\tilde{ \\mathbf b_2}^T \\mathbf{A} \\\\ \n",
    "\\vdots\\\\ \n",
    "\\tilde{ \\mathbf b}_{m}^T \\mathbf{A} \\\\ \n",
    "\\end{bmatrix}\\mathbf\\Sigma^T = \\mathbf \\Sigma\\big(\\mathbf{BA}\\mathbf \\Sigma^T\\big) = \\mathbf \\Sigma \\bigg[\\begin{array}{c|c|c|c|c|c}\n",
    "\\mathbf B \\mathbf a_1 & \\mathbf B \\mathbf a_2 &\\cdots & \\mathbf B \\mathbf a_m \n",
    "\\end{array}\\bigg] $  \n",
    "\n",
    "$= \\big(\\mathbf \\Sigma\\mathbf B\\big)\\big(\\mathbf A\\mathbf \\Sigma^T\\big) =  \\big(\\mathbf B_{1:m,1:m}\\big)\\big(\\mathbf A_{1:m,1:m}\\big) $\n",
    "\n",
    "\n",
    "taking determinants \n",
    "\n",
    "$ \\det\\big(\\mathbf \\Sigma\\mathbf B\\big)\\det\\big(\\mathbf A\\mathbf \\Sigma^T\\big) =  \\det\\big(\\mathbf A\\mathbf \\Sigma^T\\big)\\det\\big(\\mathbf \\Sigma\\mathbf B\\big) = \\det\\big(\\mathbf A\\mathbf \\Sigma^T \\mathbf \\Sigma\\mathbf B\\big) = \\det\\big(\\mathbf A \\mathbf S \\mathbf B\\big) $\n",
    "\n",
    "we may have overloaded S here as we're using it for both the projector, and for $S \\in \\binom{[n]}{m}$.  However, we hope that this (almost) overloading hits the point home.  What Cauchy Binet tells us is precisely that \n",
    "\n",
    "$\\det\\big(\\mathbf {AB}\\big) = \\sum_{\\mathbf \\Sigma} \\det\\big(\\mathbf A \\mathbf \\Sigma^T\\big) \\det\\big( \\mathbf \\Sigma \\mathbf B\\big) = \\sum_{\\mathbf \\Sigma} \\det\\big(\\mathbf A \\mathbf \\Sigma^T \\mathbf \\Sigma \\mathbf B\\big) = \\sum_{\\mathbf \\Sigma} \\det\\big(\\mathbf A \\mathbf S \\mathbf B\\big) = \\sum_{\\mathbf \\Sigma} \\det\\big(\\mathbf A \\mathbf S^2 \\mathbf B\\big)$  \n",
    "\n",
    "\n",
    "note: we cannot actually directly make the argument with $\\det\\big(\\mathbf A \\mathbf S\\big)$ or $\\det\\big( \\mathbf S \\mathbf B\\big)$ here because they are not square matrices in general.  We need to work directly with $\\mathbf \\Sigma$ which is 'almost' a projector.  \n",
    "\n",
    "\n",
    "in words: the determinant of the matrix $\\big(\\mathbf {AB}\\big)$ is equal to the summation over all determinants computed on the projections of $\\mathbf A$ in the relevant $m$ dimensional subspace times the determinant of $\\mathbf B$ in that m dimensional subspace.  The summation has $\\binom{n}{m}$ terms in it because an $n$ x $m$ matrix $\\mathbf A$ (and $m$ x $n$ matrix for $\\mathbf B$) has precisely $\\binom{n}{m}$ different $m$ dimensional subspaces to be projected down onto.  \n",
    "\n",
    "This projector interpretation is an extremely nice result and very close to the \"geometric interpretation\" section in the wikipedia writeup here: https://en.wikipedia.org/wiki/Cauchy%E2%80%93Binet_formula  \n",
    "\n",
    "in general, seeing Cauchy Binet as a kind of bridge between two different but closely related characteristic polynomials (and their elementary symmetric functions) *and* this geometric interpretation in terms of m dimensional subspaces, is a very nice result.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
