{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For (scalar) fields of characteristic zero, we can prove the general Cayley Hamilton Theorem via the use of a density argument, which at its core says that any defective n x n matrix is arbitrarily close to a 'nice' (i.e. diagonalizable) matrix which ultimately implies that the defective matrix must be annihlated by its characteristic polynomial just like diagonalizable matrices are annhilated by their characteristic polynomial.  \n",
    "\n",
    "The underlying idea is very simple.  And the proof below can be radically streamlined, depending on what tools the reader has available to use.  However the proof below really takes a basic 'introductory' route, showing that the density argument needs nothing more than \n",
    "\n",
    "(i) Schur's Triangularization theorem   \n",
    "(ii) matrices with all distinct eigenvalues are diagonalizable  \n",
    "(iii) single variable polynomials are continuous  \n",
    "(iv) repeated application of trianle inequality   \n",
    "\n",
    "note: approximately $\\frac{2}{3}$ the way down in the Vandermonde Matrices writeup there is a direct (plodding) algebraic proof of Cayley Hamilton for all (upper) triangular matrices.  This means that in $\\mathbb C$ if we know how to apply (i), we have a direct algebraic proof that all matrices obey Cayley Hamilton, since they are all similar to an upper triagular matrix  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The underlying idea behind this proof is \n",
    "\n",
    "1.) Finite dimensions   \n",
    "2.) Continuity (using say the standard metric)   \n",
    "and in particular the fact that a polynomial function   \n",
    "$p : \\mathbb C \\mapsto \\mathbb C$  \n",
    "is a continuous map  \n",
    "so we know for any $\\epsilon \\gt 0$ there exists some $\\delta \\gt 0$ such that  \n",
    "\n",
    "$p\\big(N(x, \\delta)\\big) \\subset N\\big(p(x), \\epsilon\\big)$  \n",
    "\n",
    "i.e.   \n",
    "$\\big \\vert x - x_1 \\big \\vert \\lt \\delta \\longrightarrow \\big \\vert p(x) - p(x_1)\\big \\vert \\lt \\epsilon$  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**optional background on the continuity of single variable polynomial map p**    \n",
    "\n",
    "There are two typical ways of verifying the continuity of $p$.  \n",
    "\n",
    "The *standard way* is to verify that \n",
    "\n",
    "i.) adding two continuous functions gives a continuous function (i.e. the result is immeditate as we merely select $\\frac{\\epsilon}{2} + \\frac{\\epsilon}{2} = \\epsilon$) and the result follows by induction for the sum of $n$ continuous functions.  \n",
    "\n",
    "ii.) multiplication of two continuous functions is continuous  \n",
    "(i.e. the idea is $x\\mapsto 1$ is continuous and $x \\mapsto x$ is continuous by application of $\\epsilon-\\delta$ definition.  then $x \\mapsto x^2 = g(x)\\cdot g(x)$ and $x \\mapsto x^3 = x^2 \\cdot g(x)$ and so on.  \n",
    "\n",
    "the proof of this standard result can be given proven by directly examining the definition that for any $\\epsilon \\gt 0$, with continuous functions $f,g$  and the map $h$ we want to prove is continuous given by $h(x) = f(x)g(x)$  \n",
    "$\\big \\vert h(x) - h(x_1)\\big \\vert $  \n",
    "$= \\big \\vert f(x)g(x) - f(x_1)g(x_1)\\big \\vert $  \n",
    "$= \\big \\vert\\big\\{ f(x)g(x) -f(x)g(x_1) \\big\\}- \\big\\{f(x_1)g(x_1) - f(x)g(x_1)\\big\\}\\big \\vert $   \n",
    "$\\leq \\big \\vert f(x)g(x) -f(x)g(x_1) \\big\\vert + \\big \\vert f(x_1)g(x_1) - f(x)g(x_1)\\big \\vert $   \n",
    "$\\leq \\big \\vert f(x)\\big \\vert \\cdot \\big \\vert g(x) -g(x_1) \\big\\vert + \\big \\vert g(x_1)\\big \\vert \\cdot \\big \\vert f(x_1) - f(x)\\big \\vert $   \n",
    "$\\leq \\big \\vert f(x)\\big \\vert \\cdot \\big(\\frac{\\epsilon}{2}\\frac{1}{\\big \\vert f(x)\\big \\vert + 1 }\\big) + \\big \\vert g(x_1)\\big \\vert \\cdot \\big \\vert f(x_1) - f(x)\\big \\vert $   \n",
    "$\\leq \\big \\vert f(x)\\big \\vert \\cdot \\big(\\frac{\\epsilon}{2}\\frac{1}{\\big \\vert f(x)\\big \\vert + 1 }\\big) + \\big \\vert g(x_1)\\big \\vert \\cdot \\big(\\frac{\\epsilon}{2}\\frac{1}{\\big \\vert g(x)\\big \\vert + 1 }\\big) $   \n",
    "$\\lt \\frac{\\epsilon}{2} + \\frac{\\epsilon}{2}$  \n",
    "$= \\epsilon$  \n",
    "\n",
    "where the 3rd to last inequality is given by the continuity of $g$ which implies there is some $\\delta_1$ such that if  \n",
    "\n",
    "$\\big \\vert x-x_1\\big \\vert \\lt \\delta_1 \\longrightarrow \\big \\vert g(x)-g(x_1)\\big \\vert  \\lt \\epsilon_1 := \\frac{\\epsilon}{2}\\frac{1}{\\big \\vert f(x)\\big \\vert + 1 } $    \n",
    "where $f(x)$ is just some constant  \n",
    "\n",
    "the 2nd to last inequality comes from the continuity of $f$ which implies there is some $\\delta_2 \\gt 0$ such that    \n",
    "$\\big \\vert x-x_1\\big \\vert \\lt \\delta_2 \\longrightarrow \\big \\vert f(x) - f(x_1)\\big \\vert \\lt \\epsilon_2 := \\frac{\\epsilon}{2}\\frac{1}{\\big \\vert g(x)\\big \\vert + 1 }$    \n",
    "\n",
    "there would appear to be a subtle mismatch, because we want to show  \n",
    "\n",
    "$\\big \\vert g(x_1)\\big \\vert \\cdot \\frac{\\epsilon}{2}\\frac{1}{\\big \\vert g(x)\\big \\vert + 1 } \\lt \\frac{\\epsilon }{2}$  \n",
    "and while $g(x)$ is constant, $g(x_1)$ in general is not.  However we can still bound the size of $g(x_1)$, by again calling on the continuity of $g$ we can estimate $\\big\\vert g(x_1)\\big \\vert$  via triangle inequality  \n",
    "\n",
    "$\\big\\vert g(x_1)\\big \\vert = \\big\\vert g(x) + \\big\\{g(x_1) - g(x)\\big\\}\\big \\vert  \\leq \\big\\vert g(x)\\big \\vert + \\big \\vert g(x_1) - g(x)\\big \\vert \\lt  \\big\\vert g(x)\\big \\vert + 1$   \n",
    "for some $\\delta_3 \\gt 0$ (selecting $\\epsilon_3 :=1$ by the continuity of $g$)  \n",
    "\n",
    "and finally we select $\\delta = \\min\\big(\\delta_1, \\delta_2,\\delta_3\\big)$,  which completes the proof  \n",
    "\n",
    "\n",
    "The *lazy way* is   \n",
    "i.) first prove the power rule for derivatives (in the complex plane)   \n",
    "if we want to differentiate $x^n$, then (e.g. see ex 2.5 in *Cauchy Schwarz Masterclass*)   \n",
    "$x^{n}a^0 + x^{n-1}a^{1} + x^{n-2}a^{2} + ... +x^{2}a^{n-2} + x^{1}a^{n-1} + x^{0}a^{n} = \\frac{ a^{n+1}- x^{n+1} }{ a-x}$  \n",
    "\n",
    "where the RHS is our difference quotient, and we have   \n",
    "$\\frac{d}{dx} x^{n+1} = \\lim_{a \\to x}\\frac{x^{n+1} - a^{n+1}}{x - a} = \\lim_{a \\to x}\\big(x^{n}\\cdot 1 + x^{n-1}a^{1} + x^{n-2}a^{2} + ... +x^{2}a^{n-2} + x^{1}a^{n-1} + 1\\cdot a^{n}\\big)= (n+1)\\cdot x^n$  \n",
    "\n",
    "ii.) use linearity of the derivative, and the finite degree of the polynomial, to differentiate term by term (immediate, in effect by using $\\frac{\\epsilon}{n}$ when there are n terms we want to take a limit on)  \n",
    "\n",
    "iii.) recall that differentiability implies continuity because, all we need to satisfy is  \n",
    "$\\big \\vert x - x_1 \\big \\vert \\lt \\delta \\longrightarrow \\big \\vert p(x) - p(x_1)\\big \\vert \\lt \\epsilon$    \n",
    "\n",
    "and differentiability gives us  \n",
    "$\\big \\vert x - x_1 \\big \\vert \\lt \\delta \\longrightarrow \\big \\vert \\frac{p(x) - p(x_1)}{x - x_1}- L\\big \\vert \\lt \\epsilon$    \n",
    "\n",
    "in particular for both continuity and differentiability, whenever we have a satisfying $\\delta$ we may always choose an even smaller $\\delta$ and it will still satisfy the above, hence consider $\\delta \\in \\big(0, 1\\big)$ and rescaling the RHS by $\\delta$ gives   \n",
    "\n",
    "\n",
    "$\\big \\vert x - x_1 \\big \\vert \\lt \\delta \\longrightarrow \\big \\vert \\frac{p(x) - p(x_1)}{\\omega}- \\delta L\\big \\vert \\lt \\delta \\cdot \\epsilon \\lt \\epsilon$    \n",
    "(where $\\omega$ is some point on the unit circle)  \n",
    "and applying the 'reverse' triangle inequality we have  \n",
    "\n",
    "$\\Big \\vert \\big \\vert p(x) - p(x_1)\\big \\vert - \\delta \\big \\vert  L\\big \\vert\\Big \\vert = \\Big \\vert \\big \\vert \\frac{p(x) - p(x_1)}{\\omega}\\big \\vert - \\big \\vert \\delta L\\big \\vert\\Big \\vert \\leq \\big \\vert \\frac{p(x) - p(x_1)}{\\omega}- \\delta L\\big \\vert \\lt \\delta \\cdot \\epsilon $  \n",
    "\n",
    "now we linearize the lower bound by considering two cases  \n",
    "first if \n",
    "$\\vert p(x) - p(x_1)\\big \\vert \\lt \\delta \\big \\vert  L\\big \\vert$   \n",
    "then the desired result is immediate by selecting  \n",
    "$\\delta =: \\text{min}\\big(\\frac{1}{2}\\epsilon^*, \\frac{1}{2}\\big \\vert L\\big \\vert^{-1}\\cdot \\epsilon^*\\big)$     \n",
    "where $\\epsilon^* = \\text{min}\\big(1,\\epsilon\\big)$  \n",
    "\n",
    "second if  \n",
    "$\\vert p(x) - p(x_1)\\big \\vert \\geq \\delta \\big \\vert  L\\big \\vert$   \n",
    "\n",
    "then we have  \n",
    "$\\big \\vert p(x) - p(x_1)\\big \\vert - \\delta \\big \\vert  L\\big \\vert = \\Big \\vert \\big \\vert p(x) - p(x_1)\\big \\vert - \\delta \\big \\vert  L\\big \\vert\\Big \\vert \\lt \\delta \\cdot \\epsilon $  \n",
    "\n",
    "so  \n",
    "$\\big \\vert p(x) - p(x_1)\\big \\vert \\lt + \\delta \\big \\vert  L\\big \\vert +\\delta \\cdot \\epsilon \\leq \\frac{1}{2}\\epsilon^* +\\frac{1}{2}\\epsilon{^*}\\epsilon \\leq \\frac{1}{2}\\epsilon + \\frac{1}{2}\\epsilon = \\epsilon $  \n",
    "selecting   \n",
    "$\\delta =: \\text{min}\\big(\\frac{1}{2}\\epsilon^*, \\frac{1}{2}\\big \\vert L\\big \\vert^{-1}\\cdot \\epsilon^*\\big)$      \n",
    "as before   \n",
    "\n",
    "*a small clarification*  \n",
    "the attentive reader may wonder in steps (i) and (ii) of the lazy derivation, how we know  \n",
    "\n",
    "$\\frac{d}{dx} x^{n+1} $  \n",
    "$= \\lim_{a \\to x}\\big(x^{n}\\cdot 1 + x^{n-1}a^{1} + x^{n-2}a^{2} + ... +x^{2}a^{n-2} + x^{1}a^{n-1} + 1\\cdot a^{n}\\big)$  \n",
    "$= \\lim_{a \\to x}x^{n} + \\big(\\sum_{j=1}^{n-1} \\lim_{a \\to x} x^{n-j}a^j\\big) + \\lim_{a \\to x} a^{n}\\big)$  \n",
    "$= x^{n} + \\big(\\sum_{j=1}^{n-1} \\lim_{a \\to x} x^{n-j}a^j\\big) + \\lim_{a \\to x} a^{n}\\big)$  \n",
    "$= x^{n} + \\big(\\sum_{j=1}^{n-1}  x^{n-j}x^{j}\\big) + x^{n}\\big)$  \n",
    "$= x^{n} + \\big(\\sum_{j=1}^{n-1}  x^{n}\\big) + x^{n}\\big)$  \n",
    "\n",
    "i.e. how do we justify the second to last step as it would appear to use what we want to prove -- that $f_j:x \\mapsto x^j$ is continuous.  However it in fact uses an induction hypothesis, which nicely makes use of overlapping subproblems.  In particular the cleanest form of the above proof is to first note that the constant function\n",
    "\n",
    "$f : x\\mapsto 1$  \n",
    "\n",
    "is continuous by immediate application of the $\\epsilon-\\delta$ definition, and  \n",
    "\n",
    "$f : x\\mapsto x$  \n",
    "is also continuous (e.g. select $\\delta = \\frac{\\epsilon}{2}$)   \n",
    "\n",
    "now we run the above differentiation argument on $x\\mapsto x^2$, making use of the fact that $a^n$ has a derivative written as a linear combination of lower order powers of $a$ which are continuous  \n",
    "\n",
    "$\\frac{d}{dx} x^{1+1}= x^{n} + \\lim_{a \\to x} a^{1} = 2x $  \n",
    "(n=1)  \n",
    "\n",
    "and formally we now have a strong inductive hypothesis of continuity for  \n",
    "$f : x\\mapsto x^k$ for $k \\in \\{0,1,2,...,n\\}$  \n",
    "(where it is understood that $k=0$ means the constant function)    \n",
    "\n",
    "and we then show this implies continuity in the case of $x \\mapsto x^{n+1}$ because  \n",
    "$\\frac{d}{dx} x^{n+1} = x^{n} + \\big(\\sum_{j=1}^{n-1} \\lim_{a \\to x} x^{n-j}a^j\\big) + \\lim_{a \\to x} a^{n} = x^{n} + \\big(\\sum_{j=1}^{n-1}  x^{n}\\big) + x^{n} = (n+1)x^n$  \n",
    "\n",
    "where we make use of the continuity of lower order powers of $x$ (and that the limit point is in our set i.e. the entire the open ball (/ delta neighborhood) around $x$, which includes $x$ itself  \n",
    "\n",
    "hence the above proves that the derivative exists, and as we've show in (iii) $\\text{differentiability} \\longrightarrow\\text{continuity}$ which completes the proof  \n",
    "\n",
    "**this concludes optional background material on the continuity of single variable polynomial map p**  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cayley Hamilton Theorem via density argument** for matrices in $\\mathbb C^\\text{n x n}$   \n",
    "\n",
    "We start by considering something 'nice' -- i.e. a diagonalizable $\\mathbf A$.  \n",
    "\n",
    "As shown elsewhere if a matrix $\\mathbf A$ is diagonalizable then  \n",
    "$\\mathbf 0 = p_A\\big(\\mathbf A\\big)= \\big(\\mathbf I\\lambda_1 - \\mathbf A\\big)\\big(\\mathbf I\\lambda_2 - \\mathbf A\\big)...\\big(\\mathbf I\\lambda_n - \\mathbf A\\big)$  \n",
    "(where $p_A$ is the characteristic polynomial of $\\mathbf A$) \n",
    "\n",
    "because up to a similarity transform the RHS is the product of $n$ diagonal matrices, and the first one has $d_{1,1}=0$ and the second one has $d_{2,2} = 0$ and so on with the $n$th one having $d_{n,n}=0$, so the diagonal matrix that results from the product has $0=d_{i,i} = d_{i,i}^{(1)} d_{i,i}^{(2)}... d_{i,i}^{(n)}$ and at least one of those terms on the RHS is zero, so the product is zero.  \n",
    "\n",
    "now consider some matrix $\\mathbf B$ that is defective (i.e. its eigenvectors don't form a basis). Our argument is a density argument--  that for any $\\epsilon_0 \\gt 0$ we have (at least one) nice $\\mathbf A$ where \n",
    "\n",
    "$\\big \\Vert \\mathbf A -\\mathbf B\\big \\Vert_F \\lt \\epsilon_0 \\longrightarrow p_B\\big(\\mathbf B\\big) = \\mathbf 0$  \n",
    "\n",
    "in particular consider by application of triangle inequality   \n",
    "$\\big \\Vert p_B\\big(\\mathbf B\\big) - \\mathbf 0\\big \\Vert_F  $  \n",
    "$ = \\big \\Vert p_B\\big(\\mathbf B\\big) - p_A\\big(\\mathbf A\\big)\\big \\Vert_F  $   \n",
    "$ = \\big \\Vert \\big\\{ p_B\\big(\\mathbf B\\big) - p_B\\big(\\mathbf A\\big)\\big\\} - \\big\\{p_A\\big(\\mathbf A\\big)- p_B\\big(\\mathbf A\\big)\\big\\}\\big \\Vert_F  $   \n",
    "$ \\leq \\big \\Vert p_B\\big(\\mathbf B\\big) - p_B\\big(\\mathbf A\\big)\\big \\Vert_F  + \\big \\Vert p_A\\big(\\mathbf A\\big)- p_B\\big(\\mathbf A\\big) \\big\\Vert_F  $   \n",
    "$ \\lt \\frac{\\epsilon}{2}+\\frac{\\epsilon}{2}$    \n",
    "$=\\epsilon$  \n",
    "\n",
    "which implies \n",
    "$\\big \\Vert p_B\\big(\\mathbf B\\big) - \\mathbf 0\\big \\Vert_F  = 0 \\longrightarrow p_B\\big(\\mathbf B\\big) =\\mathbf 0$  \n",
    "Frobenius norms are used for convenience.  It remains to explicitly construct $\\mathbf A$ and prove  \n",
    "$\\big \\Vert p_B\\big(\\mathbf B\\big) - p_B\\big(\\mathbf A\\big)\\big \\Vert_F  + \\big \\Vert p_A\\big(\\mathbf A\\big)- p_B\\big(\\mathbf A\\big) \\big\\Vert_F     \\lt \\frac{\\epsilon}{2}+\\frac{\\epsilon}{2}$  \n",
    "\n",
    "*constructing* $\\mathbf A$   \n",
    "using Schur's Triangularization Theorem (see *Schur Inequality* writeup)    \n",
    "we know that even defective matrices are unitarily similar to an upper triangular matrix, i.e. \n",
    "\n",
    "$\\mathbf U^* \\mathbf A \\mathbf U = \\mathbf R$  \n",
    "now if we put every eigenvalue of $\\mathbf A$ (or $\\mathbf R$) in a set, we will observe that the set has a finitely many (distinct) eigenvalues and hence there is some minimum pairwise distance between each of the pairs of eigenvalues in that set.  Call this value $m \\gt 0$.  (Note in the case where all eigenvalues are identical, the above set only has one eigenvalue in it and hence there are no pairwise sets to contemplate -- in this case we can simply assign $m:=1$ for convenience.)  \n",
    "\n",
    "now let $\\mathbf D$ be a diagonal matrix \n",
    "\n",
    "one option is to let  \n",
    "$d_{j,j} = \\frac{j\\cdot m}{k}$  \n",
    "for some natural number $k\\geq (n+1)^2$    \n",
    "\n",
    "but an easier option is to use  \n",
    "$d_{j,j} = \\frac{\\omega^{j}m}{k}$  \n",
    "for some natural number $k\\geq K' = 3$      \n",
    "where $\\omega$ is an nth root of unity (i.e. $\\omega^n$ = 1)  \n",
    "this construction simplifies future results, so we'll use it.  \n",
    "note: this implies that if two eigenvalues were the same before, then they are distinct now, since the first n of the nth roots of unity are distinct.  Further, if two eigenvalues were distinct before, then we know \n",
    "$m \\leq \\big \\vert \\lambda_i - \\lambda_j\\big \\vert$  and after this adjustment we have (by repeated application of triangle inequality)  \n",
    "\n",
    "$0 \\lt \\frac{1}{3}m = m - \\frac{2}{3}m \\leq  \\big \\vert \\lambda_i  - \\lambda_j\\big \\vert - \\big \\vert \\frac{\\omega^{i} m }{k}- \\frac{\\omega^{j} m }{k}\\big \\vert  = \\Big \\vert \\big \\vert \\lambda_i  - \\lambda_j\\big \\vert - \\big \\vert \\frac{\\omega^{i} m }{k}- \\frac{\\omega^{j} m }{k}\\big \\vert \\Big \\vert \\leq \\Big \\vert \\big(\\lambda_i + \\frac{\\omega^{i} m }{k}\\big) - \\big(\\lambda_j + \\frac{\\omega^{j} m }{k}\\big)\\Big \\vert$    \n",
    "where we note  \n",
    "$\\big \\vert \\frac{\\omega^{i}m}{k}- \\frac{\\omega^{j} m }{k}\\big \\vert \\leq \\big \\vert \\frac{\\omega^{i} m}{k}\\big \\vert + \\big \\vert \\frac{\\omega^{j} m }{k}\\big \\vert = 2\\frac{m}{k} \\lt \\frac{2}{3}m$  \n",
    "\n",
    "hence the new eigenvalues are distinct \n",
    "\n",
    "now we create \n",
    "$\\mathbf A: = \\mathbf U\\big(\\mathbf R +\\mathbf D\\big)\\mathbf U^*$  \n",
    "\n",
    "i.e. up to a unitary similarity transform, $\\mathbf A$ is the same matrix as $\\mathbf B$ except its diagonal (eigen)values have been peturbed by a small amount and are all distinct.  Since distinct eigenvalues implies diagonalizability we know that  \n",
    "$p_A\\big(\\mathbf A\\big) = \\mathbf 0$   \n",
    "\n",
    "*proving the upper bound*  \n",
    "\n",
    "we return to our estimate and and look at the first term of  \n",
    "$\\big \\Vert p_B\\big(B\\big) - p_B\\big(\\mathbf A\\big)\\big \\Vert_F + \\big \\Vert p_A\\big(\\mathbf A\\big)- p_B\\big(\\mathbf A\\big) \\big\\Vert_F$   \n",
    "so examining  \n",
    "$\\big \\Vert p_B\\big(B\\big) - p_B\\big(\\mathbf A\\big)\\big \\Vert_F $  \n",
    "$=\\big \\Vert p_B\\big(\\mathbf T\\big) - p_B\\big(\\mathbf T +\\mathbf D\\big) \\big \\Vert_F$    \n",
    "$=\\big \\Vert \\sum_{r=0}^n c_r \\Big(\\mathbf T^r - \\big(\\mathbf T +\\mathbf D\\big)^r\\Big) \\big \\Vert_F$    \n",
    "$\\leq  \\sum_{r=0}^n \\big \\vert c_r\\big \\vert \\cdot \\big \\Vert  \\big(\\mathbf T +\\mathbf D\\big)^r - \\mathbf T^r \\big \\Vert_F$    \n",
    "$\\leq \\sum_{r=0}^n \\big \\vert c_r\\big \\vert \\cdot   \\sum_{j=1}^r \\binom{r}{j}\\big \\Vert \\mathbf T\\big \\Vert_F^{r-j}\\cdot \\big \\Vert  \\mathbf D \\big \\Vert_F^j$ (binomial expand, apply subadditivity and sub multiplicativity of Frobenius norm)  \n",
    "$\\leq n \\cdot \\max_r\\big\\{ \\big \\vert c_r\\big \\vert \\big\\}\\cdot   \\sum_{j=1}^n \\binom{n}{j}\\big \\Vert \\mathbf T\\big \\Vert_F^{r-j}\\cdot \\big \\Vert  \\mathbf D \\big \\Vert_F^j$  \n",
    "$\\leq \\big \\Vert  \\mathbf D \\big \\Vert_F \\cdot n \\cdot \\max_r\\big\\{ \\big \\vert c_r\\big \\vert \\big\\}\\cdot   \\sum_{j=1}^n \\binom{n}{j}\\big \\Vert \\mathbf T\\big \\Vert_F^{r-j}$ (for $\\big \\Vert \\mathbf D\\big \\Vert\\leq 1$ which is implied by selecting $k$ large enough)  \n",
    "$\\leq \\big \\Vert  \\mathbf D \\big \\Vert_F \\cdot n \\cdot \\max_r\\big\\{ \\big \\vert c_r\\big \\vert \\big\\}\\big\\{\\text{max}_{ 1\\leq j\\leq n} \\big \\Vert \\mathbf T\\big \\Vert_F^{j}\\big\\} \\cdot 2^n$    \n",
    "$= \\big \\Vert  \\mathbf D \\big \\Vert_F \\cdot \\text{constant}$  \n",
    "$\\lt \\frac{\\epsilon}{2}$  \n",
    "\n",
    "by selecting sufficiently small enough (in norm) $\\mathbf D$, i.e. for $k \\geq K_0$, for $K_0$ large enough.  \n",
    "\n",
    "As for the second estimate, we have (where $e_r$ is the rth elementary symmetric polynomial, though it operates on triangular matrices for notational convenience (and their eigenvalues) which is formally the same as the rth principal minor)  \n",
    "\n",
    "$ \\big \\Vert p_A\\big(\\mathbf A\\big)- p_B\\big(\\mathbf A\\big) \\big\\Vert_F$   \n",
    "$= \\big \\Vert (a_0^{(A)} - a_0^{(B)}) \\mathbf I +  \\sum_{r=1}^{n-1} (a_r^{(A)} - a_r^{(B)}) \\mathbf A^r  \\big\\Vert_F$   \n",
    "$\\leq  n \\cdot \\big \\vert a_0^{(A)} - a_0^{(B)}\\big \\vert +  \\sum_{r=1}^{n-1} \\big \\vert a_r^{(A)} - a_r^{(B)}\\big \\vert \\cdot \\big \\Vert \\mathbf A  \\big\\Vert_F^r$   \n",
    "$\\leq  n \\cdot \\big \\vert a_0^{(A)} - a_0^{(B)}\\big \\vert + \\text{max}\\big(n,\\big \\Vert \\mathbf A  \\big\\Vert_F^n \\big)\\sum_{r=1}^{n-1} \\big \\vert a_r^{(A)} - a_r^{(B)}\\big \\vert $   \n",
    "$=  n \\cdot \\big \\vert e_n\\big(\\mathbf T\\big) - e_n\\big(\\mathbf T +\\mathbf D\\big) \\big \\vert + \\text{max}\\big(n,\\big \\Vert \\mathbf A  \\big\\Vert_F^n \\big)\\sum_{r=1}^{n-1} \\big \\vert e_{n-r}\\big(\\mathbf T\\big) - e_{n-r}\\big(\\mathbf T+\\mathbf D\\big)\\big \\vert $    \n",
    "$=  n \\cdot \\big \\vert e_n\\big(\\mathbf {\\Lambda 1}\\big) - e_n\\big( \\big(\\mathbf\\Lambda + \\mathbf D\\big)  \\mathbf 1\\big) \\big \\vert + \\text{max}\\big(n,\\big \\Vert \\mathbf A  \\big\\Vert_F^n \\big)\\sum_{r=1}^{n-1} \\big \\vert e_{n-r}\\big(\\mathbf {\\Lambda 1}\\big) - e_{n-r}\\big( \\big(\\mathbf\\Lambda + \\mathbf D\\big)  \\mathbf 1\\big)\\big \\vert $    \n",
    "\n",
    "\n",
    "$\\lt \\frac{\\epsilon}{2n} + \\sum_{r=1}^{n-1} \\frac{\\epsilon}{2n}$  \n",
    "$=\\frac{\\epsilon}{2}$  \n",
    "\n",
    "where the inequality follows by the fact $e_r$ is a polynomial, which is a continuous map (for clarification on this multivariable continuity argument, see discussion at the end), e.g.  for any $1\\leq j\\leq n-1$ that for any for $\\frac{\\epsilon}{2n}\\cdot  \\text{max}\\big(n,\\big \\Vert \\mathbf A  \\big\\Vert_F^n \\big)^{-1}$ there exists some $\\delta_j \\gt 0$ such that \n",
    "\n",
    "if  \n",
    "$\\big \\Vert \\big(\\mathbf {\\Lambda 1}\\big) - \\big( \\big(\\mathbf\\Lambda + \\mathbf D\\big)  \\mathbf 1\\big)\\big \\Vert_F = \\big \\Vert \\mathbf D \\big \\Vert_F \\lt \\delta_j$  \n",
    "\n",
    "then we know   \n",
    "$\\big \\vert e_{n-r}\\big(\\mathbf {\\Lambda 1}\\big) - e_{n-r}\\big( \\big(\\mathbf\\Lambda + \\mathbf D\\big)  \\mathbf 1\\big)\\lt \n",
    "\\frac{\\epsilon}{2n}\\cdot  \\text{max}\\big(n,\\big \\Vert \\mathbf A  \\big\\Vert_F^n \\big)^{-1}$  \n",
    "\n",
    "and for any fixed $\\delta_j \\gt 0$ there is some large enough $K_j$ such that \n",
    "$\\big \\Vert \\mathbf D \\big \\Vert_F \\lt \\delta_j$  \n",
    "simply because  \n",
    "$\\big \\Vert \\mathbf D \\big \\Vert_F = \\Big( \\sum_{j=1}^n \\big(\\frac{j\\cdot m}{k}\\big)^2\\Big)^\\frac{1}{2} \\leq  \\sum_{j=1}^n \\big\\vert\\frac{j\\cdot m}{k}\\big\\vert \\lt \\frac{1}{k}\\cdot n \\cdot m$  \n",
    "so selecting $n\\cdot \\frac{1}{\\delta_j} \\leq K_j$  \n",
    "will do  \n",
    "\n",
    "now we have $\\{K_1, K_2, ..., K_n\\} \\bigcup \\big \\{K_0\\}$   \n",
    "where $K_0$ is from the first estimate.  There are finitely terms, all positive, in the above set so we select   \n",
    "$K := \\text{max}\\big(\\{K_0, K_1, K_2, ..., K_n\\}\\big)$  \n",
    "\n",
    "which completes the proof\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**more details on the bound for**    \n",
    "$ \\big \\vert e_{n-r}\\big(\\mathbf {\\Lambda 1}\\big) - e_{n-r}\\big( \\big(\\mathbf\\Lambda + \\mathbf D\\big)  \\mathbf 1\\big)\\big \\vert$  \n",
    "\n",
    "or \n",
    "\n",
    "$ \\big \\vert e_{r}\\big(\\mathbf {\\Lambda 1}\\big) - e_{r}\\big( \\big(\\mathbf\\Lambda + \\mathbf D\\big)  \\mathbf 1\\big)\\big \\vert$  \n",
    "\n",
    "the one sleight of hand in the above would seem to be that we've proven continuity for single variable complex polynomials, but our elementary symmetric polynomials would seem to be multivariate polynomials and hence require additional proof if we are to using continuity to bound the above.  We could directly prove this result (see 'second approach' the the box after this).\n",
    "\n",
    "**first approach**  \n",
    "An alternative approach, is to instead map our problem down to the single variable case and show that we have the desired continuity because we inherit it from what we've already proven.  (The technique is also closely related to polarization which is a technique frequently employed in complex analysis.)   \n",
    "\n",
    "we can start by writing out the rth elementary symmetric polynomial as  \n",
    "$e_{r}\\big(\\mathbf {\\Lambda 1}\\big) = \\sum_{1 \\leq i_1 \\leq i_2 \\leq ... \\leq i_r \\leq n} \\lambda_{i_1}\\lambda_{i_2}...\\lambda_{i_r} = \\sum_{1 \\leq i_1 \\leq i_2 \\leq ... \\leq i_r \\leq n}  (-1)^r(0-\\lambda_{i_1})(0-\\lambda_{i_2})...(0-\\lambda_{i_r})$\n",
    "\n",
    "$e_{r}\\big(\\mathbf {\\Lambda 1}+\\mathbf {D1}\\big) = \\sum_{1 \\leq i_1 \\leq i_2 \\leq ... \\leq i_r \\leq n} (\\lambda_{i_1} +\\frac{\\omega^{i_1}m}{k})(\\lambda_{i_2}+\\frac{\\omega^{i_2}m}{k})...(\\lambda_{i_r}+\\frac{\\omega^{i_r}m}{k}) $   \n",
    "$= \\sum_{1 \\leq i_1 \\leq i_2 \\leq ... \\leq i_r \\leq n}  (-1)^r(-\\frac{\\omega^{i_1}m}{k}-\\lambda_{i_1})(-\\frac{\\omega^{i_2}m}{k}-\\lambda_{i_2})...(-\\frac{\\omega^{i_r}m}{k}-\\lambda_{i_r}) $\n",
    "\n",
    "where we sum over all $\\binom{n}{r}$ distinct combinations  (or equivalently the $\\binom{n}{r}$ permutations associated with those combinations)  \n",
    "\n",
    "$\\Big \\vert e_{r}\\big(\\mathbf {\\Lambda 1}\\big) -e_{r}\\big(\\mathbf {\\Lambda 1}+\\mathbf {D1}\\big)\\Big \\vert $  \n",
    "$= \\Big \\vert \\sum_{1 \\leq i_1 \\leq i_2 \\leq ... \\leq i_r \\leq n} (-1)^r\\big\\{(0-\\lambda_{i_1})(0-\\lambda_{i_2})...(0-\\lambda_{i_r})-(-\\frac{\\omega^{i_1}m}{k}-\\lambda_{i_1})(-\\frac{\\omega^{i_2}m}{k}-\\lambda_{i_2})...(-\\frac{\\omega^{i_r}m}{k}-\\lambda_{i_r})\\big\\} \\Big\\vert  $ \n",
    "$= \\Big \\vert \\sum_{1 \\leq i_1 \\leq i_2 \\leq ... \\leq i_r \\leq n} \\big\\{(0-\\lambda_{i_1})(0-\\lambda_{i_2})...(0-\\lambda_{i_r})-(-\\frac{\\omega^{i_1}m}{k}-\\lambda_{i_1})(-\\frac{\\omega^{i_2}m}{k}-\\lambda_{i_2})...(-\\frac{\\omega^{i_r}m}{k}-\\lambda_{i_r})\\big\\} \\Big\\vert  $ \n",
    "$\\leq  \\sum_{1 \\leq i_1 \\leq i_2 \\leq ... \\leq i_r \\leq n} \\Big \\vert(0-\\lambda_{i_1})(0-\\lambda_{i_2})...(0-\\lambda_{i_r})-(-\\frac{\\omega^{i_1}m}{k}-\\lambda_{i_1})(-\\frac{\\omega^{i_2}m}{k}-\\lambda_{i_2})...(-\\frac{\\omega^{i_r}m}{k}-\\lambda_{i_r}) \\Big\\vert  $ \n",
    "\n",
    "by triangle inequality.  So it remains to bound   \n",
    "$\\Big \\vert(0-\\lambda_{i_1})(0-\\lambda_{i_2})...(0-\\lambda_{i_r})-(-\\frac{\\omega^{i_1}m}{k}-\\lambda_{i_1})(-\\frac{\\omega^{i_2}m}{k}-\\lambda_{i_2})...(-\\frac{\\omega^{i_r}m}{k}-\\lambda_{i_r}) \\Big\\vert$  \n",
    "\n",
    "from here we can observe that multiplying each of the two terms inside by a scalar with magnitude one (i.e. a value on the unit circle) does not change the magnitude in the norm,  \n",
    "\n",
    "so  \n",
    "\n",
    "$\\Big \\vert(0-\\lambda_{i_1})(0-\\lambda_{i_2})...(0-\\lambda_{i_r})-(-\\frac{\\omega^{i_1}m}{k}-\\lambda_{i_1})(-\\frac{\\omega^{i_2}m}{k}-\\lambda_{i_2})...(-\\frac{\\omega^{i_r}m}{k}-\\lambda_{i_r}) \\Big\\vert$  \n",
    "$=\\Big \\vert 1 \\Big \\vert \\cdot \\Big \\vert(0-\\lambda_{i_1})(0-\\lambda_{i_2})...(0-\\lambda_{i_r})-(-\\frac{\\omega^{i_1}m}{k}-\\lambda_{i_1})(-\\frac{\\omega^{i_2}m}{k}-\\lambda_{i_2})...(-\\frac{\\omega^{i_r}m}{k}-\\lambda_{i_r}) \\Big\\vert$  \n",
    "$=\\Big \\vert (\\bar{\\omega^{i_1}}\\bar{\\omega^{i_2}}...\\bar{\\omega^{i_r}}) \\Big \\vert \\cdot \\Big \\vert(0-\\lambda_{i_1})(0-\\lambda_{i_2})...(0-\\lambda_{i_r})-(-\\frac{\\omega^{i_1}m}{k}-\\lambda_{i_1})(-\\frac{\\omega^{i_2}m}{k}-\\lambda_{i_2})...(-\\frac{\\omega^{i_r}m}{k}-\\lambda_{i_r}) \\Big\\vert$   \n",
    "$= \\Big \\vert (\\bar{\\omega^{i_1}}\\bar{\\omega^{i_2}}...\\bar{\\omega^{i_r}})(0-\\lambda_{i_1})(0-\\lambda_{i_2})...(0-\\lambda_{i_r})-(\\bar{\\omega^{i_1}}\\bar{\\omega^{i_2}}...\\bar{\\omega^{i_r}})(-\\frac{\\omega^{i_1}m}{k}-\\lambda_{i_1})(-\\frac{\\omega^{i_2}m}{k}-\\lambda_{i_2})...(-\\frac{\\omega^{i_r}m}{k}-\\lambda_{i_r}) \\Big\\vert$  \n",
    "$= \\Big \\vert (0-\\bar{\\omega^{i_1}}\\lambda_{i_1})(0-\\bar{\\omega^{i_2}}\\lambda_{i_2})...(0-\\bar{\\omega^{i_r}}\\lambda_{i_r})-(-\\frac{m}{k}-\\bar{\\omega^{i_1}}\\lambda_{i_1})(-\\frac{m}{k}-\\bar{\\omega^{i_2}}\\lambda_{i_2})...(-\\frac{m}{k}-\\bar{\\omega^{i_r}}\\lambda_{i_r}) \\Big\\vert$  \n",
    "$= \\Big \\vert p_{\\sigma_j}(0)-p_{\\sigma_j}(-\\frac{m}{k}) \\Big\\vert$    \n",
    "$\\lt \\frac{\\epsilon'}{\\binom{n}{2}}$  \n",
    "\n",
    "\n",
    "in particular with $\\epsilon' := \\frac{\\epsilon}{2n}\\cdot  \\text{max}\\big(n,\\big \\Vert \\mathbf A  \\big\\Vert_F^n \\big)^{-1}$  \n",
    "\n",
    "the final inequality follows because $p_{\\sigma_j}$ is a single variable complex polynomial, which we've already proven to be continuous, hence for any $\\epsilon^* \\gt 0$ we know there is some $\\delta\\gt 0$  neighborhood around zero that satisfies this, and for any $\\delta \\gt 0$ we simply check that \n",
    "$\\big \\vert  -\\frac{m}{k} -0\\big \\vert = \\frac{m}{k} \\lt \\delta \\longrightarrow \\frac{m}{\\delta}\\lt k$    \n",
    "\n",
    "so any $k \\geq K_{\\sigma_j}\\gt \\frac{m}{\\delta} $  suffices.  There are finitely many $K_{\\sigma_j}$ to choose from ($\\binom{n}{r}$ in fact) and we select $K_r$ to be equal to the maximum of these.  \n",
    "\n",
    "- - - - \n",
    "For another approach, see \"application\" approximately half the way down in the Schur Inequality notebook which proves coefficients of a polynomial vary contiuously with the roots, via the indirect method of Newton's Identities.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**second approach**   \n",
    "\n",
    "an alternative approach is to directly prove that the multivariate polynomial map $\\mathbb C^r \\mapsto \\mathbb C$ are continuous functions by mimicking the argument for the $r=1$ case.  I.e. as before we have the fact that a linear combination of continuous functions is continuous, which is an almost immediate implication of the definition, but we need to work to show that multiplication is continous. And the proof proceeds by induction, with \n",
    "\n",
    "$x_0$ and \n",
    "\n",
    "$\\mathbf x:= \n",
    "\\begin{bmatrix}\n",
    "x_1 \\\\\n",
    "\\vdots \\\\ \n",
    "x_{r-1}\n",
    "\\end{bmatrix}$  \n",
    "consider the function  \n",
    "\n",
    "$h(\\mathbf x') := f(x_0)g(\\mathbf x )$  \n",
    "\n",
    "We already have that $f$ and $g$ are continous in the case of $r=2$ since they both inherit this from the already proven scalar case.  And in general we have an inductive hypothesis that $f$ and $g$ are continous by said inductive hypothesis ($f$ by the one dimensonal case and $g$ for a lower dimension sub problem that we've already proven).  \n",
    "\n",
    "note: we suggest using the euclidean distance (or perhaps manhattan distance) for convenience here  \n",
    "\n",
    "We want to prove that for any $\\epsilon \\gt 0$, there exists some $\\delta \\gt 0$  where    \n",
    "$h\\big(N(\\mathbf x', \\delta)\\big) \\subset N\\big(h(\\mathbf x'), \\epsilon\\big)$  \n",
    "i.e.   \n",
    "$\\big \\vert \\mathbf x' - \\mathbf y'\\big \\vert \\lt \\delta \\longrightarrow \\big \\vert h(\\mathbf x') - h(\\mathbf y')\\big \\vert \\lt \\epsilon$  \n",
    "\n",
    "so  \n",
    "$\\big \\vert h(\\mathbf x') - h(\\mathbf y')\\big \\vert $  \n",
    "$= \\big \\vert f(x_0)g(\\mathbf x) - f(y_0)g(\\mathbf y)\\big \\vert $  \n",
    "$= \\big \\vert\\big\\{ f(x_0)g(\\mathbf x) -f(x_0)g(\\mathbf y) \\big\\}- \\big\\{f(y_0)g(\\mathbf y) - f(x_0)g(\\mathbf y)\\big\\}\\big \\vert $   \n",
    "$\\leq \\big \\vert  f(x_0)g(\\mathbf x) -f(x_0)g(\\mathbf y) \\big \\vert+ \\big \\vert f(y_0)g(\\mathbf y) - f(x_0)g(\\mathbf y) \\big \\vert $   \n",
    "$\\leq \\big \\vert f(x_0) \\big \\vert \\cdot \\big \\vert g(\\mathbf x) -g(\\mathbf y) \\big \\vert + \\big \\vert g(\\mathbf y)\\big \\vert \\cdot \\big \\vert f(y_0) - f(x_0) \\big \\vert $   \n",
    "$\\leq \\big \\vert f(x_0)\\big \\vert \\cdot \\big(\\frac{\\epsilon}{2}\\frac{1}{\\big \\vert f(x_0)\\big \\vert + 1 }\\big) + \\big \\vert g(\\mathbf y)\\big \\vert \\cdot \\big \\vert f(y_0) - f(x_0)\\big \\vert $   \n",
    "$\\leq \\big \\vert f(x_0)\\big \\vert \\cdot \\big(\\frac{\\epsilon}{2}\\frac{1}{\\big \\vert f(x_0)\\big \\vert + 1 }\\big) + \\big \\vert g(\\mathbf y)\\big \\vert \\cdot\\big(\\frac{\\epsilon}{2}\\frac{1}{\\big \\vert g(\\mathbf x)\\big \\vert + 1 }\\big)  $   \n",
    "$\\lt \\frac{\\epsilon}{2} + \\frac{\\epsilon}{2}$  \n",
    "$= \\epsilon$  \n",
    "\n",
    "with the justification for the inequalities essentially identical to the rationale used before and again selecting $\\delta:= \\min\\big(\\delta_1,\\delta_2,\\delta_3\\big)$  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
