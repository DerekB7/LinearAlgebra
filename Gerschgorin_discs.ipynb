{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a nice way to 'eyeball' the range that eigenvalues can be in.  It also has a nice picture associated with it.  I'd finally add that this leads to an immediate proof for understanding diagonally dominant matrices, a clean way to prove that symmetric (or in complex space, Hermitian) matrices are positive (semi) definite.\n",
    "\n",
    "Indeed we can use these Gerschgorin discs to make especially strong claims about matrices with special structure.  \n",
    "\n",
    "At first this post was going to be my adaptation of the proof from Kuttler's \"Linear Algebra\" (page 177).  However I ultimately decided that a more indirect route -- via Levy-Desplanques -- was more a lot more intuitive.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While I generally prefer proofs other than contradictions, this proof of Levy-Desplanques is elementary, short and sweet and immediately leads to Gerschgorin discs in a very intuitive way.\n",
    "\n",
    "The first part of this is, essentially, a direct lift from \n",
    "https://shreevatsa.wordpress.com/2007/10/03/a-nice-theorem-and-trying-to-invert-a-function/ .  However, I added extra steps in here to make the derivation a bit more deliberate.  \n",
    "\n",
    "We say that $\\mathbf A \\in \\mathbb C^{n x n}$ is (strictly) diagonally dominant if for every row i, $\\big \\vert a_{i,i}\\big \\vert \\gt \\sum_{j \\neq i}\\big \\vert a_{i,j}\\big \\vert$.\n",
    "\n",
    "**claim:** if (square) matrix $\\mathbf A$ is diagonally dominant, then $\\det(\\mathbf A) \\neq 0$\n",
    "\n",
    "The contradiction comes from assuming $\\det(\\mathbf A) =0$, i.e. that $\\mathbf A$ is not invertible.  If this is the case, there must some $\\mathbf x \\neq \\mathbf 0$ where $\\mathbf A \\mathbf x = \\mathbf 0$.  \n",
    "\n",
    "Consider the maximal coordinate on $\\mathbf x$, $x_k$, where $\\big \\vert x_k \\big \\vert \\geq \\big \\vert x_j\\big \\vert$ for $j = \\{1, 2, ..., n\\}$\n",
    "\n",
    "hence $x_k \\neq 0$, and we can say that $\\big \\vert x_k \\big \\vert \\gt 0$\n",
    "\n",
    "Now look at the kth row of $\\mathbf {Ax}$.  We have \n",
    "\n",
    "$a_{k, 1} x_1  + a_{k, 2} x_2 + ... + a_{k,n} x_n = \\sum_{j=1}^{n} a_{k,j} x_j = a_{k,k}x_k + \\sum_{j \\neq k} a_{k,j} x_j$\n",
    "\n",
    "now recall that we are considering the case where $\\mathbf {Ax} = \\mathbf 0$ for some non-zero $\\mathbf x$, hence\n",
    "\n",
    "$a_{k,k}x_k + \\sum_{j \\neq k} a_{k,j} x_j = 0$\n",
    "\n",
    "or \n",
    "\n",
    "$a_{k,k}x_k = - \\sum_{j \\neq k} a_{k,j} x_j $\n",
    "\n",
    "now take the magnitude of both sides\n",
    "\n",
    "$\\big \\vert a_{k,k}x_k \\big \\vert = \\big \\vert a_{k,k}\\big \\vert \\big \\vert x_k \\big \\vert = \\big \\vert \\sum_{j \\neq k} a_{k,j} x_j \\big \\vert \\leq  \\sum_{j \\neq k} \\big \\vert a_{k,j}\\big \\vert \\big \\vert x_j \\big \\vert \\leq \\sum_{j \\neq k} \\big \\vert a_{k,j}\\big \\vert \\big \\vert x_k \\big \\vert = \\big \\vert x_k \\big \\vert\\big( \\sum_{j \\neq k} \\big \\vert a_{k,j}\\big \\vert \\big)$\n",
    "\n",
    "\n",
    "From here the contradiction is aparent, but we can further distill this to:\n",
    "\n",
    "$\\big \\vert a_{k,k}\\big \\vert \\big \\vert x_k \\big \\vert \\leq \\big \\vert x_k \\big \\vert\\big( \\sum_{j \\neq k} \\big \\vert a_{k,j}\\big \\vert \\big)$\n",
    "\n",
    "thus, because $\\big \\vert x_k \\big \\vert \\gt 0$, we can divide it out of the above and since it is positive, the inequality sign does not change.  \n",
    "\n",
    "$\\big \\vert a_{k,k}\\big \\vert \\leq \\sum_{j \\neq k} \\big \\vert a_{k,j}\\big \\vert$\n",
    "\n",
    "yet this contradicts our (strong) defintion of diagonal dominance, because we said our matix satisfied:\n",
    "\n",
    "$\\big \\vert a_{k,k}\\big \\vert \\gt \\sum_{j \\neq k}\\big \\vert a_{k,j}\\big \\vert$\n",
    "\n",
    "hence we know $\\mathbf A \\mathbf x = \\mathbf 0$ if and only if $\\mathbf x = \\mathbf 0$, thus $det\\big(\\mathbf A\\big) \\neq 0$.\n",
    "\n",
    "# Now the good part:\n",
    "\n",
    "for each eigenvalue, $\\lambda$ in $\\mathbf A$, we can say that when we consider the matrix\n",
    "\n",
    "$\\big(\\mathbf A - \\lambda \\mathbf I\\big)$, it is not invertible, so per Levy-Desplanques  there must be some diagonal entry on $\\mathbf A$ where \n",
    "\n",
    "$\\big \\vert a_{i,i} - \\lambda \\big \\vert \\ngtr \\sum_{j\\neq i} \\big \\vert a_{i,j}\\big \\vert$\n",
    "\n",
    "we can restate this as:\n",
    "\n",
    "$\\big \\vert a_{i,i} - \\lambda \\big \\vert \\leq \\sum_{j\\neq i} \\big \\vert a_{i,j}\\big \\vert$\n",
    "\n",
    "and this is the Gershgorin disc formula.  To be clear, the above does not tell us which diagonal entry gives us the range for a given eigenvalue -- it just tells us that any given eigenvalue must be located in a disc associated with one of these diagonal entries.  \n",
    "\n",
    "It is perhaps worth noting that we can also apply this to the conjugate transpose of $\\mathbf A$ -- hence we could instead interpret this formula in terms of the columns of $\\mathbf A$.  \n",
    "\n",
    "# Why might this be useful?\n",
    "\n",
    "Gerschgorin discs are a very nice tool for identifying things like whether a Hermitian --Symmetric, in reals-- matrix is positive (semi) definite.  To be clear, the test associated with the discs will generally tell us yes it is positive semi-definite or it will say unclear.  In some cases it may allow us to reject the hypothesis as well, but we already have other tools at our disposal, like -- (a) are any of the diagonal entries negative, (b) if any of the diagonal entries are zero, then you need the entire column and row associated with that diagonal to be zero, and (c) we also have the inequality $N \\cdot trace\\big(\\mathbf A \\big) \\geq sum \\big(\\mathbf{A}\\big)$ -- proved over reals in the posting \"SPDSP_Trace_Inequality\" which must be true for a Hermitian or Symmetric matrix that is positive semi-definite.  Notably all of these other tools allow us to reject whether a matrix is positive semi-definite -- they don't allow us to confirm that it is.  However, in certain cases, Gerschgorin discs do allow us to confirm this-- the calculation involved is quite simple, and their proof is quite simple as well.   \n",
    "\n",
    "Now, if we are doing something like a second derivative test, using a Hessian Matrix, we would in fact just use numeric values at or around a critical point.  There are times, however where we may want to evaluate our Hessian symbolically over a large range of values.  \n",
    "\n",
    "for example consider the function, f, where \n",
    "\n",
    "$f(x,y,z) = x^{2} y^{2} z^{4} + z^{2}$\n",
    "\n",
    "which has the following Hessian:\n",
    "\n",
    "$\\left[\\begin{matrix}2 y^{2} z^{4} & 4 x y z^{4} & 8 x y^{2} z^{3}\\\\4 x y z^{4} & 2 x^{2} z^{4} & 8 x^{2} y z^{3}\\\\8 x y^{2} z^{3} & 8 x^{2} y z^{3} & 12 x^{2} y^{2} z^{2} + 2\\end{matrix}\\right]$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Taussky's Refinement**  \n",
    "reference pages 59 - 61 of Brualdi's *The Mutually Beneficial Relationship of Graphs and Matrices*  \n",
    "\n",
    "A *weakly* diagonally dominant square matrix is one where   \n",
    "$\\big \\vert a_{i,i} \\big\\vert \\geq \\sum_{j\\neq i} \\big \\vert a_{i,j}\\big \\vert = r_i$  (i.e. radius on row i)  \n",
    "with the inequality strict for at least one row $i$.  \n",
    "\n",
    "\n",
    "**claim:**  \n",
    "if $\\mathbf A$ is irreducible and weakly diagonally dominant, then  \n",
    "$\\det\\big(\\mathbf A\\big) \\neq 0$  \n",
    "\n",
    "*your author's approach:*  \n",
    "remark: being irreducible implies that there are no zero rows, which implies each $a_{i,i} \\neq 0$  \n",
    "\n",
    "For this proof, *we can assume WLOG that each $a_{i,i} = -1$*  \n",
    "\n",
    "- - - -  \n",
    "Why? If this isn't the case we can consider \n",
    "\n",
    "$\\mathbf A = \\mathbf {DZ}$  \n",
    "\n",
    "where $\\mathbf D$ is a normalizing diagonal matrix such that $z_{i,i} = -1$.  As such $\\det \\big( \\mathbf D\\big) \\neq 0$, so we need to determine whether $\\det\\big(\\mathbf Z\\big) = 0$ to determine the singularity of $\\mathbf A$.  \n",
    "- - - -  \n",
    "\n",
    "Hence if $\\det\\big(\\mathbf A\\big) = 0$ then there is some $\\mathbf x \\neq \\mathbf 0$ such that \n",
    "$\\mathbf {Ax} = \\mathbf 0$  \n",
    "\n",
    "Equivalently if $\\mathbf A$ is singular, then \n",
    "\n",
    "$\\big(\\mathbf A + \\mathbf I\\big)\\mathbf x = \\mathbf I \\mathbf x = \\mathbf x$  \n",
    "\n",
    "for some $\\mathbf x \\neq \\mathbf 0$ \n",
    "\n",
    "so we define \n",
    "\n",
    "$\\mathbf B := \\big(\\mathbf A + \\mathbf I\\big)$  \n",
    "\n",
    "Now, by repeated application of triangle inequality, we know  \n",
    "\n",
    "$\\big \\vert \\mathbf x \\big \\vert =  \\big \\vert \\big(\\mathbf B \\mathbf x\\big) \\big \\vert \\leq  \\big(\\big \\vert\\mathbf B\\big \\vert \\cdot \\big \\vert \\mathbf x\\big \\vert\\big) $  \n",
    "\n",
    "where the magnitude / absolute value is understood to be applied component wise and the inequality is evaluated component-wise.  (Notationally this is similar to that used in Brualdi and elsewhere in discussions of Peron Frobenius Theory).  \n",
    "\n",
    "equivalently, if we look at the scalars in row $i$, this reads  \n",
    "$\\big \\vert x_i\\big \\vert = \\big \\vert \\sum_{j=1}^n b_{i,j}x_j\\big \\vert \\leq \\sum_{j=1}^n \\big \\vert b_{i,j}x_j\\big \\vert = \\sum_{j=1}^n \\big \\vert b_{i,j}\\big \\vert \\cdot \\big \\vert x_j\\big \\vert$  \n",
    "\n",
    "and by further application of triangle inequality, we have \n",
    "\n",
    "$\\big \\vert \\mathbf x \\big \\vert = \\big \\vert\\big(\\mathbf B^2 \\mathbf x\\big) \\big \\vert \\leq \\big \\vert\\mathbf B\\big \\vert \\cdot \\big \\vert \\big(\\mathbf B \\mathbf x\\big) \\big \\vert \\leq  \\big(\\big \\vert\\mathbf B\\big \\vert \\big \\vert\\mathbf B\\big \\vert \\cdot \\big \\vert \\mathbf x\\big \\vert\\big) = \\big \\vert\\mathbf B\\big \\vert^2 \\cdot \\big \\vert \\mathbf x\\big \\vert $  \n",
    "\n",
    "and by induction we have  \n",
    "\n",
    "$\\big \\vert \\mathbf x \\big \\vert = \\big \\vert\\big(\\mathbf B^k \\mathbf x\\big)\\big \\vert \\leq \\big \\vert\\mathbf B\\big \\vert^k \\cdot \\big \\vert \\mathbf x\\big \\vert = \\mathbf P^k \\cdot \\big \\vert \\mathbf x\\big \\vert$  \n",
    "\n",
    "for all natural numbers $k$, where $\\mathbf P :=  \\big \\vert\\mathbf B\\big \\vert$  \n",
    "\n",
    "Based on our construction of weak diagonal dominance and magnitude one on the diagonal, we see that $\\mathbf P$ is an substochastic matrix associated with an irreducible markov chain.  To bring the point home, we can embed $\\mathbf P$ in an absorbing chain, where we've inserted a state 0 as the absorbing state\n",
    "\n",
    "$\\mathbf M = \\begin{bmatrix} \n",
    "1 & 0\\\\ \n",
    "* & \\mathbf P\\\\ \n",
    "\\end{bmatrix}$\n",
    "\n",
    "\n",
    "where at least one starred component is positive so that the matrix is row stochastic $\\mathbf {M1} = \\mathbf 1$.  That means that at least one state $\\mathbf P$ communicates with the absorbing state -- so said state is transient.  But since the underlying graph in $\\mathbf P$ is irreducible, and transience is a class property, *all states are transient*.  We also, of course can multiply this in blocked form:  \n",
    "\n",
    "$\\mathbf M^k = \\begin{bmatrix} \n",
    "1 & 0\\\\ \n",
    "* & \\mathbf P^k\\\\ \n",
    "\\end{bmatrix}$\n",
    "\n",
    "The end result, is that we have   \n",
    "\n",
    "$\\big \\vert \\mathbf x \\big \\vert = \\lim_{k\\to \\infty} \\big \\vert \\mathbf x \\big \\vert \\leq \\lim_{k\\to \\infty}  \\mathbf P^k \\cdot \\big \\vert \\mathbf x\\big \\vert = \\mathbf 0$ \n",
    "\n",
    "or, if the reader prefers, with \n",
    "\n",
    "$\\mathbf v := \\begin{bmatrix} \n",
    "0\\\\\n",
    "\\big \\vert \\mathbf x\\big \\vert\n",
    "\\end{bmatrix}$  \n",
    "\n",
    "\n",
    "$\\mathbf v = \\lim_{k\\to \\infty} \\mathbf v \\leq \\lim_{k\\to \\infty}  \\mathbf M^k \\cdot \\big \\vert \\mathbf v\\big \\vert = \\mathbf e_0$ \n",
    "\n",
    "which is a contradiction.  \n",
    "\n",
    "- - - - -  \n",
    "the above are standard markov chain results.  Based on zero or one laws, if a node in a communicating class is transient, then all in that class are transient.  Since a renewal does not occur with probability one, this implies that the expected number of visits to said state is finite, which means that the number of visits after time $t$ tends to zero by selecting large enough $t$ (either via Borell Cantelli or Markov Inequality).  Thus all diagonal components of $P$ tend to zero. For avoidance of doubt, this *also* implies that the off diagonal components tends to zero.  Consider:  \n",
    "\n",
    "for $j, i \\geq 1$  \n",
    "\n",
    "$\\sum_{k=1}^\\infty p_{i,j} = E\\Big[N\\Big] = E\\Big[\\big[N\\big \\vert \\text{vist state j once}\\big]\\Big] = 0 + p \\cdot \\big(1 + \\sum_{k=1}^\\infty P_{j,j}^{(k)}\\big) \\leq 1 + \\sum_{k=1}^\\infty P_{j,j}^{(k)} \\lt \\infty$  \n",
    "\n",
    "with $p$ being the total probability of ever reaching state $j$ from state $i$.  Being probability we know $p \\in [0,1]$.  \n",
    "\n",
    "this implies that \n",
    "$P_{j,j}^{(k)} \\to 0$ \n",
    "by selecting large enough $k$ (again by Borell Cantelli).  Equivalently the this is a delayed defective renewal process, where the 'real renewal' occurs at state $j$ but the transition probability to there from $i$ may be defective, and we *know* that the renewal process from $j\\to j$ is defective.   \n",
    "\n",
    "*remark:*  \n",
    "Some of the ideas here closely follow pages 400 - 402 of Feller Vol 1 (3rd edition).  \n",
    "\n",
    "*another finish -- via extremal characterization:*  \n",
    "For an irreducible chain (i.e. underlying graph is connected) that is weakly diagonally dominant.  Again for our matrix $\\mathbf A$ with diagonal components that we WLOG assume are -1, we can use an extremal argument to prove that $\\mathbf A$ is non-singular.  \n",
    "\n",
    "It is enough to prove that $\\mathbf {Av} = \\mathbf 0$ has a unique solution (equivalently, only the zero vector is in the nullspace of $\\mathbf A$).  \n",
    "\n",
    "so we know \n",
    "$\\mathbf {A0} = \\mathbf 0$  \n",
    "and aim to prove this is the only possible solution.  Suppose for a contradiction that some $\\mathbf v \\neq \\mathbf 0$ exists such that $\\mathbf {Av} = \\mathbf 0$  \n",
    "\n",
    "as such  \n",
    "$\\max_{i} \\big \\vert v_i \\big \\vert = \\alpha \\gt 0$  \n",
    "\n",
    "now partition into two cases.  First the case where the max value occurs in a row where \n",
    "$\\big \\vert a_{i,i} \\big\\vert \\gt \\sum_{j\\neq i} \\big \\vert a_{i,j}\\big \\vert = r_i$  \n",
    "\n",
    "for reasons that will become clear, we'll refer to this as a sub-convex combination and the case where \n",
    "$\\big \\vert a_{i,i} \\big\\vert = \\sum_{j\\neq i} \\big \\vert a_{i,j}\\big \\vert = r_i$  \n",
    "which we refer to as a convex combination  \n",
    "\n",
    "In either case for this maximal row $i$  \n",
    "\n",
    "$-1\\cdot v_i + \\sum_{j\\neq i} a_{i,j} v_j = a_{i,i} v_i + \\sum_{j\\neq i} a_{i,j} v_j = 0$  \n",
    "$v_i = \\sum_{j\\neq i} a_{i,j} v_j$  \n",
    "taking the magnitude of each side  \n",
    "$\\alpha $  \n",
    "$= \\big \\vert v_i\\big \\vert $  \n",
    "$ = \\big \\vert \\sum_{j\\neq i} a_{i,j} v_j\\big \\vert$  \n",
    "$\\leq  \\sum_{j\\neq i}\\big \\vert a_{i,j} v_j\\big \\vert$  \n",
    "$= \\sum_{j\\neq i}\\big \\vert a_{i,j}\\big \\vert \\big \\vert  v_j\\big \\vert$  \n",
    "$\\leq  \\sum_{j\\neq i}\\alpha \\big \\vert  v_j\\big \\vert$  \n",
    "$= \\alpha \\cdot \\sum_{j\\neq i}\\big \\vert  v_j\\big \\vert$  \n",
    "$\\leq \\alpha $  \n",
    "\n",
    "where the inequalities follow by triangle inequality, the fact that $\\big \\vert v_j \\big \\vert \\leq \\alpha$ for all $j$, and the fact that each diagonal element is magnitude 1 but the row are weakly diagonally dominant -- i.e. each row has a magnitude sum of at most one.  \n",
    "\n",
    "In the case of a \"sub-convex\" combination this last inequality is strict, i.e. with some $p \\in (0,1)$  \n",
    "giving us   \n",
    "\n",
    "$\\alpha \\leq p \\cdot \\alpha \\lt \\alpha$  \n",
    "which is impossible for any $\\alpha \\gt 0$.  (Note the inequality is not strict if $\\alpha =0$.)  \n",
    "\n",
    "In the other case where the maximal row $i$ has a proper convex combination this last inequality need not be strict.  However the equality conditions are achievable *iff* each $\\big \\vert v_j\\big \\vert = \\alpha$.  That is each node $j$ that node $i$ is connected to must have a maximal magnitude value such that $\\big \\vert v_j\\big \\vert = \\alpha$.  But the underlying graph is irreducible / connected.  This means that at least one node in the \"convex combination\" class communicates with at least one node in that \"sub-convex combination\" class, which implies that for said node $k$ in the subconvex combination class, $\\big \\vert v_k\\big \\vert = \\alpha \\leq p \\cdot \\alpha \\lt \\alpha$, which is a contradiction for any $\\alpha \\gt 0$.  \n",
    "\n",
    "Thus the  \n",
    "$\\max_{i} \\big \\vert v_i \\big \\vert = 0 \\longrightarrow \\mathbf v= 0$  afterall\n",
    "so we conclude that $\\mathbf A$ is nonsingular    \n",
    "\n",
    "\n",
    "*yet another finish:*  \n",
    "\n",
    "Let us reconsider  \n",
    "\n",
    "$\\big \\vert \\mathbf x \\big \\vert = \\big \\vert\\big(\\mathbf B^k \\mathbf x\\big) \\leq \\big \\vert\\mathbf B\\big \\vert^k \\cdot \\big \\vert \\mathbf x\\big \\vert = \\mathbf P^k \\cdot \\big \\vert \\mathbf x\\big \\vert$  \n",
    "\n",
    "in particular  \n",
    "\n",
    "$\\big \\vert \\mathbf x \\big \\vert =  \\mathbf P^k \\cdot \\big \\vert \\mathbf x\\big \\vert = \\mathbf P^k \\cdot \\big \\vert \\mathbf x\\big \\vert$  \n",
    "\n",
    "then if we sum over this result for $\\mathbf I$ and $\\mathbf P^{k}$ for $k \\in \\{1,2,...,n, n+1\\}$   \n",
    "\n",
    "i.e.  \n",
    "\n",
    "$\\mathbf S = \\frac{1}{n+1} \\big(\\mathbf P^{1} + \\mathbf P^{2} + ... + \\mathbf P^{n} + \\mathbf P^{n+1}\\big) = \\frac{1}{n+1} \\big(\\mathbf I + \\mathbf P^{1} + \\mathbf P^{2} + ... + \\mathbf P^{n}\\big) \\mathbf P$     \n",
    "\n",
    "we still have  \n",
    "$\\mathbf S  \\big \\vert \\mathbf x\\big \\vert =  \\big \\vert \\mathbf x\\big \\vert$   \n",
    "\n",
    "Further, since we have one communicating class, each state is reachable from any other state in at most $n$ iterations. Thus for each row $i$ and each state $j$, where we interpret $\\mathbf P^0 =\\mathbf I$) we have some $k$ where \n",
    "\n",
    "$\\mathbf e_i^T \\mathbf P^{k}\\mathbf e_j = w_j \\gt 0$  \n",
    "\n",
    "i.e.  \n",
    "$\\mathbf e_i^T \\mathbf P^{k}  =  w_j \\mathbf e_j + \\sum_{r\\neq j} w_r\\mathbf e_r $  \n",
    "where $\\sum_{m=1}^n w_m = 1$ and $w_m \\geq 0$  (i.e. a convex combination).  \n",
    "\n",
    "However since this applies for every $j$ that includes the (at least one) state that has out transition probabilities that sum to strictly less than one.  Hence  \n",
    "\n",
    "$\\mathbf e_i^T \\mathbf P^{k+1}\\mathbf 1 $  \n",
    "$= \\Big(\\mathbf e_i^T \\mathbf P^{k}\\big)\\mathbf P\\Big) \\mathbf 1 $  \n",
    "$= \\big( w_j \\mathbf e_j \\mathbf P\\big)\\mathbf 1 + \\sum_{r\\neq j} \\big(w_r\\mathbf e_r \\mathbf P \\big)\\mathbf 1 $  \n",
    "$= w_j \\alpha_j + \\sum_{r\\neq j} w_r \\alpha_r $  \n",
    "$\\leq w_j \\alpha_j + \\sum_{r\\neq j} w_r $  \n",
    "$= w_j \\alpha_j + \\big(1 + w_j\\cdot(-1)\\big) $  \n",
    "$= 1 + w_j \\big(-1 + \\alpha_j\\big) $  \n",
    "$\\lt 1$  \n",
    "\n",
    "because $\\alpha_r \\leq 1$ but $\\alpha_j \\lt 1$ with $0 \\lt w_j \\leq 1$  \n",
    "\n",
    "thus for any $\\mathbf e_i$ we have  \n",
    "\n",
    "$\\mathbf e_i^T \\mathbf S\\mathbf 1 = \\mathbf e_i^T  \\frac{1}{n+1} \\big(\\mathbf P^{1} + \\mathbf P^{2} + ... + \\mathbf P^{n} + \\mathbf P^{n+1}\\big)\\mathbf 1 \\leq \\frac{1}{n+1}\\alpha_j + \\frac{n}{n+1}\\lt 1 $   \n",
    "\n",
    "which tells us that *every* row of $\\mathbf S$ sums to less than one. Since all components of $\\mathbf S$ are real non-negative, a direct application our originally proven Gerschorin discs tells us that \n",
    "\n",
    "$\\big \\vert \\lambda_\\text{max}\\big(\\mathbf S\\big) \\big \\vert \\leq \\text{largest row sum }\\big(\\mathbf S\\big) \\lt 1$    \n",
    "\n",
    "but if $\\mathbf P$ has an eigenvalue of $1$, so much $\\mathbf S$.  The contrapositive is that since $\\mathbf S$ cannot have an eigenvalue of $1$, then we know that all eigenvalues of $\\mathbf P$ have magnitude $\\lt 1$.  Then using Jordan Form, or result from Gelfand, we know that $\\mathbf P^k \\to 0$ and so the strictness of the threorem is proven.  \n",
    "\n",
    "a slightly different and amusing finish would be to notice that it is sufficient to show that some multiply of $\\mathbf P$ tends to zero, i.e. for some natural number $j$ \n",
    "\n",
    "$\\big \\Vert \\big(\\mathbf P^j\\big)^k \\big \\Vert \\leq \\epsilon$  \n",
    "\n",
    "for all $\\epsilon \\gt 0$ by selecting large enough $k$.  \n",
    "\n",
    "One way to finish this is to select $j$ large enough (i.e. so the eigenvalue magnitudes are all *very* small), so that the maximal magnitude eigenvalue of $\\mathbf P$ is less than $3^{-n}$. From here, apply Cayley Hamilton, triangle inequality and positive definiteness.  \n",
    "\n",
    "I.e.  for $n$ by $n$ matrices, we have  \n",
    "\n",
    "$ a_n\\big(\\mathbf P^j\\big)^1 + a_{n-1}\\big(\\mathbf P^j\\big)^2 + ... + a_1\\big(\\mathbf P^j\\big)^{n-1}  + \\big(\\mathbf P^j\\big)^n = \\mathbf 0$  \n",
    "\n",
    "by Cayley Hamilton. Thus  \n",
    "\n",
    "$\\big(\\mathbf P^j\\big)^n  = -\\Big( a_n\\big(\\mathbf P^j\\big)^1 + a_{n-1}\\big(\\mathbf P^j\\big)^2 + ... + a_1\\big(\\mathbf P^j\\big)^{n-1} \\Big)  $  \n",
    "\n",
    "computing the norm of each side --- with some care, the argument works with any norm, however the Frobenius or operator 2 norm is suggested here for convenience.  Any submultipilicative norm will work here, however.  \n",
    "\n",
    "If $\\big \\Vert \\mathbf P\\big \\Vert \\lt 1$, then submultiplicativity is enough to prove that \n",
    "$\\big \\Vert \\mathbf P^k\\big \\Vert \\leq \\big \\Vert \\mathbf P\\big \\Vert^k \\leq \\big \\Vert \\mathbf P\\big \\Vert^{k-1} ... \\leq \\big \\Vert \\mathbf P\\big \\Vert  \\geq 1$  \n",
    "\n",
    "which is monotone decreasing, bounded below by zero, and has an obvious limit of zero.  \n",
    "\n",
    "\n",
    "For the rest of the post, we assume  \n",
    "$\\big \\Vert \\mathbf P\\big \\Vert =M \\geq 1$. \n",
    "\n",
    "Applying triangle inequality, we have  \n",
    "\n",
    "\n",
    "$\\big \\Vert \\mathbf P^{nj}\\big \\Vert $  \n",
    "$=\\big \\Vert-\\Big( a_n\\big(\\mathbf P^j\\big)^1 + a_{n-1}\\big(\\mathbf P^j\\big)^2 + ... + a_1\\big(\\mathbf P^j\\big)^{n-1} \\Big)\\big \\Vert   $  \n",
    "$ \\leq  \\vert a_n\\vert \\big \\Vert \\mathbf P^j \\big \\Vert  + \\vert a_{n-1}\\vert \\big \\Vert \\mathbf P^{2j}\\big \\Vert + ... + \\vert a_1\\vert \\big \\Vert \\mathbf P^{(n-1)j}\\big \\Vert$  \n",
    "$ \\leq  \\vert a_n\\vert \\big \\Vert \\mathbf P^j \\big \\Vert  + \\vert a_{n-1}\\vert \\big \\Vert \\mathbf P^{j}\\big \\Vert^2 + ... + \\vert a_1\\vert \\big \\Vert \\mathbf P^{j}\\big \\Vert^{(n-1)}$  \n",
    "$=  \\vert a_n\\vert \\cdot c  + \\vert a_{n-1}\\vert c^2 + ... + \\vert a_1\\vert c^{(n-1)}$  \n",
    "\n",
    "with $ c:  =\\big \\Vert \\mathbf P^j \\big \\Vert$  \n",
    "\n",
    "\n",
    "and by submultiplicativity of our norm, for $i \\in{1, 2, ... , n-1,n\\}$  \n",
    "\n",
    "$\\big \\Vert \\mathbf P^{nj + i}\\big \\Vert$  \n",
    "$\\leq  \\big \\Vert \\mathbf P \\big \\Vert^i \\cdot  \\big \\Vert \\mathbf P^{nj}\\big \\Vert $  \n",
    "$\\leq  M^i \\cdot  \\big \\Vert \\mathbf P^{nj}\\big \\Vert $  \n",
    "$\\leq  M^i \\cdot \\big(\\vert a_n\\vert \\cdot c  + \\vert a_{n-1}\\vert c^2 + ... + \\vert a_1\\vert c^{(n-1)}\\big) $    \n",
    "$\\leq  M^{n} \\cdot \\big(\\vert a_n\\vert \\cdot c  + \\vert a_{n-1}\\vert c^2 + ... + \\vert a_1\\vert c^{(n-1)}\\big) $    \n",
    "\n",
    "thus we have,  \n",
    "$\\big \\Vert \\mathbf P^{nj + i}\\big \\Vert$    \n",
    "$\\leq  M^{n} \\cdot \\big(\\vert a_n\\vert \\cdot c  + \\vert a_{n-1}\\vert c^2 + ... + \\vert a_1\\vert c^{(n-1)}\\big) $    \n",
    "\n",
    "for some arbitrary fixed constants $c, M \\gt 0$  are specified for any given $\\mathbf P$.  \n",
    "\n",
    "It thus suffices to show that for any given $\\mathbf P$, we may find \n",
    "by selecting large enough $j$,  such that \n",
    "$\\big \\Vert \\mathbf P^{nj}\\big \\Vert \\leq \\big(\\vert a_n\\vert \\cdot c  + \\vert a_{n-1}\\vert c^2 + ... + \\vert a_1\\vert c^{(n-1)}\\big) \\lt \\frac{1}{M^n} $  \n",
    "\n",
    "because once we have this, we know for $i \\in\\{1, 2, ... , n-1,n\\}$  \n",
    "$\\big \\Vert \\mathbf P^{nj +i}\\big \\Vert \\to 0$\n",
    "\n",
    "which implies $\\mathbf P^{nj +i} \\to \\mathbf 0$.  \n",
    "\n",
    "and we may select it to be arbitrarily close to zero by selecting large enough $n$  \n",
    "\n",
    "**I don't think this is quite needed here... and the discussion is getting muddled right around now** \n",
    "\n",
    "so what we want is to have \n",
    "$\\sum_{i=0}^{n-1} \\big \\Vert \\mathbf P^{nj +i}\\big \\Vert$  \n",
    "$\\leq \\sum_{i=0}^{n-1} M^i\\big \\Vert \\mathbf P^{nj}\\big \\Vert$    \n",
    "$ \\leq \\big \\Vert \\mathbf P^{nj}\\big \\Vert \\cdot \\sum_{i=0}^{n-1} M^i $    \n",
    "$ \\leq \\big \\Vert \\mathbf P^{nj}\\big \\Vert \\cdot \\frac{1-M^n}{1-M}$    \n",
    "(where the upper bound is understood to be simply $\\big \\Vert \\mathbf P^{nj}\\big \\Vert \\cdot n$ if $M=1$)  \n",
    " \n",
    "\n",
    "but, we know  \n",
    "$\\big \\vert  \\lambda_1\\big \\vert \\geq \\big \\vert \\lambda_2 \\big \\vert  \\geq ... \\geq \\big \\vert \\lambda_n\\big \\vert $  \n",
    "\n",
    "with $e_k$ being the $kth$ elementary symmetric function, \n",
    "$\\vert a_k \\vert $  \n",
    "$\\big \\vert e_k\\big(\\lambda_1 , \\lambda_2 , ..., \\lambda_n\\big)\\big \\vert$  \n",
    "$\\leq e_k\\big(\\big \\vert\\lambda_1\\big \\vert , \\big \\vert\\lambda_2\\big \\vert , ..., \\big \\vert \\lambda_n\\big \\vert\\big)$  \n",
    "$\\leq e_k\\big(\\big \\vert\\lambda_1\\big \\vert , \\big \\vert\\lambda_1\\big \\vert , ..., \\big \\vert \\lambda_1\\big \\vert\\big)$  \n",
    "$ =  \\big \\vert \\lambda_1\\big \\vert \\binom{n}{k}$   \n",
    "\n",
    "by application first of triangle inequality, then a point-wise bound (the second step is alternatively justified via maclaurin's inequalities or shur concativity of elementary symmetric functions and writing $\\lambda_1 = c \\cdot A$ where $A$ is the arithmetic mean of the magnitudes of the eigenvalues, with $c \\geq 1$)  \n",
    "\n",
    "hence we know  \n",
    "\n",
    "$ \\vert a_n\\vert + \\big \\vert a_{n-1}\\vert +  ... +  \\vert a_1\\vert$   \n",
    "$\\leq \\vert \\lambda_1 \\vert \\Big( \\binom{n}{1} + \\binom{n}{2} + ... + \\binom{n}{n-1} +  \\binom{n}{n}\\Big)$   \n",
    "$\\leq \\vert \\lambda_1 \\vert \\Big( \\binom{n}{0} + \\binom{n}{1} + \\binom{n}{2} + ... + \\binom{n}{n-1} +  \\binom{n}{n}\\Big)$   \n",
    "$= \\vert \\lambda_1 \\vert\\cdot 2^n $   \n",
    "$= \\big(\\frac{2}{3}\\big)^n $   \n",
    "\n",
    "- - - - \n",
    "the idea from here, to then look at all $k\\geq K$ where we set $K = jn\\cdot M$ or something like this, and hece we can verify that the norm for all $\\big \\Vert P^k\\big \\Vert \\lt 1$, then looking at maximum over chunks of n we see as sequence that is monotone decreasing, (less than one in norm) and bounded below by $0$ and hence the limit is zero.  \n",
    "- - - - \n",
    "\n",
    "\n",
    "From here we have a companion system /  residual life (renewal) chain  \n",
    "\n",
    "\n",
    "$\\begin{bmatrix}\n",
    "v_{n-1}\\\\ \n",
    "v_{n-2}\\\\ \n",
    "\\vdots\\\\ \n",
    "v_{2}\\\\ \n",
    "v_{1}\n",
    "\\end{bmatrix} =  \\left[\\begin{matrix}  \n",
    "\\vert a_1\\vert & \\vert a_{2}\\vert & \\vert a_{3}\\vert & \\cdots & \\vert a_n\\vert\n",
    "\\\\1 & 0 & 0 & \\cdots & 0 \n",
    "\\\\0 & 1 & 0 & \\cdots & 0 \n",
    "\\\\\\vdots & \\vdots & \\ddots & \\ddots & \\vdots \n",
    "\\\\0 & 0 & 0 & 1 & 0  \n",
    "\\end{matrix}\\right]\\begin{bmatrix}\n",
    "v_{n-1}\\\\ \n",
    "v_{n-2}\\\\ \n",
    "\\vdots\\\\ \n",
    "v_{1}\\\\ \n",
    "v_{0}\n",
    "\\end{bmatrix}\n",
    "$  \n",
    "\n",
    "\n",
    "selecting  \n",
    "$\\begin{bmatrix}\n",
    "v_{n-1}\\\\ \n",
    "v_{n-2}\\\\ \n",
    "\\vdots\\\\ \n",
    "v_{2}\\\\ \n",
    "v_{1}\n",
    "\\end{bmatrix} := \\begin{bmatrix}\n",
    "\\big \\Vert \\mathbf P^{(n-1)j}\\big \\Vert\\\\ \n",
    "\\big \\Vert \\mathbf P^{(n-2)j}\\big \\Vert\\\\ \n",
    "\\vdots\\\\ \n",
    "\\big \\Vert \\mathbf P^{2j}\\big \\Vert\\\\ \n",
    "\\big \\Vert \\mathbf P^{1j}\\big \\Vert\n",
    "\\end{bmatrix}$  \n",
    "\n",
    "recalling our earlier inequalities, we know \n",
    "\n",
    "$\\big \\Vert \\mathbf P^{(n)j}\\big \\Vert \\leq v_{n}$ \n",
    "\n",
    "and by induction \n",
    "$0 \\leq \\big \\Vert \\mathbf P^{(n+i)j}\\big \\Vert \\leq v_{n+i}$  \n",
    "for all natural numbers $i$  \n",
    "\n",
    "- - - - -  \n",
    "another approach to this companion system, is to note that if there is a right eigenvector with eigenvalue 1, i.e. a fixed point, then \n",
    "\n",
    "$\\mathbf C\\mathbf v = \\mathbf v$, but examining the second row tells us that $v_1 = v_2$, and examining the 3rd row tells us that $v_2 = v_3$, and so on, giving us $v_1 = v_2 = v_3 = ... = v_{n-1} = v_n$.  i.e. any fixed point $\\propto \\mathbf 1$.  However, focusing on the first row of this matrix, we can see that it is substochastic, so far any $c \\gt 0$  \n",
    "\n",
    "\n",
    "$c \\big(\\vert a_1\\vert \\cdot 1 + \\vert a_{2}\\vert \\cdot 1 + \\vert a_{3}\\vert\\cdot 1 + ...\\cdots + \\vert a_n\\vert \\cdot 1\\big)  $  \n",
    "$= c \\big(\\vert a_1\\vert + \\vert a_{2}\\vert + \\vert a_{3}\\vert + ...\\cdots + \\vert a_n\\vert \\big)  $  \n",
    "$\\lt c  $  \n",
    "\n",
    "i.e.  \n",
    "$c \\cdot \\mathbf e_1^T \\mathbf C \\mathbf 1 \\lt c \\cdot \\mathbf e_1^T \\mathbf 1  = c$   \n",
    "\n",
    "we can verify that if the first component decreases, then   \n",
    "$c \\cdot \\mathbf e_k^T \\mathbf C \\mathbf 1 \\leq c \\cdot \\mathbf e_1^T \\mathbf 1  = c$    \n",
    "for $k\\in \\{1,2, ..., n\\}$    \n",
    "\n",
    "i.e. we have a component-wise inequality  \n",
    "\n",
    "$c \\cdot  \\mathbf C \\mathbf 1 \\leq c \\cdot \\mathbf 1  $   \n",
    "\n",
    "\n",
    "\n",
    "then \n",
    "$c \\cdot \\mathbf e_1^T \\mathbf C^2 \\mathbf 1  = c \\cdot \\mathbf e_1^T \\mathbf C \\big( \\mathbf C \\mathbf 1\\big) \\lt c \\cdot \\mathbf e_1^T \\mathbf C \\mathbf 1 \\lt c \\cdot \\mathbf e_1^T \\mathbf 1  = c$   \n",
    "\n",
    "because we take a sub convex combination in the second iteration of positive components each of which have non-increased from the prior iteration.  \n",
    "\n",
    "and by induction we have a decreasing sequence \n",
    "\n",
    "$c \\cdot \\mathbf e_1^T \\mathbf C^r \\mathbf 1  \\lt c \\cdot \\mathbf e_1^T \\mathbf C^{r-1} \\lt ... \\lt e_1^T \\mathbf C \\mathbf 1 \\lt c \\cdot \\mathbf e_1^T \\mathbf 1  = c$   \n",
    "\n",
    "which is bounded below by 0 (i.e. a linear combination of terms where all scalars involved are non-negative gives a non-negative resul).  \n",
    "\n",
    "Hence we have a bounded monotone decreasing sequence, and we know some limit exists, i.e.  \n",
    "\n",
    "$\\lim_{r\\to\\infty} c \\cdot \\mathbf e_1^T \\mathbf C^r \\mathbf 1  = L$  \n",
    "\n",
    "however, since  \n",
    "$ c \\cdot \\mathbf e_{k}^T \\mathbf C^{r-k+1} \\mathbf 1 =  c \\cdot \\mathbf e_{1}^T \\mathbf C^{r} \\mathbf 1$    \n",
    "\n",
    "this tells us that each component must have the same limitting value:  \n",
    "$\\lim_{r\\to\\infty} c \\cdot \\mathbf e_1^T \\mathbf C^r \\mathbf 1  = L \\mathbf 1$   \n",
    "\n",
    "But $L$ must be zero -- if $L \\gt 0$ then we have \n",
    "$\\lim_{r\\to\\infty} c \\cdot \\mathbf C^r \\mathbf 1  = L \\mathbf 1$  \n",
    "which is a right eigenvector with eigenvalue 1, a contradiction.  \n",
    "\n",
    "hence \n",
    "$\\lim_{r\\to\\infty} c \\cdot \\mathbf C^r \\mathbf 1  = \\mathbf 0$    \n",
    "\n",
    "for any $c \\gt 0$.  \n",
    "\n",
    "now, for any other $n$ dimenstional $\\mathbf v$ with real non-negative components, we have  \n",
    "$\\lim_{r\\to\\infty} c \\cdot \\mathbf C^r \\mathbf v  = \\mathbf 0$    \n",
    "\n",
    "because by selecting large enough $c$, we have the component wise inequality  \n",
    "$c \\cdot \\mathbf C^r \\mathbf 1 \\geq c \\cdot \\mathbf C^r \\mathbf v$    \n",
    "for all natural numbers $r$ \n",
    "\n",
    "this may be equivalently written as  \n",
    "$ \\mathbf C^r \\big(c \\cdot\\mathbf 1 -\\mathbf v\\big) = \\mathbf C^r \\big(\\mathbf x\\big) \\geq 0$    \n",
    "\n",
    "because $\\mathbf x \\geq 0$ and (sub) convex combinations of real non-negative numbers results in real non-negative numbers.  \n",
    "\n",
    "- - - -  \n",
    "*note: making use of submultiplicativity we can further clean this up by noting that \n",
    "$\\big \\Vert \\mathbf P^{mj}\\big \\Vert \\leq \\big \\Vert \\mathbf P^{1j}\\big \\Vert^m$  \n",
    "and hence getting a Vandermonde style result of \n",
    "\n",
    "$\\begin{bmatrix}\n",
    "c^{(n-1)j}\\\\ \n",
    "c^{(n-2)j}\\\\ \n",
    "\\vdots\\\\ \n",
    "c^{2j}\\\\ \n",
    "c^{1j}\n",
    "\\end{bmatrix}$\n",
    "\n",
    "with $c := \\big \\Vert \\mathbf P^{j}\\big \\Vert$   \n",
    "- - - -  \n",
    "\n",
    "\n",
    "we now prove $\\lim_{i\\to \\infty} v_{n+i} \\to  0 $   and hence \n",
    "$0 \\leq \\lim_{i \\to \\infty} \\big \\Vert \\mathbf P^{(n+i)j}\\big \\Vert \\leq 0$  \n",
    "\n",
    "which by positive definite ness of norms implies $\\mathbf P^{(n+i)j} \\to \\mathbf 0$    \n",
    "\n",
    "referencing our Feller chp 15 notes, we note that the first row sums of our companion system is real non-negative and sums to less than one, hence we may exponentially tilt it.  I.e. via intermediate value theroem we know there is some $\\theta \\gt 1$ such that \n",
    "\n",
    "$ \\vert a_1 \\vert \\theta + \\vert a_{2}\\vert \\theta^2 + \\vert a_{3}\\vert \\theta^3 +... +  \\cdots + \\vert a_n\\vert \\theta^n =1$ \n",
    "\n",
    "so  \n",
    "\n",
    "$\\left[\\begin{matrix}  \n",
    "\\vert a_1\\theta \\vert & \\vert a_{2}\\vert \\theta^2& \\vert a_{3}\\vert \\theta^3 & \\cdots & \\vert a_n\\vert \\theta^n\n",
    "\\\\1 & 0 & 0 & \\cdots & 0 \n",
    "\\\\0 & 1 & 0 & \\cdots & 0 \n",
    "\\\\\\vdots & \\vdots & \\ddots & \\ddots & \\vdots \n",
    "\\\\0 & 0 & 0 & 1 & 0  \n",
    "\\end{matrix}\\right]\\mathbf 1 = \\mathbf 1$\n",
    "\n",
    "from here, using one of many results in the Feller chp 15 notebook, we can observe that, where \n",
    "\n",
    "$\\mu = 1 \\cdot \\vert a_1\\theta \\vert + 2 \\cdot \\vert a_{2}\\vert \\theta^2 + 3 \\cdot vert a_{3}\\vert \\theta^3 +... +  \\cdots + n \\cdot \\vert a_n\\vert \\theta^n$  \n",
    "\n",
    "hence \n",
    "$ v_{n+i} \\to \\frac{\\theta^{-i}}{\\mu}$  \n",
    "\n",
    "which may be made arbitrarily small.  \n",
    "\n",
    "referencing the section \"an even better approach\" shows us that is to note that \n",
    "\n",
    "$v_{n+i} \\leq  c \\dot \\theta^{n-i}$  \n",
    "\n",
    "selecting $i$ large enough such that \n",
    "\n",
    "$v_{n+i} \\lt \\min\\{c^{-n},1\\} $  \n",
    "\n",
    "we find that \n",
    "$\\big \\Vert \\mathbf P^{(n+i)j}\\big \\Vert \\leq v_{n+i}\\lt \\min\\{c^{-n},1\\}$  \n",
    "\n",
    "but then for *any* power $\\mathbf P^k$ for $k \\geq nj \\geq i$ (where large enough $i$ is used above) we may write it as $r = k\\% n$ and $k-r$  \n",
    "\n",
    "**below needs cleaned up, though it is intuitively clear to me...**  \n",
    "using submultiplicativity, if $r \\neq 0$     \n",
    "$\\big \\Vert \\mathbf P^k \\big \\Vert_F$   \n",
    "$\\leq \\big \\Vert \\mathbf P^r \\big \\Vert_F \\big \\Vert \\mathbf P^{(k-r)} \\big \\Vert_F$  \n",
    "$\\big \\Vert \\mathbf P^r \\big \\Vert_F \\big \\Vert \\mathbf P^{jn} \\big \\Vert_F$  \n",
    "$\\leq \\big \\Vert \\mathbf P \\big \\Vert_F^r \\big \\Vert \\mathbf P^{(k-r)} \\big \\Vert_F$  \n",
    "$\\leq c^r \\big \\Vert \\mathbf P^{(k-r)} \\big \\Vert_F$  \n",
    "$\\lt c^r \\min\\{c^{-n},1\\} $  \n",
    "$\\lt 1$  \n",
    "\n",
    "but since norms are submultiplicative, this implies any higher power of this norm shrink and the matrix will tend to zero.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we now know that muliples of $jn$ tend to zero.  This means that for any $\\epsilon \\gt 0$, there exists some $J$, such that that for all $j \\geq J$  \n",
    "$\\big \\Vert P^{jn}\\big \\Vert_F \\lt \\epsilon$  \n",
    "\n",
    "but this also tells us we have convergence for all (large enough) powers of $\\mathbf P$.   \n",
    "\n",
    "that is, but making use of submultiplicativity, we observe  the pointwise inequalities, selecting some $\\epsilon_0 \\gt 0$\n",
    "\n",
    "$\\big \\Vert \\mathbf P^{jn}\\big \\Vert_F \\lt \\epsilon_0$  \n",
    "$\\big \\Vert \\mathbf P^{jn+1}\\big \\Vert_F = \\big \\Vert \\mathbf P \\mathbf P^{jn}\\big \\Vert_F \\leq \\big \\Vert \\mathbf P\\big \\Vert_F\\Vert \\mathbf P^{jn}\\big \\Vert_F = m \\Vert \\mathbf P^{jn}\\big \\Vert_F \\lt m \\cdot \\epsilon_0$  \n",
    "$\\big \\Vert \\mathbf P^{jn+2}\\big \\Vert_F = \\big \\Vert \\mathbf P \\mathbf P  \\mathbf P^{jn}\\big \\Vert_F \\leq \\big \\Vert \\mathbf P\\big \\Vert_F \\big \\Vert \\mathbf P\\big \\Vert_F \\big \\Vert \\mathbf P^{jn}\\big \\Vert_F = m^2 \\big \\Vert \\mathbf P^{jn}\\big \\Vert_F \\lt m^2 \\cdot \\epsilon_0$  \n",
    "$\\vdots$  \n",
    "$\\big \\Vert \\mathbf P^{jn+j(n-1)}\\big \\Vert_F = \\big \\Vert \\underbrace{\\mathbf P...\\mathbf P}_{\\text{jn-1 times}}  \\mathbf P^{jn}\\big \\Vert_F \\leq  \\underbrace{ \\big \\Vert \\mathbf P\\big \\Vert_F... \\big \\Vert \\mathbf P\\big \\Vert_F}_{\\text{jn-1 times}}  \\big \\Vert \\mathbf P^{jn}\\big \\Vert_F = m^{jn-1} \\big \\Vert \\mathbf P^{jn}\\big \\Vert_F \\lt m^{jn-1} \\cdot \\epsilon_0$  \n",
    "$\\Vert \\mathbf P^{jn+(jn)}\\big \\Vert_F = \\big \\Vert \\mathbf P^{2jn}\\big \\Vert_F \\lt\\epsilon_0$  \n",
    "$\\big \\Vert \\mathbf P^{2jn + 1}\\big \\Vert_F \\lt m\\cdot \\epsilon_0$  \n",
    "$\\big \\Vert \\mathbf P^{2jn + 2}\\big \\Vert_F \\lt m^2\\cdot \\epsilon_0$  \n",
    "$\\vdots$  \n",
    "$\\big \\Vert \\mathbf P^{2jn+j(n-1)}\\big \\Vert_F = \\big \\Vert \\underbrace{\\mathbf P...\\mathbf P}_{\\text{jn-1 times}}  \\mathbf P^{2jn)}\\big \\Vert_F \\leq  \\underbrace{ \\big \\Vert \\mathbf P\\big \\Vert_F... \\big \\Vert \\mathbf P\\big \\Vert_F}_{\\text{jn-1 times}}  \\big \\Vert \\mathbf P^{2jn)}\\big \\Vert_F = m^{n-1} \\big \\Vert \\mathbf P^{2jn}\\big \\Vert_F \\lt m^{jn-1} \\cdot \\epsilon_0$   \n",
    "and this cyclic pattern continues to repeat.  \n",
    "\n",
    "Thus for any $\\epsilon \\gt 0$ we may select   \n",
    "$\\epsilon_0 := \\frac{\\epsilon}{m^{jn-1}}$   \n",
    "which tells us there is a $J$  such that for all $j\\geq J$, we have a desired upper bound on the norm of our matrix.  We can restate this as for all $k \\geq J\\cdot n$, where we examine numbers modulo $jn$,  \n",
    "$r := k\\%jn$   \n",
    "$i := k - r$  \n",
    "\n",
    "$\\big \\Vert \\mathbf P^k \\big \\Vert_F = \\big \\Vert \\mathbf P^{in +r}\\big \\Vert_F \\lt m^r \\epsilon_0 \\leq m^{jn-1} \\epsilon_0 = m^{jn-1} \\frac{\\epsilon}{m^{jn-1}} = \\epsilon$  \n",
    "\n",
    "which proves that \n",
    "$\\big \\Vert \\mathbf P^k \\big \\Vert_F \\to 0$  \n",
    "\n",
    "and hence  \n",
    "\n",
    "$\\mathbf P^k \\to \\mathbf 0$  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to finish the above is to note that we have a cauchy sequence, so by selecting appropriate $K$, for all \n",
    "\n",
    "$k,m\\gt K$ (where $v$ is $\\min\\{$k\\%n$, m\\%n\\}$)  \n",
    "\n",
    "here I try to simplify this by choosing $K$ to be a multiple of $n$   \n",
    "\n",
    "$\\big \\Vert \\mathbf P^k - \\mathbf P^m \\big \\Vert_F $  \n",
    "$\\leq \\big \\Vert \\mathbf P^k\\big \\Vert_F + \\big \\Vert \\mathbf P^m \\big \\Vert_F$  \n",
    "$\\leq  2 M^{jn} \\cdot \\big\\Vert \\mathbf P^{K}\\big \\Vert_F $  \n",
    "$\\text{insert line showing it arbitrarily small as a function of well chosen M}$  \n",
    "$\\lt \\epsilon$  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- - - - - -  \n",
    "**broken alternative close**    \n",
    "\n",
    "an alternative close would make use of the extremal characterization associated with row stochastic matrices. In particular, reconsidering  \n",
    "\n",
    "$\\big \\vert \\mathbf x \\big \\vert = \\big \\vert\\big(\\mathbf B^k \\mathbf x\\big) \\leq \\big \\vert\\mathbf B\\big \\vert^k \\cdot \\big \\vert \\mathbf x\\big \\vert = \\mathbf P^k \\cdot \\big \\vert \\mathbf x\\big \\vert$  \n",
    "\n",
    "in particular  \n",
    "\n",
    "$\\big \\vert \\mathbf x \\big \\vert =  \\mathbf P^k \\cdot \\big \\vert \\mathbf x\\big \\vert = \\mathbf P^k \\cdot \\big \\vert \\mathbf x\\big \\vert$  \n",
    "\n",
    "\n",
    "since $\\mathbf x in \\mathbb R^n$ we have finitely many points and hence there is a global maximum and a global minimum associated with $\\big \\vert \\mathbf x\\big \\vert$. \n",
    "\n",
    "\n",
    "$\\text{minimum value } \\leq \\vert x_i\\vert \\leq\\text{ maximum value } $   \n",
    "\n",
    "\n",
    "But considering that we have one communicating class here, and first considering the stochastic rows, \n",
    "since  \n",
    "$\\big \\vert x_i \\big \\vert  = \\sum_{j=1}^n w_j^{(i)} \\big \\vert x_j \\big \\vert$   \n",
    "\n",
    "where each $w_j \\geq 0$ and $\\sum_{j}w_j = 1$  \n",
    "we know  \n",
    "\n",
    "$\\big \\vert x_i \\big \\vert  \\leq \\text{max}_{j\\in \\text{1 step transition neighbors}}\\{ \\vert x_j \\vert\\} $   \n",
    "\n",
    "this inequality also holds for the substochastic row (though it isn fact strict as it involves a convex combination with a zero value -- we will return to this)  \n",
    "\n",
    "however, since there is one communicating class, each state is reachable in at most $n$ steps from any state, and since $\\big \\vert \\mathbf x\\big \\vert $ is a fixed point, we see that for $k=\\{1, 2,...,n\\}$  \n",
    "\n",
    "$\\big \\vert x_i \\big \\vert  \\leq \\text{max}_{j\\in \\text{k step transition neighbors}}\\{ \\vert x_j \\vert\\} $   \n",
    "\n",
    "taking the union over each of these inequalities, and recognizing that each $x_j$ shows up at least once in the union, we get  \n",
    "**needs cleaned up**  \n",
    "\n",
    "we find that each $i$, $\\big \\vert x_i\\big \\vert $ is bounded above by the maxmium of a set containint $\\big \\vert x_j \\big \\vert$ for any $j\\neq i$\n",
    "\n",
    "**the above should be cleaned up or perhaps more likely deleted? upon reflection it is awfully similar to the result from brualdi, below**  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- - - - - \n",
    "*Brualdi's Approach*  \n",
    "\n",
    "The standard approach in Brualdi is much shorter but does not rely on probability theory but relies on a nested contradiction.  \n",
    "\n",
    "As before $\\mathbf A$ is an $n$ x $n$ matrix that is irreducible and weakly diagonally dominant. If $\\det\\big(\\mathbf A\\big) = 0$ then there is some $\\mathbf x \\neq 0$ such that $\\mathbf {A x} = \\mathbf 0$.  \n",
    "\n",
    "Now, since the inequality is strict for at least one row of $\\mathbf A$, we can see that $\\mathbf x \\propto \\mathbf 1 \\neq \\mathbf 0$.  This means there is some maximal magnitude component of our vector, called $x_k$ as well as at least one $\\big \\vert x_i\\big \\vert \\lt \\big \\vert x_k \\big \\vert $.  So we create a bipartition -- the set $U$ has all components of $\\mathbf x$ where $\\big \\vert x_j \\big \\vert = \\big \\vert x_k\\big \\vert$, and $U^C$ has all other components of $\\mathbf x$.  Since the underlying graph is irreducible, there must be a $p \\in U$ and $q \\in U^C$ (**tbc mechanics of why this holds**) such that $a_{p,q} = \\neq 0$.  Since $p \\in U$ we can infer that $\\big \\vert x_q\\big \\vert = \\big \\vert x_k\\big \\vert$ \n",
    "\n",
    "**(needs cleaned up and finished)**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The immediate corollary is:  \n",
    "\n",
    "if $\\mathbf A$ is irreducible, then if we revisit our Gershgoring Disc formula:  \n",
    "\n",
    "$\\big \\vert a_{i,i} - \\lambda \\big \\vert \\leq \\sum_{j\\neq i} \\big \\vert a_{i,j}\\big \\vert$\n",
    "\n",
    "we find that $\\lambda$ can be an eigenvalue on the boundary of the union of discs **iff** it is a boundary point of all of the circular discs  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This matrix is small enough (3x3 means cubic root) that we can solve symbollically for the eigenvalues exactly, but those eigenvalues -- given two cells down-- are not so easy to interpret.  This gets increasingly difficult for much larger matrices.  As is, we an simply look at (1,1,1) and see that the trace inequality is not being observed, hence the Hessian is not positive semi-definite at (1,1,1) and the function is thus not convex.  (We can of course look at the fact that diagonal elements are always positive to know that the function is *not negative convex*.)  \n",
    "\n",
    "There are *many* applications where we may want to make claims about the eigenvalues / singularity of a matrix without looking at specific numerical values.\n",
    "\n",
    "\n",
    "# Application: Graph Laplacian\n",
    "\n",
    "By construction we know that the Graph Laplacian is symmetric and real.  Thus we know all eigenvalues are real.  Further, we know that the diagonal entries are positive, and all off diagonal entries are either zero or negative.  An example is shown below.\n",
    "\n",
    "$\\mathbf L = \n",
    "\\begin{bmatrix}\n",
    "3 & -1 & 0 & -1 &  -1 & 0\\\\ \n",
    "-1 & 3 & -1 & 0 & -1 & 0\\\\ \n",
    "0 & -1 & 2 & 0 & -1 &0 \\\\ \n",
    " -1& 0 & 0 & 2 & -1 & 0\\\\ \n",
    "-1 & -1 & -1 & -1 & 5 & -1\\\\ \n",
    "0 & 0 & 0 &0  & -1 & 1\n",
    "\\end{bmatrix}$\n",
    "\n",
    "\n",
    "The minus ones correspond to edges and the diagonal values represented the degree of a given node (recall that the graph is undirected).  Thus for any Laplacian, we know:\n",
    "\n",
    "$\\mathbf{L1} = \\mathbf 0$\n",
    "\n",
    "This combined with aforementioned structure (symmetric, positives on diagonal, non-positives off diagonal) tells us, that the Laplacian is singular, and using Gershgorin's discs we can observed that $\\big \\vert l_{i,i} - \\lambda \\big \\vert \\leq l_{i,i}$, which tells us that the smallest an eigenvalue can be is 0.  (If an eigenvalue were less than zero, its distance from the strictly postively valued $l_{i,i}$ would necessarily be more than $l_{i,i}$.)  Hence we observe that the graph Laplacian is Symmetric Positive Semi-Definite.  There are other ways to prove this fact, of course, but the Gerschgorin disc approach is extremely quick and intuitive.  \n",
    "\n",
    "- - - -\n",
    "\n",
    "\n",
    "# Other Applications\n",
    "\n",
    "\n",
    "And again, whether we think of this in terms of eigenvalues with Gerschgorin discs, or Levy-Desplanques, we also have a way of knowing whether or not certain matrices are invertible without going through the full calculation -- i.e. if they are diagonally dominant, their invertibility should just jump off the page at you. \n",
    "\n",
    "Thus the ability to bound eigenvalues via the simple and intuitive Gerschgorin Discs, gives us a new way of interpretting special structure in matrices.  \n",
    "\n",
    "There are of course also numeric applications in engineering where matrices with special structures are used.  In these cases if may be nice to prove that the matrices are symmetric positive semi defnite in general, irrespetive of their size -- whether they are 5x5 or 1,000 x 1,000 or more generally $n$ x $n$.  There may be alternative approaches that involve importing machinery like Cauchy Interlacing and then doing induction on $n$, but using Gerschgorin Discs gives a simple, direct and very visual way to prove this.  \n",
    "\n",
    "\n",
    "For instance consider the matrix given on page number 34 (35 of 42 according to PDF viewer) here: https://ocw.mit.edu/courses/aeronautics-and-astronautics/16-920j-numerical-methods-for-partial-differential-equations-sma-5212-spring-2003/lecture-notes/lec15.pdf\n",
    "\n",
    "Just by looking at it, we we tell it is real valued and symmetric, so we know its eigenvalues are all real.  We can use gerschgorin discs to determine that the minimum possible eigvenvalue is zero.  Hence we know the matrix is at least Symmetric Positive Semi-Definite.  We can also easily look through the implicit Gram-Schmidt and determine that the first n - 1 columns must be linearly independent, and hence the rank of this $n$ x $n$ matrix is at least n - 1.  With a small bit of more work, we can then determine that the final column must be in linearly independent as well, and thus the matrix is Symmetric Positive Definite.  But the main point is that by simply eyeballing the symmetry, and knowing about Gerschgorin discs, we were able to have deep and general understanding of the spectrum underlying this matrix for any finite $n$ x $n$ dimension that it may take on.  \n",
    "\n",
    "Finally, Levy-Desplanques and Gerschgorin Discs also are used in numerical linear algebra for evaluating conditioning, making claims on whether pivotting is needed in Gaussian Elimination, and so on.\n",
    "\n",
    "Note: with respect to time homogenous finite state Markov Chains, while there are more powerful approaches using greatest common divisor (which generaize to countable state markov chains), Gerschgorin discs (with a strictness refinement due to Taussky) immediately tell us that for a graph with a single communicating class (irreducible) and even one self-loop -- said graph cannot have periodic behavior because the only point on the unit circle touching/inside of *all* Gerschgorin Discs is the value 1 (this is the Taussky refinement), hence the only eigenvalue with magnitude 1 is an eigenvalue of one. All other eigenvalues have magnitude less than 1 and may be made arbitrarily small after a large enough number of iterations.  The fact that the eigenvalue of 1 is simple (i.e. algebraic multiplicity of one) is of course given by Perron Frobenius Theory or standard markov chain results from Kolmogorov.  \n",
    "\n",
    "Of interest: it is also implied directly by the elementary renewal theorem with a delayed start -- or perhaps better, we could formulate this as a renewal rewards problem where are reward of one is given each time we are visit any state in the graph, of a cycle starting and finishing at some arbitrary node $i$.  The renewal reward theorem (and perhaps common sense) tells us that we have a time averaged reward of 1 -- but this is equivalent to \n",
    "\n",
    "$1  =  \\lim_{t \\to \\infty}\\frac{E[r(t)]}{t}  =\\lim_{t \\to \\infty} \\frac{1}{t}\\sum_{k=1}^t \\text{trace}\\big(\\mathbf A^k\\big)$  \n",
    "\n",
    "However if the algebraic multipliciity of eigenvalue $1$ is larger than 1, (e.g. 2 or 3 or...) then the time averaged trace must be at least $2$, which is a contradiction-- this proves that the eigenvalue of one is simple. (Note that this *also*  proves simplicity of eigenvalue 1 for any irreducible time homogenous finite state markov chain -- including periodic chains.  Consider the matrix $\\mathbf A$ and its eigenvalues.  Now consider the convex combination of $\\mathbf B: = \\frac{1}{2}\\big(\\mathbf A + \\mathbf I\\big)$. Here the graph in $\\mathbf B$ is connected but has (many) self-loops and hence is aperiodic -- and the above tells us that $\\mathbf B$ has a simple eigenvalue of $1$ and all others with magnitude less than $1$.  Yet the eigenvalues of $\\mathbf B$ the average of the eigenvalues of $\\mathbf A$ and $1$.  Thus if $\\mathbf A$ had multiple eigenvalues of $1$, so would $\\mathbf B$, which tells us that $\\mathbf A$ has a simple eigenvalue of $1$.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{4*x**2*y**2*z**2 + 2*x**2*z**4/3 + 2*y**2*z**4/3 - (-1512*x**4*y**4*z**10 + 324*x**2*y**2*z**8 + sqrt((-3024*x**4*y**4*z**10 + 648*x**2*y**2*z**8 - (-108*x**2*y**2*z**2 - 18*x**2*z**4 - 18*y**2*z**4 - 18)*(-40*x**4*y**2*z**6 - 40*x**2*y**4*z**6 - 12*x**2*y**2*z**8 + 4*x**2*z**4 + 4*y**2*z**4) + 2*(-12*x**2*y**2*z**2 - 2*x**2*z**4 - 2*y**2*z**4 - 2)**3)**2 - 4*(120*x**4*y**2*z**6 + 120*x**2*y**4*z**6 + 36*x**2*y**2*z**8 - 12*x**2*z**4 - 12*y**2*z**4 + (-12*x**2*y**2*z**2 - 2*x**2*z**4 - 2*y**2*z**4 - 2)**2)**3)/2 - (-108*x**2*y**2*z**2 - 18*x**2*z**4 - 18*y**2*z**4 - 18)*(-40*x**4*y**2*z**6 - 40*x**2*y**4*z**6 - 12*x**2*y**2*z**8 + 4*x**2*z**4 + 4*y**2*z**4)/2 + (-12*x**2*y**2*z**2 - 2*x**2*z**4 - 2*y**2*z**4 - 2)**3)**(1/3)/3 + 2/3 - (120*x**4*y**2*z**6 + 120*x**2*y**4*z**6 + 36*x**2*y**2*z**8 - 12*x**2*z**4 - 12*y**2*z**4 + (-12*x**2*y**2*z**2 - 2*x**2*z**4 - 2*y**2*z**4 - 2)**2)/(3*(-1512*x**4*y**4*z**10 + 324*x**2*y**2*z**8 + sqrt((-3024*x**4*y**4*z**10 + 648*x**2*y**2*z**8 - (-108*x**2*y**2*z**2 - 18*x**2*z**4 - 18*y**2*z**4 - 18)*(-40*x**4*y**2*z**6 - 40*x**2*y**4*z**6 - 12*x**2*y**2*z**8 + 4*x**2*z**4 + 4*y**2*z**4) + 2*(-12*x**2*y**2*z**2 - 2*x**2*z**4 - 2*y**2*z**4 - 2)**3)**2 - 4*(120*x**4*y**2*z**6 + 120*x**2*y**4*z**6 + 36*x**2*y**2*z**8 - 12*x**2*z**4 - 12*y**2*z**4 + (-12*x**2*y**2*z**2 - 2*x**2*z**4 - 2*y**2*z**4 - 2)**2)**3)/2 - (-108*x**2*y**2*z**2 - 18*x**2*z**4 - 18*y**2*z**4 - 18)*(-40*x**4*y**2*z**6 - 40*x**2*y**4*z**6 - 12*x**2*y**2*z**8 + 4*x**2*z**4 + 4*y**2*z**4)/2 + (-12*x**2*y**2*z**2 - 2*x**2*z**4 - 2*y**2*z**4 - 2)**3)**(1/3)): 1, 4*x**2*y**2*z**2 + 2*x**2*z**4/3 + 2*y**2*z**4/3 - (-1/2 + sqrt(3)*I/2)*(-1512*x**4*y**4*z**10 + 324*x**2*y**2*z**8 + sqrt((-3024*x**4*y**4*z**10 + 648*x**2*y**2*z**8 - (-108*x**2*y**2*z**2 - 18*x**2*z**4 - 18*y**2*z**4 - 18)*(-40*x**4*y**2*z**6 - 40*x**2*y**4*z**6 - 12*x**2*y**2*z**8 + 4*x**2*z**4 + 4*y**2*z**4) + 2*(-12*x**2*y**2*z**2 - 2*x**2*z**4 - 2*y**2*z**4 - 2)**3)**2 - 4*(120*x**4*y**2*z**6 + 120*x**2*y**4*z**6 + 36*x**2*y**2*z**8 - 12*x**2*z**4 - 12*y**2*z**4 + (-12*x**2*y**2*z**2 - 2*x**2*z**4 - 2*y**2*z**4 - 2)**2)**3)/2 - (-108*x**2*y**2*z**2 - 18*x**2*z**4 - 18*y**2*z**4 - 18)*(-40*x**4*y**2*z**6 - 40*x**2*y**4*z**6 - 12*x**2*y**2*z**8 + 4*x**2*z**4 + 4*y**2*z**4)/2 + (-12*x**2*y**2*z**2 - 2*x**2*z**4 - 2*y**2*z**4 - 2)**3)**(1/3)/3 + 2/3 - (120*x**4*y**2*z**6 + 120*x**2*y**4*z**6 + 36*x**2*y**2*z**8 - 12*x**2*z**4 - 12*y**2*z**4 + (-12*x**2*y**2*z**2 - 2*x**2*z**4 - 2*y**2*z**4 - 2)**2)/(3*(-1/2 + sqrt(3)*I/2)*(-1512*x**4*y**4*z**10 + 324*x**2*y**2*z**8 + sqrt((-3024*x**4*y**4*z**10 + 648*x**2*y**2*z**8 - (-108*x**2*y**2*z**2 - 18*x**2*z**4 - 18*y**2*z**4 - 18)*(-40*x**4*y**2*z**6 - 40*x**2*y**4*z**6 - 12*x**2*y**2*z**8 + 4*x**2*z**4 + 4*y**2*z**4) + 2*(-12*x**2*y**2*z**2 - 2*x**2*z**4 - 2*y**2*z**4 - 2)**3)**2 - 4*(120*x**4*y**2*z**6 + 120*x**2*y**4*z**6 + 36*x**2*y**2*z**8 - 12*x**2*z**4 - 12*y**2*z**4 + (-12*x**2*y**2*z**2 - 2*x**2*z**4 - 2*y**2*z**4 - 2)**2)**3)/2 - (-108*x**2*y**2*z**2 - 18*x**2*z**4 - 18*y**2*z**4 - 18)*(-40*x**4*y**2*z**6 - 40*x**2*y**4*z**6 - 12*x**2*y**2*z**8 + 4*x**2*z**4 + 4*y**2*z**4)/2 + (-12*x**2*y**2*z**2 - 2*x**2*z**4 - 2*y**2*z**4 - 2)**3)**(1/3)): 1, 4*x**2*y**2*z**2 + 2*x**2*z**4/3 + 2*y**2*z**4/3 - (-1/2 - sqrt(3)*I/2)*(-1512*x**4*y**4*z**10 + 324*x**2*y**2*z**8 + sqrt((-3024*x**4*y**4*z**10 + 648*x**2*y**2*z**8 - (-108*x**2*y**2*z**2 - 18*x**2*z**4 - 18*y**2*z**4 - 18)*(-40*x**4*y**2*z**6 - 40*x**2*y**4*z**6 - 12*x**2*y**2*z**8 + 4*x**2*z**4 + 4*y**2*z**4) + 2*(-12*x**2*y**2*z**2 - 2*x**2*z**4 - 2*y**2*z**4 - 2)**3)**2 - 4*(120*x**4*y**2*z**6 + 120*x**2*y**4*z**6 + 36*x**2*y**2*z**8 - 12*x**2*z**4 - 12*y**2*z**4 + (-12*x**2*y**2*z**2 - 2*x**2*z**4 - 2*y**2*z**4 - 2)**2)**3)/2 - (-108*x**2*y**2*z**2 - 18*x**2*z**4 - 18*y**2*z**4 - 18)*(-40*x**4*y**2*z**6 - 40*x**2*y**4*z**6 - 12*x**2*y**2*z**8 + 4*x**2*z**4 + 4*y**2*z**4)/2 + (-12*x**2*y**2*z**2 - 2*x**2*z**4 - 2*y**2*z**4 - 2)**3)**(1/3)/3 + 2/3 - (120*x**4*y**2*z**6 + 120*x**2*y**4*z**6 + 36*x**2*y**2*z**8 - 12*x**2*z**4 - 12*y**2*z**4 + (-12*x**2*y**2*z**2 - 2*x**2*z**4 - 2*y**2*z**4 - 2)**2)/(3*(-1/2 - sqrt(3)*I/2)*(-1512*x**4*y**4*z**10 + 324*x**2*y**2*z**8 + sqrt((-3024*x**4*y**4*z**10 + 648*x**2*y**2*z**8 - (-108*x**2*y**2*z**2 - 18*x**2*z**4 - 18*y**2*z**4 - 18)*(-40*x**4*y**2*z**6 - 40*x**2*y**4*z**6 - 12*x**2*y**2*z**8 + 4*x**2*z**4 + 4*y**2*z**4) + 2*(-12*x**2*y**2*z**2 - 2*x**2*z**4 - 2*y**2*z**4 - 2)**3)**2 - 4*(120*x**4*y**2*z**6 + 120*x**2*y**4*z**6 + 36*x**2*y**2*z**8 - 12*x**2*z**4 - 12*y**2*z**4 + (-12*x**2*y**2*z**2 - 2*x**2*z**4 - 2*y**2*z**4 - 2)**2)**3)/2 - (-108*x**2*y**2*z**2 - 18*x**2*z**4 - 18*y**2*z**4 - 18)*(-40*x**4*y**2*z**6 - 40*x**2*y**4*z**6 - 12*x**2*y**2*z**8 + 4*x**2*z**4 + 4*y**2*z**4)/2 + (-12*x**2*y**2*z**2 - 2*x**2*z**4 - 2*y**2*z**4 - 2)**3)**(1/3)): 1}\n"
     ]
    }
   ],
   "source": [
    "import sympy as sp\n",
    "\n",
    "x = sp.Symbol('x')\n",
    "y = sp.Symbol('y')\n",
    "z = sp.Symbol('z')\n",
    "\n",
    "myfunc = x**2*y**2*z**4 + z**2\n",
    "\n",
    "mylist = [x, y, z]\n",
    "\n",
    "gradient = [myfunc.diff(variable) for variable in mylist]\n",
    "hessian = [[partial1.diff(variable) for variable in mylist] for partial1 in gradient]\n",
    "\n",
    "\n",
    "hessianmatrix = sp.Matrix(hessian)\n",
    "\n",
    "print(hessianmatrix.eigenvals())\n",
    "# these are NOT easy to interpret and it is a very small Hessian!\n",
    "# use a different tool!\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# extension: include Cassini Disks\n",
    "\n",
    "http://bwlewis.github.io/cassini/#br1\n",
    "\n",
    "or better: work through the Casini ovals stated (and then extended with graph properties), in this file: \n",
    "\n",
    "'CasiniOvals_extension.pdf'\n",
    "\n",
    "located in Linear Algebra folder ...\n",
    "\n",
    "also this seems to quite good\n",
    "\n",
    "http://planetmath.org/sites/default/files/texpdf/37503.pdf\n",
    "\n",
    "also this: \n",
    "\n",
    "http://www.math.kent.edu/~varga/pub/paper_232.pdf\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
