{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motivating Background: figuring out a function from data points\n",
    "\n",
    "Quite often in basic machine learning applications -- say with linear regression -- we gather $n$ samples of data and look to fit a model to it.  Note: we often have *a lot* of data, and in fact n can be any natural number.  For illustrative purposes, we start with the case of n = 5. \n",
    "\n",
    "Note that we typically also have multiple different features in our data, but *the goal of this posting is to strip down ideas to their very core*, so we consider the one feature case.  Also note that in machine learning we may use notation like $\\mathbf {Xw} = \\mathbf y$, where we solve for the weights in $\\mathbf w$.  However, this posting uses the typical Linear Algebra setup of $\\mathbf{Ax} = \\mathbf b$, where we are interested in solving for $\\mathbf x$.  \n",
    "\n",
    "So initially we may just have the equation\n",
    "\n",
    "$\\mathbf{Ax} = \\begin{bmatrix}\n",
    "a_1\\\\ \n",
    "a_2\\\\ \n",
    "a_3\\\\ \n",
    "a_4\\\\ \n",
    "a_5\n",
    "\\end{bmatrix} \\begin{bmatrix}\n",
    "x_1\\\\ \n",
    "\\end{bmatrix} = \\mathbf b$\n",
    "\n",
    "**this original 'data' matrix will also be written as **\n",
    "\n",
    "$\\mathbf a = \\begin{bmatrix}\n",
    "a_1\\\\ \n",
    "a_2\\\\ \n",
    "a_3\\\\ \n",
    "a_4\\\\ \n",
    "a_5\n",
    "\\end{bmatrix}$\n",
    "\n",
    "Note that when we gather real world data there is noise in the data, so we would be *extremely* surprised if any of the entries in $\\mathbf a$ are duplicates.  So, unless otherwise noted assume that each entry in $a_i$ is unique. Since there is only one column, the column rank of $\\mathbf A$ is one, and the column rank = row rank, thus we know that the row rank = 1. \n",
    "\n",
    "Then we decide to insert a bias /affine translation piece (in index position zero -- to use notation from Caltech's \"Learning From Data\").  \n",
    "\n",
    "Thus we end up with the following equation\n",
    "\n",
    "$\\mathbf{Ax} = \\begin{bmatrix}\n",
    "1 & a_1\\\\ \n",
    "1 & a_2\\\\ \n",
    "1 & a_3\\\\ \n",
    "1 & a_4\\\\ \n",
    "1 & a_5\n",
    "\\end{bmatrix} \\begin{bmatrix}\n",
    "x_0\\\\\n",
    "x_1\\\\ \n",
    "\\end{bmatrix} = x_0 \\mathbf 1 + x_1 \\mathbf a = \\mathbf b$\n",
    "\n",
    "Column 0 of $\\mathbf A$ is the ones vector, also denoted as $\\mathbf 1$.  \n",
    "\n",
    "At this point we know that $\\mathbf A$ still has full column rank (i.e. rank = 2) -- if this wasn't the case, this would imply that we could scale column 0 to get column 1 (i.e. everything in column 1 would have to be identical).   \n",
    "\n",
    "From here we may simply decide to do least squares and solve (which we always can do when we have full column rank, and $\\mathbf A $ has m rows and n columns, where $m \\geq n$).  \n",
    "\n",
    "Or we may decide to map this to a higher dimensional space that has a quadratic term.  \n",
    "\n",
    "$\\mathbf{Ax} = \\begin{bmatrix}\n",
    "1 & a_1 & a_1^2\\\\ \n",
    "1 & a_2 & a_2^2\\\\ \n",
    "1 & a_3 & a_3^2\\\\ \n",
    "1 & a_4 & a_4^2\\\\ \n",
    "1 & a_5 & a_5^2\n",
    "\\end{bmatrix} \\begin{bmatrix}\n",
    "x_0\\\\\n",
    "x_1\\\\ \n",
    "x_2\\\\\n",
    "\\end{bmatrix} = \\mathbf b$\n",
    "\n",
    "\n",
    "At this point we may just do least squares and solve.  But that requires $\\mathbf A$ to have full column rank.  How do we know that $\\mathbf A$ has full column rank?  An intuitive way to think about it is that squaring each $a_i$ to get column 2 is not a linear transformation, so we would not expect it to be linear combination of prior columns.  \n",
    "\n",
    "$\\mathbf a \\circ \\mathbf a \\neq \\gamma_0 \\mathbf 1 + \\gamma_1 \\mathbf a$\n",
    "\n",
    "where $\\circ$ denotes the Hadamard product.  And by earlier argument, we know $\\mathbf a \\neq \\gamma_0 \\mathbf 1$, hence each column is linearly independent.  There is another (more mathemetically exact) way to verify linear independence of these columns -- which comes from the Vandermonde Matrix, and we will address this shortly.  \n",
    "\n",
    "We may however decide we want an even higher dimensional space for our data, so we add a cubic term:\n",
    "\n",
    "$\\mathbf{Ax} = \\begin{bmatrix}\n",
    "1 & a_1 & a_1^2 & a_1^3\\\\ \n",
    "1 & a_2 & a_2^2 & a_2^3\\\\ \n",
    "1 & a_3 & a_3^2 & a_3^3\\\\ \n",
    "1 & a_4 & a_4^2 & a_4^3\\\\ \n",
    "1 & a_5 & a_5^2 & a_5^3\n",
    "\\end{bmatrix} \\begin{bmatrix}\n",
    "x_0\\\\\n",
    "x_1\\\\ \n",
    "x_2\\\\\n",
    "x_3\\\\\n",
    "\\end{bmatrix} = \\mathbf b$\n",
    "\n",
    "Again we may be confident that the columns are linearly independent because our new column -- cubing $\\mathbf a$ is not a linear transformation (or alternatively, using the hadamard product is not a linear transformation), so we write: \n",
    "\n",
    "$\\mathbf a \\circ \\mathbf a \\circ \\mathbf a \\neq \\gamma_0 \\mathbf 1 + \\gamma_1 \\mathbf a + \\gamma_2 \\big(\\mathbf a \\circ \\mathbf a\\big)$\n",
    "\n",
    "And if the above is *still* not enough, we may add a term to the fourth power:\n",
    "\n",
    "$\\mathbf{Ax} = \\begin{bmatrix}\n",
    "1 & a_1 & a_1^2 & a_1^3 & a_1^4\\\\ \n",
    "1 & a_2 & a_2^2 & a_2^3 & a_2^4\\\\ \n",
    "1 & a_3 & a_3^2 & a_3^3 & a_3^4\\\\ \n",
    "1 & a_4 & a_4^2 & a_4^3 & a_4^4\\\\ \n",
    "1 & a_5 & a_5^2 & a_5^3 & a_5^4\n",
    "\\end{bmatrix} \\begin{bmatrix}\n",
    "x_0\\\\\n",
    "x_1\\\\ \n",
    "x_2\\\\\n",
    "x_3\\\\\n",
    "x_4\\\\\n",
    "\\end{bmatrix} = \\mathbf b$\n",
    "\n",
    "Again quite confident that the above has full column rank because \n",
    "\n",
    "$\\mathbf a \\circ \\mathbf a \\circ \\mathbf a \\circ \\mathbf a \\neq \\gamma_0 \\mathbf 1 + \\gamma_1 \\mathbf a + \\gamma_2 \\big(\\mathbf a \\circ \\mathbf a\\big) + \\gamma_3 \\big(\\mathbf a \\circ \\mathbf a \\circ \\mathbf a \\big)$\n",
    "\n",
    "We may be tempted to go to an even higher dimensional space at this point, but this requires considerable justification.  Notice that $\\mathbf A$ is a square matrix now, and as we've argued, it has full column rank -- which means it also has full row rank.  Thus we can be sure to solve the above equation for a single, exact solution, where $\\mathbf x = \\mathbf A^{-1}\\mathbf b$.  If we were to go to a higher dimensional space we would be entering the world of an underdetermined system of equations -- see postings titled \"Underdetermined_System_of_Equations.ipynb\" for the L2 norm oriented solution, and \"underdetermined_regression_minimize_L1_norm.ipynb\" for the L1 norm oriented solution.  Since we can already be certain of solving for a single exact solution in this problem, we will stop mapping to higher dimensions here.  \n",
    "\n",
    "In the above equation of $\\mathbf{Ax} = \\mathbf b$, the square $\\mathbf A$ is a Vandermonde matrix.  Technical note: some texts say that $\\mathbf A$ is the Vandermonde matrix, while others say $\\mathbf A^T$ is the Vandermonde matrix.  The calculation of the determinant is identical, and for other properties, a mere small book-keeping adjustment is required.\n",
    "  \n",
    "Note that the Vandermonde matrix is well studied, has special fast matrix vector multiplication (i.e. $\\lt O(n^2)$) algorithms associated with it -- and a very special type of Vandermonde matrix is the Discrete Fourier Transform matrix.  The Vandermonde matrix  also has some very interesting properties for thinking about eigenvalues. \n",
    "\n",
    "\n",
    "There is another, more exacting way to verify that $\\mathbf A$ is full rank.  Let's look at the determinant of $\\mathbf A^T$.  There are a few different ways to prove this.  Sergei Winitzki had an interesting proof using wedge products -- that I may revisit at some point in the future.  \n",
    "\n",
    "\n",
    "# Begin Look at Vandermonde Matrices\n",
    "\n",
    "For some real valued Vandermonde matrix $\\mathbf A$, or it's transpose, we can say the following:\n",
    "\n",
    "(note the book-keeping required to evaluate this as a complex matrix, is just a very small alteration)\n",
    "\n",
    "\n",
    "$\\mathbf A^T  = \\begin{bmatrix}\n",
    "1 & 1 & 1 & \\dots & 1 & 1\\\\ \n",
    "a_1 & a_2 & a_3 & \\dots & a_{n-1} & a_n \\\\ \n",
    "a_1^2 & a_2^2 & a_3^2 & \\dots & a_{n-1}^2 & a_{n}^2\\\\ \n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\ \n",
    "a_{1}^{n-2} & a_{2}^{n-2} & a_{3}^{n-2} & \\dots & a_{n-1}^{n-2} & a_{n}^{n-2}\\\\\n",
    "a_{1}^{n-1} & a_{2}^{n-1} & a_{3}^{n-1} & \\dots & a_{n-1}^{n-1} & a_{n}^{n-1}\n",
    "\\end{bmatrix} $\n",
    "\n",
    "For the now,  I'll just notice that there is a rather obvious 'pattern' to these Vandermonde matrices, so we'll do the proof using mathematical induction, which takes advantage of this pattern / progression in polynomial terms.  \n",
    "\n",
    "\n",
    "\n",
    "**claim**: \n",
    "\n",
    "for natural number $n \\geq 2$ where $\\mathbf A \\in \\mathbb R^{n x n}$, and $\\mathbf A$ is a Vandermonde matrix, \n",
    "\n",
    "$det \\big(\\mathbf A \\big) = det \\big(\\mathbf A^T \\big) = \\prod_{1 \\leq i \\lt j \\leq n} (a_j - a_i)$\n",
    "\n",
    "*Base Case:* \n",
    "\n",
    "$n = 2$\n",
    "\n",
    "$\\mathbf A^T = \\begin{bmatrix}\n",
    "1 & 1\\\\ \n",
    "a_1 & a_2\n",
    "\\end{bmatrix}$\n",
    "\n",
    "$det \\big(\\mathbf A^T \\big) = (a_2 - a_1) = \\prod_{1 \\leq i \\lt j \\leq n} (a_j - a_i)$\n",
    "\n",
    "*sneak peak:*  \n",
    "if we follow the row operation procedure used during the inductive case, what we'd have is:\n",
    "\n",
    "$det \\big(\\mathbf A^T \\big) = det\\Big(\\begin{bmatrix}\n",
    "1 & 1\\\\ \n",
    "0 & (a_2 - a_1)\n",
    "\\end{bmatrix}\\Big) = 1*(a_2 - a_1)$\n",
    "\n",
    "\n",
    "*Inductive Case:*\n",
    "\n",
    "For $n \\gt 2$, assume formula is true where $\\mathbf C \\in \\mathbb R^{(n-1) x (n -1)}$\n",
    "\n",
    "i.e. assume true where \n",
    "\n",
    "$\\mathbf C = \\begin{bmatrix}\n",
    "1 & 1 & 1 & \\dots & 1\\\\ \n",
    "a_1 & a_2 & a_3 & \\dots & a_{n-1}\\\\ \n",
    "a_1^2 & a_2^2 & a_3^2 & \\dots & a_{n-1}^2\\\\ \n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots\\\\ \n",
    "a_{1}^{n-2} & a_{2}^{n-2} & a_{3}^{n-2} & \\dots & a_{n-1}^{n-2}\n",
    "\\end{bmatrix}$\n",
    "\n",
    "Note that we call this submatrix $\\mathbf C$ -- it will make a reappearance shortly!\n",
    "\n",
    "\n",
    "We need to show that the formula holds true where dimension of $\\mathbf A$ is $n$ x $n$. Thus consider the case where:\n",
    "\n",
    "$\\mathbf A^T  = \\begin{bmatrix}\n",
    "1 & 1 & 1 & \\dots & 1 & 1\\\\ \n",
    "a_1 & a_2 & a_3 & \\dots & a_{n-1} & a_n \\\\ \n",
    "a_1^2 & a_2^2 & a_3^2 & \\dots & a_{n-1}^2 & a_{n}^2\\\\ \n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\ \n",
    "a_{1}^{n-2} & a_{2}^{n-2} & a_{3}^{n-2} & \\dots & a_{n-1}^{n-2} & a_{n}^{n-2}\\\\\n",
    "a_{1}^{n-1} & a_{2}^{n-1} & a_{3}^{n-1} & \\dots & a_{n-1}^{n-1} & a_{n}^{n-1}\n",
    "\\end{bmatrix} $\n",
    "\n",
    "**Procedure:**\n",
    "subtract $a_1$ times the $i - 1$ row from the ith row, for  $0 \\lt i \\leq n$ **starting from the bottom of the matrix and working our way up** (i.e. the operations / subproblems do not overlap in this regard).  \n",
    "\n",
    "- - - - - \n",
    "**Justification:**\n",
    "\n",
    "First, the reason we'd like to do this is because we see an obvious pattern in the polynomial progression in each column of $\\mathbf A^T$.  Thus by following this procedure, we can zero out all entries in the zeroth column of $\\mathbf A^T$ except, the 1 located in the top left (i.e. in $a_{0,0}$).  This will allow us to, in effect, reduce our problem to the n - 1 x n - 1 dimensional case.  \n",
    "\n",
    "Also recall that the determinant of $\\mathbf A^T$ is equivalent to the determinant of $\\mathbf A$. Thus the above procedure is equivalent to subtracting a scaled version of column 0 of the original $\\mathbf A$ from column 1, and a scaled version of column 1 in the original $\\mathbf A$ from column 2, and so on.  These are standard operations that are well understood to not change the calculated determinant over any legal field. \n",
    "\n",
    "Since, your author particularly likes Gramâ€“Schmidt and orthgonality, there is an additional more visual interpretation that can be used over inner product spaces (i.e. real or complex fields).  Consider that $\\mathbf A = \\mathbf{QR}$, thus $det \\big(\\mathbf A \\big) = det \\big(\\mathbf{QR} \\big) = det \\big(\\mathbf{Q} \\big)det \\big(\\mathbf{R} \\big)$.  Notice that these column operations will have no impact on $\\mathbf Q$, and will only change the value of entries above the diagonal in $\\mathbf R$, thus there is no change in $det \\big(\\mathbf{Q} \\big)$ or $det \\big(\\mathbf{R} \\big)$ (which is given by the product of its diagonal entries).  This means there is no change in $det \\big(\\mathbf{A} \\big)$.  \n",
    "\n",
    "\n",
    "- - - - - \n",
    "\n",
    "$ = \\begin{bmatrix}\n",
    "1 & 1 & 1 & \\dots & 1 & 1\\\\ \n",
    "0 & a_2 - a_1 & a_3 - a_1 & \\dots & a_{n-1} - a_1 & a_n - a_1 \\\\ \n",
    "0 & a_2^2 - a_1 a_2 & a_3^2 - a_1 a_3 & \\dots & a_{n-1}^2 - a_1 a_{n-1} & a_{n}^2 - a_1 a_{n}\\\\ \n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\ \n",
    "0 & a_{2}^{n-2} - a_1 a_{2}^{n-3} & a_{3}^{n-2} - a_1 a_{3}^{n-3} & \\dots & a_{n-1}^{n-2} - a_1 a_{n-1}^{n-3} & a_{n}^{n-2} - a_1 a_{n}^{n-3}\\\\\n",
    "0 & a_{2}^{n-1} - a_1 a_2^{n-2} & a_{3}^{n-1} - a_1 a_3^{n-2}& \\dots & a_{n-1}^{n-1} -  a_1 a_{n-1}^{n-2}& a_{n}^{n-1} - a_1 a_{n}^{n-1}\n",
    "\\end{bmatrix} $\n",
    "\n",
    "$ = \\begin{bmatrix}\n",
    "1 & 1 & 1 & \\dots & 1 & 1\\\\ \n",
    "0 & (a_2 - a_1) 1 & (a_3 - a_1)1 & \\dots & (a_{n-1} - a_1) 1 & (a_n - a_1) 1 \\\\ \n",
    "0 & (a_2 - a_1) a_2 & (a_3 - a_1) a_3 & \\dots & (a_{n-1} - a_1) a_{n-1} & (a_n - a_1) a_{n}\\\\ \n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\ \n",
    "0 & (a_2 - a_1)a_{2}^{n-3} & (a_3 - a_1)a_{3}^{n-3} & \\dots & (a_{n-1} - a_1)a_{n-1}^{n-3} & (a_n - a_1)a_{n}^{n-3}\\\\\n",
    "0 & (a_2 - a_1)a_{2}^{n-2} & (a_3 - a_1)a_{3}^{n-2} & \\dots & (a_{n-1} - a_1)a_{n-1}^{n-2} & (a_n - a_1)a_{n}^{n-2} \n",
    "\\end{bmatrix}  $\n",
    "\n",
    "we can rewrite this as \n",
    "\n",
    "$= \\begin{bmatrix}\n",
    "1 & \\mathbf 1^T\\\\ \n",
    "\\mathbf 0 & \\mathbf{CD}\n",
    "\\end{bmatrix}$\n",
    "\n",
    "\n",
    "where \n",
    "\n",
    "$\\mathbf D = Diag\\Big(\\begin{bmatrix}\n",
    "a_2 & a_3 & a_4 & \\dots & a_n\n",
    "\\end{bmatrix}^T \\Big) - a_1 \\mathbf I =    \\begin{bmatrix}\n",
    "(a_2-a_1) & 0 &  0& \\dots & 0\\\\ \n",
    "0 & (a_3 - a_1) &0  &\\dots  &0 \\\\ \n",
    "0 & 0 & (a_4 - a_1) & \\dots & 0\\\\ \n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\ \n",
    "0 & 0 & 0 & \\dots & (a_n - a_1)\n",
    "\\end{bmatrix}$\n",
    "\n",
    "Note that $\\begin{bmatrix}\n",
    "1 & \\mathbf 1^T\\\\ \\mathbf 0 & \\mathbf{CD} \\end{bmatrix} - \\lambda \\begin{bmatrix}\n",
    "1 & \\mathbf 0^T\\\\ \\mathbf 0 & \\mathbf{I} \\end{bmatrix} = \\begin{bmatrix}\n",
    "1 - \\lambda & \\mathbf 1^T\\\\ \\mathbf 0 & \\mathbf{CD } - \\mathbf \\lambda \\mathbf I \\end{bmatrix}$, which is not invertible when $\\lambda := 1$ (because the left most column is all zeros).  \n",
    "\n",
    "Hence we know that there is an eigenvalue of 1, given by the top left diagonal entry, associated with $\\begin{bmatrix}\n",
    "1 & \\mathbf 1^T\\\\ \n",
    "\\mathbf 0 & \\mathbf{CD}\n",
    "\\end{bmatrix}$. We'll call this $\\lambda_1$ -- for the first eigenvalue of the \"MatrixAfterRowOperations\".  \n",
    "\n",
    "Thus the determinant can be written as \n",
    "\n",
    "$det\\big(\\mathbf A^T \\big) = det\\big(MatrixAfterRowOperations\\big) = (\\lambda_1) * (\\lambda_2  * \\lambda_3 * ... * \\lambda_n\\big) = (1) * \\det\\big(\\mathbf{CD}\\big) = \\det\\big(\\mathbf{C}\\big) \\det\\big(\\mathbf{D}\\big)$\n",
    "\n",
    "\n",
    "\n",
    "- - - - -\n",
    "\n",
    "**begin interlude** \n",
    "\n",
    "The fact that \n",
    "$det\\big(\\begin{bmatrix}\n",
    "1 & \\mathbf *\\\\ \n",
    "\\mathbf 0 & \\mathbf{Z}\n",
    "\\end{bmatrix}\\big) = 1 * det\\big(\\mathbf{Z}\\big)$\n",
    "\n",
    "is well understood via properities of block matrices over many fields. In particular refrence the recursive approach in 'determinants_calculations.ipynb'.  We can see that the determinant 'payoff' is scaled by zero in all cases except where we take the top entry from row one.  This also nicely lines up with how we'd think about about doing row reduction and Gaussian Elimination.  Frequently the orthogonal approach is pleasant and fits most directly with our intuition, but it is worth remembering from time to time, that determinants and linear independence are considerably more general concepts.  This features somewhat prominently at the very end of the writeup in the walkthrough of \"Proof of the Schwartz-Zippel Theorem\" which at its core, is a brilliant result for multi-variable polynomials over finite fields.  \n",
    "\n",
    "**end interlude**\n",
    "- - - - -\n",
    "We know that \n",
    "\n",
    "$\\det\\big(\\mathbf{D}\\big) = (a_2-a_1) * (a_3 - a_1) * ... * (a_n - a_1)$\n",
    "\n",
    "because the determininant of a diagonal matrix is the product of its diagonal entries (i.e. its eigenvalues)  \n",
    "\n",
    "and \n",
    "\n",
    "$det \\big(\\mathbf C \\big) = \\prod_{1 \\leq i \\lt j \\leq n-1} (a_j - a_i)$ \n",
    "\n",
    "by inductive hypothesis.  \n",
    "Thus we can say \n",
    "\n",
    "$ det\\big(\\mathbf A^T \\big) = \\big(\\prod_{1 \\leq i \\lt j \\leq n-1} (a_j - a_i)\\big) \\big((a_2-a_1) * (a_3 - a_1) * ... * (a_n - a_1)\\big) = \\prod_{1 \\leq i \\lt j \\leq n} (a_j - a_i)$\n",
    "\n",
    "And the induction is proved.  \n",
    "\n",
    "Finally, we note that $det \\big(\\mathbf A \\big) = det \\big(\\mathbf A^T \\big)$ because $\\mathbf A$ and $\\mathbf A^T$ have the same characteristic polynomials (or equivalently, they have the same eigenvalues). We have thus proved the determinant formula for $\\mathbf A$.  \n",
    "\n",
    "(Technical note: if $\\mathbf A \\in \\mathbb C^{n x n}$ then the above results still hold with respect to the magnitude of the determinant of $\\mathbf A$.  This includes the very important special case of whether or not $\\big\\vert det\\big(\\mathbf A\\big)\\big\\vert = 0$ --i.e. whether or not $\\mathbf A^{-1}$ exists.  However, with respect to the exact determinant, it would be more proper to state that $det\\big(\\mathbf A\\big) = conjugate\\Big(\\det\\big(\\mathbf A^H\\big)\\Big)$. \n",
    "- - - -\n",
    "\n",
    "This gives us another way to confirm that our Vandermonde Matrix is full rank.  We know that a square, finite dimensional matrix is singular iff it has a determinant of 0.  We then see that \n",
    "\n",
    "$\\det \\big(\\mathbf A\\big) = \\big(\\prod_{1 \\leq i \\lt j \\leq n} (a_j - a_i)\\big) = 0$ iff there is some $a_j = a_i$ where $i \\neq j$.  \n",
    "\n",
    "This of course is another way of saying that our Vandermonde Matrix is not full rank if some entry in our 'original' matrix of \n",
    "\n",
    "$\\mathbf a = \\begin{bmatrix}\n",
    "a_1\\\\ \n",
    "a_2\\\\ \n",
    "a_3\\\\ \n",
    "a_4\\\\ \n",
    "a_5\n",
    "\\end{bmatrix}$\n",
    "\n",
    "was not unique.  \n",
    "\n",
    "\n",
    "- - - -\n",
    "It is worth highlighting that if for some reason we did not like to explicitly use determinants, we could instead just repeatedly, and recursively apply the above procedure as a type of Gaussian Elimination, and in the end we would get have transformed $\\mathbf{A}^T$ into the below Row Echelon form: \n",
    "\n",
    "$\\begin{bmatrix}\n",
    "1 & * & * &  \\dots & * & *\\\\\n",
    "0 &(a_2-a_1) & * &   \\dots & * & *\\\\ \n",
    "0& 0 & (a_3 - a_1)(a_3 - a_2)  &\\dots &* &* \\\\ \n",
    "\\vdots &\\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\ \n",
    "0&0 & 0 & \\dots & \\big(\\prod_{1 \\leq i \\lt n-1} (a_{n-1} - a_i)\\big) & *\\\\ \n",
    "0& 0 & 0 & \\dots & 0 & \\big(\\prod_{1 \\leq i \\lt n} (a_{n} - a_i)\\big)\n",
    "\\end{bmatrix}\\mathbf x = \\mathbf b$\n",
    "\n",
    "(Of course, we can immediately notice that the determinant formula can be recovered by multiplying the diagonal elements of the above matrix.)\n",
    "\n",
    "It is instructive to realize that we can solve for an exact $\\mathbf x$ so long as we don't have any zeros on the diagonal of our above upper triangular /row echelon matrix.  We notice that this is the case only if and only if all $a_i$ are unique.\n",
    "\n",
    "- - - -\n",
    "\n",
    "\n",
    "\n",
    "Furthermore, notice that this determinant formula gives us a proof that we have full column rank in any thinner (i.e. more rows than columns) version of our Vandermonde matrix.  E.g. consider the case of \n",
    "\n",
    "\n",
    "$\\mathbf{A} = \\begin{bmatrix}\n",
    "1 & a_1 & a_1^2 & a_1^3\\\\ \n",
    "1 & a_2 & a_2^2 & a_2^3\\\\ \n",
    "1 & a_3 & a_3^2 & a_3^3\\\\ \n",
    "1 & a_4 & a_4^2 & a_4^3\\\\ \n",
    "1 & a_5 & a_5^2 & a_5^3\n",
    "\\end{bmatrix}$\n",
    "\n",
    "\n",
    "These columns must be linearly independent, so long as each $a_i \\neq a_j$ where $i \\neq j$.  If that was not the case, then appending additional columns until square (i.e. append $\\mathbf a \\circ \\mathbf a \\circ \\mathbf a \\circ \\mathbf a$) would mean that \n",
    "\n",
    "$\\mathbf A = \\begin{bmatrix}\n",
    "1 & a_1 & a_1^2 & a_1^3 & a_1^4\\\\ \n",
    "1 & a_2 & a_2^2 & a_2^3 & a_2^4\\\\ \n",
    "1 & a_3 & a_3^2 & a_3^3 & a_3^4\\\\ \n",
    "1 & a_4 & a_4^2 & a_4^3 & a_4^4\\\\ \n",
    "1 & a_5 & a_5^2 & a_5^3 & a_5^4\n",
    "\\end{bmatrix} $\n",
    "\n",
    "could not have full column rank either.  Yet we know this matrix is full rank via our determinant formula (again so long as each $a_i$ is unique) thus we know that the columns of any smaller  \"long and skinny\" version of this matrix must also be linearly independent.\n",
    "\n",
    "Also, when each $a_i$ is unique, since we know that our Vandermonde matrix is full rank, we know that each of its rows is linearly independent.  If for some reason we had a 'short and fat' version of the above matrix, like:\n",
    "\n",
    "$\\mathbf A = \\begin{bmatrix}\n",
    "1 & a_1 & a_1^2 & a_1^3 & a_1^4\\\\ \n",
    "1 & a_2 & a_2^2 & a_2^3 & a_2^4\\\\ \n",
    "1 & a_3 & a_3^2 & a_3^3 & a_3^4\\\\ \n",
    "\\end{bmatrix} $\n",
    "\n",
    "we would know that it is full row rank -- i.e. each of its rows are linearly independent.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Implication: A degree n-1 polynomial is completely given by n uniqe data points**\n",
    "\n",
    "Assuming there is no noise in the data -- or numeric precision issues-- the Vandermonde matrix, $\\mathbf A$, allows you to solve for the unique values in some polynomial with coefficients of \n",
    "\n",
    "\n",
    "$x_0 * 1 + x_1 a + x_2 a^2 + x_3 a^3 + x_4 a^4 = b$\n",
    "\n",
    "\n",
    "- - - - -\n",
    "$\\mathbf{Ax} = \\begin{bmatrix}\n",
    "1 & a_1 & a_1^2 & a_1^3 & a_1^4\\\\ \n",
    "1 & a_2 & a_2^2 & a_2^3 & a_2^4\\\\ \n",
    "1 & a_3 & a_3^2 & a_3^3 & a_3^4\\\\ \n",
    "1 & a_4 & a_4^2 & a_4^3 & a_4^4\\\\ \n",
    "1 & a_5 & a_5^2 & a_5^3 & a_5^4\n",
    "\\end{bmatrix} \\begin{bmatrix}\n",
    "x_0\\\\\n",
    "x_1\\\\ \n",
    "x_2\\\\\n",
    "x_3\\\\\n",
    "x_4\\\\\n",
    "\\end{bmatrix} = \\mathbf b$\n",
    "- - - - -\n",
    "\n",
    "The next extension is perhaps a bit more interesting.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extension: two ways to think about polynomials** \n",
    "\n",
    "Knowing that we can exactly specify a degree $n-1$ polynomial with $n$ distinct data points leads us to wonder:\n",
    "\n",
    "is it 'better' to think about polynomials with respect to the coefficients or the data points?  In the above vector form -- the question becomes is it better to think about the polynomial in terms of $\\mathbf x$ or $\\mathbf b$? \n",
    "\n",
    "The answer is-- it depends.  To directly evaluate a function is much quicker when we know $\\mathbf x$.  But as it turns out, when we want to multiply or convolve polynomials, it is considerably faster to know their point values contained in $\\mathbf b$.  \n",
    "\n",
    "And since the Vandermonde matrix is so helpful for encapsulating all of our knowledge about a polynomial, a natural question is -- what if we wanted to make multiplying $\\mathbf A^{-1} \\mathbf b$ to get $\\mathbf x$ at least as easy as just multiplying $\\mathbf{Ax}$ to get $\\mathbf b$?  The clear answer would mean finding a way so that you don't have to explicitly invert $\\mathbf A$.  This can be done most easily if $\\mathbf A$ is unitary (i.e. orthogonal albeit in a complex inner product space), hence $\\mathbf A^H = \\mathbf A^{-1}$.  If $\\mathbf A$ is unitary, this directly leads us to the Discrete Fourier Transform.  (And from there to the Fast Fourier Transform which is widely regarded as one of the top 10 algorithms of the last 100 years.)\n",
    "\n",
    "But first, let's work through a couple of important related ideas where we can apply Vandermonde matrices: (a) square matrices that have unique eigenvalues must be diagonalizable and (b) some interesting cyclic and structural properties underlying Permutation matrices. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "*small formatting note*: in bold face LaTeX, the capital A, $\\mathbf A$, looks very similar to the capital Lambda, given by $\\mathbf \\Lambda$, which is a diagonal matrix with eigenvalues $\\lambda_k$, along the diagonal.  The rest of this posting will stop using capital A for an operator, accordingly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Application of Vandermonde Matrices: \n",
    "# Proof of Linear Independence of Eigenvectors associated with Unique Eigenvalues\n",
    "\n",
    "This proves that if a square (finite dimensional) matrix --aka an operator --has all eigenvalues that are unique, then the eigenvectors must be linearly independent.  Put differently, this proves that such an operator is diagonalizable.  \n",
    "\n",
    "The typical argument for linear indepdence is in fact a touch shorter than this and does not need Vandermonde matrices -- however it relies on a contradiction that is not particularly satisfying.  The following proof -- adapted from Winitzki's *Linear Algebra via Exterior Products* is direct -- and to your author-- very intuitive.   \n",
    "\n",
    "Consider $\\mathbf B \\in \\mathbb C^{n x n}$ matrix, which has n unique eigenvalues -- i.e. $\\lambda_1 \\neq \\lambda_2 \\neq ... \\neq \\lambda_n$.  \n",
    "\n",
    "When looking for linear indepenence, \n",
    "\n",
    "$\\gamma_1 \\mathbf v_1 + \\gamma_2 \\mathbf v_2 + ... + \\gamma_n \\mathbf v_n = \\mathbf 0$  \n",
    "\n",
    "we can say that **the eigenvectors are linearly independent iff** $\\gamma_1 = \\gamma_2 = ... = \\gamma_n = 0$\n",
    "\n",
    "Further, for $k = \\{1, 2, ..., n\\}$, we know that  \n",
    "$\\mathbf v_k  = \\mathbf v_k$  \n",
    "$\\mathbf B \\mathbf v_k = \\lambda_k \\mathbf v_k$  \n",
    "$\\mathbf B \\mathbf B \\mathbf v_k = \\mathbf B^2 \\mathbf v_k = \\lambda_k^2 \\mathbf v_k$  \n",
    "$\\vdots $  \n",
    "\n",
    "$\\mathbf B^{n-1} \\mathbf v_k = \\lambda_k^{n-1} \\mathbf v_k$  \n",
    "\n",
    "\n",
    "Thus we can take our original linear independence test,\n",
    "\n",
    "$\\gamma_1 \\mathbf v_1 + \\gamma_2 \\mathbf v_2 + ... + \\gamma_n \\mathbf v_n = \\mathbf 0$  \n",
    "\n",
    "and left multiply by $\\mathbf B^r$ and get the following equalities, as well: \n",
    "\n",
    "$\\mathbf B^r \\mathbf 0 = \\mathbf 0 =  \\mathbf B^r \\big(\\gamma_1 \\mathbf v_1 + \\gamma_2 \\mathbf v_2 + ... + \\gamma_n \\mathbf v_n\\big) =  \\lambda_1^r \\gamma_1 \\mathbf v_1 + \\lambda_2^r  \\gamma_2 \\mathbf v_2 + ... + \\lambda_n^r \\gamma_n \\mathbf v_n $  \n",
    "\n",
    "for $r = \\{1, 2, ..., n-1\\}$\n",
    "- - - -\n",
    "\n",
    "Now let's collect these $n$ relationships in a system of equations:\n",
    "\n",
    "\n",
    "$\\bigg[\\begin{array}{c|c|c|c}\n",
    "\\gamma_1 \\mathbf v_1 & \\gamma_2 \\mathbf v_2 &\\cdots & \\gamma_n \\mathbf v_n\n",
    "\\end{array}\\bigg] \\mathbf W = \\bigg[\\begin{array}{c|c|c|c}\n",
    "  \\mathbf 0 & \\mathbf 0 &\\cdots & \\mathbf 0\n",
    "\\end{array}\\bigg]$\n",
    "\n",
    "\n",
    "where \n",
    "\n",
    "$\\mathbf W = \\begin{bmatrix}\n",
    "1 & \\lambda_1 & \\lambda_1^2 & \\dots  & \\lambda_1^{n-1}\\\\ \n",
    "1 & \\lambda_2 & \\lambda_2^2 & \\dots &  \\lambda_2^{n-1} \\\\ \n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\\\ \n",
    "1 & \\lambda_{n} & \\lambda_{n}^{2} & \\dots  & \\lambda_{n}^{n-1}\n",
    "\\end{bmatrix}$\n",
    "\n",
    "\n",
    "Notice that $\\mathbf W$ is a Vandermonde matrix. Since $\\lambda_i \\neq \\lambda_k$ if $i \\neq k$, we know that $det \\big(\\mathbf W\\big) \\neq 0$, and thus $\\mathbf W^{-1}$ exists as a unique operator.  We multiply each term on the right by $\\mathbf W^{-1}$.  \n",
    "\n",
    "$\\bigg[\\begin{array}{c|c|c|c}\n",
    "\\mathbf \\gamma_1 \\mathbf v_1 & \\gamma_2 \\mathbf v_2 &\\cdots & \\gamma_n \\mathbf v_n\n",
    "\\end{array}\\bigg]\n",
    " \\mathbf W \\mathbf W^{-1}= \\bigg[\\begin{array}{c|c|c|c}\n",
    "\\gamma_1 \\mathbf v_1 & \\gamma_2 \\mathbf v_2 &\\cdots & \\gamma_n \\mathbf v_n\n",
    "\\end{array}\\bigg]\\mathbf I = \\bigg[\\begin{array}{c|c|c|c}\n",
    "  \\mathbf 0 & \\mathbf 0 &\\cdots & \\mathbf 0\n",
    "\\end{array}\\bigg] \\mathbf W^{-1} = \\bigg[\\begin{array}{c|c|c|c}\n",
    "  \\mathbf 0 & \\mathbf 0 &\\cdots & \\mathbf 0\n",
    "\\end{array}\\bigg]$  \n",
    "\n",
    "\n",
    "Thus we know that \n",
    "\n",
    "$\\bigg[\\begin{array}{c|c|c|c}\n",
    "\\mathbf \\gamma_1 \\mathbf v_1 & \\gamma_2 \\mathbf v_2 &\\cdots & \\gamma_n \\mathbf v_n\n",
    "\\end{array}\\bigg] = \\bigg[\\begin{array}{c|c|c|c}\n",
    "  \\mathbf 0 & \\mathbf 0 &\\cdots & \\mathbf 0\n",
    "\\end{array}\\bigg]$\n",
    "\n",
    "By definition each eigenvector $\\mathbf v_k \\neq \\mathbf 0$.  This means that each scalar $\\gamma_k = 0$.  Each eigenvector has thus been proven to be linearly independent in the case where eigenvalues are unique.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Permutation Matrices and Periodic Behavior (introducting DFT)\n",
    "\n",
    "Consider an $n$ x $n$ permutation matrix $\\mathbf P$.  Note that this matrix is real valued where each column has all zeros, and a single 1.  \n",
    "\n",
    "We'll use the $^H$ to denote conjugate transpose (even though the matrix is entirely real valued), as some imaginary numbers will creep in later.\n",
    "\n",
    "\n",
    "It is easy to verify that the permutation matrix is unitary (a special case infact which is orthogonal), i.e. that \n",
    "\n",
    "$\\mathbf P^H \\mathbf P = \\mathbf I$\n",
    "\n",
    "because, by construction, each column in a permutation matrix has all zeros, except a single 1, and the Permutation matrix is full rank -- hence each column must be orthogonal.  \n",
    "\n",
    "Further, as mentioned in \"Schurs_Inequality.ipynb\", such a matrix can be diagonalized where   \n",
    "\n",
    "$\\mathbf P = \\mathbf{V\\Lambda V}^H$\n",
    "\n",
    "where $\\mathbf V$ is unitary, each eigenvalue $\\lambda_i$ is contained along the diagonal of $\\mathbf \\Lambda$ and is on the unit circle.  \n",
    "\n",
    "notice that for any permtuation matrix: \n",
    "\n",
    "$\\mathbf {P1} = \\mathbf 1$\n",
    "\n",
    "Hence such a permutation matrix has $\\lambda_1 = 1$ (i.e. a permutation matrix is stochastic -- in fact doubly so).  \n",
    "\n",
    "Because $\\mathbf P$ has all zeros, except a single 1 in each column (or equivalently, in each row), it can be interpretted as a special kind of Adjacency Matrix for a directed graph. \n",
    "- - - - \n",
    "Of particular interest is **the permutation matrix that relates to a connected graph** (i.e. where each node is reachable from each other node) with $n$ nodes.  One example, where $n = 6$ is the below:\n",
    "\n",
    "<table style=\"background-color: white\"></table><tr><td><table><tr><td><img src='images/permutation_graph_matrix.gif'style=\"width: 100%; background-color: white;\"></td><td><img src='images/permutation_matrix_graph.png'style=\"width: 50%; background-color: white;\" ></td></tr></table>\n",
    "\n",
    "\n",
    "\n",
    "*Claim:*  \n",
    "For a permutation matrix associated with a connected graph (i.e. where each node may be visited from each other node), the time it takes to repeat a visit to a node is $n$ iterations. \n",
    "\n",
    "*Proof:*  \n",
    "since each node in the directed graph has an outbound connection to only one other node, and there are $n$ nodes total, if a cycle can occur in $\\leq n - 1$ iterations, then the number of nodes you can reach from the starting node (including itself) is $\\leq n - 1$ nodes, and hence the graph is not connected -- a contradiction. (And for avoidance of doubt, if it took $\\geq n + 1$ iterations to visit the starting node, then that would mean after $n$ iterations, you've visited (at least)  one of the $n-1$ nodes, other than the starting one, more than once which means there is a cycle in the graph $\\leq n-1$ nodes, which is a contradiction, as outlined above.  This second part in many ways is not needed-- we have many other tools to deal with linear dependence after n iterations.  The point is that this type of graph, by construction, has periodicity equal to its number of nodes -- i.e. all cycles take n iterations.)\n",
    "\n",
    "\n",
    "Thus we can say that $\\mathbf P^ 0 = \\mathbf P^n = \\mathbf I$. So $trace\\big(\\mathbf P^0\\big) = trace\\big(\\mathbf P^n\\big) = trace\\big(\\mathbf I\\big) = n$.  \n",
    "\n",
    "However, the diagonal entries of $\\mathbf P^k$ are all zero for $k \\in \\{1, 2, ..., n-2, n-1\\}$. Thus we have:\n",
    "\n",
    "$trace\\big(\\mathbf P^k\\big) = 0$\n",
    "\n",
    "*note: the reader may wonder why I chose a permutation matrix associated with a connected graph -- this post was supposed to be about Vandermonde matrices! The core reasons are simple -- it is a special type of unitary (or orthogonal) matrx, it has a simple visual representation via graph theory, and its trace is extremely easy to compute.  On top of that there is a messier, related reason -- I was inspired by the n iterations cycle that is implied in the proof of linear independence of eigenvectors associated with unique eigenvalues which used a Vandermonde matrix, and I had recently used a connected graph 3 x 3 permutation matrix (and its eigenvalues) to explain to someone the that 3 complex numbers on the unit circle must be equidistant when there is a constraint they all sum to zero  -- I knew that first eigenvalue had to be one because the matrix is stochastic, I knew the trace was 0, and I knew that for a real valued matrix, complex eigenvalues numbers come in conjugate pairs and hence in the 3 x 3 case they had to be evenly spaced in the unit circle.  In many respects this posting grew out of my attempt to generalize ideas from that conversation, plus a growing interest I had in Vandermonde matrices and matrices used in convolutions. I chose to use permutation matrices associated with a connected graph for those reasons and was pleasantly surprised that I was able to derive the DFT and all of its properties in full, just using this permutation matrix and spectral theory, for an arbitrary $n$ x $n$ dimensional case.* \n",
    "\n",
    "Now consider the standard basis vectors $\\mathbf e_j$, where $j \\in \\{1, 2, ..., n-1, n\\}$ -- i.e. column slices of the identity matrix, shown below:\n",
    "\n",
    "$\\bigg[\\begin{array}{c|c|c|c}\n",
    "\\mathbf e_1 & \\mathbf  e_2 &\\cdots &\\mathbf  e_n\n",
    "\\end{array}\\bigg] = \\mathbf I$\n",
    "\n",
    "Each one of these vectors is a valid starting position for having a 'presence' on exactly one node of the graph. With no loss of generality, we could choose just one the standard basis vectors to be our starting point -- e.g. set $\\mathbf e_j := \\mathbf e_1$.  However, we'll keep the notation $\\mathbf e_j$, though the reader may decide to select a specific standard basis vector if helpful.\n",
    "\n",
    "Since we have a connected graph and can only be on one position at a time as we iterate though, we know that $\\mathbf P^k \\mathbf e_j \\perp \\mathbf P^r \\mathbf e_j$,\n",
    "\n",
    "for natural numbers $r$, $k$, $\\in \\{1, 2, ..., n-2, n-1\\}$, where $r \\neq k$\n",
    "\n",
    "Thus we can collect each location in the graph in an $n$ x $n$ matrix as below:\n",
    "\n",
    "$\\bigg[\\begin{array}{c|c|c|c|c}\n",
    "\\mathbf P^0\\mathbf e_j & \\mathbf P^1\\mathbf e_j & \\mathbf P^2\\mathbf e_j &\\cdots & \\mathbf P^{n-1}\\mathbf e_j\n",
    "\\end{array}\\bigg] = \\bigg[\\begin{array}{c|c|c|c|c}\n",
    "\\mathbf V \\mathbf \\Lambda^0 \\mathbf V^H\\mathbf e_j & \\mathbf V \\mathbf \\Lambda^1 \\mathbf V^H\\mathbf e_j & \\mathbf V \\mathbf \\Lambda^2 \\mathbf V^H\\mathbf e_j &\\cdots & \\mathbf V \\mathbf \\Lambda^{n-1} \\mathbf V^H\\mathbf e_j\n",
    "\\end{array}\\bigg]$\n",
    "\n",
    "\n",
    "\n",
    "$\\bigg[\\begin{array}{c|c|c|c|c}\n",
    "\\mathbf P^0\\mathbf e_j & \\mathbf P^1\\mathbf e_j & \\mathbf P^2\\mathbf e_j &\\cdots & \\mathbf P^{n-1}\\mathbf e_j\n",
    "\\end{array}\\bigg] = \\mathbf V \\bigg[\\begin{array}{c|c|c|c|c}\n",
    "\\mathbf \\Lambda^0 \\mathbf V^H\\mathbf e_j & \\mathbf \\Lambda^1 \\mathbf V^H\\mathbf e_j & \\mathbf \\Lambda^2 \\mathbf V^H\\mathbf e_j &\\cdots & \\mathbf \\Lambda^{n-1} \\mathbf V^H\\mathbf e_j\n",
    "\\end{array}\\bigg]$\n",
    "\n",
    "Now left multiply each side by full rank, unitary matrix $\\mathbf V^H$, and for notational simplity, let $\\mathbf y := \\mathbf V^H\\mathbf e_j$\n",
    "\n",
    "\n",
    "$\\mathbf V^H \\bigg[\\begin{array}{c|c|c|c|c}\n",
    "\\mathbf P^0\\mathbf e_j & \\mathbf P^1\\mathbf e_j & \\mathbf P^2\\mathbf e_j &\\cdots & \\mathbf P^{n-1}\\mathbf e_j\n",
    "\\end{array}\\bigg] = \\bigg[\\begin{array}{c|c|c|c|c}\n",
    "\\mathbf \\Lambda^0 \\mathbf y & \\mathbf \\Lambda^1 \\mathbf y & \\mathbf \\Lambda^2 \\mathbf y &\\cdots & \\mathbf \\Lambda^{n-1} \\mathbf y\n",
    "\\end{array}\\bigg]$\n",
    "\n",
    "For each column vector on the right hand side, we have $\\mathbf \\Lambda^m \\mathbf y$.  In various forms this can be written as \n",
    "\n",
    "$\\mathbf \\Lambda^m \\mathbf y =  \\mathbf \\Lambda^m \\big(\\mathbf{Diag}\\big(\\mathbf y\\big)\\mathbf 1\\big) = \\mathbf{Diag}\\big(\\mathbf y\\big) \\mathbf \\Lambda^m \\mathbf 1 =\n",
    "\\mathbf{Diag}\\big(\\mathbf y\\big)\\big(\\mathbf \\Lambda^m \\mathbf 1\\big)$\n",
    "\n",
    "Thus we can say: \n",
    "\n",
    "$\\bigg[\\begin{array}{c|c|c|c|c}\n",
    "\\mathbf \\Lambda^0 \\mathbf y & \\mathbf \\Lambda^1 \\mathbf y & \\mathbf \\Lambda^2 \\mathbf y &\\cdots & \\mathbf \\Lambda^{n-1} \\mathbf y\n",
    "\\end{array}\\bigg] = \\bigg[\\begin{array}{c|c|c|c|c}\n",
    "\\mathbf{Diag}\\big(\\mathbf y\\big)\\mathbf \\Lambda^0 \\mathbf 1 & \\mathbf{Diag}\\big(\\mathbf y\\big)\\mathbf \\Lambda^1 \\mathbf 1 & \\mathbf{Diag}\\big(\\mathbf y\\big)\\mathbf \\Lambda^2 \\mathbf 1 &\\cdots & \\mathbf{Diag}\\big(\\mathbf y\\big)\\mathbf \\Lambda^{n-1} \\mathbf 1\n",
    "\\end{array}\\bigg] $\n",
    "\n",
    "\n",
    "\n",
    "$\\bigg[\\begin{array}{c|c|c|c|c}\n",
    "\\mathbf \\Lambda^0 \\mathbf y & \\mathbf \\Lambda^1 \\mathbf y & \\mathbf \\Lambda^2 \\mathbf y &\\cdots & \\mathbf \\Lambda^{n-1} \\mathbf y\n",
    "\\end{array}\\bigg] = \\mathbf{Diag}\\big(\\mathbf y\\big)\\bigg[\\begin{array}{c|c|c|c|c}\n",
    "\\mathbf \\Lambda^0 \\mathbf 1 & \\mathbf \\Lambda^1 \\mathbf 1 & \\mathbf \\Lambda^2 \\mathbf 1 &\\cdots & \\mathbf \\Lambda^{n-1} \\mathbf 1\n",
    "\\end{array}\\bigg] $\n",
    "\n",
    "\n",
    "We make this substitution and see: \n",
    "\n",
    "$\\mathbf V^H \\bigg[\\begin{array}{c|c|c|c|c}\n",
    "\\mathbf P^0\\mathbf e_j & \\mathbf P^1\\mathbf e_j & \\mathbf P^2\\mathbf e_j &\\cdots & \\mathbf P^{n-1}\\mathbf e_j\n",
    "\\end{array}\\bigg] = \\mathbf{Diag}\\big(\\mathbf y\\big)\\bigg[\\begin{array}{c|c|c|c|c}\n",
    "\\mathbf \\Lambda^0 \\mathbf 1 & \\mathbf \\Lambda^1 \\mathbf 1 & \\mathbf \\Lambda^2 \\mathbf 1 &\\cdots & \\mathbf \\Lambda^{n-1} \\mathbf 1\n",
    "\\end{array}\\bigg] $\n",
    "\n",
    "From here we may notice that since the left hand side is full rank, the right hand side must be full rank as well. \n",
    "\n",
    "Actually, we know consideraably more than this -- i.e. we know that the left hand side is unitary. \n",
    "\n",
    "where we have $\\mathbf X = f(\\mathbf e_j) = \\bigg[\\begin{array}{c|c|c|c|c}\n",
    "\\mathbf P^0\\mathbf e_j & \\mathbf P^1\\mathbf e_j & \\mathbf P^2\\mathbf e_j &\\cdots & \\mathbf P^{n-1}\\mathbf e_j\n",
    "\\end{array}\\bigg]$\n",
    "\n",
    "In general $\\mathbf X = f(\\mathbf s)$, is called a **circulant matrix**.  For now we confine ourselves to the case where $\\mathbf s := \\mathbf e_j$, though we'll loosen up this restriction at the end of this writeup. \n",
    "\n",
    "earlier we noted that:\n",
    "\n",
    "$\\mathbf P^k \\mathbf e_j \\perp \\mathbf P^r \\mathbf e_j$\n",
    "\n",
    "\n",
    "and of course \n",
    "\n",
    "$\\big \\Vert \\mathbf P^m \\mathbf e_j\\big \\Vert_2^2 = \\big(\\mathbf P^m \\mathbf e_j\\big)^H\\big(\\mathbf P^m \\mathbf e_j\\big) = \\mathbf e_j^H \\big(\\mathbf P^m\\big)^H  \\mathbf P^m \\mathbf e_j = \\mathbf e_j^H \\mathbf I \\mathbf e_j = \\mathbf e_j^H \\mathbf e_j = 1$\n",
    "\n",
    "Thus each column in $\\mathbf X$ is mutually orthonormal -- and $\\mathbf U$ is $n$ x $n$ so it is a (real valued) unitary matrix. \n",
    "\n",
    "--\n",
    "\n",
    "From here we see\n",
    "\n",
    "$\\big(\\mathbf V^H \\mathbf X\\big)^H \\mathbf V^H \\mathbf X = \\mathbf X^H \\big(\\mathbf V \\mathbf V^H\\big) \\mathbf X = \\mathbf X^H \\mathbf X = \\mathbf I$\n",
    "\n",
    "So we know that the left hand side in unitary.  This means that the right handside must be unitary as well. \n",
    "\n",
    "Since the right hand side is unitary, that means it must be non-singular.  \n",
    "\n",
    "Note that with respect to determinants, we could say:\n",
    "\n",
    "$ \\Big \\vert Det\\Big(\\mathbf{Diag} \\big(\\mathbf y\\big)\\Big)\\Big \\vert*\\Big \\vert Det\\Big( \\bigg[\\begin{array}{c|c|c|c|c}\n",
    "\\mathbf \\Lambda^0 \\mathbf 1 & \\mathbf \\Lambda^1 \\mathbf 1 & \\mathbf \\Lambda^2 \\mathbf 1 &\\cdots & \\mathbf \\Lambda^{n-1} \\mathbf 1\n",
    "\\end{array}\\bigg]\\Big) \\Big \\vert = 1$\n",
    "\n",
    "Thus \n",
    "\n",
    "$Det\\Big( \\bigg[\\begin{array}{c|c|c|c|c}\n",
    "\\mathbf \\Lambda^0 \\mathbf 1 & \\mathbf \\Lambda^1 \\mathbf 1 & \\mathbf \\Lambda^2 \\mathbf 1 &\\cdots & \\mathbf \\Lambda^{n-1} \\mathbf 1\n",
    "\\end{array}\\bigg]\\Big) \\neq 0$\n",
    "\n",
    "Finally, we 'unpack' this matrix and see that\n",
    "\n",
    "$\\bigg[\\begin{array}{c|c|c|c|c}\n",
    "\\mathbf \\Lambda^0 \\mathbf 1 & \\mathbf \\Lambda^1 \\mathbf 1 & \\mathbf \\Lambda^2 \\mathbf 1 &\\cdots & \\mathbf \\Lambda^{n-1} \\mathbf 1\n",
    "\\end{array}\\bigg]=\\begin{bmatrix}\n",
    "1 & \\lambda_1 & \\lambda_1^2 & \\dots  & \\lambda_1^{n-1}\\\\ \n",
    "1 & \\lambda_2 & \\lambda_2^2 & \\dots &  \\lambda_2^{n-1} \\\\ \n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\\\ \n",
    "1 & \\lambda_{n} & \\lambda_{n}^{2} & \\dots  & \\lambda_{n}^{n-1}\n",
    "\\end{bmatrix}$\n",
    "\n",
    "This is the Vandermonde matrix, which is non-singular **iff**  each $\\lambda_i$ is unique.  Thus we conclude that each $\\lambda_i$ for our Permutation matrix of a connected graph must be unique.\n",
    "\n",
    "**claim:**\n",
    "\n",
    "$\\bigg[\\begin{array}{c|c|c|c|c}\n",
    "\\mathbf \\Lambda^0 \\mathbf 1 & \\mathbf \\Lambda^1 \\mathbf 1 & \\mathbf \\Lambda^2 \\mathbf 1 &\\cdots & \\mathbf \\Lambda^{n-1} \\mathbf 1\n",
    "\\end{array}\\bigg]^H \\bigg[\\begin{array}{c|c|c|c|c}\n",
    "\\mathbf \\Lambda^0 \\mathbf 1 & \\mathbf \\Lambda^1 \\mathbf 1 & \\mathbf \\Lambda^2 \\mathbf 1 &\\cdots & \\mathbf \\Lambda^{n-1} \\mathbf 1\n",
    "\\end{array}\\bigg] = n \\mathbf I$\n",
    "\n",
    "That is, the columns in the above matrix are mutually orthogonal (aka have an inner product of zero), and subject to some normalizing scalar constant, we know that the matrix is unitary. \n",
    "\n",
    "**proof:**  \n",
    "First notice that each column has a squared L2 norm of $n$\n",
    "\n",
    "for $m = \\{0, 1, 2, ..., n-1\\}$\n",
    "\n",
    "$\\big(\\mathbf \\Lambda^m \\mathbf 1\\big)^H \\mathbf \\Lambda^m \\mathbf 1 = \\mathbf 1^H \\big(\\mathbf \\Lambda^m\\big)^H \\mathbf \\Lambda^m \\mathbf 1 = \\mathbf 1^H \\big(\\mathbf \\Lambda^m\\big)^{-1} \\mathbf \\Lambda^m \\mathbf 1 = \\mathbf 1^H \\big(\\mathbf I\\big) \\mathbf 1 = trace \\big(\\mathbf I\\big)  = n$ \n",
    "\n",
    "note that when we say $\\mathbf 1^H \\big(\\mathbf I\\big) \\mathbf 1 = trace \\big(\\mathbf I\\big)$, we notice first that $\\mathbf 1^H \\big(\\mathbf Z\\big) \\mathbf 1$, means to sum up all entries in some operator $\\mathbf Z$, and if $\\mathbf Z$ is a diagonal matrix, then this is equivalent to just summing the entries along the diagonal of $\\mathbf Z$ which is equal to the trace of $\\mathbf Z$.\n",
    "\n",
    "Next we want to prove that the inner product of any column $j$ with some other column $\\neq j$, is zero.\n",
    "\n",
    "Thus we are interested in the cases of\n",
    "\n",
    "$\\big(\\mathbf \\Lambda^r \\mathbf 1\\big)^H \\mathbf \\Lambda^k \\mathbf 1$ \n",
    "\n",
    "for all natural numbers $k$, $r$, *first* where $0\\leq r \\lt k \\leq n-1$ and *second* where $0\\leq  k \\lt r \\leq n-1$.\n",
    "\n",
    "First we observe the $r \\lt k$ case:\n",
    "\n",
    "$\\big(\\mathbf \\Lambda^r \\mathbf 1\\big)^H \\mathbf \\Lambda^k \\mathbf 1 = \\mathbf 1^H \\big(\\mathbf \\Lambda^r\\big)^H \\mathbf \\Lambda^{k} \\mathbf 1 = \\mathbf 1^H \\Big(\\big( \\mathbf \\Lambda^{-r} \\big) \\mathbf \\Lambda^{k}\\Big) \\mathbf 1 = \\mathbf 1^H \\Big( \\mathbf \\Lambda^{k - r}\\Big) \\mathbf 1 = trace\\big(\\mathbf \\Lambda^{k - r}\\big) $ \n",
    "\n",
    "since $k \\gt r$, we know $0 \\lt k - r \\leq n-1$.  Put differently $(k - r) \\%n \\neq 0$\n",
    "\n",
    "Thus $\\big(\\mathbf \\Lambda^r \\mathbf 1\\big)^H \\mathbf \\Lambda^k \\mathbf 1 = trace\\big(\\mathbf \\Lambda^{k - r}\\big) = trace\\big(\\mathbf Q^H \\mathbf P^{k - r} \\mathbf Q\\big) = trace\\big(\\mathbf Q \\mathbf Q^H \\mathbf P^{k - r}\\big) = trace\\big(\\mathbf P^{k - r}\\big) = 0$\n",
    "\n",
    "note that inner products are symmetric -- except for complex conjugation-- so in the case of an inner product equal to zero, we have \n",
    "\n",
    "$\\Big(\\big(\\mathbf \\Lambda^r \\mathbf 1\\big)^H \\mathbf \\Lambda^k \\mathbf 1\\Big)^H = trace\\big(\\mathbf \\Lambda^{k - r}\\big)^H  = 0^H = 0$\n",
    "\n",
    "which covers the second case.\n",
    "\n",
    "\n",
    "Thus all columns in\n",
    "\n",
    "$\\bigg[\\begin{array}{c|c|c|c|c}\n",
    "\\mathbf \\Lambda^0 \\mathbf 1 & \\mathbf \\Lambda^1 \\mathbf 1 & \\mathbf \\Lambda^2 \\mathbf 1 &\\cdots & \\mathbf \\Lambda^{n-1} \\mathbf 1\n",
    "\\end{array}\\bigg]$\n",
    "\n",
    "have a squared length of $n$ and are mutually orthgonal.\n",
    "\n",
    "Hence we can say:\n",
    "\n",
    "$\\frac{1}{\\sqrt n} \\bigg[\\begin{array}{c|c|c|c|c}\n",
    "\\mathbf \\Lambda^0 \\mathbf 1 & \\mathbf \\Lambda^1 \\mathbf 1 & \\mathbf \\Lambda^2 \\mathbf 1 &\\cdots & \\mathbf \\Lambda^{n-1} \\mathbf 1\n",
    "\\end{array}\\bigg] = \\mathbf F$\n",
    "\n",
    "is a unitary matrix.  In fact this matrix $\\mathbf F$ is the discrete Fourier transform matrix.  \n",
    "\n",
    "*note: in some cases, we may use the the conjugate transpose of this matrix, or another variant, as the DFT.  This is ultimately just a book-keeping adjustment*\n",
    "\n",
    "\n",
    "\n",
    "# Claim: \n",
    "# The DFT Matrix is the collection of eigenvectors for a circulant matrix\n",
    "\n",
    "\n",
    "We say that this circulant matrix is given by $\\mathbf X$  \n",
    "\n",
    "\n",
    "When we look at \n",
    "\n",
    "$\\mathbf V^H \\bigg[\\begin{array}{c|c|c|c|c}\n",
    "\\mathbf P^0\\mathbf e_j & \\mathbf P^1\\mathbf e_j & \\mathbf P^2\\mathbf e_j &\\cdots & \\mathbf P^{n-1}\\mathbf e_j\n",
    "\\end{array}\\bigg] = \\mathbf{Diag}\\big(\\mathbf y\\big) \\bigg[\\begin{array}{c|c|c|c|c}\n",
    "\\mathbf \\Lambda^0 \\mathbf 1 & \\mathbf \\Lambda^1 \\mathbf 1 & \\mathbf \\Lambda^2 \\mathbf 1 &\\cdots & \\mathbf \\Lambda^{n-1} \\mathbf 1\n",
    "\\end{array}\\bigg]$\n",
    "\n",
    "we can re-write this as\n",
    "\n",
    "$\\mathbf V^H \\bigg[\\begin{array}{c|c|c|c|c}\n",
    "\\mathbf P^0\\mathbf e_j & \\mathbf P^1\\mathbf e_j & \\mathbf P^2\\mathbf e_j &\\cdots & \\mathbf P^{n-1}\\mathbf e_j\n",
    "\\end{array}\\bigg] = \\sqrt(n)\\mathbf{Diag}\\big(\\mathbf y\\big) \\frac{1}{\\sqrt(n)}\\bigg[\\begin{array}{c|c|c|c|c}\n",
    "\\mathbf \\Lambda^0 \\mathbf 1 & \\mathbf \\Lambda^1 \\mathbf 1 & \\mathbf \\Lambda^2 \\mathbf 1 &\\cdots & \\mathbf \\Lambda^{n-1} \\mathbf 1\n",
    "\\end{array}\\bigg] = \\sqrt(n) \\mathbf{Diag}\\big(\\mathbf y\\big) \\mathbf F$\n",
    "\n",
    "we can recongize that this is a form of the singular value decompostion on our matrix $\\mathbf X$ (so long as we relax the constraint that the diagonal matrix is real-valued, non-negative).  That is, we have \n",
    "\n",
    "$\\mathbf X = \\bigg[\\begin{array}{c|c|c|c|c}\n",
    "\\mathbf P^0\\mathbf e_j & \\mathbf P^1\\mathbf e_j & \\mathbf P^2\\mathbf e_j &\\cdots & \\mathbf P^{n-1}\\mathbf e_j\n",
    "\\end{array}\\bigg] = \\mathbf V \\Big(\\sqrt(n) \\mathbf{Diag} \\big(\\mathbf y\\big)\\Big)\\mathbf F$\n",
    "\n",
    "In the above case, $\\mathbf X$ is decomposed into unitary matrix $\\mathbf V$ times a diagonal matrix times unitary matrix $\\mathbf F$.  \n",
    "\n",
    "\n",
    "In this case, $\\mathbf X$ is itself unitary and per the note in \"Schurs_Inequality.ipynb\", that means that $\\mathbf X$ is normal.  Since $\\mathbf X$ is normal, this means that our Singular Value Decomposition in fact gives us an eigenvalue decomposition.  Put differently we can set our left singular and right singular vectors to be equal, and allocate everything else to the middle diagonal matrix.  \n",
    "\n",
    "$\\mathbf V := \\mathbf F^H $\n",
    "\n",
    "$\\mathbf X = \\mathbf F^H \\mathbf D_j \\mathbf F$\n",
    "\n",
    "or equivalently\n",
    "\n",
    "$\\mathbf F \\mathbf X \\mathbf F^H =  \\mathbf D_j $\n",
    "\n",
    "Thus in the above we can say $\\mathbf X$ is unitarily similar to a diagonal matrix $\\mathbf D_j$ with $\\mathbf F$ containing the eigenvectors.  \n",
    "\n",
    "Which is another way of saying that our unitary Vandermonde matrix $\\mathbf F$ **contains the mutually orthonormal collection of eigenvectors for** $\\mathbf X$.  \n",
    "\n",
    "This immediately motivates the question-- what if $\\mathbf X$ was a function of permuting some different, arbitrary vector $\\mathbf s$ i.e. if $\\mathbf X = f(\\mathbf s)$ -- could we still say $\\mathbf X$ is unitarily similar to a diagonal matrix with $\\mathbf F$ containing the eigenvectors?  The answer is yes, though it takes a little bit more work to show it.\n",
    "\n",
    "\n",
    "\n",
    "# Short form proof\n",
    "\n",
    "for a quick take, consider that \n",
    "\n",
    "$\\mathbf F \\big(f(\\mathbf e_j)\\big) \\mathbf F^H = \\mathbf D_j$\n",
    "\n",
    "where $\\mathbf D_j$ denotes some diagonal matrix similar to our function applied on the jth standard basis vector.\n",
    "\n",
    "The standard basis vectors form a basis so we can write $\\mathbf s$ in terms of them:  \n",
    "$\\mathbf s = \\gamma_1 \\mathbf e_1 + \\gamma_2 \\mathbf e_2 + ... + \\gamma_n \\mathbf e_n  = \\big(\\sum_{j=1}^{n} \\gamma_j \\mathbf e_j\\big)$\n",
    "\n",
    "Then, using linearity we can say \n",
    "\n",
    "$\\mathbf X = f(\\mathbf s) = f(\\gamma_1 \\mathbf e_1 + \\gamma_2 \\mathbf e_2 + ... + \\gamma_n \\mathbf e_n) = f(\\gamma_1 \\mathbf e_1) + f(\\gamma_2 \\mathbf e_2) + ... + (\\gamma_n \\mathbf e_n) = \\gamma_1f(\\mathbf e_1) + \\gamma_2 f(\\mathbf e_2) + ... + \\gamma_n (\\mathbf e_n)$\n",
    "\n",
    "reminding ourselves, again via linearity: \n",
    "\n",
    "$\\gamma_j \\mathbf F \\big(f(\\mathbf e_j)\\big) \\mathbf F^H = \\mathbf F \\big \\gamma_j (f(\\mathbf e_j)\\big) \\mathbf F^H = \\mathbf F \\big (f(\\gamma_j \\mathbf  e_j)\\big) \\mathbf F^H = \\gamma_j \\mathbf D_j$\n",
    "\n",
    "left multiply each side by $\\mathbf F$ and right multiply each side by $\\mathbf F^H$, and we get\n",
    "\n",
    "$\\mathbf F \\big(\\mathbf X\\big) \\mathbf F^H = \\mathbf F \\big(f(\\mathbf s)\\big) \\mathbf F^H= \\gamma_1 \\mathbf F \\big( f( \\mathbf e_1)\\big)\\mathbf F^H + \\gamma_2 \\mathbf F\\big(f( \\mathbf e_2)\\big)\\mathbf F^H + ... + \\gamma_n \\mathbf F\\big( f(\\mathbf e_n)\\big)\\mathbf F^H = \\gamma_1 \\mathbf D_1 + \\gamma_2 \\mathbf D_2 + ... + \\gamma_n \\mathbf D_n$\n",
    "\n",
    "The sum of a sequence of diagonal matrices is a diagonal matrix, hence we can can say that using $\\mathbf F$, we find that $\\mathbf X$ is unitarily similar to a diagonal matrix. \n",
    "\n",
    "\n",
    "# Begin Long Form Proof\n",
    "\n",
    "To begin, notice that by linearity\n",
    "\n",
    "$f(\\mathbf e_1) + f(\\mathbf e_2) = f(\\mathbf e_1 + \\mathbf e_2)$  \n",
    "\n",
    "Written in terms of a matrix, this is:  \n",
    "\n",
    "$\\bigg[\\begin{array}{c|c|c|c}\n",
    "\\mathbf P^0\\mathbf e_1 & \\mathbf P^1\\mathbf e_1 & \\cdots & \\mathbf P^{n-1}\\mathbf e_1\n",
    "\\end{array}\\bigg] + \\bigg[\\begin{array}{c|c|c|c}\n",
    "\\mathbf P^0\\mathbf e_2 & \\mathbf P^1\\mathbf e_2 & \\cdots & \\mathbf P^{n-1}\\mathbf e_2\n",
    "\\end{array}\\bigg] = \\bigg[\\begin{array}{c|c|c|c}\n",
    "\\mathbf P^0 \\mathbf e_1 +  \\mathbf P^0 \\mathbf e_2 & \\mathbf P^1 \\mathbf e_1 + \\mathbf P^1 \\mathbf e_2 & \\cdots & \\mathbf P^{n-1} \\mathbf e_1 + \\mathbf P^{n-1} \\mathbf e_2 \n",
    "\\end{array}\\bigg]$\n",
    "\n",
    "which we can restate as \n",
    "\n",
    "$\\bigg[\\begin{array}{c|c|c|c}\n",
    "\\mathbf P^0\\mathbf e_1 & \\mathbf P^1\\mathbf e_1 & \\cdots & \\mathbf P^{n-1}\\mathbf e_1\n",
    "\\end{array}\\bigg] + \\bigg[\\begin{array}{c|c|c|c}\n",
    "\\mathbf P^0\\mathbf e_2 & \\mathbf P^1\\mathbf e_2 & \\cdots & \\mathbf P^{n-1}\\mathbf e_2\n",
    "\\end{array}\\bigg] = \\bigg[\\begin{array}{c|c|c|c}\n",
    "\\mathbf P^0\\big(\\mathbf e_1 +  \\mathbf e_2\\big) & \\mathbf P^1 \\big(\\mathbf e_1 + \\mathbf e_2\\big) & \\cdots & \\mathbf P^{n-1}\\big(\\mathbf e_1 + \\mathbf e_2 \\big)\n",
    "\\end{array}\\bigg]$\n",
    "\n",
    "\n",
    "If we left multiply by $\\mathbf F$, what we get is\n",
    "\n",
    "\n",
    "$\\mathbf F \\bigg[\\begin{array}{c|c|c|c}\n",
    "\\mathbf P^0\\mathbf e_1 & \\mathbf P^1\\mathbf e_1 & \\cdots & \\mathbf P^{n-1}\\mathbf e_1\n",
    "\\end{array}\\bigg] + \\mathbf F \\bigg[\\begin{array}{c|c|c|c}\n",
    "\\mathbf P^0\\mathbf e_2 & \\mathbf P^1\\mathbf e_2 &\\cdots & \\mathbf P^{n-1}\\mathbf e_2\n",
    "\\end{array}\\bigg]= \\mathbf D_1\\mathbf F + \\mathbf D_2\\mathbf F$\n",
    "\n",
    "$= \\big(\\mathbf D_1 + \\mathbf D_2\\big) \\mathbf F = \\mathbf F \\bigg[\\begin{array}{c|c|c|c}\n",
    "\\mathbf P^0\\big(\\mathbf e_1 +  \\mathbf e_2\\big) & \\mathbf P^1 \\big(\\mathbf e_1 + \\mathbf e_2\\big) &\\cdots & \\mathbf P^{n-1}\\big(\\mathbf e_1 + \\mathbf e_2 \\big)\n",
    "\\end{array}\\bigg]$\n",
    "\n",
    "\n",
    "and to further generalize this, notice that if we added all of the standard basis vectors, we'd get the ones vector.    Where $\\mathbf D_j$ is a diagonal matrix similar to the permutation matrix given by $f(\\mathbf e_j)$.\n",
    "We can write this as:  \n",
    "\n",
    "$\\mathbf {11}^H = \\sum_{j=1}^{n}\\Big(\\bigg[\\begin{array}{c|c|c|c}\n",
    "\\mathbf P^0\\mathbf e_j & \\mathbf P^1\\mathbf e_j & \\cdots & \\mathbf P^{n-1}\\mathbf e_j\n",
    "\\end{array}\\bigg]\\Big) = \\bigg[\\begin{array}{c|c|c|c}\n",
    "\\mathbf P^0\\big(\\sum_{j=1}^{n} \\mathbf e_j \\big) & \\mathbf P^1 \\big(\\sum_{j=1}^{n} \\mathbf e_j \\big) & \\cdots & \\mathbf P^{n-1}\\big(\\sum_{j=1}^{n} \\mathbf e_j \\big)\n",
    "\\end{array}\\bigg]$  \n",
    "\n",
    "and if we left multiply by $\\mathbf F$, we get\n",
    "\n",
    "$\\mathbf F \\big(\\mathbf {11}^H \\big) = \\sum_{j=1}^{n}\\Big(\\mathbf F\\bigg[\\begin{array}{c|c|c|c}\n",
    "\\mathbf P^0\\mathbf e_j & \\mathbf P^1\\mathbf e_j & \\cdots & \\mathbf P^{n-1}\\mathbf e_j\n",
    "\\end{array}\\bigg]\\Big) = \\sum_{j=1}^{n} \\big(\\mathbf D_j \\mathbf F\\big) = \\big(\\sum_{j=1}^{n}\\mathbf D_j\\big) \\mathbf F$  \n",
    "\n",
    "From here, consider what would happen if we instead decided to scale each standard basis vector, $\\mathbf e_j$, by some arbitrary amount, $\\gamma_j$, giving us the following expression:  \n",
    "\n",
    "$\\sum_{j=1}^{n}\\Big(\\gamma_j \\bigg[\\begin{array}{c|c|c|c}\n",
    "\\mathbf P^0\\mathbf e_j & \\mathbf P^1\\mathbf e_j & \\cdots & \\mathbf P^{n-1} \\mathbf e_j\n",
    "\\end{array}\\bigg]\\Big) = \\sum_{j=1}^{n}\\Big(\\bigg[\\begin{array}{c|c|c|c}\n",
    "\\mathbf P^0\\big(\\gamma_j \\mathbf e_j\\big) & \\mathbf P^1\\big(\\gamma_j \\mathbf e_j\\big) & \\cdots & \\mathbf P^{n-1} \\big(\\gamma_j  \\mathbf e_j\\big)\n",
    "\\end{array}\\bigg]\\Big)$\n",
    "\n",
    "which can be restated as  \n",
    "\n",
    "$\\sum_{j=1}^{n}\\Big(\\gamma_j \\bigg[\\begin{array}{c|c|c|c}\n",
    "\\mathbf P^0\\mathbf e_j & \\mathbf P^1\\mathbf e_j & \\cdots & \\mathbf P^{n-1} \\mathbf e_j\n",
    "\\end{array}\\bigg]\\Big)  = \\bigg[\\begin{array}{c|c|c|c|c}\n",
    "\\mathbf P^0\\big(\\sum_{j=1}^{n} \\gamma_j \\mathbf e_j \\big) & \\mathbf P^1 \\big(\\sum_{j=1}^{n}\\gamma_j \\mathbf e_j \\big) & \\cdots & \\mathbf P^{n-1}\\big(\\sum_{j=1}^{n} \\gamma_j \\mathbf e_j \\big)\n",
    "\\end{array}\\bigg]$ \n",
    "\n",
    "again, left multiply this expression by $\\mathbf F$ and we see\n",
    "\n",
    "$\\mathbf F\\bigg[\\begin{array}{c|c|c|c|c}\n",
    "\\mathbf P^0\\big(\\sum_{j=1}^{n} \\gamma_j \\mathbf e_j \\big) & \\mathbf P^1 \\big(\\sum_{j=1}^{n}\\gamma_j \\mathbf e_j \\big) & \\cdots & \\mathbf P^{n-1}\\big(\\sum_{j=1}^{n} \\gamma_j \\mathbf e_j \\big)\n",
    "\\end{array}\\bigg] = \\sum_{j=1}^{n}\\Big(\\gamma_j \\mathbf F\\bigg[\\begin{array}{c|c|c|c}\n",
    "\\mathbf P^0\\mathbf e_j & \\mathbf P^1\\mathbf e_j & \\cdots & \\mathbf P^{n-1} \\mathbf e_j\n",
    "\\end{array}\\bigg]\\Big)$\n",
    "\n",
    "from here notice  \n",
    "\n",
    "$ \\sum_{j=1}^{n}\\Big(\\gamma_j \\mathbf F\\bigg[\\begin{array}{c|c|c|c}\n",
    "\\mathbf P^0\\mathbf e_j & \\mathbf P^1\\mathbf e_j & \\cdots & \\mathbf P^{n-1} \\mathbf e_j\n",
    "\\end{array}\\bigg]\\Big) = \\sum_{j=1}^{n}\\gamma_j\\Big( \\mathbf F\\bigg[\\begin{array}{c|c|c|c}\n",
    "\\mathbf P^0\\mathbf e_j & \\mathbf P^1\\mathbf e_j & \\cdots & \\mathbf P^{n-1} \\mathbf e_j\n",
    "\\end{array}\\bigg]\\Big) = \\sum_{j=1}^{n} \\mathbf \\gamma_j \\big(\\mathbf D_j \\mathbf F\\big) = \\big(\\sum_{j=1}^{n}\\mathbf \\gamma_j \\mathbf D_j\\big) \\mathbf F$\n",
    "\n",
    "Thus we say\n",
    "\n",
    "$\\mathbf F\\bigg[\\begin{array}{c|c|c|c|c}\n",
    "\\mathbf P^0\\big(\\sum_{j=1}^{n} \\gamma_j \\mathbf e_j \\big) & \\mathbf P^1 \\big(\\sum_{j=1}^{n}\\gamma_j \\mathbf e_j \\big) & \\cdots & \\mathbf P^{n-1}\\big(\\sum_{j=1}^{n} \\gamma_j \\mathbf e_j \\big)\n",
    "\\end{array}\\bigg] = \\big(\\sum_{j=1}^{n}\\mathbf \\gamma_j \\mathbf D_j\\big) \\mathbf F$\n",
    "\n",
    "\n",
    "Right multiply each side by $\\mathbf F^H$:  \n",
    "\n",
    "$\\mathbf F\\bigg[\\begin{array}{c|c|c|c|c}\n",
    "\\mathbf P^0\\big(\\sum_{j=1}^{n} \\gamma_j \\mathbf e_j \\big) & \\mathbf P^1 \\big(\\sum_{j=1}^{n}\\gamma_j \\mathbf e_j \\big) & \\cdots & \\mathbf P^{n-1}\\big(\\sum_{j=1}^{n} \\gamma_j \\mathbf e_j \\big)\n",
    "\\end{array}\\bigg] \\mathbf F^H = \\big(\\sum_{j=1}^{n}\\mathbf \\gamma_j \\mathbf D_j\\big) \\mathbf F \\mathbf F^H  = \\big(\\sum_{j=1}^{n}\\mathbf \\gamma_j \\mathbf D_j\\big) $\n",
    "\n",
    "Since the sum of a finite sequence of $n$ x $n$ diagonal matrices is itself a diagonal matrix, this tells us that our matrix is unitarily similar to a diagonal matrix, and the mutually orthonormal eigenvectors are contained in $\\mathbf F$ (or technically, the right eigenvectors are contained as columns in $\\mathbf F^H$ -- which again, is just a small bookkeeping adjustment).  \n",
    "\n",
    "Now consider the general case where $\\mathbf X = f(\\mathbf s)$.  This looks quite formidable -- the circulant matrix is given by: \n",
    "\n",
    "$\\mathbf X = \\bigg[\\begin{array}{c|c|c|c}\n",
    "\\mathbf P^0\\mathbf s & \\mathbf P^1\\mathbf s & \\cdots & \\mathbf P^{n-1}\\mathbf s\n",
    "\\end{array}\\bigg]  = \\begin{bmatrix}\n",
    "s_0 & s_{n-1} & s_{n-2} & \\dots & s_2 & s_1 \\\\ \n",
    "s_1 & s_0 & s_{n-1} & \\dots & s_3 & s_2 \\\\ \n",
    "s_2 & s_1 & s_0 & \\dots & s_4 & s_3 \\\\\n",
    "\\vdots & \\vdots  & \\vdots &\\ddots & \\vdots & \\vdots\\\\ \n",
    "s_{n-2} & s_{n-3} & s_{n-4} & \\dots & s_0  & s_{n-1} \\\\ \n",
    "s_{n-1} & s_{n-2}  & s_{n-3} & \\dots & s_1 &  s_0\n",
    "\\end{bmatrix}$\n",
    "\n",
    "But, we simply need to recall that the standard basis vectors in fact form a basis, so we can uniquely write $\\mathbf s$ in terms of them. \n",
    "\n",
    "$\\mathbf s = \\gamma_1 \\mathbf e_1 + \\gamma_2 \\mathbf e_2 + ... + \\gamma_n \\mathbf e_n  = \\big(\\sum_{j=1}^{n} \\gamma_j \\mathbf e_j\\big)$\n",
    "\n",
    "Thus we have \n",
    "\n",
    "\n",
    "$\\mathbf X = \\bigg[\\begin{array}{c|c|c|c}\n",
    "\\mathbf P^0\\mathbf s & \\mathbf P^1\\mathbf s & \\cdots & \\mathbf P^{n-1}\\mathbf s\n",
    "\\end{array}\\bigg] = \\bigg[\\begin{array}{c|c|c|c|c}\n",
    "\\mathbf P^0\\big(\\sum_{j=1}^{n} \\gamma_j \\mathbf e_j \\big) & \\mathbf P^1 \\big(\\sum_{j=1}^{n}\\gamma_j \\mathbf e_j \\big) & \\cdots & \\mathbf P^{n-1}\\big(\\sum_{j=1}^{n} \\gamma_j \\mathbf e_j \\big)\n",
    "\\end{array}\\bigg]$\n",
    "\n",
    "left multiply each side by $\\mathbf F$ and right multiply each side by $\\mathbf F^H$, and we get \n",
    "\n",
    "$\\mathbf F \\big(\\mathbf X\\big) \\mathbf F^H = \\mathbf F \\bigg[\\begin{array}{c|c|c|c|c}\n",
    "\\mathbf P^0\\big(\\sum_{j=1}^{n} \\gamma_j \\mathbf e_j \\big) & \\mathbf P^1 \\big(\\sum_{j=1}^{n}\\gamma_j \\mathbf e_j \\big) & \\cdots & \\mathbf P^{n-1}\\big(\\sum_{j=1}^{n} \\gamma_j \\mathbf e_j \\big)\n",
    "\\end{array}\\bigg] \\mathbf F^H = \\big(\\sum_{j=1}^{n}\\mathbf \\gamma_j \\mathbf D_j\\big)$\n",
    "\n",
    "\n",
    "which states that $\\mathbf X = f(\\mathbf s)$ is unitarily similar to a diagonal matrix, with vectors in $\\mathbf F$ forming the basis of mutually orthonormal eigenvectors.  This completes the proof that a circulant matrix $\\mathbf X$ is unitarily diagonalizable via the \"help\" of $\\mathbf F$. \n",
    "\n",
    "\n",
    "# End Long Form Proof\n",
    "\n",
    "- - - - - \n",
    "# But what the heck do the components of the DFT look like? \n",
    "\n",
    "\n",
    "When we consider that (a) each $\\lambda_i$, contained in position $\\mathbf \\Lambda_{i,i}$, is distinct and also that (b) each $\\lambda_i^n - 1 = 0$\n",
    "\n",
    "as a reminder: this is because (a) the associated Vandermonde matrix is non-singular, and (b) $\\mathbf \\Lambda^n = \\mathbf Q^H \\mathbf P^n \\mathbf Q = \\mathbf Q^H \\mathbf I \\mathbf Q = \\mathbf I $, hence each diagonal element raised to the nth power equals one.  \n",
    "\n",
    "We know that $\\lambda_1 = 1$, because $\\mathbf {P1} = \\mathbf 1$.  From here we can say, $\\lambda_1$ has polor coordinate (1, $2\\pi \\frac{(1 - 1) }{n}$) which is to say it has magnitude 1, and an angle of $0 \\pi$ i.e. it is all real valued = 1.  \n",
    "\n",
    "$\\lambda_2$ has polar coordinate of (1 , $2\\pi\\frac{(2-1)}{n} $)  \n",
    "$\\lambda_3$ has polar coordinate of (1,  $2\\pi\\frac{(3-1)}{n} $)  \n",
    "$\\vdots$  \n",
    "$\\lambda_{n-1}$ has polar coordinate of (1, $2\\pi\\frac{(n-1 -1)}{n} $)  \n",
    "$\\lambda_n$ has polar coordinate of (1, $2\\pi\\frac{(n-1)}{n}$).  \n",
    "\n",
    "There is a variant of the Pidgeon Hole principle here: we have have $n$ $\\lambda_j$'s, each of which must be unique, and there are only $n$ unique nth roots of unity$^{(1)}$ -- hence each nth root has one and only one $\\lambda_j$ \"in\" it.  (This Wolfram alpha link is worth visiting, for its nice graphic: http://mathworld.wolfram.com/RootofUnity.html )\n",
    "\n",
    "\n",
    "\n",
    "Thus **the Vandermonde matrix in the following form is unitary**:  (due to GitHub $\\LaTeX$ rendering issues, the below formula has been inserted as an image)\n",
    "\n",
    "\n",
    "![F_components](images/unitary_vandermondF_components.gif)\n",
    "\n",
    "when each $\\lambda_j$ has polar coordinate of (1, $2\\pi\\frac{(j-1)}{n} $)\n",
    "- - - - -\n",
    "\n",
    "$^{(1)}$ **Side note: How do we know there are exactly n roots of unity?** \n",
    "\n",
    "The reader may wonder how we know that there are \"only $n$ unique nth roots of unity\" available for us to choose from, for any natural number $n$.  One way to support this claim comes from using the fundamental theorem of algebra, which is rather high powered machinery that is not introduced or proved anywhere in this posting.  \n",
    "\n",
    "The other approach is self contained and comes from using Vandermonde matrices.  Consider a degree $n$ polynomial (specifically the polynomial we are interested in is $\\lambda_i^n - 1$, but any degree $n$ polynomial --that isn't the zero polynomial-- is valid here). Such a polynomial would have the following Vandermonde matrix associated with it:\n",
    "\n",
    "\n",
    "$\\mathbf S = \\begin{bmatrix}\n",
    "1 & s_1 & s_1^2 & \\dots  & s_1^{n-1} &s_1^{n} \\\\ \n",
    "1 & s_2 & s_2^2 & \\dots &  s_2^{n-1} & s_2^{n} \\\\ \n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\ \n",
    "1 & s_{n} & s_{n}^{2} & \\dots  & s_{n}^{n-1} & s_{n}^{n} \\\\\n",
    "1 & s_{n+1} & s_{n+1}^{2} & \\dots  & s_{n+1}^{n-1} & s_{n+1}^{n}\n",
    "\\end{bmatrix}$\n",
    "\n",
    "That is, we evaluate our polynomial at $s_i$ where $i = \\{1, 2, ... , n, n+1\\}$.  The polynomial has coefficients associated with it, which are given in $\\mathbf t$.  When we evaluate the polynomial at each $s_i$ we get the resulting value at each $b_i$.  Setting this up as an equation:   \n",
    "\n",
    "$\\begin{bmatrix}\n",
    "1 & s_1 & s_1^2 & \\dots  & s_1^{n-1} &s_1^{n} \\\\ \n",
    "1 & s_2 & s_2^2 & \\dots &  s_2^{n-1} & s_2^{n} \\\\ \n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\ \n",
    "1 & s_{n} & s_{n}^{2} & \\dots  & s_{n}^{n-1} & s_{n}^{n} \\\\\n",
    "1 & s_{n+1} & s_{n+1}^{2} & \\dots  & s_{n+1}^{n-1} & s_{n+1}^{n}\n",
    "\\end{bmatrix}\\begin{bmatrix}\n",
    "t_0\\\\ \n",
    "t_1\\\\ \n",
    "\\vdots\\\\ \n",
    "t_{n-1}\\\\ \n",
    "t_{n}\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "b_1\\\\ \n",
    "b_2\\\\ \n",
    "\\vdots\\\\ \n",
    "b_{n}\\\\ \n",
    "b_{n+1}\n",
    "\\end{bmatrix}$\n",
    "\n",
    "or more succinctly, \n",
    "\n",
    "$\\mathbf{St} = \\mathbf b$ \n",
    "\n",
    "**For a contradiction:** assume that each of the $n+1$ data points is a distinct root of the polynomial.  Then every resulting value in $\\mathbf b$ is zero, which reduces this to:\n",
    "\n",
    "$\\mathbf{St} = \\mathbf 0$.  However since each $s_i$ is distinct, the Vandermonde matrix is invertible, which gives us \n",
    "\n",
    "$\\mathbf S^{-1} \\mathbf{St} = \\mathbf t = \\mathbf S^{-1} \\mathbf 0 = \\mathbf 0$   \n",
    "\n",
    "thus \n",
    "\n",
    "$\\mathbf t = \\mathbf 0$   \n",
    "\n",
    "Since every coefficient is zero, we in fact have the zero polynomial -- which is a contradiction.  However, if at least one of the $s_j$ is not a root (i.e. $\\mathbf b \\neq \\mathbf 0$), then $\\mathbf t \\neq \\mathbf 0$ and hence we may still have a degree $n$ polynomial.  This gives us an upper bound which tells us that a degree $n$ polynomial can have at most $n$ distinct roots.   \n",
    "\n",
    "Now, for our DFT matrix $\\mathbf F$, we are using eigenvalues from the connected graph permutation matrix and they have a constraint given by $\\lambda^n - 1 = 0$.  Put differently we are looking for roots of a degree $n$ polynomial, where the polynomial is $\\lambda^n - 1$.  These roots are called roots of unity.  Per the above, we upper bound the number of unique roots as being $\\leq n$.  Now our matrix $\\mathbf F$ is a unitary Vandermonde Matrix, which means it is non-singular, thus we determine that each $\\lambda_k$ for $k = \\{1, 2, ..., n-1, n \\}$ must be distinct. This means there must be $\\geq n$ distinct roots of unity.  Since our upper bound and lower bound are equal, we have a sandwich and conclude that there are **exactly** $n$ unique roots of unity.  \n",
    "\n",
    "** For an simple example of using a circulant matrix for convolutions of discrete probability distributions, see \n",
    "**\n",
    "https://github.com/DerekB7/LinearAlgebra/blob/master/circulant_convolution_example.ipynb  \n",
    "\n",
    "The code was originally part of this file, but it caused problems in formatting / rendering this file in Github, and hence is treated separately\n",
    "\n",
    "# Full cycle trace relations and nilpotent matrices\n",
    "\n",
    "**claim:**  \n",
    "for $\\mathbf B \\in \\mathbb C^{n x n}$ , \n",
    "\n",
    "if $trace\\big(\\mathbf B^r\\big) = 0$, for $r = \\{1, 2, ... ,n-1, n \\}$ every eigenvalue, $\\lambda_i$, of $\\mathbf B$ is equal to zero, i.e.  $\\lambda_i = 0$ for $i = \\{1, 2, ... ,n-1, n \\}$ .  \n",
    "\n",
    "\n",
    "**comment:**  \n",
    "\n",
    "This is a problem in Zhang's \"Linear Algebra: Challenging Problems for Students\".  Your author worked through several other interesting trace relations given in that book, and they are located about halfway through  \"Fun_with_Trace_and_Quadratic_Forms_and_CauchySchwarz.ipynb\".  \n",
    "\n",
    "This problem fit extremely nicely in with Vandermonde matrices, and nudged your author to think more about nilpotence, and also how after enough traces, we recover the 'signature' of a matrix.  \n",
    "\n",
    "remark: since the trace gives the sum of the eigenvalues and any complex matrix is similar to an upper triangular matrix, it is clearly true that if all eigenvalues are zero, then the trace will be zero for $\\mathbf B^r$ for any natural number $r$ -- including the case where $1 \\leq r \\leq n$ . What is not immediately clear is that this is an **iff**. \n",
    "\n",
    "In the derivation of the DFT, we used $trace\\big(\\mathbf P^r\\big) = 0$, for $r = \\{1, 2, ... ,n-1 \\}$, yet our matrix $\\mathbf P$ had all eigenvalues with magnitude $= 1$.  Extending the range of $r$ to also includes $n$ radically changes things and makes all eigenvalues have magnitude $= 0$.  \n",
    "\n",
    "\n",
    "Note that the posting called \"Fun_with_Trace_and_Quadratic_Forms_and_CauchySchwarz.ipynb\" proved that a nilpotent $n$ x $n$ matrix (in $\\mathbb C$) must have all eigenvalues equal to zero.  \n",
    "\n",
    "**proof:**  \n",
    "start by constructing the Vandermonde matrix:\n",
    "\n",
    "\n",
    "$\\mathbf W = \\begin{bmatrix}\n",
    "1 & \\lambda_1 & \\lambda_1^2 & \\dots  & \\lambda_1^{n-1}\\\\ \n",
    "1 & \\lambda_2 & \\lambda_2^2 & \\dots &  \\lambda_2^{n-1} \\\\ \n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\\\ \n",
    "1 & \\lambda_{n} & \\lambda_{n}^{2} & \\dots  & \\lambda_{n}^{n-1}\n",
    "\\end{bmatrix}$\n",
    "\n",
    "\n",
    "We want to reflect our constraint as\n",
    "\n",
    "$\\mathbf 1^H \\mathbf {\\Lambda W} = \\mathbf 0^H$\n",
    "\n",
    "i.e. as\n",
    "\n",
    "$\\mathbf 1^H \\begin{bmatrix}\n",
    "\\lambda_1 & \\lambda_1^2 & \\dots  & \\lambda_1^{n-1} & \\lambda_1^{n}\\\\ \n",
    "\\lambda_2 & \\lambda_2^2 & \\dots &  \\lambda_2^{n-1}& \\lambda_2^{n} \\\\ \n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\\\ \n",
    "\\lambda_{n} & \\lambda_{n}^{2} & \\dots  & \\lambda_{n}^{n-1}& \\lambda_n^{n}\n",
    "\\end{bmatrix} = \\mathbf 0^H$\n",
    "\n",
    "\n",
    "But we aren't sure if there are any eigenvalues equal to zero in $\\mathbf \\Lambda$ so we need to remove them first.  Why? First: if any eigenvalues are zero, the left side of the equation is not invertible.  Second, we are interested in the trace relations and we know that any eigenvalues of zero have no impact on the trace calculations, hence they may safely be removed.  \n",
    "\n",
    "*The contradiction kicks in at this stage*\n",
    "\n",
    "We remove all eigenvalues equal to zero and have an $m$ x $m$ matrix for some natural number $m$, where $ m \\leq n$.  Assume $m \\geq 2$, i.e. that some non-zero eigenvalues exist that satisfy our stated trace constraint.  \n",
    "\n",
    "- - - \n",
    "(*Two bookkeeping notes that may be skipped:* First: after removing all zero eigenvalues, $m \\neq 1$ -- because if $m=1$, then $trace\\big(\\mathbf B\\big) = \\lambda_1 = 0$ and the sole remaining eigenvalue $\\lambda_1 \\neq 0$ hence $m$ cannot be equal to one.  Second: we make the adjustment so that $\\mathbf 1$ and $\\mathbf 0$ are $m$ x $1$ column vectors.)  \n",
    "\n",
    "Our equation becomes: \n",
    "\n",
    "$\\mathbf 1^H \\begin{bmatrix}\n",
    "\\lambda_1 & \\lambda_1^2 & \\dots  & \\lambda_1^{m-1} & \\lambda_1^{m}\\\\ \n",
    "\\lambda_2 & \\lambda_2^2 & \\dots &  \\lambda_2^{m-1}& \\lambda_2^{m} \\\\ \n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\\\ \n",
    "\\lambda_{m} & \\lambda_{m}^{2} & \\dots  & \\lambda_{m}^{m-1}& \\lambda_m^{m}\n",
    "\\end{bmatrix} = \\mathbf 0^H$\n",
    "\n",
    "we re-write the above as \n",
    "\n",
    "$\\mathbf y^H \\begin{bmatrix}\n",
    "\\lambda_1 & \\lambda_1^2 & \\dots  & \\lambda_1^{m-1} & \\lambda_1^{m}\\\\ \n",
    "\\lambda_2 & \\lambda_2^2 & \\dots &  \\lambda_2^{m-1}& \\lambda_2^{m} \\\\ \n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\\\ \n",
    "\\lambda_{m} & \\lambda_{m}^{2} & \\dots  & \\lambda_{m}^{m-1}& \\lambda_m^{m}\n",
    "\\end{bmatrix} = \\mathbf 0^H$\n",
    "\n",
    "where $\\mathbf y = \\mathbf 1$.  It is important to note the identity: $\\mathbf y^H \\mathbf 1 = m$.\n",
    "\n",
    "Now we further prune our adjusted Vandermonde matrix to only include unique eigenvalues.  Thus we keep the $k$ unique eigenvalues where $2 \\leq k \\leq m$, and we adjust $\\mathbf y$ so that the trace math is identical. (Again note that $k \\neq 1$, because if so then there is only one unique eigenvalue $\\lambda_1$ and thus $\\frac{1}{m} trace \\big(\\mathbf B\\big) = \\lambda_1 = 0$, but we know that $\\lambda_1 \\neq 0$.)  \n",
    "\n",
    "For example, if all eigenvalues were unique except $\\lambda_m = \\lambda_{m-1}$ we'd remove the mth row and mth column from our adjusted Vandermonde matrix, and now $\\mathbf y$ would be an $m-1$ x $1$ column vector (as would the zero vector), where we have \n",
    "\n",
    "$\\mathbf y = \\begin{bmatrix}\n",
    "1\\\\ \n",
    "1\\\\ \n",
    "\\vdots\\\\ \n",
    "1 \\\\\n",
    "2\n",
    "\\end{bmatrix}$\n",
    "\n",
    "**Put differently, at this stage $y_i$ has algebraic multiplicity for each unique non-zero eigenvalue $\\lambda_i$.**\n",
    "\n",
    "The underlying math with respect to traces is the same, and we still have the key identity $\\mathbf y^H \\mathbf 1 = m$.\n",
    "\n",
    "our equation is thus:   \n",
    "\n",
    "$\\mathbf y^H \\begin{bmatrix}\n",
    "\\lambda_1 & \\lambda_1^2 & \\dots  & \\lambda_1^{k-1} & \\lambda_1^{k}\\\\ \n",
    "\\lambda_2 & \\lambda_2^2 & \\dots &  \\lambda_2^{k-1}& \\lambda_2^{k} \\\\ \n",
    "\\vdots & \\vdots & \\ddots & \\vdots & \\vdots & \\\\ \n",
    "\\lambda_{k} & \\lambda_{k}^{2} & \\dots  & \\lambda_{k}^{k-1}& \\lambda_k^{k}\n",
    "\\end{bmatrix} = \\mathbf 0^H$\n",
    "\n",
    "This matrix has each $\\lambda_i \\neq 0$ and each $\\lambda_i$ is unique. We can factor out a diagonal matrix $\\mathbf D$ if we'd like.  Thus we have \n",
    "\n",
    "$\\mathbf y^H \\mathbf D \\begin{bmatrix}\n",
    "1 & \\lambda_1 & \\lambda_1^2 & \\dots  & \\lambda_1^{k-1} \\\\ \n",
    "1 & \\lambda_2 & \\lambda_2^2 & \\dots &  \\lambda_2^{k-1} \\\\ \n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\\\ \n",
    "1& \\lambda_{k} & \\lambda_{k}^{2} & \\dots  & \\lambda_{k}^{k-1}\n",
    "\\end{bmatrix} = \\mathbf 0^H$\n",
    "\n",
    "letting $\\mathbf K$ be our adjusted Vandermonde matrix in this equation, i.e. $\\mathbf K = \\bigg[\\begin{array}{c|c|c|c|c}\n",
    "\\mathbf D^0 \\mathbf 1 & \\mathbf D^1 \\mathbf 1 & \\mathbf D^2 \\mathbf 1 &\\cdots & \\mathbf D^{k-1} \\mathbf 1\n",
    "\\end{array}\\bigg]$\n",
    "\n",
    "we have \n",
    "\n",
    "$\\mathbf y^H \\mathbf D \\mathbf K = \\mathbf 0^H$\n",
    "\n",
    "Because all $\\lambda_i$'s are unique, $\\mathbf K$ is non-singular and so must be $\\mathbf D$ (it is diagonal with no zero eigenvalues).  We right multiply both sides of our equation by the inverse of $\\mathbf K$ and this gives us \n",
    "\n",
    "\n",
    "$\\mathbf y^H \\mathbf D = \\mathbf y^H \\mathbf D \\mathbf {KK}^{-1} = \\mathbf 0^H \\mathbf K^{-1} = \\mathbf 0^H$ \n",
    "\n",
    "now right multiply both sides by $\\mathbf D^{-1}$, and we have \n",
    "\n",
    "$\\mathbf y^H = \\mathbf y^H \\mathbf D \\mathbf D^{-1} =  \\mathbf 0^H \\mathbf D^{-1} = \\mathbf 0^H$ \n",
    "\n",
    "This tells us that $\\mathbf y^H = \\mathbf 0^H$.  Yet this is a contradiction, because \n",
    "\n",
    "$\\mathbf y^H \\mathbf 1 = m \\neq \\mathbf 0^H \\mathbf 1 = 0$\n",
    "\n",
    "\n",
    "hence we know that $m \\ngeq 2$, and as mentioned earlier $m \\neq 1$.  Thus $m = 0$.  Put differently, $\\mathbf K$ does not exist (i.e. it must be a $0$ x $0$ matrix).  This proves the claim that all eigenvalues of $\\mathbf B$ must be equal to zero if \n",
    "\n",
    "$trace\\big(\\mathbf B^r\\big) = 0$, for $r = \\{1, 2, ... ,n-1, n \\}$ \n",
    "\n",
    "- - - -\n",
    "- - - -\n",
    "- - - -\n",
    "**alternative approach:** if the reader feels the above contradiction to be unsatifying, consider instead the following setup:\n",
    "\n",
    "\n",
    "$\\mathbf y^H \\mathbf D \\begin{bmatrix}\n",
    "1 & \\lambda_1 & \\lambda_1^2 & \\dots  & \\lambda_1^{k-1} & \\lambda_1^{k} \\\\ \n",
    "1 & \\lambda_2 & \\lambda_2^2 & \\dots &  \\lambda_2^{k-1} & \\lambda_2^{k} \\\\ \n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\\\ \n",
    "1& \\lambda_{k} & \\lambda_{k}^{2} & \\dots  & \\lambda_{k}^{k-1} & \\lambda_k^{k} \\\\\n",
    "1& 0 & 0 & \\dots  & 0 & 0\n",
    "\\end{bmatrix} = \\mathbf 0^H$\n",
    "\n",
    "Here we have $k + 1$ rows in our Vandermonde matrix -- where the eigenvalue of zero is contained in the final row. $\\mathbf D$ now is a $k+1 $ x $k+1$ diagonal matrix that has a zero in its bottom right corner, and $\\mathbf y$ has the algebraic multiplicity for each of the $k+1$ unique eigenvalues (inclusive of the eigenvalue equal to zero, which is given by $y_{k+1}$).  We know that $\\mathbf B$ has $n$ eigenvalues thus $\\mathbf y^H \\mathbf 1 = n$.\n",
    "\n",
    "Our Vandermonde Matrix is invertible so we multiply both sides on the right by its inverse, giving us \n",
    "\n",
    "$\\mathbf y^H \\mathbf D = \\mathbf 0^H$\n",
    "\n",
    "or equivalently\n",
    "\n",
    "$\\mathbf D^H \\mathbf y = \\mathbf 0$\n",
    "\n",
    "now we notice that $\\mathbf D$ is singular, with all non-zero entries along its diagonal except the entry in the bottom right corner.  However the above equation tells us that \n",
    "\n",
    "for $i = \\{1, 2, ..., k\\}$\n",
    "\n",
    "$y_i \\bar{\\lambda_i} = 0$\n",
    "\n",
    "\n",
    "we observe that this is also true in the $k+1$ case:  $y_{k+1} \\bar{\\lambda_{k+1}} = 0$\n",
    "\n",
    "First we deal with $i = \\{1, 2, ..., k\\}$ noticing that $\\bar{\\lambda_i} \\neq 0$\n",
    "\n",
    "$y_i \\bar{\\lambda_i} = 0$\n",
    "\n",
    "divide both sides by $\\bar{\\lambda_i}$ and see that \n",
    "\n",
    "$y_i = 0$\n",
    "\n",
    "Finally for $y_{k+1}$, we have  \n",
    "\n",
    "$y_{k+1} \\bar{\\lambda_{k+1}} = y_{k+1} 0 = 0$\n",
    "\n",
    "but we also have the constraint $n = \\mathbf y^H \\mathbf 1 = \\mathbf 1^H \\mathbf y = \\big(y_0 + y_1 + ... y_{k-1} + y_k \\big) + y_{k+1} = \\big(0 \\big) + y_{k+1} $ \n",
    "\n",
    "hence we see that $y_{k+1} = n$.  Put differently, all of the eigenvalues for $\\mathbf B$ are zero -- i.e. $\\mathbf B$ is nilpotent.\n",
    "\n",
    "*Book-keeping note: in either case, we've determined that* $\\mathbf B$* is nilpotent (i.e. all of its eigenvalues are zero), but only considered traces of powers up to k.  From here we simply mention that for k + 1, k +2, ... , n, we know that* $trace\\big(\\mathbf B^k\\big)= \\sum_{i=1}^n \\lambda_i^k = \\sum_{i=1}^n 0^k = 0$\n",
    "\n",
    "# Cayley Hamilton \n",
    "\n",
    "**claim**: each operator, $\\mathbf B \\in \\mathbb C^{n x n}$ obeys its characteristic polynomial.  i.e. \n",
    "\n",
    "$c_0 \\mathbf I + c_1 \\mathbf B + c_2 \\mathbf B^2 + ... + c_{n-1}\\mathbf B^{n-1} + c_{n}\\mathbf B^n = c_0 \\mathbf I +  \\sum_{r=1}^{n} c_r \\mathbf B^r = \\bigg[\\begin{array}{c|c|c|c}\n",
    "  \\mathbf 0 & \\mathbf 0 &\\cdots & \\mathbf 0\n",
    "\\end{array}\\bigg]$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**proof: non-defective case, where $\\mathbf B$ has $n$ linearly independent eigenvectors.**\n",
    "\n",
    "We know that each eigenvalue is a root to the characteristic polynomial.  \n",
    "\n",
    "Put differently, we know that for $k = \\{1, 2, ..., n-1, n\\}$, we have an eigenpair of $\\mathbf x_k, \\lambda_k$\n",
    "\n",
    "$c_0 +  \\sum_{r=1}^{n} c_r \\lambda_k^r = 0$\n",
    "\n",
    "\n",
    "we can multiply this by $\\mathbf x_k$:\n",
    "\n",
    "$\\big(c_0 +  \\sum_{r=1}^{n} c_r \\lambda_k^r \\big) \\mathbf x_k = \\mathbf 0$\n",
    "\n",
    "\n",
    "or equivalently\n",
    "\n",
    "$\\big(c_0 \\mathbf I  + \\sum_{r=1}^{n} c_r \\mathbf B^r \\big) \\mathbf x_k = \\mathbf 0$\n",
    "\n",
    "\n",
    "\n",
    "We collect these $n$ relationships in a system of equations.  Let:\n",
    "\n",
    "$\\mathbf X = \\bigg[\\begin{array}{c|c|c|c}\n",
    "\\mathbf x_1 & \\mathbf x_2 &\\cdots & \\mathbf x_n\n",
    "\\end{array}\\bigg]$  \n",
    "\n",
    "\n",
    "$\\big(c_0 \\mathbf I + \\sum_{r=1}^{n} c_r \\mathbf B^r \\big)\\mathbf X = \\bigg[\\begin{array}{c|c|c|c}\n",
    "  \\mathbf 0 & \\mathbf 0 &\\cdots & \\mathbf 0\n",
    "\\end{array}\\bigg]$\n",
    "\n",
    "because the eigenvectors are stated to be linearly independent, we multiply each side on the right by $\\mathbf X^{-1}$ seeing that\n",
    "\n",
    "\n",
    "$\\big(c_0 \\mathbf I + \\sum_{r=1}^{n} c_r \\mathbf B^r \\big) = \\bigg[\\begin{array}{c|c|c|c}\n",
    "  \\mathbf 0 & \\mathbf 0 &\\cdots & \\mathbf 0\n",
    "\\end{array}\\bigg]$\n",
    "\n",
    "which proves that $\\mathbf B$ follows its characteristic polynomial at least in the case where its eigenvectors form a basis.\n",
    "\n",
    "- - - -\n",
    "*begin alternative proof: non-defective case* \n",
    "\n",
    "an alternative aproach, which foreshadows the algebraic approach to the defective case, is to factor the characteristic polynomial applied to $\\mathbf B$\n",
    "\n",
    "\n",
    "$c_0 \\mathbf I + c_1 \\mathbf B + c_2 \\mathbf B^2 + ... + c_{n-1}\\mathbf B^{n-1} + c_{n}\\mathbf B^n = c_0 \\mathbf I +   \\sum_{r=1}^{n} c_r \\mathbf B^r  = \\big(\\mathbf B - \\lambda_{1} \\mathbf I\\big)\\big(\\mathbf B - \\lambda_{2} \\mathbf I\\big)...\\big(\\mathbf B - \\lambda_{n-1} \\mathbf I\\big)\\big(\\mathbf B - \\lambda_n \\mathbf I\\big) $\n",
    "\n",
    "Because $\\mathbf B$ is not defective, we diagonalize it, as shown below\n",
    "\n",
    "$\\mathbf B = \\mathbf{PD}\\mathbf P^{-1}$\n",
    "\n",
    "$ c_0 \\mathbf I +  \\sum_{r=1}^{n} c_r \\mathbf B^r = \\big(\\mathbf{PD}\\mathbf P^{-1} - \\lambda_{1} \\mathbf I\\big)\\big(\\mathbf{PD}\\mathbf P^{-1} - \\lambda_{2} \\mathbf I\\big)...\\big(\\mathbf{PD}\\mathbf P^{-1} - \\lambda_{n-1} \\mathbf I\\big)\\big(\\mathbf{PD}\\mathbf P^{-1} - \\lambda_n \\mathbf I\\big) $\n",
    "\n",
    "$ c_0 \\mathbf I +  \\sum_{r=1}^{n} c_r \\mathbf B^r = \\big(\\mathbf{PD}\\mathbf P^{-1} - \\lambda_{1} \\mathbf P \\mathbf I \\mathbf P^{-1}\\big)\\big(\\mathbf{PD}\\mathbf P^{-1} - \\lambda_{2} \\mathbf P \\mathbf I \\mathbf P^{-1}\\big)...\\big(\\mathbf{PD}\\mathbf P^{-1} - \\lambda_{n-1} \\mathbf P\\mathbf I\\mathbf P^{-1}\\big)\\big(\\mathbf{PD}\\mathbf P^{-1} - \\lambda_n \\mathbf P\\mathbf I\\mathbf P^{-1}\\big)$\n",
    "\n",
    "$ c_0 \\mathbf I +  \\sum_{r=1}^{n} c_r \\mathbf B^r = \\big(\\mathbf{P}\\big(\\mathbf D - \\lambda_{1}\\mathbf I\\big) \\mathbf P^{-1}\\big)\\big(\\mathbf{P}\\big(\\mathbf D - \\lambda_{2}\\mathbf I\\big)\\mathbf P^{-1} \\big)...\\big(\\mathbf{P}\\big(\\mathbf D - \\lambda_{n-1}\\mathbf I\\big)\\mathbf P^{-1}\\big)\\big(\\mathbf{P}\\big(\\mathbf D - \\lambda_{n}\\mathbf I\\big)\\mathbf P^{-1}\\big)$\n",
    "\n",
    "\n",
    "$ c_0 \\mathbf I +  \\sum_{r=1}^{n} c_r \\mathbf B^r = \\mathbf P\\Big(\\big(\\mathbf D - \\lambda_{1}\\mathbf I\\big)\\big(\\mathbf D - \\lambda_{2}\\mathbf I\\big)...(\\mathbf D - \\lambda_{n-1}\\mathbf I\\big)(\\mathbf D - \\lambda_{n}\\mathbf I\\big)\\Big)\\mathbf P^{-1}$\n",
    "\n",
    "$ c_0 \\mathbf I +  \\sum_{r=1}^{n} c_r \\mathbf B^r = \\mathbf P\\Big(Diag\\big(\\mathbf 0\\big)\\Big)\\mathbf P^{-1} = \\bigg[\\begin{array}{c|c|c|c}\n",
    "  \\mathbf 0 & \\mathbf 0 &\\cdots & \\mathbf 0\n",
    "\\end{array}\\bigg]$\n",
    "\n",
    "*end alternative proof: non-defective case* \n",
    "- - - -\n",
    "\n",
    "**proof: defective case:**  \n",
    "\n",
    "a sketch of the analysis proof covers two things\n",
    "\n",
    "First, it is clear from Schur decomposition that any $n$ x $n$ matrix in $\\mathbb C$ is unitarily similar to an upper triangular matrix\n",
    "\n",
    "$\\mathbf Q^H \\mathbf{BQ} = \\mathbf T$ or \n",
    "\n",
    "$\\mathbf{B} = \\mathbf {QTQ}^H$\n",
    "\n",
    "\n",
    "The eigenvalues of $\\mathbf T$ obey their characteristic polynomial, hence the characteristic polynomial of $\\mathbf B$ or equivalently $\\mathbf T$, must be a nilpotent matrix.  However Cayley Hamilton makes a stronger claim that in fact it is not just any nilpotent matrix, but the zero matrix.  \n",
    "\n",
    "*A key takeway from this, however, is that if a matrix was not nilpotent (i.e. it has at least one non-zero eigenvalue), and it becomes nilpotent after applying some other matrix's characteristic polynomial, then that means your matrix has roots to that other matrices characteristic polynomial -- i.e. your non-zero eigenvalues are non-zero eigenvalues for that other matrix. *\n",
    "\n",
    "an outline of the analysis approach is that:\n",
    "\n",
    "we can find an upper triangular matrix $\\mathbf R$ where all entries are identical to $\\mathbf T$ except diagonal elements are perturbed by small enough $\\delta$, $\\delta^2$, $\\delta^3$, and so on as needed for all duplicate eigenvalues.  Afterward, we have \n",
    "\n",
    "$\\big \\Vert \\mathbf{T} - \\mathbf R \\big \\Vert_F^2 \\lt \\epsilon$\n",
    "\n",
    "for any $\\epsilon \\gt 0$ \n",
    "\n",
    "where $\\mathbf C = \\mathbf{QRQ}^H$\n",
    "\n",
    "But now each eigenvalue is unique and per the proof near the top of this posting $\\mathbf C$  is now diagonalizable aka non-defective, and the earlier part of this cell -- i.e. the proof that all diagonalizable matrices obey Cayley Hamilton-- may be used. (There is a tecnical nit that by perturbing the eigenvalues, we have changed the characteristic polynomial, but this change is $O(\\delta)$ and becomes arbitrarily small as $\\delta \\to 0$).  \n",
    "\n",
    "Thus we may say, up to any arbitrary level of precision we can approximate $\\mathbf B$ or $\\mathbf T$ and find that those approximations all obey Cayley Hamilton, hence $\\mathbf B$ obeys Cayley Hamilton as well. \n",
    "\n",
    "*note: there are purely algebraic proofs of Cayley Hamilton for defective matrices that do not require limits / analysis.  The analysis view is quite intuitive, but requires some heavy duty machinery to be fully rigorous.  In any case these different approaches are complementary.*  \n",
    "\n",
    "**A purely algebraic proof is below.**  \n",
    "\n",
    "** Special structures in multiplying with Diagonal matrices and Triangular matrices**\n",
    "\n",
    "Matrix multiplication is associative but generally does not commute.  However in certain special cases it does.  In particular, when you have an $n$ x $n$ matrix $\\mathbf B$ and scaled form of the identity matrix, multiplication does commute. \n",
    "\n",
    "e.g. \n",
    "\n",
    "$\\big(\\gamma_1 \\mathbf B - \\lambda_1 \\mathbf I\\big)\\big(\\gamma_2 \\mathbf B - \\lambda_2 \\mathbf I\\big) = \\big(\\gamma_2 \\gamma_1 \\mathbf B^2 - (\\lambda_2 \\gamma_1 + \\lambda_1 \\gamma_2) \\mathbf B + \\lambda_1 \\lambda_2 \\mathbf I\\big) = \\big(\\gamma_2 \\mathbf B - \\lambda_2 \\mathbf I\\big)\\big(\\gamma_1 \\mathbf B - \\lambda_1 \\mathbf I\\big)$\n",
    "\n",
    "which we may verify by inspection.\n",
    "\n",
    "More generally, when we multiply this out over k terms\n",
    "\n",
    "$\\big(\\gamma_1 \\mathbf B - \\lambda_1 \\mathbf I\\big)\\big(\\gamma_2 \\mathbf B - \\lambda_2 \\mathbf I\\big)\\big(\\gamma_3 \\mathbf B - \\lambda_3 \\mathbf I\\big)...\\big(\\gamma_k \\mathbf B - \\lambda_k \\mathbf I\\big)$\n",
    "\n",
    "we may permute the ordering any way we like an get the same result.  \n",
    "\n",
    "Put differently, we may swap any two adjacent terms, where \n",
    "\n",
    "$\\big(\\gamma_{r-1} \\mathbf B - \\lambda_{r-1} \\mathbf I\\big)\\big(\\gamma_r \\mathbf B - \\lambda_r \\mathbf I\\big) = \\big(\\gamma_r \\mathbf B - \\lambda_r \\mathbf I\\big)\\big(\\gamma_{r-1} \\mathbf B - \\lambda_{r-1} \\mathbf I\\big)$\n",
    "\n",
    "for $r = \\{2, 3, 4, ... , k\\}$\n",
    "\n",
    "and using associativity, we see that the product is unchanged.  I.e. \n",
    "\n",
    "$\\Big(\\big(\\gamma_1 \\mathbf B - \\lambda_1 \\mathbf I\\big)\\big(\\gamma_2 \\mathbf B - \\lambda_2 \\mathbf I\\big)... \\Big(\\big(\\gamma_{r-1} \\mathbf B - \\lambda_{r-1} \\mathbf I\\big)\\big(\\gamma_r \\mathbf B - \\lambda_r \\mathbf I\\big)\\Big)\\Big)\\Big(\\big(\\gamma_{r+1} \\mathbf B - \\lambda_{r+1} \\mathbf I\\big) ...\\big(\\gamma_k \\mathbf B - \\lambda_k \\mathbf I\\big)\\Big)$\n",
    "\n",
    "is equivalent to \n",
    "\n",
    "$\\Big(\\big(\\gamma_1 \\mathbf B - \\lambda_1 \\mathbf I\\big)\\big(\\gamma_2 \\mathbf B - \\lambda_2 \\mathbf I\\big)... \\Big(\\big(\\gamma_{r} \\mathbf B - \\lambda_{r} \\mathbf I\\big)\\big(\\gamma_{r-1} \\mathbf B - \\lambda_{r-1} \\mathbf I\\big)\\Big)\\Big)\\Big(\\big(\\gamma_{r+1} \\mathbf B - \\lambda_{r+1} \\mathbf I\\big) ...\\big(\\gamma_k \\mathbf B - \\lambda_k \\mathbf I\\big)\\Big)$\n",
    "\n",
    "\n",
    "and we may repeatedly use this to move any term from any starting position $r$ to any ending position $j$.  This is equivalent to permuting a deck of cards (slowly) by deciding which card we want at the top of the deck and doing pairwise swaps until it is there.  Then deciding which card we want in the 2nd to top place and doing pairwise swaps until it is there, and so on.  (Note: in the language of permuting symmetric groups on $k$ elements, this pairwise swap would be called a \"basic transposition\", a term that sometimes comes up when discussing determinants, though the use of 'transpose' with a different meaning is a bit unfortunate here.)\n",
    "\n",
    "**Triangular Matrices**\n",
    "\n",
    "Upper triangular matrices are particularly easy to work with and may be interpretted as a directed graph without cycles -- except there may be a very special cycle-- i.e. a self-loop-- if there are non-zero eigenvalues.  (The same arguments may be made with lower triangular matrices, but the convention is to use upper triangular, and that is what this posting does.)  \n",
    "\n",
    "The standard basis vectors, are particularly easy to use when making arguments with upper triangular matrices. \n",
    "\n",
    "$\\bigg[\\begin{array}{c|c|c|c}\n",
    "  \\mathbf e_1 & \\mathbf e_2 &\\cdots & \\mathbf e_n\n",
    "\\end{array}\\bigg] = \\mathbf I$\n",
    "\n",
    "For instance: Consider the case where we have a nilpotent, i.e. strictly upper triangular matrix, $\\mathbf T$.  In this case we know that \n",
    "\n",
    "$\\mathbf {T e}_1 = \\mathbf 0$\n",
    "\n",
    "and we know that \n",
    "\n",
    "$\\mathbf {T e}_2 = t_{1,2} \\mathbf e_1$\n",
    "\n",
    "hence $\\mathbf  T^2 \\mathbf  e_2 = \\mathbf {TT e}_2 = t_{1,2} \\mathbf T\\mathbf e_1 = \\mathbf 0$\n",
    "\n",
    "and the same idea occurs with \n",
    "\n",
    "$\\mathbf  T^3 \\mathbf  e_3 = \\mathbf {TTT e}_3 =  t_{1,3}\\mathbf T \\big(\\mathbf T \\mathbf e_1\\big) +  t_{2,3}\\mathbf T^2\\mathbf e_2 = \\mathbf 0$\n",
    "\n",
    "and in general for any $\\mathbf e_k$, we have \n",
    "\n",
    "$\\mathbf  T \\mathbf  e_k =   t_{1,k}\\mathbf e_1 + t_{2,k}\\mathbf e_2 + ... +  t_{k-1,k}\\mathbf e_{k-1}$\n",
    "\n",
    "Repeated left multiplication by $\\mathbf T$ generates smaller and smaller subproblems.  I.e. if we left multiply the above by $\\mathbf T$ again, we get a linear combination of $\\{\\mathbf e_1, \\mathbf e_2, ... , \\mathbf e_{k-2}\\}$ And hence we can repeatedly left multiply any $\\mathbf  e_k$ by $\\mathbf T$ for $k - 2$ times, and at worse we recover a linear combination of $\\mathbf e_2$ and $\\mathbf e_1$, which we have already shown becomes the zero vector after at most two left multiplications of $\\mathbf T$.  \n",
    "\n",
    "Hence we conclude that $\\mathbf T^k \\mathbf e_k = \\mathbf 0$ \n",
    "\n",
    "Now, the only difference between the general upper triangular case and the strictly upper triangular case is the possibility of self-loops in the former.  \n",
    "\n",
    "*That is, we now assume* $\\mathbf T$ *is not necesarily nilpotent.  Instead, it is unitarily similar to *$\\mathbf B$, via Schur Decomposition.  I.e. $\\mathbf Q^H \\mathbf {BQ} = \\mathbf T$\n",
    "\n",
    "Then we now have \n",
    "\n",
    "$\\mathbf  T \\mathbf  e_k =   t_{1,k}\\mathbf e_1 + t_{2,k}\\mathbf e_2 + ... +  t_{k-1,k}\\mathbf e_{k-1} +  t_{k,k}\\mathbf e_{k} = t_{1,k}\\mathbf e_1 + t_{2,k}\\mathbf e_2 + ... +  t_{k-1,k}\\mathbf e_{k-1} +  \\lambda_k \\mathbf e_{k}$\n",
    "\n",
    "And thus we see the role of the non-zero eigenvalue in diagonal position $k$.  \n",
    "\n",
    "If we take \n",
    "\n",
    "$\\big(\\mathbf T - \\lambda_n \\mathbf I\\big)$, we can be sure that it has a zero in its bottom right (i.e. nth) diagonal position.  \n",
    "\n",
    "Thus we have \n",
    "\n",
    "$\\big(\\mathbf T - \\lambda_n \\mathbf I\\big)\\mathbf e_n = t_{1,k}\\mathbf e_1 + t_{2,k}\\mathbf e_2 + ... +  t_{k-1,k}\\mathbf e_{k-1} = \\sum_{i = 1}^{n-1} t_{i, n} \\mathbf e_i$\n",
    "\n",
    "Now suppose we left multiply this by \n",
    "\n",
    "$\\big(\\mathbf T - \\lambda_{n-1} \\mathbf I\\big)$\n",
    "\n",
    "we then have \n",
    "\n",
    "$\\big(\\mathbf T - \\lambda_{n-1} \\mathbf I\\big)\\big(\\mathbf T - \\lambda_n \\mathbf I\\big)\\mathbf e_n = \\big(\\mathbf T - \\lambda_{n-1} \\mathbf I\\big) \\sum_{i = 1}^{n-1} t_{i, n} \\mathbf e_i = \\sum_{i = 1}^{n-1}  \\Big(t_{i, n} \\big(\\mathbf T - \\lambda_{n-1} \\mathbf I\\big) \\mathbf e_i\\Big)= \\sum_{i = 1}^{n-2} \\gamma_i \\mathbf e_i $\n",
    "\n",
    "where $\\gamma_i$ denotes the appropriate scalar -- whose value we are not particularly interested in.  We verify the right hand side by noting that at worst, $\\big(\\mathbf T - \\lambda_{n-1} \\mathbf I\\big)$ has a non-zero eigenvalue for each $\\mathbf e_r$ where $r \\lt n-1$, and $\\big(\\mathbf T - \\lambda_{n-1} \\mathbf I\\big)$ has a zero eigenvalue in the n-1 spot, so $\\big(\\mathbf T - \\lambda_{n-1} \\mathbf I\\big)\\mathbf e_{n-1}$ returns a linear combination of $\\{\\mathbf e_1, \\mathbf e_2, ..., \\mathbf e_{n-2}\\}$ and $\\big(\\mathbf T - \\lambda_{n-1} \\mathbf I\\big)\\mathbf e_{r}$ returns a linear combination that at most has elements in $\\{\\mathbf e_1, \\mathbf e_2, ..., \\mathbf e_{n-2}\\}$.\n",
    "\n",
    "We now left multiply by $\\big(\\mathbf T - \\lambda_{n-2} \\mathbf I\\big)$ and see \n",
    "\n",
    "$\\big(\\mathbf T - \\lambda_{n-2} \\mathbf I\\big)\\big(\\mathbf T - \\lambda_{n-1} \\mathbf I\\big)\\big(\\mathbf T - \\lambda_n \\mathbf I\\big)\\mathbf e_n = \\sum_{i = 1}^{n-3} \\gamma_i \\mathbf e_i$\n",
    "\n",
    "And we continue this process until we have \n",
    "\n",
    "$\\big(\\mathbf T - \\lambda_{2} \\mathbf I\\big)\\big(\\mathbf T - \\lambda_{3} \\mathbf I\\big)...\\big(\\mathbf T - \\lambda_{n-1} \\mathbf I\\big)\\big(\\mathbf T - \\lambda_n \\mathbf I\\big)\\mathbf e_n = \\sum_{i = 1}^{1} \\gamma_i \\mathbf e_i = \\gamma_1 \\mathbf e_1$\n",
    "\n",
    "One more appropriate left multiplication gives us\n",
    "\n",
    "$\\big(\\mathbf T - \\lambda_{1} \\mathbf I\\big)\\big(\\mathbf T - \\lambda_{2} \\mathbf I\\big)\\big(\\mathbf T - \\lambda_{3} \\mathbf I\\big)...\\big(\\mathbf T - \\lambda_{n-1} \\mathbf I\\big)\\big(\\mathbf T - \\lambda_n \\mathbf I\\big)\\mathbf e_n = \\gamma_1 \\big(\\mathbf T - \\lambda_{1} \\mathbf I\\big) \\mathbf e_1 = \\mathbf 0$\n",
    "\n",
    "The left hand side has $n$ terms in it that we multiply by $\\mathbf e_n$ and get the zero vector.  \n",
    "\n",
    "It naturally follows that we can use *any* $\\mathbf e_k$ on the left hand side and get the zero vector.  There are a few ways to argue this.  The easiest, perhaps, is to remember that these multiplications commute so we may choose the ordering as we please.  \n",
    "\n",
    "Hence if $\\mathbf e_k \\neq \\mathbf e_n$ we simply reorder the equation, (using, in effect, a cyclic property that we get for free with commutativity):\n",
    "\n",
    "i.e. we know that \n",
    "\n",
    "$\\big(\\mathbf T - \\lambda_{k+1} \\mathbf I\\big)...\\big(\\mathbf T - \\lambda_n \\mathbf I\\big)\\big(\\mathbf T - \\lambda_{1} \\mathbf I\\big)...\\big(\\mathbf T - \\lambda_{k-1} \\mathbf I\\big)\\big(\\mathbf T - \\lambda_k \\mathbf I\\big) = \\big(\\mathbf T - \\lambda_{1} \\mathbf I\\big)\\big(\\mathbf T - \\lambda_{2}\\mathbf I\\big)...\\big(\\mathbf T - \\lambda_n \\mathbf I\\big)$\n",
    "\n",
    "Thus we have \n",
    "\n",
    "$\\Big(\\big(\\mathbf T - \\lambda_{k+1} \\mathbf I\\big)...\\big(\\mathbf T - \\lambda_n \\mathbf I\\big)\\big(\\mathbf T - \\lambda_{1} \\mathbf I\\big)\\big(\\mathbf T - \\lambda_{2} \\mathbf I\\big)...\\big(\\mathbf T - \\lambda_{k-1} \\mathbf I\\big)\\big(\\mathbf T - \\lambda_k \\mathbf I\\big)\\Big)\\mathbf e_k$\n",
    "\n",
    "now making further use of associativity  \n",
    "\n",
    "$\\Big(\\big(\\mathbf T - \\lambda_{k+1} \\mathbf I\\big)...\\big(\\mathbf T - \\lambda_n \\mathbf I\\big)\\Big)\\Big(\\big(\\mathbf T - \\lambda_{1} \\mathbf I\\big)\\big(\\mathbf T - \\lambda_{2} \\mathbf I\\big)...\\big(\\mathbf T - \\lambda_{k-1} \\mathbf I\\big)\\big(\\mathbf T - \\lambda_k \\mathbf I\\big)\\mathbf e_k\\Big)  = \\Big(\\big(\\mathbf T - \\lambda_{k+1} \\mathbf I\\big)...\\big(\\mathbf T - \\lambda_n \\mathbf I\\big)\\Big)\\mathbf 0 = \\mathbf 0$\n",
    "\n",
    "collect these relationships for each $\\mathbf e_k$ in matrix form, and we find \n",
    "\n",
    "$\\Big(\\big(\\mathbf T - \\lambda_{1} \\mathbf I\\big)\\big(\\mathbf T - \\lambda_{2} \\mathbf I\\big)\\big(\\mathbf T - \\lambda_{3} \\mathbf I\\big)...\\big(\\mathbf T - \\lambda_{n-1} \\mathbf I\\big)\\big(\\mathbf T - \\lambda_n \\mathbf I\\big)\\Big)  \\bigg[\\begin{array}{c|c|c|c}\n",
    "  \\mathbf e_1 & \\mathbf e_2 &\\cdots & \\mathbf e_n\n",
    "\\end{array}\\bigg]  = \\bigg[\\begin{array}{c|c|c|c}\n",
    "  \\mathbf 0 & \\mathbf 0 &\\cdots & \\mathbf 0\n",
    "\\end{array}\\bigg]$\n",
    "\n",
    "\n",
    "normally this posting would say to invert and right multiply by the full rank matrix we formed with $\\mathbf e_k$'s, however we simply recall that \n",
    "\n",
    "$\\bigg[\\begin{array}{c|c|c|c}\n",
    "  \\mathbf e_1 & \\mathbf e_2 &\\cdots & \\mathbf e_n\n",
    "\\end{array}\\bigg] = \\mathbf I$\n",
    "\n",
    "Thus we have \n",
    "\n",
    "$\\big(\\mathbf T - \\lambda_{1} \\mathbf I\\big)\\big(\\mathbf T - \\lambda_{2} \\mathbf I\\big)\\big(\\mathbf T - \\lambda_{3} \\mathbf I\\big)...\\big(\\mathbf T - \\lambda_{n-1} \\mathbf I\\big)\\big(\\mathbf T - \\lambda_n \\mathbf I\\big)   \n",
    " = \\bigg[\\begin{array}{c|c|c|c}\n",
    "  \\mathbf 0 & \\mathbf 0 &\\cdots & \\mathbf 0\n",
    "\\end{array}\\bigg]$\n",
    "\n",
    "\n",
    "From here we may notice that the factorization of \n",
    "\n",
    "$\\big(\\mathbf T - \\lambda_{1} \\mathbf I\\big)\\big(\\mathbf T - \\lambda_{2} \\mathbf I\\big)\\big(\\mathbf T - \\lambda_{3} \\mathbf I\\big)...\\big(\\mathbf T - \\lambda_{n-1} \\mathbf I\\big)\\big(\\mathbf T - \\lambda_n \\mathbf I\\big)$\n",
    "\n",
    "is just factoring the characteristic polynomial of $\\mathbf B$ (or equivalently $\\mathbf T$, since they are similar), thus \n",
    "\n",
    "$\\big(\\mathbf T - \\lambda_{1} \\mathbf I\\big)\\big(\\mathbf T - \\lambda_{2} \\mathbf I\\big)...\\big(\\mathbf T - \\lambda_{n-1} \\mathbf I\\big)\\big(\\mathbf T - \\lambda_n \\mathbf I\\big) = c_0 \\mathbf I + c_1 \\mathbf T + c_2 \\mathbf T^2 + ... + c_{n-1}\\mathbf T^{n-1} + c_{n}\\mathbf T^n = \\bigg[\\begin{array}{c|c|c|c}\n",
    "  \\mathbf 0 & \\mathbf 0 &\\cdots & \\mathbf 0\n",
    "\\end{array}\\bigg]$\n",
    "\n",
    "recalling that, $\\mathbf B = \\mathbf {QTQ}^H$\n",
    "\n",
    "we left multiply each side of the above by $\\mathbf Q$ and right multiply each side of the above by $\\mathbf Q^H$\n",
    "\n",
    "$\\mathbf Q\\big(c_0 \\mathbf I + c_1 \\mathbf T + c_2 \\mathbf T^2 + ... + c_{n-1}\\mathbf T^{n-1} + c_{n}\\mathbf T^n\\big)\\mathbf Q^H = \\mathbf Q\\bigg[\\begin{array}{c|c|c|c}\n",
    "  \\mathbf 0 & \\mathbf 0 &\\cdots & \\mathbf 0\n",
    "\\end{array}\\bigg] \\mathbf Q^H = \\bigg[\\begin{array}{c|c|c|c}\n",
    "  \\mathbf 0 & \\mathbf 0 &\\cdots & \\mathbf 0\n",
    "\\end{array}\\bigg]$\n",
    "\n",
    "$c_0 \\mathbf Q \\mathbf I \\mathbf Q^H + c_1 \\mathbf Q\\mathbf T\\mathbf Q^H + c_2 \\mathbf Q\\mathbf T^2\\mathbf Q^H + ... + c_{n-1}\\mathbf Q\\mathbf T^{n-1}\\mathbf Q^H + c_{n}\\mathbf Q\\mathbf T^n \\mathbf Q^H = \\bigg[\\begin{array}{c|c|c|c}\n",
    "  \\mathbf 0 & \\mathbf 0 &\\cdots & \\mathbf 0\n",
    "\\end{array}\\bigg]$\n",
    "\n",
    "and finally:\n",
    "\n",
    "$c_0 \\mathbf I + c_1 \\mathbf B + c_2 \\mathbf B^2 + ... + c_{n-1}\\mathbf B^{n-1} + c_{n}\\mathbf B^n = c_0 \\mathbf I +  \\sum_{r=1}^{n} c_r \\mathbf B^r = \\bigg[\\begin{array}{c|c|c|c}\n",
    "  \\mathbf 0 & \\mathbf 0 &\\cdots & \\mathbf 0\n",
    "\\end{array}\\bigg]$\n",
    "\n",
    "This is the long-form algebraic proof of Cayley Hamilton when $\\mathbf B$ is defective.  \n",
    "- - - -\n",
    "\n",
    "# The traces of a matrix over large enough k iterations, uniquely characterize the non-zero eigenvalues\n",
    "\n",
    "We now combine two different results: \"Full cycle trace relations and nilpotent matrices\" and the above \"Cayley Hamilton\" proofs.  \n",
    "\n",
    "**claim:**  \n",
    "\n",
    "If for $n$ x $n$ matrix $\\mathbf X$ and $m$ x $m$ matrix $\\mathbf Y$:\n",
    "\n",
    "$trace\\big(\\mathbf X^k\\big)$ = $trace\\big(\\mathbf Y^k \\big)$  for natural numbers $k = \\{1, 2, 3, ...\\}$,  \n",
    "\n",
    "then $\\mathbf X$ and $\\mathbf Y$ have the same non-zero eigenvalues (with same algebraic multiplicities).\n",
    "\n",
    "\n",
    "**commentary: ** \n",
    "\n",
    "There is an alternative proof which shows a slightly stronger claim using only Vandermonde Matrices after this one. Strictly speaking this claim only requires the traces to be equivalent for $k = \\{1, 2, 3, ..., \\big(max(m,n) + 1\\big)^2\\}$, i.e. the number of iterations is approximately the size of the bigger dimension, squared, whereas in the alternative proof, the number of iterations is required to be only approx twice the size of the bigger dimension (I.e. two full 'cycles').  Your author knows that with considerably more work the claim could be made while requiring only one cycle instead of two cycle, though the tighter claim requires considerably more machinery and may not be as intuitive.  Hence in a manner like teaching Kosaraju's algorithm instead of Tarjan's for Strongly Connected Components, the 2 cycle approach is emphasized as it is considerably more intuitive than the one cycle approach. It also offers some unique insights on the type of roots a polynomial has, and how many unique roots it has.  (Indeed it generalizes nicely to probability problems that have distributions that may not have a finite number of terms.)  That said, an illustrative sketch of the one cycle approach is given at the end of this posting, under the heading of \"Enter the Companion Matrix\", and then the underlying formula for recovering the characteristic polynomial via a full cycle is derived in two different ways under the sections at the very end with \"Newton's Indetities\" in their title.  All in all, we have 4 different proofs of the fact that a large enough number of traces uniquely characterizes the matrices eigenvalues (assuming they exist -- which is always the case if $\\mathbb C$ is your field.)\n",
    "\n",
    "The idea of this particular proof is that the trace of a matrix raised to repeated powers gives a 'signature' that uniquely specifies the non-zero eigenvalues.  The idea may be useful in cases where perhaps we know the traces, or even the eigenvalues, of $\\mathbf X$ and want to make inferences about $\\mathbf Y$.  \n",
    "\n",
    "One interpretation underpinning this is that there is a way to recreat a characteristic polynomial purely from traces of matrix powers -- so of course the trace of repeated matrix powers 'gives away' the eigenvalues.  An alternative interpretation comes in the analogy with the method of moments in probability -- if we have $E[X^k]$ for $k = \\{1, 2, 3, ...,\\}$, then the underlying probability distribution is uniquely specified.  Notice that $\\frac{1}{m} trace\\big(\\mathbf X^k\\big)$ is analogous to $E[X^k]$ (though there is the question as to whether we want to us the m total eigenvalues in $\\frac{1}{m}$, or perhaps instead just use $\\frac{1}{s}$ where $s:= n - z$, where we have z eigenvalues equal to zero.  In any case the analogoy with methods of moments is quite interesting.  \n",
    "\n",
    "*Subsequent note: My second approach, including collecting 2n - 1 iterations of the \"expected values\" in a Hankel matrix, seems awfully similar to that used in certain cases for the \"Hamburger Moment Problem\", https://en.wikipedia.org/wiki/Hamburger_moment_problem#Uniqueness_of_solutions .  It is unfortunate, but not surprising that my 'discoveries' seem to have been done before.*\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**proof:**\n",
    "\n",
    "for convenience notice that, if for $r = \\{1, 2, 3, ... , max(m,n)\\}$\n",
    "\n",
    "$trace\\big(\\mathbf X^r\\big) = trace\\big(\\mathbf Y^r\\big)  = 0 $, then both $\\mathbf X$ and $\\mathbf Y$ are nilpotent.  \n",
    "\n",
    "*The rest of the writeup assumes that they are not nilpotent matrices.*\n",
    "\n",
    "Now suppose we know the eigenvalues of $\\mathbf X$, and in particular the non-zero eigenvalues of $\\mathbf X$.  Then we know the characteristic polynomial, $p$ and use Cayley Hamilton to see the below mapping:  \n",
    "\n",
    "$p\\big(\\mathbf X\\big) = c_0 \\mathbf I + c_1 \\mathbf X + c_2 \\mathbf X^2 + ... + c_{n-1}\\mathbf X^{n-1} + c_{n}\\mathbf X^n = c_0 \\mathbf I +  \\sum_{j=1}^{n} c_j \\mathbf X^j = \\bigg[\\begin{array}{c|c|c|c}\n",
    "  \\mathbf 0 & \\mathbf 0 &\\cdots & \\mathbf 0\n",
    "\\end{array}\\bigg]$\n",
    "\n",
    "We right multiply the above by $\\mathbf X$\n",
    "\n",
    "$p\\big(\\mathbf X\\big)\\mathbf X = c_0 \\mathbf X + c_1 \\mathbf X^2 + c_2 \\mathbf X^3 + ... + c_{n-1}\\mathbf X^{n} + c_{n}\\mathbf X^{n+1} =\\bigg[\\begin{array}{c|c|c|c}\n",
    "  \\mathbf 0 & \\mathbf 0 &\\cdots & \\mathbf 0\n",
    "\\end{array}\\bigg] \\mathbf X = \\bigg[\\begin{array}{c|c|c|c}\n",
    "  \\mathbf 0 & \\mathbf 0 &\\cdots & \\mathbf 0\n",
    "\\end{array}\\bigg]$\n",
    "\n",
    "Notice that if we square the above, we have \n",
    "\n",
    "$\\big(p\\big(\\mathbf X\\big)\\mathbf X\\big)^2 = \\big(c_0 \\mathbf X + c_1 \\mathbf X^2 + c_2 \\mathbf X^3 + ... + c_{n-1}\\mathbf X^{n} + c_{n}\\mathbf X^{n+1}\\big)^2 = \\bigg[\\begin{array}{c|c|c|c}\n",
    "  \\mathbf 0 & \\mathbf 0 &\\cdots & \\mathbf 0\n",
    "\\end{array}\\bigg] $\n",
    "\n",
    "$\\big(p\\big(\\mathbf X\\big)\\mathbf X\\big)^2 = c_0^2 \\mathbf X^2 + c_1^2 \\mathbf X^4 + c_2^2 \\mathbf X^6 + ... + c_{n-1}^2\\mathbf X^{2n} + c_{n}^2\\mathbf X^{2n+2} + 2c_0 c_1 \\mathbf X^3 + 2c_1 c_2 \\mathbf X^5 + ... + 2c_{n-1}c_{n}  \\mathbf X^{2n+1}= \\bigg[\\begin{array}{c|c|c|c}\n",
    "  \\mathbf 0 & \\mathbf 0 &\\cdots & \\mathbf 0\n",
    "\\end{array}\\bigg]$\n",
    "\n",
    "or equivalently\n",
    "\n",
    "\n",
    "$\\big(p\\big(\\mathbf X\\big)\\mathbf X\\big)^2 = \\big(\\sum_{j=1}^{n+1} (c_{j-1})\\mathbf X^j\\big) \\big(\\sum_{j=1}^{n+1} (c_{j-1})\\mathbf X^j\\big) = \\sum_i \\gamma_i \\mathbf X^i = \\bigg[\\begin{array}{c|c|c|c}\n",
    "  \\mathbf 0 & \\mathbf 0 &\\cdots & \\mathbf 0\n",
    "\\end{array}\\bigg]$\n",
    "\n",
    "where $\\gamma_i$ is the appropriate scalar that comes from multiplying out the respective $c_j$ terms.  We will return to this momentarily.\n",
    "\n",
    "Taking the trace of $\\big(p\\big(\\mathbf X\\big)\\mathbf X \\big)$, we see:  \n",
    "\n",
    "$trace\\big(p\\big(\\mathbf X\\big)\\mathbf X \\big) = c_0 trace\\big(\\mathbf X\\big) + c_1 trace\\big(\\mathbf X^2\\big) + c_2 trace\\big(\\mathbf X^3\\big) + ... + c_{n-1} trace\\big(\\mathbf X^{n}\\big) + c_{n} trace\\big(\\mathbf X^{n+1}\\big) = 0$\n",
    "\n",
    "but this is equivalent to \n",
    "\n",
    "$trace\\big(p\\big(\\mathbf Y\\big)\\mathbf Y \\big) = c_0 trace\\big(\\mathbf Y\\big) + c_1 trace\\big(\\mathbf Y^2\\big) + c_2 trace\\big(\\mathbf Y^3\\big) + ... + c_{n-1} trace\\big(\\mathbf Y^{n}\\big) + c_{n} trace\\big(\\mathbf Y^{n+1}\\big) = 0$\n",
    "\n",
    "\n",
    "and more generally, we see that \n",
    "\n",
    "for $r = \\{1, 2, 3, ... , max(m,n)\\}$\n",
    "\n",
    "$trace\\Big( \\big(p\\big(\\mathbf X\\big)\\mathbf X \\big)\\big)^r\\Big)= trace\\Big(\\big(\\sum_{j=1}^{n+1} (c_{j-1})\\mathbf X^j\\big)^r\\Big) = trace\\big(\\sum_i \\gamma_i \\mathbf X^i\\big) =\\sum_i \\gamma_i trace\\big(\\mathbf X^i\\big) = 0$\n",
    "\n",
    "where again, $\\gamma_i$ indicates the scalar result of multiplying the relevant $c_j$ terms.  We then recall that for each term in this finite series, for the relevant natural numbers $i$, \n",
    "\n",
    "$trace\\big(\\mathbf X^i\\big) = trace\\big(\\mathbf Y^i\\big)$  \n",
    "\n",
    "and scaling this by some $\\gamma_i$,\n",
    "\n",
    "$\\gamma_i trace\\big(\\mathbf X^i\\big) = \\gamma_i trace\\big(\\mathbf Y^i\\big)$  \n",
    "\n",
    "hence \n",
    "\n",
    "$0 = \\sum_i \\gamma_i trace\\big(\\mathbf X^i\\big) = \\sum_i \\gamma_itrace\\big(\\mathbf Y^i\\big)$  \n",
    "\n",
    "We then conclude that for $r = \\{1, 2, 3, ... , max(m,n)\\}$\n",
    "\n",
    "$trace\\Big( \\big(p\\big(\\mathbf X\\big)\\mathbf X \\big)\\big)^r\\Big) = trace\\Big( \\big(p\\big(\\mathbf Y\\big)\\mathbf Y \\big)\\big)^r\\Big) = 0$  \n",
    "\n",
    "We now know that the matrix given by $\\Big(p\\big(\\mathbf Y\\big)\\mathbf Y\\Big)$ is nilpotent.  Recalling that $p\\big(\\mathbf Y\\big)$ is just a finite series of $ \\mathbf Y^k$ with particular scalars applied for each appropriate $k$, we do Schur Decomposition and see  \n",
    "\n",
    "$ p\\big(\\mathbf Y\\big) = \\mathbf {QTQ}^H $ and $\\mathbf Y = \\mathbf {QRQ}^H$, then \n",
    "\n",
    "$\\Big(p\\big(\\mathbf Y\\big)\\mathbf Y\\Big) = \\Big(\\mathbf {QTQ}^H \\mathbf {QRQ}^H \\Big) = \\mathbf {QTRQ}^H$\n",
    "\n",
    "since $\\mathbf Y$ is not nilpotent (i.e. $\\mathbf R$ is not nilpotent) but $\\big(\\mathbf{TR}\\big)$ is nilpotent, this tells us that $\\mathbf T$ is strictly upper triangular -- i.e. $\\mathbf T$ is nilpotent, which means that the matrix given by $p\\big(\\mathbf Y\\big)$ is nilpotent.  \n",
    "\n",
    "We thus see that all non-zero diagonal elements of $\\mathbf R$ -- aka the all non-zero eigenvalues of $\\mathbf Y$ obey the characteristic polynomial given by $p$. \n",
    "\n",
    "At a bare minimum, the above shows that the set of unique non zero eigenvalues of $\\mathbf Y$ is a subset of the set of unique non zero eigenvalues of $\\mathbf X$.  We denote this as $\\lambda\\big(\\mathbf Y\\big) \\subset \\lambda\\big(\\mathbf X\\big)$. \n",
    "\n",
    "Now, do the exact same argument used above, except swap $\\mathbf X$ for $\\mathbf Y$. \n",
    "\n",
    "(In many programming languages we would say:  \n",
    "(a) $\\mathbf X, \\mathbf Y := \\mathbf Y, \\mathbf X$  \n",
    "(b) call on argument used above, once.\n",
    ")  \n",
    "\n",
    "At a minimum, doing that shows that the set of unique non zero eigenvalues of $\\mathbf X$ is a subset of the unique non zero eigenvalues of $\\mathbf Y$.  \n",
    "\n",
    "hence with respect to unique non-zero eigenvalues we have $\\lambda\\big(\\mathbf X\\big) \\subset \\lambda\\big(\\mathbf Y\\big)$ and from before, with respect to unique nonzero eigenvalues, we have $\\lambda\\big(\\mathbf Y\\big) \\subset \\lambda\\big(\\mathbf X\\big)$ which proves that with respect to unique non-zero eigenvalues $\\lambda\\big(\\mathbf Y\\big) = \\lambda\\big(\\mathbf X\\big)$.  \n",
    "\n",
    "As in the nilpotence by trace proof, we now collect these unique non-zero eigenvalues in a diagonal matrix $\\mathbf D$.  There are $t$ non-zero eigenvalues, and $\\mathbf D$ is $t$ x $t$.  \n",
    "\n",
    "Collect the algebraic multiplicities for these unique nonzero eigenvalues of $\\mathbf X$ in $\\mathbf a_x$ and collect the algebraic multiplicities for the unique nonzero eigenvalues of $\\mathbf Y$ in $\\mathbf a_y$.  (As reminder, because they are algebraic multiplicities, each entry in $\\mathbf a_x$ and $\\mathbf a_y$ must be an integer $\\geq 1$.)\n",
    "\n",
    "Thus we have \n",
    "$\\mathbf W = \\bigg[\\begin{array}{c|c|c|c|c}\n",
    "\\mathbf D^0 \\mathbf 1 & \\mathbf D^1 \\mathbf 1 & \\mathbf D^2 \\mathbf 1 &\\cdots & \\mathbf D^{t-1} \\mathbf 1\n",
    "\\end{array}\\bigg]$\n",
    "\n",
    "and we show that these traces are equal with the following expression:  \n",
    "\n",
    "\n",
    "$\\mathbf a_x^H \\mathbf D \\mathbf W = \\mathbf a_y^H \\mathbf D \\mathbf W $  \n",
    "$\\mathbf a_x^H \\big(\\mathbf D \\mathbf W\\big)\\big(\\mathbf D \\mathbf W\\big)^{-1} = \\mathbf a_x^H \\mathbf D \\mathbf W \\mathbf W^{-1} \\mathbf D^{-1} = \\mathbf a_x^H = \\mathbf a_y^H = \\mathbf a_y^H \\mathbf D \\mathbf W \\mathbf W^{-1} \\mathbf D^{-1}  = \\mathbf a_y^H  \\big(\\mathbf D \\mathbf W\\big)\\big(\\mathbf D \\mathbf W\\big)^{-1} $  \n",
    "\n",
    "hence $\\mathbf a_x^H = \\mathbf a_y^H$\n",
    "\n",
    "and equivalently: $\\mathbf a_x = \\mathbf a_y$\n",
    "\n",
    "and we see that $\\mathbf X$ and $\\mathbf Y$ not only have the same unique non-zero eigenvalues, but that each one of those unique non-zero eigenvalues has the same algebraic multiplicity.\n",
    "- - - -\n",
    "# Alternative proof that a large enough number of traces uniquely gives the eigenvalues of a matrix\n",
    "\n",
    "**claim: over a Complex (or real) field, for n x n matrix** $\\mathbf Y$ **and n x n matrix** $\\mathbf X$, if:\n",
    "\n",
    "$trace\\big(\\mathbf X^k \\big) = trace \\big(\\mathbf Y^k\\big)$\n",
    "\n",
    "for $k = \\{1, 2, 3,... , 2n-1\\}$,\n",
    "\n",
    "then $\\mathbf X$ and $\\mathbf Y$ have the same non-zero eigenvalues, with same algebraic multiplicities.  And because they have the same dimension, then they have the same number of non-zero eigenvalues as well.  \n",
    "\n",
    "**proof:**\n",
    "\n",
    "*The proof progresses in three stages.  First we prove that these matrices must have the same number of unique non-zero eigenvalues (cardinality of sets of non-zero eigenvalues).  Then we prove that these non-zero eigenvalues must be the same for both $\\mathbf X$ and $\\mathbf Y$.  Finally we prove that these unique non-zero eigenvalues has the same algebraic multiplicity in each case.  Since the matrices are the same dimension, this means they have the same number of eigenvalues equal to zero as well.  After this is done, we offer a couple of extensions / generalization. *  \n",
    "\n",
    "**part one**\n",
    "\n",
    "We start by supposing that $\\mathbf X$ has more unique non-zero eigenvalues than $\\mathbf Y$.  \n",
    "\n",
    "Thus we assume that $\\mathbf Y$ has $q$ unique non-zero eigenvalues and $\\mathbf X$ has $r$ unique non-zero eigenvalues, where $1 \\leq q \\lt r$.  Note that if $q = 0$, then $\\mathbf Y$ would be nilpotent (see earlier cell in this posting on nilpotence and full cycle trace relations), and $\\mathbf X$ would have to be nilpotent as well.  Thus we are interested in $1 \\leq q$.  \n",
    "\n",
    "We collect the algebraic multiplicities for the $q$ unique non-zero eigenvalues $\\mathbf Y$ in $\\mathbf a_y$.  As a reminder each entry in $\\mathbf a_y$ (and $\\mathbf a_x$) is a natural number $\\geq 1$.  \n",
    "\n",
    "We create a short, fat $q$ x $r$ Vandermonde matrix for $\\mathbf Y$, below \n",
    "\n",
    "$\\mathbf W_{\\mathbf Y} = \\begin{bmatrix}\n",
    "1 & \\lambda_1 & \\lambda_1^2 & \\dots  & \\lambda_1^{r-1}\\\\ \n",
    "1 & \\lambda_2 & \\lambda_2^2 & \\dots &  \\lambda_2^{r-1} \\\\ \n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\\\ \n",
    "1 & \\lambda_{q} & \\lambda_{q}^{2} & \\dots  & \\lambda_{q}^{r-1}\n",
    "\\end{bmatrix}= \\bigg[\\begin{array}{c|c|c|c|c}\n",
    "\\mathbf D^0 \\mathbf 1 & \\mathbf D^1 \\mathbf 1 & \\mathbf D^2 \\mathbf 1 &\\cdots & \\mathbf D^{r-1} \\mathbf 1\n",
    "\\end{array}\\bigg]$\n",
    "\n",
    "$rank\\big(\\mathbf W_{\\mathbf Y}\\big) = q$ \n",
    "\n",
    "now we setup the trace relation as \n",
    "\n",
    "$\\big(\\mathbf a_y^H\\big) \\mathbf D \\mathbf W_{\\mathbf Y} = \\big(\\mathbf 1^H Diag\\big(\\mathbf a_y^H \\big)\\big) \\mathbf D \\mathbf W_{\\mathbf Y} =\\big(\\mathbf 1^H Diag\\big(\\mathbf a_y\\big)\\big) \\mathbf D \\mathbf W_{\\mathbf Y} = \\begin{bmatrix} trace\\big(\\mathbf Y\\big) & trace\\big(\\mathbf Y^2\\big)  & trace\\big(\\mathbf Y^3\\big) & \\cdots & trace\\big(\\mathbf Y^r\\big) \\end{bmatrix}$  \n",
    "\n",
    "from here we build this out to an $r$ x $r$ matrix, where we have \n",
    "\n",
    "$\\mathbf H = \\mathbf H(r) = \\Bigg[\\begin{matrix}\n",
    "trace\\big(\\mathbf Y\\big)  & trace\\big(\\mathbf Y^2\\big)  & trace\\big(\\mathbf Y^3\\big)  &\\cdots  &trace\\big(\\mathbf Y^r\\big) \\\\ \n",
    "trace\\big(\\mathbf Y^2\\big)  & trace\\big(\\mathbf Y^3\\big)  & trace\\big(\\mathbf Y^4\\big)  & \\cdots & trace\\big(\\mathbf Y^{r+1}\\big) \\\\ \n",
    "trace\\big(\\mathbf Y^3\\big) &  trace\\big(\\mathbf Y^4\\big)& trace\\big(\\mathbf Y^5\\big)  & \\cdots &trace\\big(\\mathbf Y^{r+2}\\big) \\\\ \n",
    "\\vdots & \\vdots & \\vdots &  \\ddots & \\vdots \\\\ \n",
    "trace\\big(\\mathbf Y^r\\big) & trace\\big(\\mathbf Y^{r+1}\\big) &trace\\big(\\mathbf Y^{r+2}\\big)  & \\cdots & trace\\big(\\mathbf Y^{2r-1}\\big)\n",
    "\\end{matrix}\\Bigg]$  \n",
    "\n",
    "note: While there may be some LaTeX rendering issues, it is clear that the above matrix $\\mathbf H$ is a Hankel matrix.  It is symmetric, but if any of the entries are complex, it is not Hermitian. From here, notice that while the first row is given by\n",
    "\n",
    "$\\begin{bmatrix}\n",
    "1 & 1 & 1 & \\cdots  &1 \n",
    "\\end{bmatrix}Diag\\big(\\mathbf a_y\\big) \\mathbf D \\mathbf W_{\\mathbf Y}$\n",
    "\n",
    "the second row is given by \n",
    "\n",
    "$\\begin{bmatrix}\n",
    "\\lambda_1 & \\lambda_2 & \\lambda_3 & \\cdots  &\\lambda_q\n",
    "\\end{bmatrix}Diag\\big(\\mathbf a_y\\big) \\mathbf D \\mathbf W_{\\mathbf Y}$\n",
    "\n",
    "the third row is given by \n",
    "\n",
    "$\\begin{bmatrix}\n",
    "\\lambda_1^2 & \\lambda_2^2 & \\lambda_3^2 & \\cdots  &\\lambda_q^2 \\end{bmatrix}Diag\\big(\\mathbf a_y\\big) \\mathbf D \\mathbf W_{\\mathbf Y}$\n",
    "\n",
    "... and the final, rth row is given by \n",
    "\n",
    "$\\begin{bmatrix}\n",
    "\\lambda_1^{r-1} & \\lambda_2^{r-1} & \\lambda_3^{r-1} & \\cdots  &\\lambda_q^{r-1} \\end{bmatrix}Diag\\big(\\mathbf a_y\\big) \\mathbf D \\mathbf W_{\\mathbf Y}$\n",
    "\n",
    "thus we have the following matrix\n",
    "\n",
    "$\\big(\\mathbf W_{\\mathbf Y}^T Diag\\big(\\mathbf a_y\\big) \\mathbf D \\mathbf W_{\\mathbf Y}\\big) = \\mathbf H$\n",
    "\n",
    "notice that the above $\\mathbf W^T \\neq \\mathbf W^H$ except in the special case where all $\\lambda_i$'s are real.  The above matrix is square and it is *not* full rank. It is at most rank $q$ because $rank\\big(\\mathbf W_{\\mathbf Y}\\big) = q$, and as a reminder we have assumed $q \\lt r$.  \n",
    "\n",
    "Put differently $det\\big(\\mathbf H\\big) = 0$.  \n",
    "\n",
    "If we work through the exact same calculations, for $\\mathbf X$, we find that \n",
    "\n",
    "$\\mathbf W_{\\mathbf X}^T Diag\\big(\\mathbf a_y\\big) \\mathbf \\Lambda \\mathbf W_{\\mathbf X} = \\mathbf H$\n",
    "\n",
    "Note that $ \\mathbf \\Lambda$ has $r$ entries along its diagonal, i.e. $\\lambda_{i}$ for $i = \\{1, 2, ..., r\\}$. \n",
    "\n",
    "$\\mathbf W_{\\mathbf X}$ is an $r$ x $r$ matrix with each of those $r$ unique non-zero eigenvalues in a Vandermonde Matrix, i.e. \n",
    "\n",
    "$\\mathbf W_{\\mathbf X} = \\bigg[\\begin{array}{c|c|c|c|c}\n",
    "\\mathbf \\Lambda^0 \\mathbf 1 & \\mathbf\\Lambda^1 \\mathbf 1 & \\mathbf\\Lambda^2 \\mathbf 1 &\\cdots & \\mathbf \\Lambda^{r-1} \\mathbf 1\n",
    "\\end{array}\\bigg]$\n",
    "\n",
    "further notice that $a_{x, i}$ for $i = \\{1, 2, ...., r\\} \\geq 1$, hence $Diag\\big(\\mathbf a_x\\big)$  is invertible. And since each eigenvalue in $\\mathbf W_{\\mathbf X}$ is unique, the square matrix given by $\\mathbf W_{\\mathbf X}$ is full rank.  \n",
    "\n",
    "Thus we have \n",
    "\n",
    "$det\\big(\\mathbf W_{\\mathbf X}^T Diag\\big(\\mathbf a_x\\big) \\mathbf \\Lambda \\mathbf W_{\\mathbf X}\\big) = det\\big(\\mathbf H \\big) \\neq 0 = det\\big(\\mathbf H \\big) = det\\big(\\mathbf W_{\\mathbf Y}^T Diag\\big(\\mathbf a_y\\big) \\mathbf D \\mathbf W_{\\mathbf Y}\\big) $\n",
    "\n",
    "which is a contradiction.  Or equivalently\n",
    "\n",
    "$rank\\big(\\mathbf W_{\\mathbf X}^T Diag\\big(\\mathbf a_x\\big) \\mathbf \\Lambda \\mathbf W_{\\mathbf X}\\big) = rank\\big(\\mathbf H \\big) = r = rank\\big(\\mathbf H \\big) = rank\\big(\\mathbf W_{\\mathbf Y}^T Diag\\big(\\mathbf a_y\\big) \\mathbf D \\mathbf W_{\\mathbf Y}\\big) \\leq q$\n",
    "\n",
    "This is a contradiction, because we have assumed that $q \\lt r$, so it cannot be the case that $r \\leq q$.  \n",
    "\n",
    "We repeat the argument and suppose that $q \\gt r$, \n",
    "\n",
    "Thus $\\mathbf W_{\\mathbf Y}$ is a $q$ x $q$ Vandermonde matrix for $\\mathbf Y$, and $\\mathbf W_{\\mathbf X}$ is a short fat $r$ x $q$ Vandermonde matrix for $\\mathbf X$\n",
    "\n",
    "and $\\mathbf H = \\mathbf H(q)$\n",
    "\n",
    "And again we have\n",
    "\n",
    "$det\\big(\\mathbf W_{\\mathbf X}^T Diag\\big(\\mathbf a_x\\big) \\mathbf \\Lambda \\mathbf W_{\\mathbf X}\\big) = det\\big(\\mathbf H \\big) = 0 \\neq det\\big(\\mathbf H \\big) = det\\big(\\mathbf W_{\\mathbf Y}^T Diag\\big(\\mathbf a_y\\big) \\mathbf D \\mathbf W_{\\mathbf Y}\\big) $\n",
    "\n",
    "Which again is a contradiction.   Which means it cannot be the case that $q \\neq r$.  \n",
    "\n",
    "Notice that if both sides are composed solely of square matrices of dimension $r$, the relation of course can be true if $\\mathbf a_y =\\mathbf a_x$ and $\\mathbf D = \\mathbf \\Lambda$ (and allowing any permutation of the ordering of the eigenvalues contained in either of the diagonal matrices).  But it also seems possible that other configurations could exist as well.  The next step is to prove that that the unique non-zero eigenvalue that are contained on the diagonal elements of $\\mathbf D$ and $\\mathbf \\Lambda$ must be the same.  \n",
    "\n",
    "To conclude part 1, we have observed that the the number of unique non-zero eigenvalues for $\\mathbf Y$, given by $q$ must be equal to the number of unique non-zero eigenvalues of $\\mathbf X$, given by $r$.  \n",
    "\n",
    "**part 2** \n",
    "\n",
    "Thus $\\mathbf Y$ and $\\mathbf X$ each have $r$ unique non-zero eigenvalues.  We now need to prove these non-zero eigenvalues are the same.  \n",
    "\n",
    "To prove the first thing, we iterate through each unique non-zero eigenvalue for $\\mathbf X$, given in $\\{\\lambda_1, \\lambda_2, ..., \\lambda_r\\}$, or equivalently: $\\lambda_i$ for $i = \\{1, 2, ...., r\\}$ and consider whether or not there is at least least one case where \n",
    "\n",
    "$\\big(\\mathbf Y - \\lambda_i \\mathbf I\\big)$ is not a singular matrix but $\\big(\\mathbf X - \\lambda_i \\mathbf I\\big)$ is of course singular. \n",
    "\n",
    "Put differently, we are going to prove that $\\lambda_i$ is in fact an eigenvalue for $\\mathbf X$ and for $\\mathbf Y$ for $i = \\{1,2, 3,..., r\\}$.  Then we will have proved that the set of $r$ unique eigenvalues of $\\mathbf X$ is equivalent to set of $r$ unique non-zero eigenvalues contained in $\\mathbf Y$.  \n",
    "\n",
    "*begin note on multiplication*\n",
    "\n",
    "$\\big(\\mathbf X - \\lambda_i \\mathbf I\\big)^k = (-1)^k \\lambda_i ^k \\mathbf I + \\sum_{j=0}^{k-1} \\big(\\lambda_i^j (-1)^j \\binom{k}{j} \\mathbf X^{k-j}\\big)$\n",
    "\n",
    "by the binomial theorem, and the fact that a scaled version of the identity matrix commutes with any other matrix\n",
    "\n",
    "thus we have \n",
    "\n",
    "$trace\\Big(\\big(\\mathbf X - \\lambda_i \\mathbf I\\big)^k\\Big) = trace\\Big((-1)^k \\lambda_i ^k \\mathbf I + \\sum_{j=0}^{k-1} \\big(\\lambda_i^j (-1)^j \\binom{k}{j} \\mathbf X^{k-j}\\big)\\Big) = (-1)^k \\lambda_i^k trace\\big(\\mathbf I\\big) + \\sum_{j=0}^{k-1} \\lambda_i^j (-1)^j \\binom{k}{j} trace\\big(\\mathbf X^{k-j}\\big)$\n",
    "\n",
    "then notice\n",
    "\n",
    "$(-1)^k \\lambda_i ^k trace\\big(\\mathbf I\\big) + \\sum_{j=0}^{k-1} \\lambda_i^j (-1)^j \\binom{k}{j} trace\\big(\\mathbf X^{k-j}\\big) = (-1)^k \\lambda_i ^k trace\\big(\\mathbf I\\big) + \\sum_{j=0}^{k-1} \\lambda_i^j (-1)^j \\binom{k}{j} trace\\big(\\mathbf Y^{k-j}\\big) = trace\\Big(\\big(\\mathbf Y - \\lambda_i \\mathbf I\\big)^k \\Big)$\n",
    "\n",
    "thus\n",
    "\n",
    "$trace\\Big(\\big(\\mathbf X - \\lambda_i \\mathbf I\\big)^k\\Big) = trace\\Big(\\big(\\mathbf Y - \\lambda_i \\mathbf I\\big)^k\\Big)$\n",
    "\n",
    "for $k = \\{1, 2, ..., 2n-1\\}$\n",
    "\n",
    "*end note on multiplication*  \n",
    "\n",
    "As before we model out these relationships in Vandermonde matrices and form a Hankel matrix.  \n",
    "\n",
    "This time, a touch of special handling is needed.  \n",
    "\n",
    "If $\\mathbf X$ is singular, then we collect all of its  unique eigenvalues *including* the eigenvalue = zero in an $r+1$ x $r+1$ diagonal matrix and call this $\\mathbf \\Lambda$. If $\\mathbf X$ is non-singular, then $\\mathbf \\Lambda$ is $r$ x $r$.  \n",
    "\n",
    "The same apples for $\\mathbf Y$ and its unique eigenvalues, including that of zero, if applicable, are collected in $\\mathbf D$.  \n",
    "\n",
    "It is a bit awkward that the dimensions may not line up, but the argument is the same, just more verbose. \n",
    "\n",
    "As a reminder, in all cases $\\mathbf a_x$ and $\\mathbf a_y$ contain the algebraic multiplicities of each unique eigenvalue associated with $\\mathbf X$ and $\\mathbf Y$.  These multiplicities are *always* natural numbers $\\geq 1$.  Hence $Diag\\big(\\mathbf a_x\\big)$ and $Diag\\big(\\mathbf a_y\\big)$ are both always invertible.  \n",
    "\n",
    "**First**  \n",
    "assume both digaonal matrices containing eigenvalues for $\\mathbf X$ and $\\mathbf Y$ are each $r$ x $r$.\n",
    "\n",
    "$\\mathbf W_{\\mathbf X} = \\bigg[\\begin{array}{c|c|c|c|c}\n",
    " \\mathbf 1 & \\big(\\mathbf \\Lambda - \\lambda_i\\mathbf I\\big)^1 \\mathbf 1 & \\big(\\mathbf \\Lambda - \\lambda_i\\mathbf I\\big)^2 \\mathbf 1 &\\cdots & \\big(\\mathbf \\Lambda - \\lambda_i\\mathbf I\\big)^{r-1} \\mathbf 1\n",
    "\\end{array}\\bigg]$\n",
    "\n",
    "\n",
    "$\\mathbf W_{\\mathbf Y} = \\bigg[\\begin{array}{c|c|c|c|c}\n",
    " \\mathbf 1 & \\big(\\mathbf D - \\lambda_i\\mathbf I\\big)^1 \\mathbf 1 & \\big(\\mathbf D - \\lambda_i\\mathbf I\\big)^2 \\mathbf 1 &\\cdots & \\big(\\mathbf D - \\lambda_i\\mathbf I\\big)^{r-1} \\mathbf 1\n",
    "\\end{array}\\bigg]$\n",
    "\n",
    "\n",
    "Again, since we know $\\mathbf X$ and $\\mathbf Y$ have the same number of unique non-zero eigenvalues ($r$ of them, to be exact), then $\\mathbf X$ and $\\mathbf Y$ have the same eigenvalues if and only if for every $\\lambda_i$, $  \\big(\\mathbf Y - \\lambda_i \\mathbf I\\big)$ and $\\big(\\mathbf X - \\lambda_i \\mathbf I\\big)$ are both singular (i.e. in effect subracting $\\lambda_i \\mathbf I$ zeroes out at least one eigenvalue from $\\mathbf X$ by design and it also does so for $\\mathbf Y$ in all cases if they have the same set of unique non-zero eigenvalues)\n",
    "\n",
    "We are again interested in the trace relations, collected in the form of a Hankel matrix, this time given as, $\\mathbf H\\big(r\\big)$  \n",
    "\n",
    "$\\mathbf H = \\mathbf H\\big(r\\big) = \\Bigg[\\begin{matrix}\n",
    "trace\\big(\\big(\\mathbf X - \\lambda_i \\mathbf I \\big)^1\\big)  & trace\\big(\\big(\\mathbf X - \\lambda_i \\mathbf I\\big)^2\\big)  & trace\\big(\\big(\\mathbf X - \\lambda_i \\mathbf I\\big)^3 \\big)  &\\cdots  &trace\\big( \\big(\\mathbf X - \\lambda_i\\mathbf I\\big)^r\\big) \\\\ \n",
    "trace\\big(\\big(\\mathbf X - \\lambda_i \\mathbf I \\big)^2\\big)  & trace\\big(\\big(\\mathbf X - \\lambda_i \\mathbf I\\big)^3\\big)  & trace\\big(\\big(\\mathbf X - \\lambda_i \\mathbf I\\big)^4 \\big)  &\\cdots  &trace\\big( \\big(\\mathbf X - \\lambda_i \\mathbf I \\big)^{r+1}\\big) \\\\ \n",
    "\\vdots & \\vdots & \\vdots &  \\ddots & \\vdots \\\\ \n",
    "trace\\big(\\big(\\mathbf X - \\lambda_i \\mathbf I \\big)^r\\big)  & trace\\big(\\big(\\mathbf X - \\lambda_i \\mathbf I\\big)^{r+1}\\big)  & trace\\big(\\big(\\mathbf X - \\lambda_i \\mathbf I\\big)^{r+2} \\big)  &\\cdots  &trace\\big( \\big(\\mathbf X - \\lambda_i\\mathbf I\\big)^{2r-1}\\big) \\end{matrix}\\Bigg]$  \n",
    "\n",
    "We summarize this relationship as:  \n",
    "\n",
    "$\\mathbf W_{\\mathbf Y}^T Diag\\big(\\mathbf a_y\\big)\\big( \\mathbf D - \\lambda_i \\mathbf I\\big) \\mathbf W_{\\mathbf Y} = \\mathbf H = \\mathbf W_{\\mathbf X}^T Diag\\big(\\mathbf a_x\\big)\\big( \\mathbf \\Lambda - \\lambda_i\\mathbf I\\big)\\mathbf W_{\\mathbf X} $\n",
    "\n",
    "Notice that by construction the right hand side is singular aka has determinant = 0.  (Again, the simplest reason is that $\\big( \\mathbf \\Lambda -\\lambda_i\\big)$ is a diagonal matrix with a zero on its diagonal.)\n",
    "\n",
    "Thus $\\mathbf H$ must be rank deficient, aka $det\\big(\\mathbf H\\big) = 0$ which means that the left hand side must be rank deficient as well.  Everything on the left hand side is square and hence easy to work with.  As always, $Diag\\big(\\mathbf a_y\\big)$ is invertible, and our Vandermonde matrix $\\mathbf W_{\\mathbf Y}$ is invertible, and so is its transpose,  thus, the left hand side, is singular if and only if $\\big(\\mathbf D - \\lambda_i\\mathbf I\\big)$ has a zero along its diagonal. This occurs if and only if $\\lambda_i$ is an eigenvalue of $\\mathbf Y$, and thus we conlcude that $\\lambda_i$ is an eigenvalue for $\\mathbf Y$.  \n",
    "\n",
    "**Second**  \n",
    "assume the diagonal matrix $\\mathbf \\Lambda$ containing the unique eigenvalues for $\\mathbf X$ is $r+1$ x $r + 1$ and the same for $\\mathbf Y$ (i.e. $\\mathbf D$ is $r+1$ x $r + 1$ as well.)  \n",
    "\n",
    "\n",
    "$\\mathbf W_{\\mathbf X} = \\bigg[\\begin{array}{c|c|c|c|c}\n",
    " \\mathbf 1 & \\big(\\mathbf \\Lambda - \\lambda_i\\mathbf I\\big)^1 \\mathbf 1 & \\big(\\mathbf \\Lambda - \\lambda_i\\mathbf I\\big)^2 \\mathbf 1 &\\cdots & \\big(\\mathbf \\Lambda - \\lambda_i\\mathbf I\\big)^{r} \\mathbf 1\n",
    "\\end{array}\\bigg]$\n",
    "\n",
    "\n",
    "$\\mathbf W_{\\mathbf Y} = \\bigg[\\begin{array}{c|c|c|c|c}\n",
    " \\mathbf 1 & \\big(\\mathbf D - \\lambda_i\\mathbf I\\big)^1 \\mathbf 1 & \\big(\\mathbf D - \\lambda_i\\mathbf I\\big)^2 \\mathbf 1 &\\cdots & \\big(\\mathbf D - \\lambda_i\\mathbf I\\big)^{r} \\mathbf 1\n",
    "\\end{array}\\bigg]$\n",
    "\n",
    "$\\mathbf H = \\mathbf H\\big(r +1\\big)$\n",
    "\n",
    "$\\mathbf H = \\mathbf H\\big(r + 1\\big) = \\Bigg[\\begin{matrix}\n",
    "trace\\big(\\big(\\mathbf X - \\lambda_i \\mathbf I \\big)^1\\big)  & trace\\big(\\big(\\mathbf X - \\lambda_i \\mathbf I\\big)^2\\big)  & trace\\big(\\big(\\mathbf X - \\lambda_i \\mathbf I\\big)^3 \\big)  &\\cdots  &trace\\big( \\big(\\mathbf X - \\lambda_i\\mathbf I \\big)^{r+1}\\big) \\\\ \n",
    "trace\\big(\\big(\\mathbf X - \\lambda_i \\mathbf I \\big)^2\\big)  & trace\\big(\\big(\\mathbf X - \\lambda_i \\mathbf I\\big)^3\\big)  & trace\\big(\\big(\\mathbf X - \\lambda_i \\mathbf I\\big)^4 \\big)  &\\cdots  &trace\\big( \\big(\\mathbf X - \\lambda_i\\mathbf I\\big)^{r+2}\\big)\\\\ \n",
    "\\vdots & \\vdots & \\vdots &  \\ddots & \\vdots \\\\ \n",
    "trace\\big(\\big(\\mathbf X - \\lambda_i \\mathbf I \\big)^r\\big)  & trace\\big(\\big(\\mathbf X - \\lambda_i \\mathbf I\\big)^{r+1}\\big)  & trace\\big(\\big(\\mathbf X - \\lambda_i \\mathbf I\\big)^{r+2} \\big)  &\\cdots  &trace\\big( \\big(\\mathbf X - \\lambda_i\\mathbf I \\big)^{2r}\\big) \\\\ \n",
    "trace\\big(\\big(\\mathbf X - \\lambda_i \\mathbf I\\big)^{r+1}\\big) & trace\\big(\\big(\\mathbf X - \\lambda_i \\mathbf I\\big)^{r+2} \\big) & trace\\big(\\big(\\mathbf X - \\lambda_i \\mathbf I\\big)^{r+3} \\big)  &\\cdots  &trace\\big( \\big(\\mathbf X - \\lambda_i\\mathbf I\\big)^{2r+1}\\big) \n",
    "\\end{matrix}\\Bigg]$  \n",
    "\n",
    "\n",
    "\n",
    "$\\mathbf W_{\\mathbf Y}^T Diag\\big(\\mathbf a_y\\big)\\big( \\mathbf D - \\lambda_i \\mathbf I\\big) \\mathbf W_{\\mathbf Y} = \\mathbf H = \\mathbf W_{\\mathbf X}^T Diag\\big(\\mathbf a_x\\big)\\big( \\mathbf \\Lambda -\\lambda_i\\mathbf I\\big)\\mathbf W_{\\mathbf X} $\n",
    "\n",
    "And again, the right hand side is singular aka has determinant = 0.  (Again, the simplest reason is that $\\big( \\mathbf \\Lambda -\\lambda_i\\big)$ is a diagonal matrix with a zero on its diagonal.)  So the left hand side must be as well, and thus we conclude there is a zero along the diagonal of $\\big(\\mathbf D - \\lambda_i \\mathbf I\\big)$, i.e. that $\\lambda_i$ is an eigenvalue of $\\mathbf Y$ as well.\n",
    "\n",
    "\n",
    "**Third**   \n",
    "assume the diagonal matrix containing unique eigenvalues for $\\mathbf X$ is $r$ x $r$ and the respetive one for $\\mathbf Y$'s is $r + 1$ x $r + 1$.\n",
    "\n",
    "shorter, slightly fatter  \n",
    "$\\mathbf W_{\\mathbf X} = \\bigg[\\begin{array}{c|c|c|c|c}\n",
    " \\mathbf 1 & \\big(\\mathbf \\Lambda - \\lambda_i\\mathbf I\\big)^1 \\mathbf 1 & \\big(\\mathbf \\Lambda - \\lambda_i\\mathbf I\\big)^2 \\mathbf 1 &\\cdots & \\big(\\mathbf \\Lambda - \\lambda_i\\mathbf I\\big)^{r} \\mathbf 1\n",
    "\\end{array}\\bigg]$\n",
    "\n",
    "square  \n",
    "$\\mathbf W_{\\mathbf Y} = \\bigg[\\begin{array}{c|c|c|c|c}\n",
    " \\mathbf 1 & \\big(\\mathbf D - \\lambda_i\\mathbf I\\big)^1 \\mathbf 1 & \\big(\\mathbf D - \\lambda_i\\mathbf I\\big)^2 \\mathbf 1 &\\cdots & \\big(\\mathbf D - \\lambda_i\\mathbf I\\big)^{r} \\mathbf 1\n",
    "\\end{array}\\bigg]$\n",
    "\n",
    "\n",
    "$\\mathbf H = \\mathbf H\\big( r+1\\big)$ \n",
    "i.e. the same as in the second case  \n",
    "\n",
    "$\\mathbf W_{\\mathbf Y}^T Diag\\big(\\mathbf a_y\\big)\\big( \\mathbf D - \\lambda_i \\mathbf I\\big) \\mathbf W_{\\mathbf Y} = \\mathbf H = \\mathbf W_{\\mathbf X}^T Diag\\big(\\mathbf a_x\\big)\\big( \\mathbf \\Lambda -\\lambda_i\\mathbf I\\big)\\mathbf W_{\\mathbf X} $\n",
    "\n",
    "\n",
    "Again, the right hand side is rank deficient which means that $det\\big(\\mathbf H\\big) = 0$. \n",
    "\n",
    "The left hand side thus must be rank deficient as well.  So we conclude that $\\lambda_i$ must be an eigenvalue of $\\mathbf Y$ in this case, too.  \n",
    "\n",
    "\n",
    "**Fourth**   \n",
    "assume the diagonal matrix containing unique eigenvalues for $\\mathbf X$ is $r+1$ x $r+1$ and for $\\mathbf Y$ it is $r$ x $r$.\n",
    "\n",
    "\n",
    "taller, slightly skinnier  \n",
    "$\\mathbf W_{\\mathbf X} = \\bigg[\\begin{array}{c|c|c|c|c}\n",
    " \\mathbf 1 & \\big(\\mathbf \\Lambda - \\lambda_i\\mathbf I\\big)^1 \\mathbf 1 & \\big(\\mathbf \\Lambda - \\lambda_i\\mathbf I\\big)^2 \\mathbf 1 &\\cdots & \\big(\\mathbf \\Lambda - \\lambda_i\\mathbf I\\big)^{r-1} \\mathbf 1\n",
    "\\end{array}\\bigg]$\n",
    "\n",
    "square  \n",
    "$\\mathbf W_{\\mathbf Y} = \\bigg[\\begin{array}{c|c|c|c|c}\n",
    " \\mathbf 1 & \\big(\\mathbf D - \\lambda_i\\mathbf I\\big)^1 \\mathbf 1 & \\big(\\mathbf D - \\lambda_i\\mathbf I\\big)^2 \\mathbf 1 &\\cdots & \\big(\\mathbf D - \\lambda_i\\mathbf I\\big)^{r-1} \\mathbf 1\n",
    "\\end{array}\\bigg]$\n",
    "\n",
    "\n",
    "$\\mathbf H = \\mathbf H\\big(r\\big)$\n",
    "i.e. the same as in the first case  \n",
    "\n",
    "$\\mathbf W_{\\mathbf Y}^T Diag\\big(\\mathbf a_y\\big)\\big( \\mathbf D - \\lambda_i \\mathbf I\\big) \\mathbf W_{\\mathbf Y} = \\mathbf H = \\mathbf W_{\\mathbf X}^T Diag\\big(\\mathbf a_x\\big)\\big( \\mathbf \\Lambda -\\lambda_i \\mathbf I\\big)\\mathbf W_{\\mathbf X} $\n",
    "\n",
    "\n",
    "And once again the right hand side is rank deficient which means that $det\\big(\\mathbf H\\big) = 0$. Notice that the left hand side is easy to evaluate because everything is square and the argument is the same as above.  We know that here, too, there must be a zero along the diagonal of $\\big( \\mathbf D - \\lambda_i \\mathbf I\\big)$ for each and every $\\lambda_i$. \n",
    "\n",
    "\n",
    "**final step**  \n",
    "\n",
    "In all cases we know that $\\lambda_i \\neq 0$  is an eigenvalue for $\\mathbf X$ and also for $\\mathbf Y$.  Since $\\mathbf X$ and $\\mathbf Y$ have the same number of unique non-zero eigenvalues, after enumerating each of the $r$ unique non-zero eigenvalues of $\\mathbf X$ -- i.e. $\\lambda_i$ for $i = \\{1, 2, 3, ..., r\\}$ -- we will have necessarily enumerated each of the unique non-zero eigenvalues for $\\mathbf Y$ as well. \n",
    "\n",
    "Thus we know that $\\mathbf X$ and $\\mathbf Y$ have the same set of unique non-zero eigenvalues ($r$ of them in total).  It remains for us to prove that these eigenvalues have the same algebraic multiplicities.  After proving that, it then naturally follows, since both matrices are n x n,  that they have the same number of eigenvalues equal to zero (i.e. sum of algebraic multiplicities of non-zero unique eigenvalues + algebraic multiplictity of zeros = $n$).  \n",
    "\n",
    "*To finish off the proof*  \n",
    "\n",
    "At this point we have $r$ sytems of equations for both $\\mathbf X$ and $\\mathbf Y$, that is:  \n",
    "\n",
    "$\\sum_{i=1}^{r} a_{y,i}\\lambda_{y,i}^k = \\sum_{i=1}^{r} a_{y,i}\\lambda_{x,i}^k = trace\\big(\\mathbf Y^k\\big) = trace\\big(\\mathbf X^k\\big) = \\sum_{i=1}^{r} a_{x,i}\\lambda_{y,i}^k = \\sum_{i=1}^{r} a_{x,i}\\lambda_{x,i}^k$  \n",
    "\n",
    "for $k = \\{1, 2, ..., r\\}$, recalling that $\\lambda_{y_i} = \\lambda_{x,i}$\n",
    "\n",
    "For both $\\mathbf X$ and $\\mathbf Y$ they have the same scalar result, *and* these are $r$ equations with $r$ terms are linearly indepedendent.  (Why are they linearly independent? No matter how we encode them in a Vandermonde matrix, said matrix is square with determinant $\\neq 0$, aka it is full rank.) We can 'eyeball' the above and see it is consistent if $a_{y,i} = a_{x,i}$.  Since we have $r$ terms in $r$ linearly independent equations, we know that there is one and only one solution to the above equation, and hence the 'eyeyball' one is unique.  \n",
    "\n",
    "Alternatively, we can once again explicitly use a Vandermonde matrix, as shown below.  \n",
    "\n",
    "$\\mathbf a_x^H \\mathbf D \\mathbf W_{\\mathbf X} = \\mathbf a_y^H \\mathbf D \\mathbf W_{\\mathbf X} = \\begin{bmatrix} trace\\big(\\mathbf X\\big) & trace\\big(\\mathbf X^2\\big)  & trace\\big(\\mathbf X^3\\big) & \\cdots & trace\\big(\\mathbf X^r\\big) \\end{bmatrix} $\n",
    "\n",
    "$\\mathbf a_x^H  = \\mathbf a_y^H = \\begin{bmatrix} trace\\big(\\mathbf X\\big) & trace\\big(\\mathbf X^2\\big)  & trace\\big(\\mathbf X^3\\big) & \\cdots & trace\\big(\\mathbf X^r\\big) \\end{bmatrix}\\big(\\mathbf D \\mathbf W_{\\mathbf X}\\big)^{-1}  $\n",
    "\n",
    "- - - - \n",
    "or if the reader prefers: \n",
    "\n",
    "$ \\mathbf a_y^H \\mathbf \\Lambda \\mathbf W_{\\mathbf Y} = \\mathbf a_x^H \\mathbf \\Lambda \\mathbf W_{\\mathbf Y}= \\begin{bmatrix} trace\\big(\\mathbf Y\\big) & trace\\big(\\mathbf Y^2\\big)  & trace\\big(\\mathbf Y^3\\big) & \\cdots & trace\\big(\\mathbf Y^r\\big) \\end{bmatrix} $\n",
    "\n",
    "$\\mathbf a_y^H = \\mathbf a_x^H  = \\begin{bmatrix} trace\\big(\\mathbf Y\\big) & trace\\big(\\mathbf Y^2\\big)  & trace\\big(\\mathbf Y^3\\big) & \\cdots & trace\\big(\\mathbf Y^r\\big) \\end{bmatrix}\\big(\\mathbf \\Lambda \\mathbf W_{\\mathbf Y}\\big)^{-1}$\n",
    "- - - - \n",
    "\n",
    "hence $\\mathbf a_x^H = \\mathbf a_y^H$\n",
    "\n",
    "and equivalently: $\\mathbf a_x = \\mathbf a_y$\n",
    "\n",
    "Thus we know that $\\mathbf X$ and $\\mathbf Y$ have the same non-zero eigenvalues with same algebraic multiplicities, and in fact have the same number of zeros, as well.  \n",
    "\n",
    "Thus, if $trace\\big(\\mathbf X^k\\big) = trace\\big(\\mathbf Y^k \\big)$  for natural numbers $k = \\{1, 2, 3, ..., 2n-1\\}$, then $\\mathbf X$ and $\\mathbf Y$ have the same non-zero eigenvalues (with same algebraic multiplicity for each non-zero eigenvalue).\n",
    "\n",
    "\n",
    "**extension:**\n",
    "\n",
    "consider the more general case where $trace\\big(\\mathbf X^k\\big) = trace\\big(\\mathbf Y^k \\big)$  for natural numbers $k = \\{1, 2, 3, ..., 2n-1\\}$  \n",
    "\n",
    "but $\\mathbf X$ is $m$ x $m$ and $\\mathbf Y$ is $n$ x $n$, where for simplicity, we assume $n \\gt m$.  \n",
    "\n",
    "The easiest way to extend this is if we take each of the eigenvalues (including repeats) of $\\mathbf X$, and place them in the first $m$ diagonal entries of an $n$ x $n$ zero matrix, $\\mathbf Z$. Thus $\\mathbf Z$ becomes a diagonal matrix, and if $\\mathbf X$ had $s$ non-zero (including repeats) eigenvalues, then there will be the respective $s$ non-zero entries along the diagonal of $\\mathbf Z$.  \n",
    "\n",
    "Alternatively, we *could* make $\\mathbf Z$ be equal to $\\mathbf X$, but we just append rows zeros below and columns of zeros to the right, until $\\mathbf Z$ has the same dimension as $\\mathbf Y$.  This alternative approach directly preserves the traces of $\\mathbf X$ and clearly is appending eigenvalues of zero to it as well.  The former approach of explicitly creating $\\mathbf Z$ with $\\mathbf X$'s eigenvalues is a bit easier to work so that is what we discuss below.  \n",
    "\n",
    "Thus by construction, we know that $\\mathbf X$ has the same non-zero eigenvalues as $\\mathbf Z$.  We also know that \n",
    "\n",
    "$trace\\big(\\mathbf Z^k\\big) = trace\\big(\\mathbf X^k\\big) = trace\\big(\\mathbf Y^k \\big)$  for natural numbers $k = \\{1, 2, 3, ..., 2n-1\\}$\n",
    "\n",
    "now repeat the argument used above, with respect to the same dimensioned $\\mathbf X$ and $\\mathbf Y$, except this time use it on $\\mathbf Z$ and $\\mathbf Y$.  We find that $\\mathbf Z$ and $\\mathbf Y$ have the same non-zero eigenvalues with same algebraic multiplicities, and because $\\mathbf Z$ has the same non-zero eigenvalues with same algebraic multiplicities as $\\mathbf X$, we know that $\\mathbf X$ and $\\mathbf Y$ have the same non-zero eigenvalues with same algebraic multiplicities.  \n",
    "\n",
    "\n",
    "# extension: \n",
    "**for some some $m$ x $n$ matrix ** $\\mathbf G$ **and some $n$ x $m$ matrix ** $\\mathbf H$, then $\\mathbf {GH}$ and $\\mathbf {HG}$ have the same non-zero eigenvalues (in terms of algebraic multiplicity).\n",
    "\n",
    "first notice \n",
    "\n",
    "$trace\\Big(\\big(\\mathbf {GH}\\big)\\Big) = trace\\Big(\\big(\\mathbf {HG}\\big)\\Big)$\n",
    "\n",
    "via the cyclic property of the trace.  Now in general, we can say\n",
    "\n",
    "for $r = \\{2, 3, ... \\}$\n",
    "\n",
    "$trace\\Big(\\big(\\mathbf {GH}\\big)^r\\Big) = trace\\Big(\\mathbf{GH} \\big(\\mathbf G\\mathbf H\\big)^{r-1}\\Big) = trace\\Big( \\mathbf{H} \\big(\\mathbf G\\mathbf H\\big)^{r-1} \\mathbf G\\Big) = trace\\Big(\\big(\\mathbf {HG}\\big)^r\\Big)$\n",
    "\n",
    "thus we have \n",
    "\n",
    "$trace\\Big(\\big(\\mathbf {GH}\\big)^k\\Big) = trace\\Big(\\big(\\mathbf {HG}\\big)^k\\Big)$ \n",
    "\n",
    "for $k = \\{1, 2,  3, ... \\}$\n",
    "\n",
    "We now apply either of the two preceding proofs, and know that $\\big(\\mathbf {GH}\\big)$ has the same non-zero eigenvalues (with the same algebraic multiplicties) as $\\big(\\mathbf {HG}\\big)$ \n",
    "\n",
    "\n",
    "# extension:  \n",
    "\n",
    "for some $n$ x $n$ matrix $\\mathbf X$, we can determine the number of unique non-zero eigenvalues it has by collecting its traces in a Hankel matrix.  (Note that we can determine whether or not it has an eigenvalue of zero via the use of determinants, or Gaussian Elimination, or whatever other nullspace oriented tool.)\n",
    "\n",
    "i.e. where we have  \n",
    "\n",
    "$\\mathbf H(n) = \\Bigg[\\begin{matrix}\n",
    "trace\\big(\\mathbf X\\big)  & trace\\big(\\mathbf X^2\\big)  & trace\\big(\\mathbf X^3\\big)  &\\cdots  &trace\\big(\\mathbf X^n\\big) \\\\ \n",
    "trace\\big(\\mathbf X^2\\big)  & trace\\big(\\mathbf X^3\\big)  & trace\\big(\\mathbf X^4\\big)  & \\cdots & trace\\big(\\mathbf X^{n+1}\\big) \\\\ \n",
    "trace\\big(\\mathbf X^3\\big) &  trace\\big(\\mathbf X^4\\big)& trace\\big(\\mathbf X^5\\big)  & \\cdots &trace\\big(\\mathbf X^{n+2}\\big) \\\\ \n",
    "\\vdots & \\vdots & \\vdots &  \\ddots & \\vdots \\\\ \n",
    "trace\\big(\\mathbf X^n\\big) & trace\\big(\\mathbf X^{n+1}\\big) &trace\\big(\\mathbf X^{n+2}\\big)  & \\cdots & trace\\big(\\mathbf X^{2n-1}\\big)\n",
    "\\end{matrix}\\Bigg]$  \n",
    "\n",
    "\n",
    "$rank\\big(\\mathbf H(n)\\big) = $ number of unique non-zero eigenvalues\n",
    "\n",
    "**updated idea from the problem that follows**\n",
    "\n",
    "if we take the convention that $\\mathbf X^0 = \\mathbf I$, we can actually determine the number of unique eigenvalues (including zeros) by looking at the rank of the below Hankel matrix \n",
    "\n",
    "\n",
    "i.e. where we have  \n",
    "\n",
    "$\\mathbf H(n) = \\Bigg[\\begin{matrix}\n",
    "trace\\big(\\mathbf X^0\\big)  & trace\\big(\\mathbf X^1\\big)  & trace\\big(\\mathbf X^2\\big)  &\\cdots  &trace\\big(\\mathbf X^{n-1}\\big) \\\\ \n",
    "trace\\big(\\mathbf X^1\\big)  & trace\\big(\\mathbf X^2\\big)  & trace\\big(\\mathbf X^3\\big)  & \\cdots & trace\\big(\\mathbf X^{n}\\big) \\\\ \n",
    "trace\\big(\\mathbf X^2\\big) &  trace\\big(\\mathbf X^3\\big)& trace\\big(\\mathbf X^4\\big)  & \\cdots &trace\\big(\\mathbf X^{n+1}\\big) \\\\ \n",
    "\\vdots & \\vdots & \\vdots &  \\ddots & \\vdots \\\\ \n",
    "trace\\big(\\mathbf X^{n-1}\\big) & trace\\big(\\mathbf X^{n}\\big) &trace\\big(\\mathbf X^{n+1}\\big)  & \\cdots & trace\\big(\\mathbf X^{2n-2}\\big)\n",
    "\\end{matrix}\\Bigg]$  \n",
    "\n",
    "\n",
    "$rank\\big(\\mathbf H(n)\\big) = $ number of unique eigenvalues (including zeros)\n",
    "\n",
    "\n",
    "# extension: \n",
    "\n",
    "\n",
    "The following is an adaptation of problem 112, in\n",
    "\n",
    "\"Matrices : Theory & Applications:  \n",
    "Additional exercises\"\n",
    "\n",
    "found here: \n",
    "\n",
    "http://perso.ens-lyon.fr/serre/DPF/exobis.pdf\n",
    "\n",
    "In general the field is $\\mathbb C$ for this problem.  \n",
    "\n",
    "**claim:**  \n",
    "\n",
    "We have a $n$ x $n$ matrix $\\mathbf X$ with strictly real entries in it, and $\\mathbf X$ has $m$ unique eigenvalues. \n",
    "\n",
    "$\\mathbf H$ collects the traces of $\\mathbf X$ raised to different powers, where we use the convention that $\\mathbf X^0 = \\mathbf I$.  \n",
    "\n",
    "where $s_k = trace\\big(\\mathbf X^k\\big)$\n",
    "\n",
    "$\\mathbf H(m) = \\begin{bmatrix}\n",
    "s_0 & s_1  & s_2 & \\cdots & s_{m-1}\\\\ \n",
    "s_1 &  s_2&  s_3 & \\cdots & s_m \\\\ \n",
    "s_2& s_3 & s_4 & \\cdots & s_{m+1}\\\\ \n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots\\\\ \n",
    "s_{m-1} & s_{m} & s_{m+1}  & \\cdots  & s_{2m-2}\n",
    "\\end{bmatrix}$\n",
    "\n",
    "$\\mathbf H(m)$ **is positive definite if and only if all eigenvalues of $\\mathbf X$ are real, and:**  \n",
    "$\\mathbf H(n)$ **is positive semi-definite if and only if all eigenvalues of $\\mathbf X$ are real.**  \n",
    "\n",
    "\n",
    "\n",
    "Let us first consider the case where our Hankel matrix $\\mathbf H$ is $m$ x $m$, and our polynomial has $m$ unique roots (including zeros).  Alternatively put, \n",
    "\n",
    "by the earlier decomposition that we have in this writeup (with slight modification), \n",
    "\n",
    "$\\mathbf H(m) = \\mathbf H = \\mathbf W^T Diag\\big(\\mathbf a\\big) \\mathbf W = \\mathbf W^T Diag\\big(\\mathbf a\\big)^{\\frac{1}{2}} Diag\\big(\\mathbf a\\big)^{\\frac{1}{2}} \\mathbf W$\n",
    "\n",
    "recalling that $\\mathbf a$ only contains natural numbers $\\geq 1$.  \n",
    "\n",
    "In our problem $\\mathbf H$ is always symmetric and real, however $\\mathbf W$ may not be. \n",
    "\n",
    "let $\\mathbf B :=  \\mathbf W^T Diag\\big(\\mathbf a\\big)^{\\frac{1}{2}} $\n",
    "\n",
    "$\\mathbf H = \\mathbf B \\mathbf B^T$\n",
    "\n",
    "Note that $\\mathbf B$ is $m$ x $m$ as is $\\mathbf W^T$ which contains $m$ unique eigenvalues and hence the Vandermonde matrix is full rank.  By construction $Diag\\big(\\mathbf a\\big)$ only contains natural numbers $\\geq 1$, thus $Diag\\big(\\mathbf a\\big)^{\\frac{1}{2}}$ must be full rank as well.  Hence we have full rank $\\mathbf B$, and thus $\\mathbf H$ is full rank as well.  Put differently $det\\big(\\mathbf H\\big) = det\\big(\\mathbf B\\big) det\\big(\\mathbf B^T\\big) \\neq 0$. \n",
    "\n",
    "\n",
    "**Consider the case where all eigenvalues of** $\\mathbf X$ **are real**: i.e.  if all of $\\mathbf B$ is real, we have \n",
    "\n",
    "$\\mathbf H = \\mathbf B \\mathbf B^T = \\mathbf B \\mathbf B^H$\n",
    "\n",
    "which guarantees that $\\mathbf H$  is Hermitian (specifically real symmetric) positive definite.  \n",
    "\n",
    "However, the more subtle issue is that $\\mathbf H$ is symmetric positive definite **iff** all of elements in $\\mathbf W$ (aka all components of $\\mathbf B$) are real.  Proving the other leg of this this a bit trickier. \n",
    "\n",
    "**We now examine the case where ** $\\mathbf X$  ** has eigenvalues that contain non-zero imaginary parts**, i.e. $\\mathbf B$ has complex numbers in it (which come in conjugate pairs, of course).  \n",
    "\n",
    "The argument works irrespective of ordering, however let us make it as simple as possible by having one of these complex eigenvalues located in $\\mathbf b_{m-1}$ and its conjugate in $\\mathbf b_{m}$.  \n",
    "\n",
    "That is, we have \n",
    "\n",
    "$\\mathbf B = \n",
    "\\bigg[\\begin{array}{c|c|c|c|c|c}\n",
    "\\mathbf b_1 & \\mathbf b_2 &\\cdots &  \\mathbf b_{m-2} &  \\mathbf b_{m-1} &  \\mathbf b_{m}\n",
    "\\end{array}\\bigg]$\n",
    "\n",
    "$\\mathbf H = \\mathbf {BB}^T = \\mathbf b_1 \\mathbf b_1^T + \\mathbf b_2 \\mathbf b_2^T + ... +\\mathbf b_{m-2}\\mathbf b_{m-2}^T + \\mathbf b_{m-1} \\mathbf b_{m-1}^T + \\mathbf b_{m}\\mathbf b_{m}^T$\n",
    "\n",
    "key facts to consider: \n",
    "\n",
    "*1) * $\\overline{\\mathbf b_{m}} = \\mathbf b_{m-1}$, *because the final two column slices of* $\\mathbf W^T$ *are identical except for complex conjugation, and the algebraic multiplicities of eigenvalues coming in conjugate pairs must be the same, hence the the final two columns in* $\\mathbf B$ *are the same except for complex conjugation*.  \n",
    "\n",
    "*2) $\\mathbf H$ is real symmetric, and hence diagonalizable with mutually orthonormal eigenvectors and real eigenvalues.  This creates a partition of the vector space.  Further $\\mathbf H$ is full rank, aka $det\\big(\\mathbf H\\big) \\neq 0$ hence $\\mathbf H$ does not have an eigenvalue equal to zero.  This means that if we find some $\\mathbf y$ where  $0 \\neq \\big \\Vert \\mathbf y \\big \\Vert_2^2 = \\big \\Vert \\mathbf c \\big \\Vert_2^2 = \\big \\Vert \\mathbf {Pc} \\big \\Vert_2^2$ where $\\mathbf y^H \\mathbf H \\mathbf y = \\sum_{i=1}^{n} \\lambda_i \\big \\vert c_i \\big \\vert^2  = 0$, where $\\mathbf H = \\mathbf {PDP}^H$ and $\\mathbf P^H \\mathbf y = \\mathbf c$, then $\\mathbf H$ must have mixed eigenvalues (i.e. at least one positive eigenvalue and at least one negative eigenvalue).  Thus, in such a case, our full rank* $\\mathbf H$ *cannot be postive definite.*\n",
    "\n",
    "\n",
    "**The proof** \n",
    "\n",
    "suppose we run $\\mathbf {QR}$ factorization (i.e. Gram Schmidt) on $\\mathbf B$\n",
    "\n",
    "$\\mathbf B = \n",
    "\\bigg[\\begin{array}{c|c|c|c|c|c}\n",
    "\\mathbf b_1 & \\mathbf b_2 &\\cdots &  \\mathbf b_{m-2} &  \\mathbf b_{m-1} &  \\mathbf b_{m}\n",
    "\\end{array}\\bigg] = \\mathbf {QR} = \n",
    "\\bigg[\\begin{array}{c|c|c|c|c|c}\n",
    "\\mathbf q_1 & \\mathbf q_2 &\\cdots &  \\mathbf q_{m-2} &  \\mathbf q_{m-1} &  \\mathbf q_{m}\n",
    "\\end{array}\\bigg] \\mathbf R\n",
    "$   \n",
    "\n",
    "for avoidance of doubt, because our $m$ x $m$ $\\mathbf B$ is full rank, this means that each diagonal element of $\\mathbf R$, is non-zero, i.e. $r_{i,i} \\neq 0$.  Also recall that each $\\mathbf q_k$ is mutually orthonormal (and $\\mathbf Q$ is a full rank, unitary matrix.) \n",
    "\n",
    "now consider the quadratic form \n",
    "\n",
    "$\\mathbf q_m^H \\mathbf H \\mathbf q_m = \\mathbf q_m^H \\big(\\mathbf {BB}^T\\big)\\mathbf q_m = \\mathbf q_m^H \\big(\\mathbf b_1 \\mathbf b_1^T + \\mathbf b_2 \\mathbf b_2^T + ... + \\mathbf b_{m-2}\\mathbf b_{m-2}^T + \\mathbf b_{m-1} \\mathbf b_{m-1}^T + \\mathbf b_{m}\\mathbf b_{m}^T\\big)\\mathbf q_m $  \n",
    "\n",
    "$\\mathbf q_m^H \\mathbf H \\mathbf q_m  = \\big( \\mathbf q_m^H \\mathbf b_1 \\mathbf b_1^T + \\mathbf q_m^H  \\mathbf b_2 \\mathbf b_2^T + ... +\\mathbf q_m^H \\mathbf b_{m-2}\\mathbf b_{m-2}^T + \\mathbf q_m^H \\mathbf b_{m-1} \\mathbf b_{m-1}^T + \\mathbf q_m^H \\mathbf b_{m}\\mathbf b_{m}^T\\big)\\mathbf q_m = \\big( \\mathbf 0^T + \\mathbf 0^T + ... + \\mathbf 0^T + \\mathbf 0^T + r_{m,m}\\mathbf b_{m}^T\\big)\\mathbf q_m $  \n",
    "\n",
    "$\\mathbf q_m^H \\mathbf H \\mathbf q_m  =  r_{m,m}\\big(\\mathbf b_{m}^T \\mathbf q_m\\big) = r_{m,m}\\Big(\\big(\\mathbf b_{m}^T \\mathbf q_m\\big)^H\\Big)^H = r_{m,m}\\Big(\\mathbf q_m^H \\overline{\\mathbf b_{m}} \\Big)^H = r_{m,m}\\Big(\\mathbf q_m^H \\mathbf b_{m-1} \\Big)^H = r_{m,m}\\Big(0 \\Big)^H  = r_{m,m}0 = 0$  \n",
    "\n",
    "conclusion for the complex roots case:\n",
    "\n",
    "because $\\mathbf H$ is real symmetric and $det\\big(\\mathbf H\\big) \\neq 0$, yet we have found a $\\mathbf y \\neq \\mathbf 0$, where $\\mathbf y^H \\mathbf H \\mathbf y = 0$, this means $\\mathbf H$ cannot be positive definite and hence has at least one negative eigenvalue.  And since $\\mathbf H$ has at least one negative eigenvalue, this means there is some $\\mathbf z$ where $\\mathbf z^H \\mathbf {Hz} \\lt 0$.  \n",
    "\n",
    "Hence we conclude that $\\mathbf H(m)$ is positive definite **iff** all eigenvalues of $\\mathbf X$ are real.  \n",
    "\n",
    "*extension*:  \n",
    "\n",
    "The original problem asks us to consider the case of $\\mathbf H(n)$ not $\\mathbf H(m)$ because $\\mathbf X$ is $n$ x $n$ and has $n$ roots to its characteristic polynomial (repeated with multiplicity).  We know that $ 1 \\leq  m \\leq n$.  If $m \\neq n$, then we 'grow' our underlying $\\mathbf B$ such that it has now has $n$ rows, but still $m$ columns (and of course has rank of $m$ still).  In this case $rank\\big(\\mathbf H(n)\\big)  = m \\lt n$ i.e. $det\\big(\\mathbf H(n)\\big) = 0$, \n",
    "\n",
    "if all of $\\mathbf B$ is real (i.e. $\\mathbf X$ has strictly real eigenvalues), then $\\mathbf H(n) = \\mathbf {BB^T} = \\mathbf {BB}^H$ and $\\mathbf H(n)$ is positive semi definite.  \n",
    "\n",
    "if $\\mathbf B$ has some non-zero imaginary parts: (i.e. $\\mathbf X$ has complex eigenvalues),\n",
    "we know $\\mathbf H(n)$ is not positive semi definite.  We can use several arguments here.  The simplest is to just use an 'expanded' $\\mathbf z$ from before.  I.e. observe that \n",
    "\n",
    "where \n",
    "\n",
    "$\\mathbf v= \\begin{bmatrix}\n",
    "z_1\\\\ \n",
    "z_2\\\\ \n",
    "\\vdots\\\\ \n",
    "z_{m}\\\\ \n",
    "0\\\\ \n",
    "0\\\\ \n",
    "\\vdots\\\\ \n",
    "0\\\\ \n",
    "\\end{bmatrix}$\n",
    "\n",
    "then it still follows that  \n",
    "\n",
    "$\\mathbf v^H \\big(\\mathbf H(n)\\big) \\mathbf v \\lt 0$ \n",
    "\n",
    "and hence in the expanded complex roots case $\\mathbf H(n)$ is still real symmetric but not positive semi definite.  \n",
    "\n",
    "Thus we conclude that $\\mathbf H(n)$ is positive semi definite **iff** all eigenvalues of $\\mathbf X$ are real.  \n",
    "\n",
    "And as an important special case: if $\\mathbf X$ has $n$ distinct eigenvalues, then we can tighten this to say that our $\\mathbf H(n)$ is positive definite **iff** all eigenvalues of $\\mathbf X$ are real.\n",
    "\n",
    "It is also worth mentioning that we can determine whether $\\mathbf H(m)$ is positive definite without ever wading into eigenvalue finding -- if a real symmetric matrix is positive definite, we can run Cholesky decomposition on it.  And if is is not positive definite, Cholesky will fail.  (Note that in principle Cholesky can be done exactly, whereas finding eigenvalues of polynomials with degree $\\geq 5$ cannot be -- also Cholesky has a low coefficient associated with it.)  The technical nit is that Cholesky will fail on $\\mathbf H(n)$ if said matrix is singular -- though the error message will say it is due to singularity.  The solution is to either find the right sizing for $\\mathbf H(m)$ or do $\\mathbf {LDL}^T$ decomposition instead.\n",
    "\n",
    "**extension: making more granular claims about the above Hankel matrix and eigenvalues of ** $\\mathbf X$    \n",
    "\n",
    "\n",
    "where we use the appropriate permutation matrix $\\mathbf P$ to effect pairwise column swaps in our Vandermonde matrix, to effect: \n",
    "\n",
    "$\\mathbf W^T \\mathbf P = \\mathbf W^H$\n",
    "\n",
    "in particular these these pairwise column swaps relate to conjugate pairs of complex eigenvalues.  Put differently, we are able to 'replicate' the effect to conjugation in our transpose, by right multiplying by the suitable $\\mathbf P$\n",
    "\n",
    "- - - -\n",
    "*special structure in this permutation matrix:*    \n",
    "Since $\\mathbf P$ either leaves each column in its same place, or *only* does a pairwise column swap (with the conjugate of that column -- and recall that by construction this Vandermonde matrix has maximal rank), we know that if we apply it twice, all columns will be back in their original position.  \n",
    "\n",
    "This means we know: $\\mathbf P^2 = \\mathbf I$, i.e. this is an involution.  Further, left multiplying each side by $ \\mathbf P^{-1} $ tells us that \n",
    "\n",
    "$\\mathbf P^{-1} = \\mathbf P^{T} = \\mathbf P^{H} = \\mathbf P$\n",
    "\n",
    "which tells us that $\\mathbf P$ is a real symmetric permutation (orthogonal) matrix.  \n",
    "\n",
    "*A further, unnecessary note:* we could model this problem such that, if we have $k$ unique complex conjugate eigenvalue pairs in $\\mathbf X$ (where order doesn't matter), then we have $m-2k$ unique real eigenvalues, and we could specifically model our problem by putting all unique real eigenvalues at the top of $\\mathbf W$ then have each unique conjugate pair be adjacent to eachother and below whatever row is above it.  This would lead us to : \n",
    "\n",
    "$\\mathbf P = \\begin{bmatrix}\n",
    "\\mathbf I_{m-2k} &  \\mathbf 0 &  \\mathbf 0 &  \\cdots &  \\mathbf 0 \\\\ \n",
    "\\mathbf 0& \\mathbf U &  \\mathbf 0 &  \\cdots &   \\mathbf 0 \\\\ \n",
    " \\mathbf 0&   \\mathbf 0&  \\mathbf U &  \\cdots  &  \\mathbf 0 \\\\ \n",
    " \\mathbf 0&   \\mathbf 0&   \\mathbf 0&  \\ddots &  \\mathbf 0 \\\\ \n",
    " \\mathbf 0&   \\mathbf 0 &  \\mathbf 0&   \\cdots&  \\mathbf U\n",
    "\\end{bmatrix}$\n",
    "\n",
    "where \n",
    "\n",
    "$\\mathbf U = \\begin{bmatrix}\n",
    "0 & 1\\\\ \n",
    "1 & 0\n",
    "\\end{bmatrix}$\n",
    "\n",
    "\n",
    "and for avoidance of doubt $\\mathbf P$ is block diagonal, where there are $k$ instances of $\\mathbf U$ along the diagonal.   Any change from this setup is really just a graph isomorphism.  \n",
    "\n",
    "This additional structure isn't really needed in that the argument works irrespective of how we order the columns in $\\mathbf W$.  However, using this setup with a clean Blocked structure simplifies the argument / visualization of the process quite a bit.  In general taking advantage of blocked matrices is a smart thing to do in your author's view.  \n",
    "\n",
    "- - - -\n",
    "now revisiting the $m$  x  $m$ Hankel matrix\n",
    "\n",
    "$\\mathbf H(m) = \\mathbf W^T Diag\\big(\\mathbf a\\big) \\mathbf W  = \\mathbf W^T \\big(\\mathbf I\\big) Diag\\big(\\mathbf a\\big) \\mathbf W  = \\mathbf W^T \\big(\\mathbf {PP}^H\\big) Diag\\big(\\mathbf a\\big) \\mathbf W  = \\big(\\mathbf W^T \\mathbf P\\big) \\big(\\mathbf P^H Diag\\big(\\mathbf a\\big)\\big) \\mathbf W = \\mathbf W^H  \\big(\\mathbf P Diag\\big(\\mathbf a\\big)\\big) \\mathbf W$\n",
    "\n",
    "As before we'd look at \n",
    "\n",
    "$\\mathbf x^H \\mathbf H(m) \\mathbf x $\n",
    "\n",
    "for $\\mathbf x \\neq \\mathbf 0$  \n",
    "\n",
    "which motivated looking at the spectrum of our real symmetric Hankel matrix.  This extension will do an indirect look via two applications of Sylvester's Law of Intertia-- once now, and once a bit later.  \n",
    "\n",
    "First we use Sylvester's Law of Intertia and recognize that we instead of looking at\n",
    "\n",
    "$\\mathbf x^H \\mathbf W^H  \\big(\\mathbf P Diag\\big(\\mathbf a\\big)\\big) \\mathbf {Wx} $\n",
    "\n",
    "(note: $\\big(\\mathbf P Diag\\big(\\mathbf a\\big)\\big)$ is actually real symmetric, as will be demonstrated in a few lines)  \n",
    "\n",
    "where $\\mathbf W$ is nonsingular, we could instead consider\n",
    "\n",
    "$\\mathbf y:= \\mathbf{Wx}$\n",
    "\n",
    "then look at:  \n",
    "\n",
    "$\\mathbf y^H \\big(\\mathbf P Diag\\big(\\mathbf a\\big)\\big) \\mathbf y $\n",
    "\n",
    "so we are ultimately interested in the quadratic form over: $\\Big( \\mathbf P Diag\\big(\\mathbf a\\big)\\Big)$.  Put differently, we are interested in demonstrating that $\\Big( \\mathbf P Diag\\big(\\mathbf a\\big)\\Big)$ is real symmetric, and then looking at the signs of its eigenvalues.   \n",
    "\n",
    "- - - -\n",
    "We now use some very special structure in the problem.  We know that $ Diag\\big(\\mathbf a\\big)$ has strictly positive entries on its diagonal and zeros elsewhere.  We further know that the positive entries are at least pairwise identical with respect to $\\mathbf X$'s complex eigenvalues which come in conjugate pairs (and hence have same algebraic multiplicities), *and* we know that these are the only values that are being pairwise permuted by $\\mathbf P$.\n",
    "\n",
    "This means that $\\Big( \\mathbf P Diag\\big(\\mathbf a\\big)\\Big)$ must be real symmetric (aka Hermitian over reals).  To be certain all values in $\\Big( \\mathbf P Diag\\big(\\mathbf a\\big)\\Big)$ are real, and zero if not on the diagonal, *except* for the case where we are doing pairwise swaps.  \n",
    "\n",
    "To be sure, suppose $\\Big( \\mathbf P Diag\\big(\\mathbf a\\big)\\Big)$ is not symmetric, then $\\mathbf P\\Big( \\mathbf P Diag\\big(\\mathbf a\\big)\\Big) = \\mathbf I Diag\\big(\\mathbf a\\big) = Diag\\big(\\mathbf a\\big) $ does not have matching algebraic multiplicities for $\\mathbf X$'s complex eigenvalues -- a contradiction.  \n",
    "\n",
    "- - - -\n",
    "Thus, while we know $\\Big( \\mathbf P Diag\\big(\\mathbf a\\big)\\Big)$ is non-singular and real symmetric, we'd like to know how many positive and how many negative eigenvalues it has. For convenience, we define:\n",
    "\n",
    "$\\mathbf S: = Diag\\big(\\mathbf a\\big)^{\\frac{1}{2}}$, again making use of the fact that the algebraic multiplicities contained in $\\mathbf a$ are strictly positive, and $Diag\\big(\\mathbf a\\big)$ is diagonal.  This gives us: \n",
    "\n",
    "$\\mathbf S\\Big( \\mathbf P Diag\\big(\\mathbf a\\big)\\Big) \\mathbf S^{-1}  =  \\mathbf S\\Big( \\mathbf P \\mathbf S^2\\Big) \\mathbf S^{-1} = \\mathbf S \\mathbf P \\mathbf S = \\mathbf S^H \\mathbf P \\mathbf S$ \n",
    "\n",
    "i.e. our real symmetric matrix $\\Big( \\mathbf P Diag\\big(\\mathbf a\\big)\\Big) = \\mathbf P \\mathbf S^2$ is similar to  \n",
    "\n",
    "$ \\mathbf S \\mathbf P \\mathbf S = \\mathbf S^H \\mathbf P \\mathbf S  $\n",
    "\n",
    "when recall that $\\mathbf P$ is itself real symmetric, we can apply Sylvester's Law of Intertia once more and recognize that the the signs of the eigenvalues of $\\mathbf S^H \\mathbf P \\mathbf S$ are in fact given by the eigenvalues of $\\mathbf P$. The number of eigenvalues for $\\mathbf P$ that are $= -1$ correspond *exactly* with the number of pairwise swaps we needed to effect conjugation when transposing $\\mathbf W$, which corresponds *exactly* our original matrix $\\mathbf X$'s number of unique conjugate pairs of complex eigenvalues (where ordering does not matter so we take care to not double count).  \n",
    "\n",
    "Finally, because $\\mathbf S^H \\mathbf P \\mathbf S$ is similar to the matrix we are actually interested in, the real symmetric matrix given by $\\Big( \\mathbf P Diag\\big(\\mathbf a\\big)\\Big)$, we know that $\\Big( \\mathbf P Diag\\big(\\mathbf a\\big)\\Big)$ has the same eigenvalues as $\\mathbf S^H \\mathbf P \\mathbf S$ -- and in particular those two matrices have the same number of negative eigenvalues and same number of positive eigenvalues (and neither matrix has eigenvalues equal to zero or has eigenvalues with non-zero imaginary components.)  \n",
    "\n",
    "Putting this all togther:  what this tells us, is that if we were to do $\\mathbf H(m) = \\mathbf {LDL}^T$, the number $k$ of negative entries in $\\mathbf D$ would tell us exactly the number $k$ of conjugate pairs of complex eigenvalues (where order doesn't matter and we don't double count) in the characteristic polynomial of $\\mathbf X$.  Put differently, we would know that $\\mathbf X$ has $2k$ unique complex eigenvalues, and $m-2k$ unique eigenvalues on the real line.  \n",
    "\n",
    "As before, we could look instead at $\\mathbf H(n) = \\mathbf {LDL}^T$, and the number, $r$, of zero entries in $\\mathbf D$ would tell us that $\\mathbf X$ has $m = n- r$ unique eigenvalues in the characteristic polynomial of $\\mathbf X$ and we have $2k$ unique complex eigenvalues and $m-2k$ unique real eigenvalues.  \n",
    "\n",
    "\n",
    "\n",
    "- - - - \n",
    "# Enter the Companion Matrix \n",
    "\n",
    "Over a complex field, let $\\mathbf C$ be the $n$ x $n$ matrix callled the Companion Matrix\n",
    "\n",
    "$\\mathbf C = \\begin{bmatrix}\n",
    "0 & 0& 0&  \\cdots&  0& -c_o\\\\ \n",
    "1 & 0& 0&  \\cdots&  0& -c_1\\\\ \n",
    "0 & 1& 0&  \\cdots&  0& -c_2\\\\ \n",
    "0 & 0& 1&  \\cdots&  0& -c_3\\\\ \n",
    "\\vdots & \\vdots& \\vdots&  \\ddots&  \\vdots& \\vdots\\\\ \n",
    "0 & 0& 0&  \\cdots& 1 & -c_{n-1} \n",
    "\\end{bmatrix}$\n",
    "\n",
    "Notice that this looks *extremely* familiar -- it is the the Permutation matrix associated with a connected graph, except the final column looks rather different.  This time, in the final column, we have the characteristic polynomial (where $c_n = 1$ and is not shown.)  \n",
    "\n",
    "*comment:  This section sketches out many interesting relations but does not fully prove many of them, as the Companion Matrix originally seemed beyond the scope, though I ultimately could not resist touching on it. *  \n",
    "\n",
    "Now consider the kth left eigenpair $\\lambda_k$, $ \\tilde{\\mathbf w_k^T}$\n",
    "\n",
    "$ \\tilde{\\mathbf w_k^T} \\mathbf C = \\begin{bmatrix}\n",
    "w_{k,2}& w_{k,3} &  w_{k,4}& \\cdots & w_{k,n} & \\sum_{i = 1}^{n}-c_{i-1}w_{k,i}\n",
    "\\end{bmatrix}$\n",
    "\n",
    "by the definition of vector matrix multiplication, and we see\n",
    "\n",
    "$ \\tilde{\\mathbf w_k^T} \\mathbf C = \\lambda_k \\tilde{\\mathbf w_k^T} = \\begin{bmatrix}\n",
    "\\lambda_k w_{k,1} & \\lambda_k w_{k,2} &  \\lambda_k w_{k,3}& \\lambda_k w_{k,4}  & \\cdots & \\lambda_k w_{k,n}\n",
    "\\end{bmatrix}$  \n",
    "by the definition of an eigenvector\n",
    "\n",
    "\n",
    "hence we see\n",
    "\n",
    "$\\tilde{\\mathbf w_k^T} \\mathbf C = \\begin{bmatrix}\n",
    "w_{k,2}& w_{k,3} &  w_{k,4}& \\cdots & w_{k,n} & \\sum_{i = 1}^{n}-c_{i-1}w_{k,i}\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "\\lambda_k w_{k,1} & \\lambda_k w_{k,2} &  \\lambda_k w_{k,3}&  \\cdots & \\lambda_k w_{k,n-1} & \\lambda_k w_{k,n}\n",
    "\\end{bmatrix}$ \n",
    "\n",
    "To solve this, we recognize that we have a base value $w_{k,2} = \\lambda_k w_{k,1}$ we can then forward propagate to see that we know $w_{k,3} = \\lambda_k w_{k,2} = \\lambda_k^2 w_{k,1}$ and so forth, as shown below.   \n",
    "\n",
    "$\\tilde{\\mathbf w_k^T} \\mathbf C = \\begin{bmatrix}\n",
    "w_{k,2}& w_{k,3} &  w_{k,4}& \\cdots & w_{k,n} & \\sum_{i = 1}^{n}-c_{i-1}w_{k,i}\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "\\lambda_k w_{k,1} & \\lambda_k^2 w_{k,1} &  \\lambda_k^3 w_{k,1} & \\cdots & w_{k,1} \\lambda_k^{n-1} &  w_{k,1} \\lambda_k^{n}\n",
    "\\end{bmatrix}$ \n",
    "\n",
    "\n",
    "in general $w_{k,1}$ may be any scalar value, except, as always special care is needed when dealing with zeros.  The case of $w_{k,1} = 0$ is easily dealt with as we see it means the determinant is zero -- most simplisitically the Companion matrix in such a case has all zeros on its top row which means such matrix must be singular, and we use typical nullspace tools in this case.    \n",
    "\n",
    "$\\tilde{\\mathbf w_k^T} \\mathbf C = \\lambda_k \\begin{bmatrix}\n",
    " w_{k,1} &  w_{k,2} &  w_{k,3}&  \\cdots &  w_{k,n-1} &  w_{k,n}\n",
    "\\end{bmatrix} = w_{k,1} \\lambda_k \\begin{bmatrix}\n",
    "1 & \\lambda_k &  \\lambda_k^2 & \\cdots & \\lambda_k^{n-2} & \\lambda_k^{n-1}\n",
    "\\end{bmatrix}$ \n",
    "\n",
    "i.e. we have: \n",
    "\n",
    "$\\begin{bmatrix}\n",
    " w_{k,1} &  w_{k,2} &  w_{k,3}&  \\cdots &  w_{k,n-1} &  w_{k,n}\n",
    "\\end{bmatrix} = w_{k,1} \\begin{bmatrix}\n",
    "1 & \\lambda_k &  \\lambda_k^2 & \\cdots & \\lambda_k^{n-2} & \\lambda_k^{n-1}\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "1 & \\lambda_k &  \\lambda_k^2 & \\cdots & \\lambda_k^{n-2} & \\lambda_k^{n-1}\n",
    "\\end{bmatrix}$ \n",
    "\n",
    "where $w_{k,1} = 1$\n",
    "\n",
    "note that this implies $\\sum_{i = 1}^{n}-c_{i-1}w_{k,i} = \\lambda_k^{n}$, which makes sense -- we in effect are putting in $-p(\\lambda_k) = 0$, except the degree n term is missing, and if we add it to each side of the equation, while evaluating at $\\lambda_k$, we get $\\sum_{i = 1}^{n}-c_{i-1}w_{k,i} = \\lambda_k^n + -p(\\lambda_k) = \\lambda_k^n$  \n",
    "\n",
    "Again, recalling our Vandermonde matrix,\n",
    "\n",
    "$\\mathbf W = \\begin{bmatrix}\n",
    "1 & \\lambda_1 & \\lambda_1^2 & \\dots  & \\lambda_1^{n-1}\\\\ \n",
    "1 & \\lambda_2 & \\lambda_2^2 & \\dots &  \\lambda_2^{n-1} \\\\ \n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\\\ \n",
    "1 & \\lambda_{n} & \\lambda_{n}^{2} & \\dots  & \\lambda_{n}^{n-1}\n",
    "\\end{bmatrix}$\n",
    "\n",
    "we collect these relationships over all $n$ eigenpairs, and see\n",
    "\n",
    "$\\mathbf{WC} = \\mathbf {\\Lambda W}$\n",
    "\n",
    "If the eigenvectors are linearly independent, then the companion matrix is diagonalizable, and hence we have:  \n",
    "\n",
    "$\\mathbf{C} = \\mathbf W^{-1}\\mathbf {\\Lambda  W}$\n",
    "\n",
    "**remarks**   \n",
    "It is interesting to note that $\\mathbf W^{-1}$ exists **iff** all $\\lambda_k$ are unique.  If we recall, much earlier on, we proved that an $n$ x $n$ matrix is always diagonalizable if all of its eigenvalues are unique (but it may be diagonalizable, despite repeated eigenvalues, for orther reasons too -- e.g. it is always the diagonalizable if the matrix is normal, or we may just be fortunate as there are non-defective matrices that have repeated eigenvalues), but $\\mathbf C$ is much more brittle and *always* defective unless all of its eigenvalues are unique. (There are some deeper ties in here with minimal polynomials as well.)  \n",
    "\n",
    "\n",
    "\n",
    "Of course, over a complex numbers field, the Companion matrix, like any other $n$ x $n$ matrix is still similar to an upper triangular matrix.  \n",
    "\n",
    "Recalling our earlier use of the permutation matrix for a connected graph, it is interesting to think about \n",
    "\n",
    "$\\mathbf P = \\begin{bmatrix}\n",
    "0 & 0& 0&  \\cdots&  0& -c_o\\\\ \n",
    "1 & 0& 0&  \\cdots&  0& 0\\\\ \n",
    "0 & 1& 0&  \\cdots&  0& 0\\\\ \n",
    "0 & 0& 1&  \\cdots&  0& 0\\\\ \n",
    "\\vdots & \\vdots& \\vdots&  \\ddots&  \\vdots& \\vdots\\\\ \n",
    "0 & 0& 0&  \\cdots& 1 & 0 \n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "0 & 0& 0&  \\cdots&  0& 1\\\\ \n",
    "1 & 0& 0&  \\cdots&  0& 0\\\\ \n",
    "0 & 1& 0&  \\cdots&  0& 0\\\\ \n",
    "0 & 0& 1&  \\cdots&  0& 0\\\\ \n",
    "\\vdots & \\vdots& \\vdots&  \\ddots&  \\vdots& \\vdots\\\\ \n",
    "0 & 0& 0&  \\cdots& 1 & 0 \n",
    "\\end{bmatrix}$\n",
    "\n",
    "which is to say that we were looking at the characteristic polynomial of $\\lambda^n + 0 + 0 + .... + 0 + c_0 = \\lambda^n - 1$, i.e. the two matrices are the same if $c_0 = -1$ and $c_r = 0$ for $r = \\{2, 3, ..., n-1\\}$.  Given that we ultimately found distinct roots of unity as the eigenvalues, this is perhaps not a surprise.  \n",
    "\n",
    "Using the companion matrix, we now offer a high level sketch of recovering the characteristic polynomial via using $trace\\big(\\mathbf C^k \\big)$ for $k = \\{1, 2, 3, ..., n\\}$.  \n",
    "\n",
    "The ideas underlying this approach should seem quite intuitive, though the exact proof seems a bit too tedious and hence is omitted.  \n",
    "\n",
    "*general idea: *   \n",
    "\n",
    "Thus far, we've shown that $\\mathbf C$ with respect to unique eigenvalues, they must be roots of the characteristic polynomial given in its right most column.  To verify that the algebraic multiplicities are intact, we'd calculate $det\\big(\\mathbf C - \\lambda \\mathbf I\\big)$, use induction with Laplace Expansion and confirm that $\\mathbf C$ itself has the characteristic polynomial given in its right most column.  \n",
    "\n",
    "From here we consider our problem where we have two $n$ x $n$ matrices, $\\mathbf X$ and $\\mathbf Y$, and we know their traces are the same over some interval -- i.e. $trace\\big(\\mathbf X^k\\big) = trace\\big(\\mathbf Y^k\\big)$ for $k = \\{1, 2, 3, ..., 2n-1\\}$.  We've already proved that these two matrices have the same eigenvalues with same algebraic multiplicities -- i.e. the same characteristic polynomial.  \n",
    "\n",
    "We can further extend this, and endcode that characteristic polynomial in an $n$ x $n$ companion matrix. Hence \n",
    "\n",
    "$trace\\big(\\mathbf C^k\\big) = trace\\big(\\mathbf X^k\\big) = trace\\big(\\mathbf Y^k\\big)$ for $k = \\{1, 2, 3, ..., 2n-1\\}$\n",
    "\n",
    "for illustrative purposes, consider the case where $n = 5$\n",
    "\n",
    "Thus we have \n",
    "\n",
    "$\\mathbf C^1 = \\left[\\begin{matrix}0 & 0 & 0 & 0 & - c_{0}\\\\1 & 0 & 0 & 0 & - c_{1}\\\\0 & 1 & 0 & 0 & - c_{2}\\\\0 & 0 & 1 & 0 & - c_{3}\\\\0 & 0 & 0 & 1 & - c_{4}\\end{matrix}\\right]$\n",
    "\n",
    "$trace\\big(\\mathbf C^1\\big) = -c_{4} = -c_{n-1}$ \n",
    "\n",
    "which is what we'd expect, as it is possible to define the trace as the negative coefficient of second highest term of the characteristic polynomial (in monic form).  \n",
    "\n",
    "$\\mathbf C^2 = \\left[\\begin{matrix}0 & 0 & 0 & - c_{0} & c_{0} c_{4}\\\\0 & 0 & 0 & - c_{1} & - c_{0} + c_{1} c_{4}\\\\1 & 0 & 0 & - c_{2} & - c_{1} + c_{2} c_{4}\\\\0 & 1 & 0 & - c_{3} & - c_{2} + c_{3} c_{4}\\\\0 & 0 & 1 & - c_{4} & - c_{3} + c_{4}^{2}\\end{matrix}\\right]$\n",
    "\n",
    "$trace\\big(\\mathbf C^2\\big) =- 2 c_{3} + c_{4}^{2}$\n",
    "\n",
    "notice there is one new term here and one old one. After computing $trace\\big(\\mathbf C^2\\big)$, we may extract the value for $c_3$ in terms of the the value of $c_4$ which was given when we computed $trace\\big(\\mathbf C^1\\big)$\n",
    "\n",
    "\n",
    "$\\mathbf C^3 = \\left[\\begin{matrix}0 & 0 & - c_{0} & c_{0} c_{4} & c_{0} c_{3} - c_{0} c_{4}^{2}\\\\0 & 0 & - c_{1} & - c_{0} + c_{1} c_{4} & c_{1} c_{3} - c_{4} \\left(- c_{0} + c_{1} c_{4}\\right)\\\\0 & 0 & - c_{2} & - c_{1} + c_{2} c_{4} & - c_{0} + c_{2} c_{3} - c_{4} \\left(- c_{1} + c_{2} c_{4}\\right)\\\\1 & 0 & - c_{3} & - c_{2} + c_{3} c_{4} & - c_{1} + c_{3}^{2} - c_{4} \\left(- c_{2} + c_{3} c_{4}\\right)\\\\0 & 1 & - c_{4} & - c_{3} + c_{4}^{2} & - c_{2} + c_{3} c_{4} - c_{4} \\left(- c_{3} + c_{4}^{2}\\right)\\end{matrix}\\right]$\n",
    "\n",
    "$trace\\big(\\mathbf C^3\\big) = - 3 c_{2} + 2 c_{3} c_{4} - c_{4} \\left(- c_{3} + c_{4}^{2}\\right)$\n",
    "\n",
    "We start to notice that while the the matrix (and even the trace expressions) are getting complicated, the sub-problems are nicely overlapping in the traces -- we can compute $trace\\big(\\mathbf C^3\\big)$ and then solve for $c_2$ since it is the only new term in here. (Also notice that each time a new term is introduced it is in a simple form -- i.e. just scaled by some real valued and added to some old values we already know -- there is no possibility of multiple answers like in the case of having a, say, squared term $c_2$ that we need to take a square root to find.)\n",
    "\n",
    "$\\mathbf C^4 = \\left[\\begin{matrix}0 & - c_{0} & c_{0} c_{4} & c_{0} c_{3} - c_{0} c_{4}^{2} & c_{0} c_{2} - c_{0} c_{3} c_{4} - c_{4} \\left(c_{0} c_{3} - c_{0} c_{4}^{2}\\right)\\\\0 & - c_{1} & - c_{0} + c_{1} c_{4} & c_{1} c_{3} - c_{4} \\left(- c_{0} + c_{1} c_{4}\\right) & c_{1} c_{2} - c_{3} \\left(- c_{0} + c_{1} c_{4}\\right) - c_{4} \\left(c_{1} c_{3} - c_{4} \\left(- c_{0} + c_{1} c_{4}\\right)\\right)\\\\0 & - c_{2} & - c_{1} + c_{2} c_{4} & - c_{0} + c_{2} c_{3} - c_{4} \\left(- c_{1} + c_{2} c_{4}\\right) & c_{2}^{2} - c_{3} \\left(- c_{1} + c_{2} c_{4}\\right) - c_{4} \\left(- c_{0} + c_{2} c_{3} - c_{4} \\left(- c_{1} + c_{2} c_{4}\\right)\\right)\\\\0 & - c_{3} & - c_{2} + c_{3} c_{4} & - c_{1} + c_{3}^{2} - c_{4} \\left(- c_{2} + c_{3} c_{4}\\right) & - c_{0} + c_{2} c_{3} - c_{3} \\left(- c_{2} + c_{3} c_{4}\\right) - c_{4} \\left(- c_{1} + c_{3}^{2} - c_{4} \\left(- c_{2} + c_{3} c_{4}\\right)\\right)\\\\1 & - c_{4} & - c_{3} + c_{4}^{2} & - c_{2} + c_{3} c_{4} - c_{4} \\left(- c_{3} + c_{4}^{2}\\right) & - c_{1} + c_{2} c_{4} - c_{3} \\left(- c_{3} + c_{4}^{2}\\right) - c_{4} \\left(- c_{2} + c_{3} c_{4} - c_{4} \\left(- c_{3} + c_{4}^{2}\\right)\\right)\\end{matrix}\\right]$\n",
    "\n",
    "$trace\\big(\\mathbf C^4\\big) = - 4 c_{1} + 2 c_{2} c_{4} + c_{3}^{2} - c_{3} \\left(- c_{3} + c_{4}^{2}\\right) - c_{4} \\left(- c_{2} + c_{3} c_{4}\\right) - c_{4} \\left(- c_{2} + c_{3} c_{4} - c_{4} \\left(- c_{3} + c_{4}^{2}\\right)\\right)$\n",
    "\n",
    "Again, if we computed the earlier traces (and memo-ized our results for $c_r$ for $r = \\{2, 3, 4\\}$) we can uniquely solve for $c_1$ after computing $trace\\big(\\mathbf C^4\\big)$\n",
    "\n",
    "$\\mathbf C^5 = \\left[\\begin{matrix}- c_{0} & c_{0} c_{4} & c_{0} c_{3} - c_{0} c_{4}^{2} & c_{0} c_{2} - c_{0} c_{3} c_{4} - c_{4} \\left(c_{0} c_{3} - c_{0} c_{4}^{2}\\right) & c_{0} c_{1} - c_{0} c_{2} c_{4} - c_{3} \\left(c_{0} c_{3} - c_{0} c_{4}^{2}\\right) - c_{4} \\left(c_{0} c_{2} - c_{0} c_{3} c_{4} - c_{4} \\left(c_{0} c_{3} - c_{0} c_{4}^{2}\\right)\\right)\\\\- c_{1} & - c_{0} + c_{1} c_{4} & c_{1} c_{3} - c_{4} \\left(- c_{0} + c_{1} c_{4}\\right) & c_{1} c_{2} - c_{3} \\left(- c_{0} + c_{1} c_{4}\\right) - c_{4} \\left(c_{1} c_{3} - c_{4} \\left(- c_{0} + c_{1} c_{4}\\right)\\right) & c_{1}^{2} - c_{2} \\left(- c_{0} + c_{1} c_{4}\\right) - c_{3} \\left(c_{1} c_{3} - c_{4} \\left(- c_{0} + c_{1} c_{4}\\right)\\right) - c_{4} \\left(c_{1} c_{2} - c_{3} \\left(- c_{0} + c_{1} c_{4}\\right) - c_{4} \\left(c_{1} c_{3} - c_{4} \\left(- c_{0} + c_{1} c_{4}\\right)\\right)\\right)\\\\- c_{2} & - c_{1} + c_{2} c_{4} & - c_{0} + c_{2} c_{3} - c_{4} \\left(- c_{1} + c_{2} c_{4}\\right) & c_{2}^{2} - c_{3} \\left(- c_{1} + c_{2} c_{4}\\right) - c_{4} \\left(- c_{0} + c_{2} c_{3} - c_{4} \\left(- c_{1} + c_{2} c_{4}\\right)\\right) & c_{1} c_{2} - c_{2} \\left(- c_{1} + c_{2} c_{4}\\right) - c_{3} \\left(- c_{0} + c_{2} c_{3} - c_{4} \\left(- c_{1} + c_{2} c_{4}\\right)\\right) - c_{4} \\left(c_{2}^{2} - c_{3} \\left(- c_{1} + c_{2} c_{4}\\right) - c_{4} \\left(- c_{0} + c_{2} c_{3} - c_{4} \\left(- c_{1} + c_{2} c_{4}\\right)\\right)\\right)\\\\- c_{3} & - c_{2} + c_{3} c_{4} & - c_{1} + c_{3}^{2} - c_{4} \\left(- c_{2} + c_{3} c_{4}\\right) & - c_{0} + c_{2} c_{3} - c_{3} \\left(- c_{2} + c_{3} c_{4}\\right) - c_{4} \\left(- c_{1} + c_{3}^{2} - c_{4} \\left(- c_{2} + c_{3} c_{4}\\right)\\right) & c_{1} c_{3} - c_{2} \\left(- c_{2} + c_{3} c_{4}\\right) - c_{3} \\left(- c_{1} + c_{3}^{2} - c_{4} \\left(- c_{2} + c_{3} c_{4}\\right)\\right) - c_{4} \\left(- c_{0} + c_{2} c_{3} - c_{3} \\left(- c_{2} + c_{3} c_{4}\\right) - c_{4} \\left(- c_{1} + c_{3}^{2} - c_{4} \\left(- c_{2} + c_{3} c_{4}\\right)\\right)\\right)\\\\- c_{4} & - c_{3} + c_{4}^{2} & - c_{2} + c_{3} c_{4} - c_{4} \\left(- c_{3} + c_{4}^{2}\\right) & - c_{1} + c_{2} c_{4} - c_{3} \\left(- c_{3} + c_{4}^{2}\\right) - c_{4} \\left(- c_{2} + c_{3} c_{4} - c_{4} \\left(- c_{3} + c_{4}^{2}\\right)\\right) & - c_{0} + c_{1} c_{4} - c_{2} \\left(- c_{3} + c_{4}^{2}\\right) - c_{3} \\left(- c_{2} + c_{3} c_{4} - c_{4} \\left(- c_{3} + c_{4}^{2}\\right)\\right) - c_{4} \\left(- c_{1} + c_{2} c_{4} - c_{3} \\left(- c_{3} + c_{4}^{2}\\right) - c_{4} \\left(- c_{2} + c_{3} c_{4} - c_{4} \\left(- c_{3} + c_{4}^{2}\\right)\\right)\\right)\\end{matrix}\\right]$\n",
    "\n",
    "$trace\\big(\\mathbf C^5\\big) = - 5 c_{0} + 2 c_{1} c_{4} + 2 c_{2} c_{3} - c_{2} \\left(- c_{3} + c_{4}^{2}\\right) - c_{3} \\left(- c_{2} + c_{3} c_{4}\\right) - c_{3} \\left(- c_{2} + c_{3} c_{4} - c_{4} \\left(- c_{3} + c_{4}^{2}\\right)\\right) - c_{4} \\left(- c_{1} + c_{2} c_{4}\\right) - c_{4} \\left(- c_{1} + c_{3}^{2} - c_{4} \\left(- c_{2} + c_{3} c_{4}\\right)\\right) - c_{4} \\left(- c_{1} + c_{2} c_{4} - c_{3} \\left(- c_{3} + c_{4}^{2}\\right) - c_{4} \\left(- c_{2} + c_{3} c_{4} - c_{4} \\left(- c_{3} + c_{4}^{2}\\right)\\right)\\right) $\n",
    "\n",
    "And once again the subproblems in terms of traces nicely overlap --despite a horror inducing matrix given by $\\mathbf C^5$.  We can also uniquely find $c_0$ here just from having $trace\\big(\\mathbf C^5\\big)$ as well as the results from $trace\\big(\\mathbf C^r\\big)$ for $ r = \\{1,2, 3, 4\\}$.  \n",
    "\n",
    "**What's the point?**  \n",
    "\n",
    "What the above example aimed to show, though *not prove*, is that we actually have the option of early stopping. Put differently, we knew that if we had\n",
    "\n",
    "$trace\\big(\\mathbf C^k\\big) = trace\\big(\\mathbf X^k\\big) = trace\\big(\\mathbf Y^k\\big)$ for $k = \\{1, 2, 3, ..., 2n-1\\}$\n",
    "\n",
    "then we could conlcude that all three $n$ by $n$ matrices had the same characteristic polynomial (and hence same eigenvalues with same algebraic multiplicities).  \n",
    "\n",
    "But if we are clever (and patient) enough, we can actually uniquely recover the characteristic polynomial of a matrix over just 'one cycle', i.e. \n",
    "\n",
    "$trace\\big(\\mathbf C^k\\big) = trace\\big(\\mathbf X^k\\big) = trace\\big(\\mathbf Y^k\\big)$ for $k = \\{1, 2, 3, ..., n\\}$\n",
    "\n",
    "Thus we could tighten our earlier result of showing two $n$ x $n$ matrices have same eigenvalues if they have the same trace over a certain number of iterations -- and decrease the required iterations from $2n -1 $ to  $n$.  \n",
    "\n",
    "The companion matrix was originally going to be omitted in this writeup, but because its eigenvectors are given in the Vandermonde matrix, and it gives some new insights on information from traces, it was included, though again, certain proofs have been omitted due to tedium. \n",
    "\n",
    "# A Much simpler look at the Circulant matrices, and the Discrete Fourier Tranform\n",
    "\n",
    "where\n",
    "\n",
    "$\\mathbf P = \\begin{bmatrix}\n",
    "0 & 0 &0 & \\cdots &0  & 1\\\\\n",
    "1 & 0 &0 &\\cdots &0  & 0\\\\\n",
    "0 & 1 &0 & \\cdots &0  &0 \\\\\n",
    "0 & 0 &1& \\cdots &0  & 0\\\\\n",
    "0 & 0 &0& \\ddots &0  & 0\\\\\n",
    "0 & 0 &0& \\cdots &1  & 0\n",
    "\\end{bmatrix}$\n",
    "\n",
    "any Circulant matrix can be written as:  \n",
    "\n",
    "$\\mathbf S = \\sum_{k=0}^{n-1} s_k \\mathbf P^k = \\begin{bmatrix}\n",
    "s_0 & s_{n-1} & s_{n-2} & \\dots & s_2 & s_1 \\\\ \n",
    "s_1 & s_0 & s_{n-1} & \\dots & s_3 & s_2 \\\\ \n",
    "s_2 & s_1 & s_0 & \\dots & s_4 & s_3 \\\\\n",
    "\\vdots & \\vdots  & \\vdots &\\ddots & \\vdots & \\vdots\\\\ \n",
    "s_{n-2} & s_{n-3} & s_{n-4} & \\dots & s_0  & s_{n-1} \\\\ \n",
    "s_{n-1} & s_{n-2}  & s_{n-3} & \\dots & s_1 &  s_0\n",
    "\\end{bmatrix}$     \n",
    "\n",
    "\n",
    "$\\mathbf P$ can be interpreted as a permutation matrix (that is associated with a connected graph).  This means it is unitarily diagonalizable and all of its eigenvalues have magnitude 1.\n",
    "\n",
    "$\\mathbf P$ can further be interpreted as a Companion matrix associated with $p(x) = x^n - 1$, and hence its eigenvalues are nth roots of unity.  Hence $\\mathbf P= \\mathbf F \\mathbf \\Lambda \\mathbf F^H $, where $\\mathbf \\Lambda$ is a diagonal matrix that contains nth roots of unity, and $\\mathbf F$ is the unitary version of the Vandermonde matrix, i.e. Discrete Fourier Transform matrix.\n",
    "\n",
    "The fact that $\\mathbf P$ is a companion matrix associated with $p(x) = x^n - 1$, and a permutation (read: normal) matrix, and $\\mathbf P^k$ serves as a basis for generating circulant matrices -- the combination of those three things gives us the interesting behavior associated with Circulant matrices.\n",
    "\n",
    "Using all of the above we can re-write things as:\n",
    "\n",
    "$\\mathbf S = \\sum_{k=0}^{n-1} s_k \\mathbf P^k = \\sum_{k=0}^{n-1} s_k \\big(\\mathbf F \\mathbf \\Lambda \\mathbf F^H\\big)^k = \\sum_{k=0}^{n-1} s_k \\mathbf F \\mathbf \\Lambda^k \\mathbf F^H = \\sum_{k=0}^{n-1} \\mathbf F \\big(s_k \\mathbf \\Lambda^k\\big) \\mathbf F^H =  \\mathbf F\\big(\\sum_{k=0}^{n-1} s_k \\mathbf \\Lambda^k\\big) \\mathbf F^H$\n",
    "\n",
    "# Enter Newton's Identities\n",
    "\n",
    "this derivation is the natural extension of the above worked example of traces on the Companion Matrix. \n",
    "\n",
    "Start with the characteristic polynomial for some $n$ x $n$ matrix $\\mathbf C$, shown below in factored and expanded form. In general, the field for this problem is $\\mathbb C$.  \n",
    "\n",
    "As in the method of moments case (i.e. Hankel matrix case), we denote\n",
    "\n",
    "$trace\\big(\\mathbf C^k\\big) = s_k = \\lambda_1^k + \\lambda_2^k + ... + \\lambda_n^k$\n",
    "\n",
    "*notational convention note:*  \n",
    "and we use the convention that $\\mathbf C^0 = \\mathbf I$ and thus $s_0 = n$.  Note: even if $\\mathbf C$ is singular this, as in the moments section, still holds.  The benefit of this approach is it simplifies notation quite a bit and allows us to see patterns more easily.  With a small bit of juggling, we could separately write $\\mathbf I$ as was done in the Cayley Hamilton section and not directly reference $s_0$, etc. but this approach simplifies things a good bit.  And of course if we wanted a fancier approach, we may note that $\\big(\\mathbf C + \\delta \\mathbf I\\big)^0 = \\mathbf I$ for any $ 0 \\lt \\delta \\neq \\{\\big \\vert\\lambda_1 \\big \\vert, \\big \\vert\\lambda_2 \\big \\vert, ..., \\big \\vert\\lambda_n \\big \\vert\\}$ though this is overkill as we're really just using a helpful notational simplification and convention here.   \n",
    "*return to the proof*\n",
    "\n",
    "The characteristic polynomial is the same whether it is factored or expanded.  Here it is in both forms. \n",
    "\n",
    "in factored form:  \n",
    "$p(x) = (x-\\lambda_1)(x-\\lambda_2)(x - \\lambda_3)...(x-\\lambda_n)$\n",
    "\n",
    "or in expanded form:  \n",
    "$p(x) = a_n x^n + a_{n-1}x^{n-1} + a_{n-2}x^{n-2} +... + a_{1}x^{1}+ a_0 = x^n + a_{n-1}x^{n-1} + a_{n-2}x^{n-2} +... + a_{1}x^{1}+ a_0$\n",
    "\n",
    "Where we have the characteristic polynomical in monic form, hence $a_n = 1$ and it may be omitted for the $x^n$ term.\n",
    "\n",
    "\n",
    "- - - - \n",
    "**important convergence consideration:** \n",
    "$x$ may be any number where $\\big \\vert x \\big \\vert \\gt max\\{\\big \\vert\\lambda_1 \\big \\vert, \\big \\vert\\lambda_2 \\big \\vert, ..., \\big \\vert\\lambda_n \\big \\vert\\}$\n",
    "\n",
    "for simplicity, we may select $x$ to be a real, positive number where $x \\gt max\\{\\big \\vert\\lambda_1 \\big \\vert, \\big \\vert\\lambda_2 \\big \\vert, ..., \\big \\vert\\lambda_n \\big \\vert\\}$\n",
    "\n",
    "- - - - \n",
    "\n",
    "**The key idea: **\n",
    "the derivative of the characteristic polynomial with respect to $x$ is the same, whether we do it on the factored form or the expanded form.\n",
    "\n",
    "*First: we take the derivative of the factored form with respect to x*\n",
    "\n",
    "using the product rule we see:\n",
    "\n",
    "$p'(x) = (x-\\lambda_2)(x-\\lambda_3)...(x-\\lambda_n) + (x-\\lambda_1)(x-\\lambda_3)...(x-\\lambda_n) + ... + (x-\\lambda_1)(x-\\lambda_2)(x-\\lambda_3)...(x-\\lambda_{n-1})$  \n",
    "$p'(x) = p(x)\\big(\\frac{1}{x - \\lambda_1} + \\frac{1}{x - \\lambda_2}+ \\frac{1}{x - \\lambda_3} +... + \\frac{1}{x - \\lambda_n}\\big)$\n",
    "\n",
    "*alternatively: consider the logarithm form, and use the chain rule:*\n",
    "\n",
    "$log(p(x)) = \\sum_{i=1}^{n} log(x - \\lambda_i)$\n",
    "\n",
    "differentiate with respect to $x$\n",
    "\n",
    "$\\frac{1}{p(x)}p'(x) = \\sum_{i=1}^{n} \\frac{1}{(x - \\lambda_i)}$\n",
    "\n",
    "$p'(x) = p(x)\\sum_{i=1}^{n} \\frac{1}{(x - \\lambda_i)} = p(x)\\big(\\frac{1}{x - \\lambda_1} + \\frac{1}{x - \\lambda_2}+ \\frac{1}{x - \\lambda_3} +... + \\frac{1}{x - \\lambda_n}\\big)$\n",
    "\n",
    "*Second: calculate* $p'(x)$ *using the expanded form of the polynomial*  \n",
    "\n",
    "$p'(x) = n x^{n-1} + (n-1) a_{n-1}x^{n-2} + (n-2) a_{n-2}x^{n-3} +... + a_{1}$\n",
    "\n",
    "because $p'(x) = p'(x)$, we have:  \n",
    "\n",
    "$n x^{n-1} + (n-1) a_{n-1}x^{n-2} + (n-2) a_{n-2}x^{n-3} +... + a_{1} = p(x)\\sum_{i=1}^{n} \\frac{1}{(x - \\lambda_i)}$\n",
    "\n",
    "now we spend some time examining the term inside the summation on the right hand side:\n",
    "\n",
    "$\\frac{1}{(x - \\lambda_i)} = \\frac{\\frac{1}{x}}{\\frac{1}{x}}  \\frac{1}{(x - \\lambda_i)} =  \\frac{1}{x}\\Big( \\frac{1}{(1 - \\frac{\\lambda_i}{x})}\\Big)$\n",
    "\n",
    "we confirm that $\\Big \\vert \\frac{\\lambda_i}{x}\\Big \\vert \\lt 1$ and hence we are in the radius of convergence of the geometric series, which we now expand.   \n",
    "\n",
    "\n",
    "$\\Big(\\frac{1}{(x - \\lambda_i)}\\Big) = \\frac{1}{x}\\Big( \\frac{1}{(1 - \\frac{\\lambda_i}{x})}\\Big)= \\frac{1}{x}\\Big(1 + \\frac{\\lambda_i}{x} + \\frac{\\lambda_i^2}{x^2} + \\frac{\\lambda_i^3}{x^3} + ...  \\Big) = \\Big( \\frac{1}{x} + \\frac{\\lambda_i}{x^2} + \\frac{\\lambda_i^2}{x^3} + \\frac{\\lambda_i^3}{x^4} + ...  \\Big)$\n",
    "\n",
    "\n",
    "hence we have:\n",
    "\n",
    "$p'(x) =n x^{n-1} + (n-1) a_{n-1}x^{n-2} + (n-2) a_{n-2}x^{n-3} +... + a_{1}  =  p(x)\\sum_{i=1}^{n} \\frac{1}{(x - \\lambda_i)} = p(x) \\sum_{i=1}^{n}\\Big( \\frac{1}{x} + \\frac{\\lambda_i}{x^2} + \\frac{\\lambda_i^2}{x^3} + \\frac{\\lambda_i^3}{x^4} + ...  \\Big)$\n",
    "\n",
    "Now, we use an approach that is quite similar to the way Variance was solved for in the second part of  \"markov_chains_absorbing_state_recut.ipynb\" (in the Probability folder).  \n",
    "\n",
    "Specifically, we are once again using something in the radius of convergence of a geometric series, and this time we have a table with $n$ rows and countably infinite columns.\n",
    "\n",
    "$\\left.\\begin{matrix}\n",
    "Line_{1}=  & \\frac{1}{x} + &   \\frac{\\lambda_1}{x^2} + & \\frac{\\lambda_1^2}{x^3}  + & \\frac{\\lambda_1^3}{x^4}+ & ...\\\\\n",
    "Line_{2}= & \\frac{1}{x}+ &  \\frac{\\lambda_2}{x^2} +& \\frac{\\lambda_2^2}{x^3} +  & \\frac{\\lambda_2^3}{x^4}+ & ... \\\\\n",
    "Line_{3}= & \\frac{1}{x}+ &   \\frac{\\lambda_3}{x^2}+ & \\frac{\\lambda_3^2}{x^3} + & \\frac{\\lambda_3^3}{x^4}+ & ...\\\\\n",
    "\\vdots  & \\vdots+ &  \\vdots+ & \\vdots + & \\vdots+ & \\ddots \\\\\n",
    "Line_{n}= & \\frac{1}{x}+ &  \\frac{\\lambda_n}{x^2}+ & \\frac{\\lambda_n^2}{x^3}  + & \\frac{\\lambda_n^3}{x^4}+ & ...\\\\\n",
    "\\end{matrix}\\right.$\n",
    "\n",
    "\n",
    "Thus we are left with   \n",
    "$p'(x) =  p(x)\\sum_{i=1}^{n} \\frac{1}{(x - \\lambda_i)} = p(x) \\big(Line_1 + Line_2 + Line_3 + ... + Line_n\\big)$\n",
    "\n",
    "however if we interchange the way we evaluate this infinite table, from one row at a time to one column at a time we see that \n",
    "\n",
    "$p'(x) =  p(x) \\big(Line_1 + Line_2 + Line_3 + ... + Line_n\\big) = p(x)\\big(\\frac{s_0}{x} + \\frac{s_1}{x^2} + \\frac{s_2}{x^3} + \\frac{s_3}{x^4} + ...\\big)$ \n",
    "\n",
    "$p'(x) = \\big(x^n + a_{n-1}x^{n-1} + a_{n-2}x^{n-2} +... + a_{1}x^{1}+ a_0\\big)\\big(\\frac{s_0}{x} + \\frac{s_1}{x^2} + \\frac{s_2}{x^3} + \\frac{s_3}{x^4} + ...\\big)$\n",
    "\n",
    "and we now expand this series into a table with $n + 1$ columns and a countably infinite number of rows. \n",
    "\n",
    "For avoidance of doubt, we first multiply $x^n$ by each term in the infinite series, and that is column one.  Then we multiply $a_{n-1}x^{n-1}$ by each term in the infinite series, and that is column 2, and so on until we multiply $a_0$ by every term in the infinite series and that is the final $(n+1)^{th}$ column. \n",
    "\n",
    "$\\left.\\begin{matrix}\n",
    "\\frac{x^n s_0}{x} + &  a_{n-1}x^{n-1}\\frac{s_0}{x}   + & a_{n-2}x^{n-2}\\frac{s_0}{x}  + & \\cdots + & a_{0}x^0\\frac{s_0}{x}\\\\\n",
    "\\frac{x^n s_1}{x^2} + &  a_{n-1}x^{n-1}\\frac{s_1}{x^2}  +& a_{n-2}x^{n-2}\\frac{s_1}{x^2} +  & \\cdots + & a_{0}x^0\\frac{s_1}{x^2}\\\\\n",
    " \\frac{x^n s_2}{x^3} + &   a_{n-1}x^{n-1}\\frac{s_2}{x^3} + & a_{n-2}x^{n-2}\\frac{s_2}{x^3} + & \\cdots + & a_{0}x^0\\frac{s_2}{x^3}\\\\\n",
    "\\frac{x^n s_3}{x^4} + &   a_{n-1}x^{n-1}\\frac{s_3}{x^4}+ &  a_{n-2}x^{n-2}\\frac{s_3}{x^4} + & \\cdots + & a_{0}x^0\\frac{s_3}{x^4}\\\\\n",
    "\\vdots  & \\vdots &  \\vdots & \\vdots  & \\vdots  \\\\\n",
    "\\end{matrix}\\right.$\n",
    "\n",
    "in some respects, the patterns are most clear above, but in others respects they are more clear in the below, simplified table:  \n",
    "\n",
    "$\\left.\\begin{matrix}\n",
    "x^{n-1} s_0 + &  a_{n-1}x^{n-2}s_0   + & a_{n-2}x^{n-3}s_0  + & \\cdots + & a_{0}x^{-1} s_0\\\\\n",
    "x^{n-2} s_1 + &  a_{n-1}x^{n-3}s_1  + & a_{n-2}x^{n-4}s_1 +  & \\cdots + & a_{0}x^{-2}s_1\\\\\n",
    " x^{n-3} s_2 + &   a_{n-1}x^{n-4}s_2 + & a_{n-2}x^{n-5}s_2 + & \\cdots + & a_{0}x^{-3}s_2\\\\\n",
    "x^{n-4} s_3 + &   a_{n-1}x^{n-5}s_3 + &  a_{n-2}x^{n-6}s_3 + & \\cdots + & a_{0}x^{-4}s_3\\\\\n",
    "\\vdots  & \\vdots &  \\vdots & \\vdots  & \\vdots  \\\\\n",
    "\\end{matrix}\\right.$\n",
    "\n",
    "Notice that the above table is reminiscent of a Hankel matrix, as there is a special pattern along the anti-diagonal entries.  Specifically notice:\n",
    "\n",
    "terms with degree $x^{n-1}$ -- there is only one and it is in the top left corner.  For terms of degree $x^{n-2}$, notice that there is one in position, (1,2) and if we go down one and to the left one to (2,1) then we see the other term of degree $x^{n-2}$.  From here consider terms of degree $x^{n-3}$.  The first one is in (1,3), and as we go down one and to the left one (i.e. along the associated anti-diagonal) we uncover the other terms of this degree. And so on for lower order terms of $x$. \n",
    "\n",
    "*Now* we compare the two different representations of $p'(x)$.  \n",
    "\n",
    "$(n)x^{n-1} + (n-1) a_{n-1}x^{n-2} + (n-2) a_{n-2}x^{n-3} +... + a_{1} = \\left.\\begin{matrix}\n",
    "x^{n-1} s_0 + &  a_{n-1}x^{n-2}s_0   + & a_{n-2}x^{n-3}s_0  + & \\cdots + & a_{0}x^{-1} s_0\\\\\n",
    "x^{n-2} s_1 + &  a_{n-1}x^{n-3}s_1  + & a_{n-2}x^{n-4}s_1 +  & \\cdots + & a_{0}x^{-2}s_1\\\\\n",
    " x^{n-3} s_2 + &   a_{n-1}x^{n-4}s_2 + & a_{n-2}x^{n-5}s_2 + & \\cdots + & a_{0}x^{-3}s_2\\\\\n",
    "x^{n-4} s_3 + &   a_{n-1}x^{n-5}s_3 + &  a_{n-2}x^{n-6}s_3 + & \\cdots + & a_{0}x^{-4}s_3\\\\\n",
    "\\vdots  & \\vdots &  \\vdots & \\vdots  & \\vdots  \\\\\n",
    "\\end{matrix}\\right.$\n",
    "\n",
    "start comparing expressions for \n",
    "\n",
    "$x^{n-1}$  \n",
    "\n",
    "$(n)x^{n-1} = x^{n-1}s_0$  \n",
    "\n",
    "recalling that $s_0 = n$\n",
    "\n",
    "$x^{n-1} = x^{n-1}$  \n",
    "\n",
    "$1 = 1 = a_n$\n",
    "\n",
    "which is not particularly insightful. \n",
    "\n",
    "next: $x^{n-2}$  \n",
    "\n",
    "$(n-1) a_{n-1}x^{n-2} = a_{n-1}x^{n-2}s_0  + x^{n-2} s_1 = n*a_{n-1}x^{n-2}  + x^{n-2} s_1 $   \n",
    "divide out $x^{n-2}$  \n",
    "$(n-1) a_{n-1} = n*a_{n-1}  + s_1 $ \n",
    "\n",
    "$-s_1  = n*a_{n-1} - (n-1) a_{n-1}$  \n",
    "$-s_1  = a_{n-1}$   \n",
    "$-trace\\big(\\mathbf C\\big) = a_{n-1}$\n",
    "\n",
    "which is as we'd expect\n",
    "\n",
    "next $x^{n-3}$  \n",
    "\n",
    "$(n-2) a_{n-2}x^{n-3} = a_{n-2}x^{n-3}s_0 + a_{n-1}x^{n-3}s_1 +  x^{n-3} s_2$  \n",
    "divide out $x^{n-3}$  \n",
    "$(n-2) a_{n-2} = a_{n-2}n + a_{n-1}s_1 + s_2$  \n",
    "$-(a_{n-1}s_1 + s_2) = (n - (n-2))a_{n-2}$  \n",
    "$-(a_{n-1}s_1 + s_2) = 2 a_{n-2}$   \n",
    "$-\\frac{1}{2}(a_{n-1}s_1 + s_2) = a_{n-2}$   \n",
    "$ a_{n-2} = -\\frac{1}{2}\\big(a_{n-1}trace\\big(\\mathbf C^1\\big) + trace\\big(\\mathbf C^2\\big) \\big)$  \n",
    "\n",
    "\n",
    "**Now in general for** $0 \\leq r \\lt n$, we see:  \n",
    "\n",
    "$ (n-r) a_{n-r} x^{n-r-1} = a_{n-r}x^{n-r-1} s_0 + \\sum_{j=1}^{r} x^{n-r-1} a_{n-r + j}*s_j $ \n",
    "\n",
    "$ (n-r) a_{n-r} = a_{n-r}n + \\sum_{j=1}^{r} a_{n-r + j}*s_j $ \n",
    "\n",
    "$ - \\sum_{j=1}^{r} a_{n-r + j}*s_j = ( n - (n-r)) a_{n-r} $ \n",
    "\n",
    "$ - \\sum_{j=1}^{r} a_{n-r + j}*s_j = r* a_{n-r} $   \n",
    "$ -\\frac{1}{r} \\sum_{j=1}^{r} a_{n-r + j}*s_j = a_{n-r} $ \n",
    "\n",
    "$a_{n-r}  = -\\frac{1}{r} \\sum_{j=1}^{r} a_{n-r + j}*trace\\big(\\mathbf C^j\\big) = -\\frac{1}{r}\\Big( a_{n-r +1}*trace\\big(\\mathbf C^1\\big) + a_{n-r +2}*trace\\big(\\mathbf C^2\\big) + ... + a_{n}*trace\\big(\\mathbf C^r\\big) \\Big)$  \n",
    "\n",
    "recalling that $a_n = 1$\n",
    "\n",
    "**As for the residual case of ** $r \\geq n$:\n",
    "\n",
    "The left hand side is $0$, but the right hand side seems to have quite a few terms.  Specifically we see:\n",
    "\n",
    "$ 0 = a_0  x^{n-r-1} s_{r-n+0} + a_1  x^{n-r-1} s_{r-n + 1} +  a_2  x^{n-r-1} s_{r-n + 2} + ... + a_n  x^{n-r-1} s_{r-n + n}$  \n",
    "$ 0 = a_0s_{r-n} + a_1 s_{r-n + 1} +  a_2 s_{r-n + 2} + ... + a_n  s_{r}$\n",
    "$ 0 = a_0 *trace \\big(\\mathbf C^{r-n}\\big) + a_1 * trace \\big(\\mathbf C^{r- n + 1}\\big) +  a_2 * trace \\big(\\mathbf C^{r-n+2}\\big)  + ... + a_n* trace \\big(\\mathbf C^{r}\\big)$\n",
    "\n",
    "$ 0 =  trace \\Big(a_0 \\mathbf C^{r-n} + a_1 \\mathbf C^{r-n+1} +  a_2 \\mathbf C^{r-n+2}  + ... + a_n \\mathbf C^{r}\\Big)$\n",
    "\n",
    "$ 0 =  trace \\Big(\\mathbf C^{r-n} \\big(a_0 \\mathbf I + a_1 \\mathbf C^{1} +  a_2 \\mathbf C^{2}  + ... + a_n \\mathbf C^{n}\\big)\\Big)$\n",
    "\n",
    "$ 0 =  trace \\Big(\\mathbf C^{r-n} \\big(\\mathbf {00}^H\\big) \\Big) = trace \\Big(\\mathbf {00}^H \\Big) = 0$\n",
    "\n",
    "per Cayley Hamilton.  \n",
    "\n",
    "\n",
    "**remarks**\n",
    "\n",
    "This final bit of using Cayley Hamilton was a direct port from Dan Kalman's \"A Matrix Proof of Newton's Identities\".  Originally this posting was going to be a walkthrough of that writeup, however your author found the steps around the time of synthetic division (approx $\\frac{2}{3}$ the way through) to be hard to follow both symbollically, and intuitively.  However, the use of Cayley Hamilton to extinguish sums of terms that are outside \"on the fringe\" of the characteristic polynomial was a nice touch, and included here. *Update: A walkthrough of Kalman's proof of Newton Identities follows in 3 paragraphs*  \n",
    "\n",
    "The essence of this approach shown above is in problem $7.1.23$ of Meyer's *Matrix Analysis and Applied linear algebra*, which says,   \n",
    "\n",
    "(a) show $p'(x) = p(x) \\sum_{i=1}^n (x - \\lambda_i)^{-1}$  \n",
    "(b) Use geometric series expansion for $(x-\\lambda_i)^{-1}$ to show that for $\\big \\vert x \\big \\vert \\ \\gt max(\\big \\vert \\lambda_i\\big \\vert)$, $\\sum_{i=1}^n \\frac{1}{x - \\lambda_i} = \\frac{s_0}{x} + \\frac{s_1}{x^2} + \\frac{s_2}{x^3} + ...$  \n",
    "(c) combine these results and equate like powers of $\\lambda$.  \n",
    "\n",
    "The approach didn't explicitly cover the case of terms that could be 'beyond' the characteristic polynomial, but the layout of the problem seemed more intuitive than most discussions that I've seen.  This long-form writeup on Newton's Identities is an attempt to carefully work through those steps, and to do so in a way that is somewhat visual and intuitive.  That said, there is a considerable amount of symbol manipulation involved in deriving Newton's Identities.  Hence the 'visualization' associated with this really comes down to the earlier worked example of traces on the Companion matrix.  Further the exact recurrence shown in here is of interest to your author, but the Hankel matrix formed by two Vandermonde matrices (with a diagonal matrix containing algebraic multiplicities in between) -- i.e. for what turns out to be the \"Hamburger Moment Problem\", is in many ways still more intuitive.\n",
    "\n",
    "# An adaptation of Kalman's 'A Matrix Proof of Newton's Identities'  \n",
    "\n",
    "located here: http://dankalman.net/preprints/newtonsID.pdf  \n",
    "\n",
    "\n",
    "**for the main case, of recovering terms** $\\{a_0, a_1, ..., a_{n-1}, a_n\\}$ **of our characteristic polynomial** \n",
    "\n",
    "suppose we have our characteristic polynomial for some $n$ x $n$ matrix $\\mathbf C$, and apply it to some matrix, $\\mathbf X = x \\mathbf I$.\n",
    "\n",
    "Thus \n",
    "\n",
    "$ p\\big(\\mathbf X\\big) = a_n \\mathbf X^n + a_{n-1}\\mathbf X^{n-1} + a_{n-2}\\mathbf X^{n-2} +... + a_{1}\\mathbf X^{1}+ a_0 \\mathbf I $\n",
    "\n",
    "recalling that our polynomial is in monic form, thus $a_n = 1$.  \n",
    "\n",
    "now suppose we wanted to factor $\\big(\\mathbf X - \\mathbf C\\big)$ from it.  \n",
    "\n",
    "That is, we'd have $p\\big(\\mathbf X\\big) = \\big(\\mathbf X - \\mathbf C\\big)\\big(\\mathbf{???}\\big)$\n",
    "\n",
    "from here use synthetic division to find the terms in $\\big(\\mathbf{???}\\big)$.  Note that your author needed a review of synthetic divsion.  The presentation mimics, as closely as possible, that of slide 24 on here:\n",
    "\n",
    "https://academics.utep.edu/Portals/1788/CALCULUS%20MATERIAL/2_3%20POLYNOMIAL%20AND%20SYNTHETIC%20DIVISION.pdf\n",
    "\n",
    "\n",
    "The synthetic division process is shown below:\n",
    "\n",
    "\\begin{bmatrix}\n",
    "x^n & x^{n-1} &x^{n-2}  &x^{n-3}  & \\cdots & x^{1} & x^{0}\\\\ \n",
    "a_{n} & a_{n-1} & a_{n-2} & a_{n-3} & \\cdots & a_{1} & a_{0}\\\\ \n",
    " & a_{n}\\mathbf C & a_{n}\\mathbf C^2 + a_{n-1} \\mathbf C & \\sum_{r=1}^3 a_{n+1-r}\\mathbf C^r &\\cdots  & \\sum_{r=1}^{n-1} a_{n+1-r}\\mathbf C^r & \\sum_{r=1}^{n} a_{n+1-r}\\mathbf C^r\\\\ \n",
    "a_n\\mathbf I & a_n \\mathbf C + a_{n-1}\\mathbf I & a_n \\mathbf C^2 + a_{n-1}\\mathbf C + a_{n-2}\\mathbf I & a_{n-3} \\mathbf I  + \\sum_{r=1}^{3} a_{n+1-r}\\mathbf C^{r+1} &  \\cdots  & a_{1} \\mathbf I  + \\sum_{r=1}^{n-1} a_{n+1-r}\\mathbf C^{r+1} & p\\big(\\mathbf C)\\\\ \n",
    "\\end{bmatrix}\n",
    "\n",
    "\n",
    "where we recall that $a_n = 1$, and $p\\big(\\mathbf C\\big) = 0$ per Cayley Hamilton.  \n",
    "\n",
    "$p\\big(\\mathbf X\\big) = \\big(\\mathbf X - \\mathbf C\\big)\\Big(\\big(\\mathbf I\\big) x^{n-1} + \\big(\\mathbf C + a_{n-1}\\mathbf I\\big)x^{n-2} +\\big( \\mathbf C^2 + a_{n-1}\\mathbf C + a_{n-2}\\mathbf I\\big)x^{n-3}  + \\big(a_{n-3} \\mathbf I  + \\sum_{r=1}^{3} a_{n+1-r}\\mathbf C^{r+1}\\big)x^{n-4} + \\cdots + \\big(\\sum_{r=1}^{n-1} a_{n+1-r}\\mathbf C^{r+1}\\big)x^0 + 0  \\Big) $\n",
    "\n",
    "or, more succinctly: \n",
    "\n",
    "$p\\big(\\mathbf X\\big) = \\big(\\mathbf X - \\mathbf C\\big)\\Big(\\sum_{r=1}^{n} \\big( a_r \\mathbf I + \\sum_{j=r+1}^{n} a_{j}*\\mathbf C^{j-r}\\big)x^{r-1} \\Big)$\n",
    "\n",
    "\n",
    "Next, note that $\\big(\\mathbf X - \\mathbf C\\big) = \\big( x\\mathbf I - \\mathbf C\\big)$, and this matrix is invertible for all $x \\neq \\lambda_k$ for $k = \\{1,2, 3, ..., n\\}$\n",
    "\n",
    "In the event $x$ is an eigenvalue of $\\mathbf C$, then we see:\n",
    "\n",
    "$\\mathbf {00}^H = \\mathbf {00}^H$\n",
    "\n",
    "now we confine ourselves to cases where $x$ is not an eigenvalue of $\\mathbf C$, and where $x \\neq 0$\n",
    "\n",
    "$\\big(\\mathbf X - \\mathbf C\\big)^{-1} p\\big(\\mathbf X\\big) = \\Big(\\sum_{r=1}^{n} \\big( a_r \\mathbf I + \\sum_{j=r+1}^{n} a_{j}*\\mathbf C^{j-r}\\big)x^{r-1} \\Big)$\n",
    "\n",
    "$\\big(\\mathbf X - \\mathbf C\\big)^{-1} p\\big(\\mathbf X\\big) = \\big(\\mathbf X - \\mathbf C\\big)^{-1} p\\big(x\\mathbf I\\big) = \\big(\\mathbf X - \\mathbf C\\big)^{-1}\\mathbf I p\\big(x\\big)= \\big(\\mathbf X - \\mathbf C\\big)^{-1} p\\big(x\\big) = \\Big(\\sum_{r=1}^{n} \\big( a_r \\mathbf I + \\sum_{j=r+1}^{n} a_{j}*\\mathbf C^{j-r}\\big)x^{r-1} \\Big)$\n",
    "\n",
    "recalling that $p\\big(x\\big)$ is a collection of scalars, and $x^r$ is a scalar, we can re-arrange and see:\n",
    "\n",
    "$p\\big(x\\big)\\big(\\mathbf X - \\mathbf C\\big)^{-1}  = \\Big(\\sum_{r=1}^{n} x^{r-1}\\big( a_r \\mathbf I + \\sum_{j=r+1}^{n} a_{j}*\\mathbf C^{j-r}\\big) \\Big)$\n",
    "\n",
    "since both sides are equal, their traces are equal. \n",
    "\n",
    "$trace\\Big(p\\big(x\\big)\\big(\\mathbf X - \\mathbf C\\big)^{-1}\\Big)  = trace \\Big(\\sum_{r=1}^{n} x^{r-1}\\big( a_r \\mathbf I + \\sum_{j=r+1}^{n} a_{j}*\\mathbf C^{j-r}\\big) \\Big)) $\n",
    "\n",
    "using linearity of the trace, and recognizing that $trace\\big(\\mathbf I\\big) = n$, we can simplify this to:  \n",
    "\n",
    "$ p\\big(x\\big) trace\\Big(\\big(\\mathbf X - \\mathbf C\\big)^{-1}\\Big) = \\Big(\\sum_{r=1}^{n} x^{r-1}\\big( a_r * n + \\sum_{j=r+1}^{n} a_{j}*trace\\big(\\mathbf C^{j-r}\\big)\\big) \\Big)$ \n",
    "\n",
    "\n",
    "\n",
    "when we evaluate the left hand side we see:\n",
    "\n",
    "$p\\big(x\\big) trace\\Big(\\big(\\mathbf X - \\mathbf C\\big)^{-1}\\Big) = p\\big(x\\big) trace\\Big(\\big(x\\mathbf I - \\mathbf C\\big)^{-1}\\Big)$\n",
    "\n",
    "which is to say we have a scalar valued polynomial $p\\big(x\\big)$ that we are multiplying by some trace.  \n",
    "\n",
    "note that the matrix $\\big(x\\mathbf I - \\mathbf C\\big)$ has eigenvalues of $(x-\\lambda_k)$, and the matrix $\\big(x\\mathbf I - \\mathbf C\\big)^{-1}$ has eigenvalues of $\\big(\\frac{1}{x-\\lambda_k}\\big)$\n",
    "\n",
    "Thus we see:  \n",
    "$trace\\Big(\\big(x\\mathbf I - \\mathbf C\\big)^{-1}\\Big) = \\Big(\\frac{1}{x-\\lambda_1} + \\frac{1}{x-\\lambda_2} + \\frac{1}{x-\\lambda_3} + ... + \\frac{1}{x-\\lambda_n}\\Big) $\n",
    "\n",
    "Thus \n",
    "\n",
    "$p\\big(x\\big) trace\\Big(\\big(\\mathbf X - \\mathbf C\\big)^{-1}\\Big)  =  p\\big(x\\big)\\Big(\\frac{1}{x-\\lambda_1} + \\frac{1}{x-\\lambda_2} + \\frac{1}{x-\\lambda_3} + ... + \\frac{1}{x-\\lambda_n}\\Big) = n x^{n-1} + (n-1) a_{n-1}x^{n-2} + (n-2) a_{n-2}x^{n-3} +... + a_{1} $\n",
    "\n",
    "note from the earlier derivation: we are not actually taking a derivative here -- we could simply multiply all terms out to get the equality.  However, if we were interested in taking the derivative, we'd see\n",
    "\n",
    "$n x^{n-1} + (n-1) a_{n-1}x^{n-2} + (n-2) a_{n-2}x^{n-3} +... + a_{1}  = p'(x) = p\\big(x\\big)\\Big(\\frac{1}{x-\\lambda_1} + \\frac{1}{x-\\lambda_2} + \\frac{1}{x-\\lambda_3} + ... + \\frac{1}{x-\\lambda_n}\\Big)$ \n",
    "\n",
    "which justifies the equality.  A few lines up, we had,\n",
    "\n",
    "$p\\big(x\\big) trace\\Big(\\big(\\mathbf X - \\mathbf C\\big)^{-1}\\Big) =  \\Big(\\sum_{r=1}^{n} x^{r-1}\\big( a_r * n + \\sum_{j=r+1}^{n} a_{j}*trace\\big(\\mathbf C^{j-r}\\big)\\big) \\Big)$\n",
    "\n",
    "And we now have a new relation for the left hand side, and make the appropriate substitution. \n",
    "\n",
    "$n* a_n* x^{n-1} + (n-1) a_{n-1}x^{n-2} + (n-2) a_{n-2}x^{n-3} +... + a_{1} =  \\Big(\\sum_{r=1}^{n} x^{r-1}\\big( a_r * n + \\sum_{j=r+1}^{n} a_{j}*trace\\big(\\mathbf C^{j-r}\\big)\\big) \\Big)$\n",
    "\n",
    "we now equate like powers of $x$\n",
    "\n",
    "$(r)a_r x^{r-1} =  x^{r-1}\\big( a_r *n + \\sum_{j=r+1}^{n} a_{j}*trace\\big(\\mathbf C^{j-r}\\big) \\big)$\n",
    "\n",
    "divide by $-x^{r-1}$\n",
    "\n",
    "$-(r)a_r =  -a_r *n -\\sum_{j=r+1}^{n} a_{j}*trace\\big(\\mathbf C^{j-r}\\big)$\n",
    "\n",
    "add $a_r (n)$ to both sides\n",
    "\n",
    "$a_r (n)+ a_r(-r) =  a_r *(n -r) =  - \\sum_{j=r+1}^{n} a_{j}*trace\\big(\\mathbf C^{j-r}\\big)$\n",
    "\n",
    "for $r = \\{1, 2, ..., n\\}$\n",
    "\n",
    "$ a_r  =  -\\frac{1}{n -r} \\sum_{j=r+1}^{n} a_{j}*trace\\big(\\mathbf C^{j-r}\\big)$\n",
    "\n",
    "\n",
    "This is the same formula as before, albeit with a different indexing scheme. \n",
    "- - - -\n",
    "For example, suppose $n=5$ and $r = 2$.  Using this setup, we see\n",
    "\n",
    "$ a_2  =  -\\frac{1}{3} \\sum_{j=3}^{5} a_{j}*trace\\big(\\mathbf C^{j-r}\\big) =  -\\frac{1}{3}\\Big( a_3*trace\\big(\\mathbf C^{1}\\big) + a_4*trace\\big(\\mathbf C^{2}\\big) + a_5 * trace\\big(\\mathbf C^{3}\\big)\\Big)$\n",
    "- - - -\n",
    "And for the case of $a_0$, we use Cayley Hamilton.  \n",
    "\n",
    "**remarks **  \n",
    "\n",
    "This synthetic division process is elementary and taught as part of basic algebra or single variable calculus.  However, it remains mechanically simple but for whatever reason, not quite intuitive to your author.  The major advantage of this approach is it does not require any notion of infinity, whether that means an infinite series, or the notion of a limit for a derivative.  The time where we recognize $p'(x)$ in two forms is merely for convenience (or alternatively, it is akin to formally differentiating, much like formally differentiating a power series where no underlying notions of limits are required).  Thus the above approach is complementary to one that needs convergence concepts from analysis.  Put differently, your author always finds it nice to prove things two different ways, especially when one of those ways is purely algebraic.  \n",
    "\n",
    "\n",
    "**For examples of working code that recovers the characteristic polynomial from traces of successive powers of a matrix, see: **\n",
    "\n",
    "https://github.com/DerekB7/LinearAlgebra/blob/master/traces_newtons_identities_code.ipynb\n",
    "\n",
    "Note: the code is mathemathematically correct, however there may be numeric precision issues when working with floating point numbers.\n",
    "\n",
    "- - - - - \n",
    "# Proof of the Schwartz-Zippel Theorem.  \n",
    "*this is an adaptation of a key theorem stated in miniature 24 in \"Thirty-Three Miniatures\"  *  \n",
    "\n",
    "**claim: **  \n",
    "when dealing with a multivariate polynomial $p(x_1, x_2, ...,x_m)$ with degree at most $d$ with $m$ variables, and that exists in some field $\\mathbb K$, but we work in some finite field $\\mathbb F_n$ (where $n$ is an appropriately sized prime number larger than $d$) that is a subset of $\\mathbb K$, the number of m-tuples $(r_1, r_2, ..., r_m) \\in \\mathbb F_n^m$ that evaluate as roots --i.e. where $p(r_1, r_2, ..., r_m) = 0$ -- is at most $dn^{m-1}$. This is equivalent to saying that if we uniformly randomly generated each $x_k$ coordinate in our finite field, the probability that the resulting tuple would be a root is *at most* $\\frac{d}{n}$.  \n",
    "\n",
    "*commentary: as outlined in the miniature, this approach is directly useful in computing perfect matchings, something tied in with Hall's Marriage Problem, and many other things in combinatorial theory. Something of particular interest is how the finite fields can rapidly help with numeric stability and memory sizing issues on one hand, but still get the benefits of random sampling on another hand, and because they are finite we can use a basic combinatorial approach to estimating or bounding underlying probabilities. *\n",
    "\n",
    "*This is very interesting and has very useful applications in CS (or at least in theoretical CS).  At its core the idea is polynomials don't have that many zeros, even though otherfunction may have lots of them.  This statement is very clear and accurate in the single variable case.  It gets hard to make the statement meaningful, however, in the multivariable case as there are may be uncountably many zeros.  However, if we drew a picture of a twovariable (x,y) polynomial and shaded in the zeros, we would see that the area isn't that much.  One approach would be to specify probability distributions or measures.  However a much more slick approach is to, in effect, truncate our domain to be a finite set -- in particular a finite field-- which allows simple counting arguments and a uniform distribution.  On top of that the finite field has major benefits in terms of avoiding numeric stability issues, e.g. when we are trying to 'back door' a permanent calculation, like in looking for at least one perfect matching in a bipatite graph (ala Hall's marriage problem).  This miniature was significantly subtle that your author did not really get how extremely nice, and powerful the result in here is.  *  \n",
    "\n",
    "\n",
    "*Something to keep in mind, is that if you have a term of say* $x_1^2 x_2 x_3^4$, *then we'd say  this term has degree of 7.  (Keep in mind homogeniety of terms in relations of things like power sums...)*  \n",
    "\n",
    "\n",
    "**proof:**  \n",
    "\n",
    "**base case: **$m = 1$ \n",
    "In this case we see that a $d$ degree polynomial (i.e. a polynomial that is not identically zero polynomial) in a finite field (and indeed in even in field with characteristic zero) has at most $d$ root, by earlier analysis of the Vandermonde matrix.  This is dealt with under the heading \"Implication: A degree n-1 polynomial is completely given by n uniqe data points \" --i.e. at most n-1 roots, plus one other data point to characterize the leading coefficient (i.e. whether the polynomial is monic, or has something else as a coefficient with that term).  It is also explicitly addressed as a worked example under \" Side note: How do we know there are exactly n roots of unity?\".  \n",
    "\n",
    "\n",
    "**inductive case: ** $m \\gt 1$   \n",
    "\n",
    "note: at this stage we have assumed it is true through $m -1$ and need to prove the hypothesis is true for $m$.  Thus if we have any number of variables less than $m$ (in particular, consider: $m-1$) and some degree $t$, then \n",
    "\n",
    "**our inductive hypothesis tells us such a set of roots has cardinality** $\\leq t n^{(m-1) - 1} = t n^{m-2 }$. \n",
    "- - - \n",
    "without loss of generality we may assume $x_1$ occurs in at least one term of this polynomial.  (If it doesn't then call this a polynomial with $m-1$ terms, and re-label to have $p(x_1, x_2, ...,x_{m-1})$, restart this argument.)  \n",
    "\n",
    "now re-write the polynomial as a polynomial in $x_1$ such what we have \n",
    "\n",
    "$p(x_1, x_2, ...,x_m) = \\sum_{i=0}^k x_1^i p_i(x_2, x_3,..., x_m)$ \n",
    "- - - \n",
    "(where $k$ is the maximum exponent of $x_1$ in $p(x_1, x_2, ...,x_m)$.  Note that for cases where $x_1$ isn't 'involved', that would correspond to the case of $x_1^0$ and there'd be an appropriate $p_0(x_2, x_3,..., x_m)$ associated with that.   The fact that we've isolated one of the terms, sets this up for inductive chaining.  \n",
    "\n",
    "From here we partition the roots into two:\n",
    "\n",
    "i.e. we divdie the m-tuples $(r_1, r_2,..., r_m)$ with $p(r_1, r_2,..., r_m) = 0$ into two disjoint sets/collection. The partition is $R_1$ where we have roots and $p_k(r_1, r_2, ..., r_n)=0$, and $R_2$ where we still have roots but $p_k(r_1, r_2, ..., r_n)\\neq 0$.  \n",
    "\n",
    "The first collection, $R_1$, has the m-tuples associated with that maximum exponent of $x_1$, $k$.  That is these consist of $p_k(r_2, r_3,..., r_m) = 0$  After removing the value of $x_1^k$ each remain term in $p_k(x_2, x_3,..., x_m)$ has degree $t = d-k$.  The inductive hypothesis then says that we have $\\big \\vert R_1 \\big \\vert \\leq  (t)n^{m-2} = (d-k)n^{m-2} \\leq (d-k)n^{m-1}$.  \n",
    "\n",
    "$R_2$ is the other portion of our partition, where $p(r_1, r_2,..., r_m) = 0$, but $p_k(r_2,..., r_m)\\neq 0$. (Note: it complicates the algebra and we don't actually do this, but the *spirit* of this is to divide each lower order terms by $p_k(r_2,..., r_m)$ to get things in monic form, i.e.\n",
    "\n",
    "$p^*(x_1, x_2, ..., x_n) = x^k + \\sum_{i=0}^{k-1} x_1^i\\frac{p_i(x_2, x_3,..., x_m)}{p_i(x_2, x_3,..., x_m)}$    \n",
    "\n",
    "\n",
    "Again, strictly speaking, we don't actually do the conversion to monic form (or worry about how to interpret this this in a finite field), but it is a very nice way to think about this.  Specifically, putting things into monic form is key to working with and having manageable polynomials... (this is an ongoing theme) and it is key to understanding why we'd want to partition things this way between $R_1$ and $R_2$ partition.  \n",
    "\n",
    "That is, $R_1$ makes direct use of the inductive hypothesis -- i.e. we've reduced the number of variables from $m$ to $m-1$ which is a clear form of progress, and $R_2$ is now in 'pseudo-monic' form, and hence easier to work with.  In this form, we can say for any given *fixed* choices of $\\{x_2, x_3, ..., x_n\\}$ -- and there are $n$ of them for each of these $m-1$ terms, giving us $n^{m-1}$ possible selections:  \n",
    "\n",
    "we have a single variable polynomial in $p_1$.  Said fixed polynomial in $p_1$ has degree $k$ and hence most $k$ roots, (again via Vandermonde Matrix, and because the polynomial isn't identically zero -- i.e. we couldn't have gone through that pseudo monic form setup if it was identically zero).  Thus we find $\\big \\vert R_2 \\big \\vert \\leq kn^{m-1}$.  \n",
    "\n",
    "Combining our partitions, we get that the total number of roots in our finite set has an upper bound given by  \n",
    "$\\big \\vert R_1 \\big \\vert + \\big \\vert R_2 \\big \\vert \\leq \\big \\vert R_1 \\big \\vert + kn^{m-1} \\leq (d-k)n^{m-1} + (k)n^{m-1} = (d)n^{m-1}$   \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [py35]",
   "language": "python",
   "name": "Python [py35]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
