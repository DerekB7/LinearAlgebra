{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motivating Background: figuring out a function from data points\n",
    "\n",
    "Quite often in basic machine learning applications -- say with linear regression -- we gather $n$ samples of data and look to fit a model to it.  Note: we often have *a lot* of data, and in fact n can be any natural number.  For illustrative purposes, we start with the case of n = 5. \n",
    "\n",
    "Note that we typically also have multiple different features in our data, but *the goal of this posting is to strip down ideas to their very core*, so we consider the one feature case.  Also note that in machine learning we may use notation like $\\mathbf {Xw} = \\mathbf y$, where we solve for the weights in $\\mathbf w$.  However, this posting uses the typical Linear Algebra setup of $\\mathbf{Ax} = \\mathbf b$, where we are interested in solving for $\\mathbf x$.  \n",
    "\n",
    "So initially we may just have the equation\n",
    "\n",
    "$\\mathbf{Ax} = \\begin{bmatrix}\n",
    "a_1\\\\ \n",
    "a_2\\\\ \n",
    "a_3\\\\ \n",
    "a_4\\\\ \n",
    "a_5\n",
    "\\end{bmatrix} \\begin{bmatrix}\n",
    "x_1\\\\ \n",
    "\\end{bmatrix} = \\mathbf b$\n",
    "\n",
    "**this original 'data' matrix will also be written as **\n",
    "\n",
    "$\\mathbf a = \\begin{bmatrix}\n",
    "a_1\\\\ \n",
    "a_2\\\\ \n",
    "a_3\\\\ \n",
    "a_4\\\\ \n",
    "a_5\n",
    "\\end{bmatrix}$\n",
    "\n",
    "Note that when we gather real world data there is noise in the data, so we would be *extremely* surprised if any of the entries in $\\mathbf a$ are duplicates.  So, unless otherwise noted assume that each entry in $a_i$ is unique. Since there is only one column, the column rank of $\\mathbf A$ is one, and the column rank = row rank, thus we know that the row rank = 1. \n",
    "\n",
    "Then we decide to insert a bias /affine translation piece (in index position zero -- to use notation from Caltech's \"Learning From Data\").  \n",
    "\n",
    "Thus we end up with the following equation\n",
    "\n",
    "$\\mathbf{Ax} = \\begin{bmatrix}\n",
    "1 & a_1\\\\ \n",
    "1 & a_2\\\\ \n",
    "1 & a_3\\\\ \n",
    "1 & a_4\\\\ \n",
    "1 & a_5\n",
    "\\end{bmatrix} \\begin{bmatrix}\n",
    "x_0\\\\\n",
    "x_1\\\\ \n",
    "\\end{bmatrix} = x_0 \\mathbf 1 + x_1 \\mathbf a = \\mathbf b$\n",
    "\n",
    "Column 0 of $\\mathbf A$ is the ones vector, also denoted as $\\mathbf 1$.  \n",
    "\n",
    "At this point we know that $\\mathbf A$ still has full column rank (i.e. rank = 2) -- if this wasn't the case, this would imply that we could scale column 0 to get column 1 (i.e. everything in column 1 would have to be identical).   \n",
    "\n",
    "From here we may simply decide to do least squares and solve (which we always can do when we have full column rank, and $\\mathbf A $ has m rows and n columns, where $m \\geq n$).  \n",
    "\n",
    "Or we may decide to map this to a higher dimensional space that has a quadratic term.  \n",
    "\n",
    "$\\mathbf{Ax} = \\begin{bmatrix}\n",
    "1 & a_1 & a_1^2\\\\ \n",
    "1 & a_2 & a_2^2\\\\ \n",
    "1 & a_3 & a_3^2\\\\ \n",
    "1 & a_4 & a_4^2\\\\ \n",
    "1 & a_5 & a_5^2\n",
    "\\end{bmatrix} \\begin{bmatrix}\n",
    "x_0\\\\\n",
    "x_1\\\\ \n",
    "x_2\\\\\n",
    "\\end{bmatrix} = \\mathbf b$\n",
    "\n",
    "\n",
    "At this point we may just do least squares and solve.  But that requires $\\mathbf A$ to have full column rank.  How do we know that $\\mathbf A$ has full column rank?  An intuitive way to think about it is that squaring each $a_i$ to get column 2 is not a linear transformation, so we would not expect it to be linear combination of prior columns.  \n",
    "\n",
    "$\\mathbf a \\circ \\mathbf a \\neq \\gamma_0 \\mathbf 1 + \\gamma_1 \\mathbf a$\n",
    "\n",
    "where $\\circ$ denotes the Hadamard product.  And by earlier argument, we know $\\mathbf a \\neq \\gamma_0 \\mathbf 1$, hence each column is linearly independent.  There is another (more mathemetically exact) way to verify linear independence of these columns -- which comes from the Vandermonde Matrix, and we will address this shortly.  \n",
    "\n",
    "We may however decide we want an even higher dimensional space for our data, so we add a cubic term:\n",
    "\n",
    "$\\mathbf{Ax} = \\begin{bmatrix}\n",
    "1 & a_1 & a_1^2 & a_1^3\\\\ \n",
    "1 & a_2 & a_2^2 & a_2^3\\\\ \n",
    "1 & a_3 & a_3^2 & a_3^3\\\\ \n",
    "1 & a_4 & a_4^2 & a_4^3\\\\ \n",
    "1 & a_5 & a_5^2 & a_5^3\n",
    "\\end{bmatrix} \\begin{bmatrix}\n",
    "x_0\\\\\n",
    "x_1\\\\ \n",
    "x_2\\\\\n",
    "x_3\\\\\n",
    "\\end{bmatrix} = \\mathbf b$\n",
    "\n",
    "Again we may be confident that the columns are linearly independent because our new column -- cubing $\\mathbf a$ is not a linear transformation (or alternatively, using the hadamard product is not a linear transformation), so we write: \n",
    "\n",
    "$\\mathbf a \\circ \\mathbf a \\circ \\mathbf a \\neq \\gamma_0 \\mathbf 1 + \\gamma_1 \\mathbf a + \\gamma_2 \\big(\\mathbf a \\circ \\mathbf a\\big)$\n",
    "\n",
    "And if the above is *still* not enough, we may add a term to the fourth power:\n",
    "\n",
    "$\\mathbf{Ax} = \\begin{bmatrix}\n",
    "1 & a_1 & a_1^2 & a_1^3 & a_1^4\\\\ \n",
    "1 & a_2 & a_2^2 & a_2^3 & a_2^4\\\\ \n",
    "1 & a_3 & a_3^2 & a_3^3 & a_3^4\\\\ \n",
    "1 & a_4 & a_4^2 & a_4^3 & a_4^4\\\\ \n",
    "1 & a_5 & a_5^2 & a_5^3 & a_5^4\n",
    "\\end{bmatrix} \\begin{bmatrix}\n",
    "x_0\\\\\n",
    "x_1\\\\ \n",
    "x_2\\\\\n",
    "x_3\\\\\n",
    "x_4\\\\\n",
    "\\end{bmatrix} = \\mathbf b$\n",
    "\n",
    "Again quite confident that the above has full column rank because \n",
    "\n",
    "$\\mathbf a \\circ \\mathbf a \\circ \\mathbf a \\circ \\mathbf a \\neq \\gamma_0 \\mathbf 1 + \\gamma_1 \\mathbf a + \\gamma_2 \\big(\\mathbf a \\circ \\mathbf a\\big) + \\gamma_3 \\big(\\mathbf a \\circ \\mathbf a \\circ \\mathbf a \\big)$\n",
    "\n",
    "We may be tempted to go to an even higher dimensional space at this point, but this requires considerable justification.  Notice that $\\mathbf A$ is a square matrix now, and as we've argued, it has full column rank -- which means it also has full row rank.  Thus we can be sure to solve the above equation for a single, exact solution, where $\\mathbf x = \\mathbf A^{-1}\\mathbf b$.  If we were to go to a higher dimensional space we would be entering the world of an underdetermined system of equations -- see postings titled \"Underdetermined_System_of_Equations.ipynb\" for the L2 norm oriented solution, and \"underdetermined_regression_minimize_L1_norm.ipynb\" for the L1 norm oriented solution.  Since we can already be certain of solving for a single exact solution in this problem, we will stop mapping to higher dimensions here.  \n",
    "\n",
    "In the above equation of $\\mathbf{Ax} = \\mathbf b$, the square $\\mathbf A$ is a Vandermonde matrix.  Technical note: some texts say that $\\mathbf A$ is the Vandermonde matrix, while others say $\\mathbf A^T$ is the Vandermonde matrix.  The calculation of the determinant is identical, and for other properties, a mere small book-keeping adjustment is required.\n",
    "  \n",
    "Note that the Vandermonde matrix is well studied, has special fast matrix vector multiplication (i.e. $\\lt O(n^2)$) algorithms associated with it -- and a very special type of Vandermonde matrix is the Discrete Fourier Transform matrix.  The Vandermonde matrix  also has some very interesting properties for thinking about eigenvalues. \n",
    "\n",
    "\n",
    "There is another, more exacting way to verify that $\\mathbf A$ is full rank.  Let's look at the determinant of $\\mathbf A^T$.  There are a few different ways to prove this.  Sergei Winitzki had an interesting proof using wedge products -- that I may revisit at some point in the future.  \n",
    "\n",
    "\n",
    "# Begin Look at Vandermonde Matrices\n",
    "\n",
    "For some real valued Vandermonde matrix $\\mathbf A$, or it's transpose, we can say the following:\n",
    "\n",
    "(note the book-keeping required to evaluate this as a complex matrix, is just a very small alteration)\n",
    "\n",
    "\n",
    "$\\mathbf A^T  = \\begin{bmatrix}\n",
    "1 & 1 & 1 & \\dots & 1 & 1\\\\ \n",
    "a_1 & a_2 & a_3 & \\dots & a_{n-1} & a_n \\\\ \n",
    "a_1^2 & a_2^2 & a_3^2 & \\dots & a_{n-1}^2 & a_{n}^2\\\\ \n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\ \n",
    "a_{1}^{n-2} & a_{2}^{n-2} & a_{3}^{n-2} & \\dots & a_{n-1}^{n-2} & a_{n}^{n-2}\\\\\n",
    "a_{1}^{n-1} & a_{2}^{n-1} & a_{3}^{n-1} & \\dots & a_{n-1}^{n-1} & a_{n}^{n-1}\n",
    "\\end{bmatrix} $\n",
    "\n",
    "For the now,  I'll just notice that there is a rather obvious 'pattern' to these Vandermonde matrices, so we'll do the proof using mathematical induction, which takes advantage of this pattern / progression in polynomial terms.  \n",
    "\n",
    "\n",
    "\n",
    "**claim**: \n",
    "\n",
    "for natural number $n \\geq 2$ where $\\mathbf A \\in \\mathbb R^{n x n}$, and $\\mathbf A$ is a Vandermonde matrix, \n",
    "\n",
    "$det \\big(\\mathbf A \\big) = det \\big(\\mathbf A^T \\big) = \\prod_{1 \\leq i \\lt j \\leq n} (a_j - a_i)$\n",
    "\n",
    "*Base Case:* \n",
    "\n",
    "$n = 2$\n",
    "\n",
    "$\\mathbf A^T = \\begin{bmatrix}\n",
    "1 & 1\\\\ \n",
    "a_1 & a_2\n",
    "\\end{bmatrix}$\n",
    "\n",
    "$det \\big(\\mathbf A^T \\big) = (a_2 - a_1) = \\prod_{1 \\leq i \\lt j \\leq n} (a_j - a_i)$\n",
    "\n",
    "*sneak peak:*  \n",
    "if we follow the row operation procedure used during the inductive case, what we'd have is:\n",
    "\n",
    "$det \\big(\\mathbf A^T \\big) = det\\Big(\\begin{bmatrix}\n",
    "1 & 1\\\\ \n",
    "0 & (a_2 - a_1)\n",
    "\\end{bmatrix}\\Big) = 1*(a_2 - a_1)$\n",
    "\n",
    "\n",
    "*Inductive Case:*\n",
    "\n",
    "For $n \\gt 2$, assume formula is true where $\\mathbf C \\in \\mathbb R^{(n-1) x (n -1)}$\n",
    "\n",
    "i.e. assume true where \n",
    "\n",
    "$\\mathbf C = \\begin{bmatrix}\n",
    "1 & 1 & 1 & \\dots & 1\\\\ \n",
    "a_1 & a_2 & a_3 & \\dots & a_{n-1}\\\\ \n",
    "a_1^2 & a_2^2 & a_3^2 & \\dots & a_{n-1}^2\\\\ \n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots\\\\ \n",
    "a_{1}^{n-2} & a_{2}^{n-2} & a_{3}^{n-2} & \\dots & a_{n-1}^{n-2}\n",
    "\\end{bmatrix}$\n",
    "\n",
    "Note that we call this submatrix $\\mathbf C$ -- it will make a reappearance shortly!\n",
    "\n",
    "\n",
    "We need to show that the formula holds true where dimension of $\\mathbf A$ is $n$ x $n$. Thus consider the case where:\n",
    "\n",
    "$\\mathbf A^T  = \\begin{bmatrix}\n",
    "1 & 1 & 1 & \\dots & 1 & 1\\\\ \n",
    "a_1 & a_2 & a_3 & \\dots & a_{n-1} & a_n \\\\ \n",
    "a_1^2 & a_2^2 & a_3^2 & \\dots & a_{n-1}^2 & a_{n}^2\\\\ \n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\ \n",
    "a_{1}^{n-2} & a_{2}^{n-2} & a_{3}^{n-2} & \\dots & a_{n-1}^{n-2} & a_{n}^{n-2}\\\\\n",
    "a_{1}^{n-1} & a_{2}^{n-1} & a_{3}^{n-1} & \\dots & a_{n-1}^{n-1} & a_{n}^{n-1}\n",
    "\\end{bmatrix} $\n",
    "\n",
    "**Procedure:**\n",
    "subtract $a_1$ times the $i - 1$ row from the ith row, for  $0 \\lt i \\leq n$ **starting from the bottom of the matrix and working our way up** (i.e. the operations / subproblems do not overlap in this regard).  \n",
    "\n",
    "- - - - - \n",
    "**Justification:**\n",
    "\n",
    "First, the reason we'd like to do this is because we see an obvious pattern in the polynomial progression in each column of $\\mathbf A^T$.  Thus by following this procedure, we can zero out all entries in the zeroth column of $\\mathbf A^T$ except, the 1 located in the top left (i.e. in $a_{0,0}$).  This will allow us to, in effect, reduce our problem to the n - 1 x n - 1 dimensional case.  \n",
    "\n",
    "Also recall that the determinant of $\\mathbf A^T$ is equivalent to the determinant of $\\mathbf A$. Thus the above procedure is equivalent to subtracting a scaled version of column 0 of the original $\\mathbf A$ from column 1, and a scaled version of column 1 in the original $\\mathbf A$ from column 2, and so on.  These are standard operations that are well understood to not change the calculated determinant over any legal field. \n",
    "\n",
    "Since, your author particularly likes Gramâ€“Schmidt and orthgonality, there is an additional more visual interpretation that can be used over inner product spaces (i.e. real or complex fields).  Consider that $\\mathbf A = \\mathbf{QR}$, thus $det \\big(\\mathbf A \\big) = det \\big(\\mathbf{QR} \\big) = det \\big(\\mathbf{Q} \\big)det \\big(\\mathbf{R} \\big)$.  Notice that these column operations will have no impact on $\\mathbf Q$, and will only change the value of entries above the diagonal in $\\mathbf R$, thus there is no change in $det \\big(\\mathbf{Q} \\big)$ or $det \\big(\\mathbf{R} \\big)$ (which is given by the product of its diagonal entries).  This means there is no change in $det \\big(\\mathbf{A} \\big)$.  \n",
    "\n",
    "\n",
    "- - - - - \n",
    "\n",
    "$ = \\begin{bmatrix}\n",
    "1 & 1 & 1 & \\dots & 1 & 1\\\\ \n",
    "0 & a_2 - a_1 & a_3 - a_1 & \\dots & a_{n-1} - a_1 & a_n - a_1 \\\\ \n",
    "0 & a_2^2 - a_1 a_2 & a_3^2 - a_1 a_3 & \\dots & a_{n-1}^2 - a_1 a_{n-1} & a_{n}^2 - a_1 a_{n}\\\\ \n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\ \n",
    "0 & a_{2}^{n-2} - a_1 a_{2}^{n-3} & a_{3}^{n-2} - a_1 a_{3}^{n-3} & \\dots & a_{n-1}^{n-2} - a_1 a_{n-1}^{n-3} & a_{n}^{n-2} - a_1 a_{n}^{n-3}\\\\\n",
    "0 & a_{2}^{n-1} - a_1 a_2^{n-2} & a_{3}^{n-1} - a_1 a_3^{n-2}& \\dots & a_{n-1}^{n-1} -  a_1 a_{n-1}^{n-2}& a_{n}^{n-1} - a_1 a_{n}^{n-1}\n",
    "\\end{bmatrix} $\n",
    "\n",
    "$ = \\begin{bmatrix}\n",
    "1 & 1 & 1 & \\dots & 1 & 1\\\\ \n",
    "0 & (a_2 - a_1) 1 & (a_3 - a_1)1 & \\dots & (a_{n-1} - a_1) 1 & (a_n - a_1) 1 \\\\ \n",
    "0 & (a_2 - a_1) a_2 & (a_3 - a_1) a_3 & \\dots & (a_{n-1} - a_1) a_{n-1} & (a_n - a_1) a_{n}\\\\ \n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\ \n",
    "0 & (a_2 - a_1)a_{2}^{n-3} & (a_3 - a_1)a_{3}^{n-3} & \\dots & (a_{n-1} - a_1)a_{n-1}^{n-3} & (a_n - a_1)a_{n}^{n-3}\\\\\n",
    "0 & (a_2 - a_1)a_{2}^{n-2} & (a_3 - a_1)a_{3}^{n-2} & \\dots & (a_{n-1} - a_1)a_{n-1}^{n-2} & (a_n - a_1)a_{n}^{n-2} \n",
    "\\end{bmatrix}  $\n",
    "\n",
    "we can rewrite this as \n",
    "\n",
    "$= \\begin{bmatrix}\n",
    "1 & \\mathbf 1^T\\\\ \n",
    "\\mathbf 0 & \\mathbf{CD}\n",
    "\\end{bmatrix}$\n",
    "\n",
    "\n",
    "where \n",
    "\n",
    "$\\mathbf D = Diag\\Big(\\begin{bmatrix}\n",
    "a_2 & a_3 & a_4 & \\dots & a_n\n",
    "\\end{bmatrix}^T \\Big) - a_1 \\mathbf I =    \\begin{bmatrix}\n",
    "(a_2-a_1) & 0 &  0& \\dots & 0\\\\ \n",
    "0 & (a_3 - a_1) &0  &\\dots  &0 \\\\ \n",
    "0 & 0 & (a_4 - a_1) & \\dots & 0\\\\ \n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\ \n",
    "0 & 0 & 0 & \\dots & (a_n - a_1)\n",
    "\\end{bmatrix}$\n",
    "\n",
    "Note that $\\begin{bmatrix}\n",
    "1 & \\mathbf 1^T\\\\ \\mathbf 0 & \\mathbf{CD} \\end{bmatrix} - \\lambda \\begin{bmatrix}\n",
    "1 & \\mathbf 0^T\\\\ \\mathbf 0 & \\mathbf{I} \\end{bmatrix} = \\begin{bmatrix}\n",
    "1 - \\lambda & \\mathbf 1^T\\\\ \\mathbf 0 & \\mathbf{CD } - \\mathbf \\lambda \\mathbf I \\end{bmatrix}$, which is not invertible when $\\lambda := 1$ (because the left most column is all zeros).  \n",
    "\n",
    "Hence we know that there is an eigenvalue of 1, given by the top left diagonal entry, associated with $\\begin{bmatrix}\n",
    "1 & \\mathbf 1^T\\\\ \n",
    "\\mathbf 0 & \\mathbf{CD}\n",
    "\\end{bmatrix}$. We'll call this $\\lambda_1$ -- for the first eigenvalue of the \"MatrixAfterRowOperations\".  \n",
    "\n",
    "Thus the determinant can be written as \n",
    "\n",
    "$det\\big(\\mathbf A^T \\big) = det\\big(MatrixAfterRowOperations\\big) = (\\lambda_1) * (\\lambda_2  * \\lambda_3 * ... * \\lambda_n\\big) = (1) * \\det\\big(\\mathbf{CD}\\big) = \\det\\big(\\mathbf{C}\\big) \\det\\big(\\mathbf{D}\\big)$\n",
    "\n",
    "\n",
    "\n",
    "- - - - -\n",
    "\n",
    "**begin interlude** \n",
    "\n",
    "The fact that \n",
    "$det\\big(\\begin{bmatrix}\n",
    "1 & \\mathbf *\\\\ \n",
    "\\mathbf 0 & \\mathbf{Z}\n",
    "\\end{bmatrix}\\big) = 1 * det\\big(\\mathbf{Z}\\big)$\n",
    "\n",
    "is well understood via properities of block matrices over many fields.  However, as is often the case, there is an additional interpretation over inner product spaces that makes use of orthogonality.  **This interlude is a bit overkill and may safely be skipped**.\n",
    "\n",
    "Another way to think about this, is we can borrow from the Schur Decomposition $\\mathbf X = \\mathbf V \\mathbf R \\mathbf V^{H}$ where $\\mathbf V$ is unitary and $\\mathbf R$ is upper triangular.  Equivalently, $ \\mathbf V^H \\mathbf X  \\mathbf V = \\mathbf R$.  Also, we know the  eigenvector associated with $\\lambda_1$ (which is $\\begin{bmatrix}1 \\\\ \\mathbf 0 \\\\ \\end{bmatrix}$) can be chosen to be the left most column of $\\mathbf V$.  Since all columns in $\\mathbf V$ are mutually orthonormal, and hence all other columns must have a zero in the upper-most position.  Writing this out, and working through the blocked multiplication we get the following:\n",
    "\n",
    "(note that $^H$ denotes conjugate transpose -- and of course if the values are real, then this acts like a regular transpose operation)\n",
    "\n",
    "$\\mathbf V^{H} \\begin{bmatrix}\n",
    "1 & \\mathbf 1^H\\\\ \n",
    "\\mathbf 0 & \\mathbf{CD}\n",
    "\\end{bmatrix} \\mathbf V = \\mathbf R$\n",
    "\n",
    "$\\mathbf V^{H} \\begin{bmatrix}\n",
    "1 & \\mathbf 1^H\\\\ \n",
    "\\mathbf 0 & \\mathbf{CD}\n",
    "\\end{bmatrix} \\mathbf V = \\begin{bmatrix}1 & \\mathbf 0^H \\\\ \n",
    "\\mathbf 0 & \\mathbf{Q}\n",
    "\\end{bmatrix}^H \\begin{bmatrix}\n",
    "1 & \\mathbf 1^H\\\\ \n",
    "\\mathbf 0 & \\mathbf{CD} \\end{bmatrix} \\begin{bmatrix}\n",
    "1 & \\mathbf 0^H \\\\ \n",
    "\\mathbf 0 & \\mathbf{Q}\n",
    "\\end{bmatrix}= \\begin{bmatrix}1 & \\mathbf 0^H \\\\ \n",
    "\\mathbf 0 & \\mathbf Q^H\n",
    "\\end{bmatrix} \\Big(\\begin{bmatrix}\n",
    "1 & \\mathbf 1^H\\\\ \n",
    "\\mathbf 0 & \\mathbf{CD} \\end{bmatrix} \\begin{bmatrix}\n",
    "1 & \\mathbf 0^H \\\\ \n",
    "\\mathbf 0 & \\mathbf{Q}\n",
    "\\end{bmatrix}\\Big) $\n",
    "\n",
    "\n",
    "$\\mathbf V^{H} \\begin{bmatrix}\n",
    "1 & \\mathbf 1^H\\\\ \n",
    "\\mathbf 0 & \\mathbf{CD}\n",
    "\\end{bmatrix} \\mathbf V =  \\begin{bmatrix} 1 & \\mathbf 0^H \\\\ \n",
    "\\mathbf 0 & \\mathbf Q^H\n",
    "\\end{bmatrix} \\Big(\\begin{bmatrix}1 & \\mathbf 1^H \\mathbf Q \\\\ \n",
    "\\mathbf 0 & \\mathbf{CDQ}\n",
    "\\end{bmatrix}\\Big) = \\begin{bmatrix}1 & \\mathbf 1^H \\mathbf Q \\\\ \n",
    "\\mathbf 0 & \\mathbf{Q}^H\\mathbf{ CDQ}\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "1 & \\mathbf 1^H \\mathbf Q \\\\ \n",
    "\\mathbf 0 & \\mathbf{T}\n",
    "\\end{bmatrix}= \\mathbf R$\n",
    "\n",
    "Thus we know that the determinant we want comes from a similar matrix $\\mathbf R$, who's determinant is the product of its eigenvalues (which are along its diagonal).  We further know that this is equal to $1 * det\\big(\\mathbf T\\big) = 1 *det\\big(\\mathbf Q^H \\mathbf{CDQ}\\big) = det\\big(\\mathbf{Q}^H\\big) det\\big(\\mathbf C\\big)\\det(\\mathbf D\\big) det\\big(\\mathbf Q\\big) = det\\big(\\mathbf C\\big)det\\big(\\mathbf D\\big)$, via the fact that upper triangular matrix $\\mathbf T = \\mathbf Q^H \\mathbf{CDQ}$, then applying multiplicative properties of determinants (and perhaps noticing that $\\mathbf{CD}$ is similar to $\\mathbf T$).  \n",
    "\n",
    "Thus $det\\Big(\\begin{bmatrix}\n",
    "1 & \\mathbf 1^H\\\\ \n",
    "\\mathbf 0 & \\mathbf{CD}\n",
    "\\end{bmatrix}\\Big) = det\\Big(\\begin{bmatrix}\n",
    "1 & \\mathbf *\\\\ \n",
    "\\mathbf 0 & \\mathbf{CD}\n",
    "\\end{bmatrix}\\Big) = det\\big(\\mathbf{CD}\\big) = det\\big(\\mathbf{C}\\big) det\\big(\\mathbf{D}\\big)$\n",
    "\n",
    "**end interlude**\n",
    "- - - - -\n",
    "We know that \n",
    "\n",
    "$\\det\\big(\\mathbf{D}\\big) = (a_2-a_1) * (a_3 - a_1) * ... * (a_n - a_1)$\n",
    "\n",
    "because the determininant of a diagonal matrix is the product of its diagonal entries (i.e. its eigenvalues)  \n",
    "\n",
    "and \n",
    "\n",
    "$det \\big(\\mathbf C \\big) = \\prod_{1 \\leq i \\lt j \\leq n-1} (a_j - a_i)$ \n",
    "\n",
    "by inductive hypothesis.  \n",
    "Thus we can say \n",
    "\n",
    "$ det\\big(\\mathbf A^T \\big) = \\big(\\prod_{1 \\leq i \\lt j \\leq n-1} (a_j - a_i)\\big) \\big((a_2-a_1) * (a_3 - a_1) * ... * (a_n - a_1)\\big) = \\prod_{1 \\leq i \\lt j \\leq n} (a_j - a_i)$\n",
    "\n",
    "And the induction is proved.  \n",
    "\n",
    "Finally, we note that $det \\big(\\mathbf A \\big) = det \\big(\\mathbf A^T \\big)$ because $\\mathbf A$ and $\\mathbf A^T$ have the same characteristic polynomials (or equivalently, they have the same eigenvalues). We have thus proved the determinant formula for $\\mathbf A$.  \n",
    "\n",
    "(Technical note: if $\\mathbf A \\in \\mathbb C^{n x n}$ then the above results still hold with respect to the magnitude of the determinant of $\\mathbf A$.  This includes the very important special case of whether or not $\\big\\vert det\\big(\\mathbf A\\big)\\big\\vert = 0$ --i.e. whether or not $\\mathbf A^{-1}$ exists.  However, with respect to the exact determinant, it would be more proper to state that $det\\big(\\mathbf A\\big) = conjugate\\Big(\\det\\big(\\mathbf A^H\\big)\\Big)$. \n",
    "- - - -\n",
    "\n",
    "This gives us another way to confirm that our Vandermonde Matrix is full rank.  We know that a square, finite dimensional matrix is singular iff it has a determinant of 0.  We then see that \n",
    "\n",
    "$\\det \\big(\\mathbf A\\big) = \\big(\\prod_{1 \\leq i \\lt j \\leq n} (a_j - a_i)\\big) = 0$ iff there is some $a_j = a_i$ where $i \\neq j$.  \n",
    "\n",
    "This of course is another way of saying that our Vandermonde Matrix is not full rank if some entry in our 'original' matrix of \n",
    "\n",
    "$\\mathbf a = \\begin{bmatrix}\n",
    "a_1\\\\ \n",
    "a_2\\\\ \n",
    "a_3\\\\ \n",
    "a_4\\\\ \n",
    "a_5\n",
    "\\end{bmatrix}$\n",
    "\n",
    "was not unique.  \n",
    "\n",
    "\n",
    "- - - -\n",
    "It is worth highlighting that if for some reason we did not like to explicitly use determinants, we could instead just repeatedly, and recursively apply the above procedure as a type of Gaussian Elimination, and in the end we would get have transformed $\\mathbf{A}^T$ into the below Row Echelon form: \n",
    "\n",
    "$\\begin{bmatrix}\n",
    "1 & 1 & 1 &  \\dots & 1 & 1\\\\\n",
    "0 &(a_2-a_1) & 1 &   \\dots & 1 & 1\\\\ \n",
    "0& 0 & (a_3 - a_1)(a_3 - a_2)  &\\dots &1 &1 \\\\ \n",
    "\\vdots &\\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\ \n",
    "0&0 & 0 & \\dots & \\big(\\prod_{1 \\leq i \\lt n-1} (a_{n-1} - a_i)\\big) & 1\\\\ \n",
    "0& 0 & 0 & \\dots & 0 & \\big(\\prod_{1 \\leq i \\lt n} (a_{n} - a_i)\\big)\n",
    "\\end{bmatrix}\\mathbf x = \\mathbf b$\n",
    "\n",
    "(Of course, we can immediately notice that the determinant formula can be recovered by multiplying the diagonal elements of the above matrix.)\n",
    "\n",
    "It is instructive to realize that we can solve for an exact $\\mathbf x$ so long as we don't have any zeros on the diagonal of our above upper triangular /row echelon matrix.  We notice that this is the case only if and only if all $a_i$ are unique.\n",
    "\n",
    "- - - -\n",
    "\n",
    "\n",
    "\n",
    "Furthermore, notice that this determinant formula gives us a proof that we have full column rank in any thinner (i.e. more rows than columns) version of our Vandermonde matrix.  E.g. consider the case of \n",
    "\n",
    "\n",
    "$\\mathbf{A} = \\begin{bmatrix}\n",
    "1 & a_1 & a_1^2 & a_1^3\\\\ \n",
    "1 & a_2 & a_2^2 & a_2^3\\\\ \n",
    "1 & a_3 & a_3^2 & a_3^3\\\\ \n",
    "1 & a_4 & a_4^2 & a_4^3\\\\ \n",
    "1 & a_5 & a_5^2 & a_5^3\n",
    "\\end{bmatrix}$\n",
    "\n",
    "\n",
    "These columns must be linearly independent, so long as each $a_i \\neq a_j$ where $i \\neq j$.  If that was not the case, then appending additional columns until square (i.e. append $\\mathbf a \\circ \\mathbf a \\circ \\mathbf a \\circ \\mathbf a$) would mean that \n",
    "\n",
    "$\\mathbf A = \\begin{bmatrix}\n",
    "1 & a_1 & a_1^2 & a_1^3 & a_1^4\\\\ \n",
    "1 & a_2 & a_2^2 & a_2^3 & a_2^4\\\\ \n",
    "1 & a_3 & a_3^2 & a_3^3 & a_3^4\\\\ \n",
    "1 & a_4 & a_4^2 & a_4^3 & a_4^4\\\\ \n",
    "1 & a_5 & a_5^2 & a_5^3 & a_5^4\n",
    "\\end{bmatrix} $\n",
    "\n",
    "could not have full column rank either.  Yet we know this matrix is full rank via our determinant formula (again so long as each $a_i$ is unique) thus we know that the columns of any smaller  \"long and skinny\" version of this matrix must also be linearly independent.\n",
    "\n",
    "Also, when each $a_i$ is unique, since we know that our Vandermonde matrix is full rank, we know that each of its rows is linearly independent.  If for some reason we had a 'short and fat' version of the above matrix, like:\n",
    "\n",
    "$\\mathbf A = \\begin{bmatrix}\n",
    "1 & a_1 & a_1^2 & a_1^3 & a_1^4\\\\ \n",
    "1 & a_2 & a_2^2 & a_2^3 & a_2^4\\\\ \n",
    "1 & a_3 & a_3^2 & a_3^3 & a_3^4\\\\ \n",
    "\\end{bmatrix} $\n",
    "\n",
    "we would know that it is full row rank -- i.e. each of its rows are linearly independent.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Implication: A degree n-1 polynomial is completely given by n uniqe data points**\n",
    "\n",
    "Assuming there is no noise in the data -- or numeric precision issues-- the Vandermonde matrix, $\\mathbf A$, allows you to solve for the unique values in some polynomial with coefficients of \n",
    "\n",
    "\n",
    "$x_0 * 1 + x_1 a + x_2 a^2 + x_3 a^3 + x_4 a^4 = b$\n",
    "\n",
    "\n",
    "- - - - -\n",
    "$\\mathbf{Ax} = \\begin{bmatrix}\n",
    "1 & a_1 & a_1^2 & a_1^3 & a_1^4\\\\ \n",
    "1 & a_2 & a_2^2 & a_2^3 & a_2^4\\\\ \n",
    "1 & a_3 & a_3^2 & a_3^3 & a_3^4\\\\ \n",
    "1 & a_4 & a_4^2 & a_4^3 & a_4^4\\\\ \n",
    "1 & a_5 & a_5^2 & a_5^3 & a_5^4\n",
    "\\end{bmatrix} \\begin{bmatrix}\n",
    "x_0\\\\\n",
    "x_1\\\\ \n",
    "x_2\\\\\n",
    "x_3\\\\\n",
    "x_4\\\\\n",
    "\\end{bmatrix} = \\mathbf b$\n",
    "- - - - -\n",
    "\n",
    "The next extension is perhaps a bit more interesting.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extension: two ways to think about polynomials** \n",
    "\n",
    "Knowing that we can exactly specify a degree $n-1$ polynomial with $n$ distinct data points leads us to wonder:\n",
    "\n",
    "is it 'better' to think about polynomials with respect to the coefficients or the data points?  In the above vector form -- the question becomes is it better to think about the polynomial in terms of $\\mathbf x$ or $\\mathbf b$? \n",
    "\n",
    "The answer is-- it depends.  To directly evaluate a function is much quicker when we know $\\mathbf x$.  But as it turns out, when we want to multiply or convolve polynomials, it is considerably faster to know their point values contained in $\\mathbf b$.  \n",
    "\n",
    "And since the Vandermonde matrix is so helpful for encapsulating all of our knowledge about a polynomial, a natural question is -- what if we wanted to make multiplying $\\mathbf A^{-1} \\mathbf b$ to get $\\mathbf x$ at least as easy as just multiplying $\\mathbf{Ax}$ to get $\\mathbf b$?  The clear answer would mean finding a way so that you don't have to explicitly invert $\\mathbf A$.  This can be done most easily if $\\mathbf A$ is unitary (i.e. orthogonal albeit in a complex inner product space), hence $\\mathbf A^H = \\mathbf A^{-1}$.  If $\\mathbf A$ is unitary, this directly leads us to the Discrete Fourier Transform.  (And from there to the Fast Fourier Transform which is widely regarded as one of the top 10 algorithms of the last 100 years.)\n",
    "\n",
    "But first, let's work through a couple of important related ideas where we can apply Vandermonde matrices: (a) square matrices that have unique eigenvalues must be diagonalizable and (b) some interesting cyclic and structural properties underlying Permutation matrices. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "*small formatting note*: in bold face LaTeX, the capital A, $\\mathbf A$, looks very similar to the capital Lambda, given by $\\mathbf \\Lambda$, which is a diagonal matrix with eigenvalues $\\lambda_k$, along the diagonal.  The rest of this posting will stop using capital A for an operator, accordingly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Application of Vandermonde Matrices: Proof of Linear Independence of Eigenvectors associated with Unique Eigenvalues**\n",
    "\n",
    "This proves that if a square (finite dimensional) matrix --aka an operator --has all eigenvalues that are unique, then the eigenvectors must be linearly independent.  Put differently, this proves that such an operator is diagonalizable.  \n",
    "\n",
    "The typical argument for linear indepdence is in fact a touch shorter than this and does not need Vandermonde matrices -- however it relies on a contradiction that is not particularly satisfying.  The following proof -- adapted from Winitzki's *Linear Algebra via Exterior Products* is direct -- and to your author-- very intuitive.   \n",
    "\n",
    "Consider $\\mathbf B \\in \\mathbb C^{n x n}$ matrix, which has n unique eigenvalues -- i.e. $\\lambda_1 \\neq \\lambda_2 \\neq ... \\neq \\lambda_n$.  \n",
    "\n",
    "When looking for linear indepenence, \n",
    "\n",
    "$\\gamma_1 \\mathbf v_1 + \\gamma_2 \\mathbf v_2 + ... + \\gamma_n \\mathbf v_n = \\mathbf 0$  \n",
    "\n",
    "we can say that **the eigenvectors are linearly independent iff** $\\gamma_1 = \\gamma_2 = ... = \\gamma_n = 0$\n",
    "\n",
    "Further, for $k = \\{1, 2, ..., n\\}$, we know that  \n",
    "$\\mathbf v_k  = \\mathbf v_k$  \n",
    "$\\mathbf B \\mathbf v_k = \\lambda_k \\mathbf v_k$  \n",
    "$\\mathbf B \\mathbf B \\mathbf v_k = \\mathbf B^2 \\mathbf v_k = \\lambda_k^2 \\mathbf v_k$  \n",
    "$\\vdots $  \n",
    "\n",
    "$\\mathbf B^{n-1} \\mathbf v_k = \\lambda_k^{n-1} \\mathbf v_k$  \n",
    "\n",
    "\n",
    "Thus we can take our original linear independence test,\n",
    "\n",
    "$\\gamma_1 \\mathbf v_1 + \\gamma_2 \\mathbf v_2 + ... + \\gamma_n \\mathbf v_n = \\mathbf 0$  \n",
    "\n",
    "and left multiply by $\\mathbf B^r$ and get the following equalities, as well: \n",
    "\n",
    "$\\mathbf B^r \\mathbf 0 = \\mathbf 0 =  \\mathbf B^r \\big(\\gamma_1 \\mathbf v_1 + \\gamma_2 \\mathbf v_2 + ... + \\gamma_n \\mathbf v_n\\big) =  \\lambda_1^r \\gamma_1 \\mathbf v_1 + \\lambda_2^r  \\gamma_2 \\mathbf v_2 + ... + \\lambda_n^r \\gamma_n \\mathbf v_n $  \n",
    "\n",
    "for $r = \\{1, 2, ..., n-1\\}$\n",
    "- - - -\n",
    "\n",
    "Now let's collect these $n$ relationships in a system of equations:\n",
    "\n",
    "\n",
    "$\\bigg[\\begin{array}{c|c|c|c}\n",
    "\\gamma_1 \\mathbf v_1 & \\gamma_2 \\mathbf v_2 &\\cdots & \\gamma_n \\mathbf v_n\n",
    "\\end{array}\\bigg] \\mathbf W = \\bigg[\\begin{array}{c|c|c|c}\n",
    "  \\mathbf 0 & \\mathbf 0 &\\cdots & \\mathbf 0\n",
    "\\end{array}\\bigg]$\n",
    "\n",
    "\n",
    "where \n",
    "\n",
    "$\\mathbf W = \\begin{bmatrix}\n",
    "1 & \\lambda_1 & \\lambda_1^2 & \\dots  & \\lambda_1^{n-1}\\\\ \n",
    "1 & \\lambda_2 & \\lambda_2^2 & \\dots &  \\lambda_2^{n-1} \\\\ \n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\\\ \n",
    "1 & \\lambda_{n} & \\lambda_{n}^{2} & \\dots  & \\lambda_{n}^{n-1}\n",
    "\\end{bmatrix}$\n",
    "\n",
    "\n",
    "Notice that $\\mathbf W$ is a Vandermonde matrix. Since $\\lambda_i \\neq \\lambda_k$ if $i \\neq k$, we know that $det \\big(\\mathbf W\\big) \\neq 0$, and thus $\\mathbf W^{-1}$ exists as a unique operator.  We multiply each term on the right by $\\mathbf W^{-1}$.  \n",
    "\n",
    "$\\bigg[\\begin{array}{c|c|c|c}\n",
    "\\mathbf \\gamma_1 \\mathbf v_1 & \\gamma_2 \\mathbf v_2 &\\cdots & \\gamma_n \\mathbf v_n\n",
    "\\end{array}\\bigg]\n",
    " \\mathbf W \\mathbf W^{-1}= \\bigg[\\begin{array}{c|c|c|c}\n",
    "\\gamma_1 \\mathbf v_1 & \\gamma_2 \\mathbf v_2 &\\cdots & \\gamma_n \\mathbf v_n\n",
    "\\end{array}\\bigg]\\mathbf I = \\bigg[\\begin{array}{c|c|c|c}\n",
    "  \\mathbf 0 & \\mathbf 0 &\\cdots & \\mathbf 0\n",
    "\\end{array}\\bigg] \\mathbf W^{-1} = \\bigg[\\begin{array}{c|c|c|c}\n",
    "  \\mathbf 0 & \\mathbf 0 &\\cdots & \\mathbf 0\n",
    "\\end{array}\\bigg]$  \n",
    "\n",
    "\n",
    "Thus we know that \n",
    "\n",
    "$\\bigg[\\begin{array}{c|c|c|c}\n",
    "\\mathbf \\gamma_1 \\mathbf v_1 & \\gamma_2 \\mathbf v_2 &\\cdots & \\gamma_n \\mathbf v_n\n",
    "\\end{array}\\bigg] = \\bigg[\\begin{array}{c|c|c|c}\n",
    "  \\mathbf 0 & \\mathbf 0 &\\cdots & \\mathbf 0\n",
    "\\end{array}\\bigg]$\n",
    "\n",
    "By definition each eigenvector $\\mathbf v_k \\neq \\mathbf 0$.  This means that each scalar $\\gamma_k = 0$.  Each eigenvector has thus been proven to be linearly independent in the case where eigenvalues are unique.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Permutation Matrices and Periodic Behavior (introducting DFT)\n",
    "\n",
    "Consider an $n$ x $n$ permutation matrix $\\mathbf P$.  Note that this matrix is real valued where each column has all zeros, and a single 1.  \n",
    "\n",
    "We'll use the $^H$ to denote conjugate transpose (even though the matrix is entirely real valued), as some imaginary numbers will creep in later.\n",
    "\n",
    "\n",
    "It is easy to verify that the permutation matrix is unitary (a special case infact which is orthogonal), i.e. that \n",
    "\n",
    "$\\mathbf P^H \\mathbf P = \\mathbf I$\n",
    "\n",
    "because, by construction, each column in a permutation matrix has all zeros, except a single 1, and the Permutation matrix is full rank -- hence each column must be orthogonal.  \n",
    "\n",
    "Further, as mentioned in \"Schurs_Inequality.ipynb\", such a matrix can be diagonalized where   \n",
    "\n",
    "$\\mathbf P = \\mathbf{V\\Lambda V}^H$\n",
    "\n",
    "where $\\mathbf V$ is unitary, each eigenvalue $\\lambda_i$ is contained along the diagonal of $\\mathbf \\Lambda$ and is on the unit circle.  \n",
    "\n",
    "notice that for any permtuation matrix: \n",
    "\n",
    "$\\mathbf {P1} = \\mathbf 1$\n",
    "\n",
    "Hence such a permutation matrix has $\\lambda_1 = 1$ (i.e. a permutation matrix is stochastic -- in fact doubly so).  \n",
    "\n",
    "Because $\\mathbf P$ has all zeros, except a single 1 in each column (or equivalently, in each row), it can be interpretted as a special kind of Adjacency Matrix for a directed graph. \n",
    "- - - - \n",
    "Of particular interest is **the permutation matrix that relates to a connected graph** (i.e. where each node is reachable from each other node) with $n$ nodes.  One example, where $n = 6$ is the below:\n",
    "\n",
    "<table style=\"background-color: white\"></table><tr><td><table><tr><td><img src='images/permutation_graph_matrix.gif'style=\"width: 100%; background-color: white;\"></td><td><img src='images/permutation_matrix_graph.png'style=\"width: 50%; background-color: white;\" ></td></tr></table>\n",
    "\n",
    "\n",
    "\n",
    "*Claim:*  \n",
    "For a permutation matrix associated with a connected graph (i.e. where each node may be visited from each other node), the time it takes to repeat a visit to a node is $n$ iterations. \n",
    "\n",
    "*Proof:*  \n",
    "since each node in the directed graph has an outbound connection to only one other node, and there are $n$ nodes total, if a cycle can occur in $\\leq n - 1$ iterations, then the number of nodes you can reach from the starting node (including itself) is $\\leq n - 1$ nodes, and hence the graph is not connected -- a contradiction. (And for avoidance of doubt, if it took $\\geq n + 1$ iterations to visit the starting node, then that would mean after $n$ iterations, you've visited (at least)  one of the $n-1$ nodes, other than the starting one, more than once which means there is a cycle in the graph $\\leq n-1$ nodes, which is a contradiction, as outlined above.  This second part in many ways is not needed-- we have many other tools to deal with linear dependence after n iterations.  The point is that this type of graph, by construction, has periodicity equal to its number of nodes -- i.e. all cycles take n iterations.)\n",
    "\n",
    "\n",
    "Thus we can say that $\\mathbf P^ 0 = \\mathbf P^n = \\mathbf I$. So $trace\\big(\\mathbf P^0\\big) = trace\\big(\\mathbf P^n\\big) = trace\\big(\\mathbf I\\big) = n$.  \n",
    "\n",
    "However, the diagonal entries of $\\mathbf P^k$ are all zero for $k \\in \\{1, 2, ..., n-2, n-1\\}$. Thus we have:\n",
    "\n",
    "$trace\\big(\\mathbf P^k\\big) = 0$\n",
    "\n",
    "*note: the reader may wonder why I chose a permutation matrix associated with a connected graph -- this post was supposed to be about Vandermonde matrices! The core reasons are simple -- it is a special type of unitary (or orthogonal) matrx, it has a simple visual representation via graph theory, and its trace is extremely easy to compute.  On top of that there is a messier, related reason -- I was inspired by the n iterations cycle that is implied in the proof of linear independence of eigenvectors associated with unique eigenvalues which used a Vandermonde matrix, and I had recently used a connected graph 3 x 3 permutation matrix (and its eigenvalues) to explain to someone the that 3 complex numbers on the unit circle must be equidistant when there is a constraint they all sum to zero  -- I knew that first eigenvalue had to be one because the matrix is stochastic, I knew the trace was 0, and I knew that for a real valued matrix, complex eigenvalues numbers come in conjugate pairs and hence in the 3 x 3 case they had to be evenly spaced in the unit circle.  In many respects this posting grew out of my attempt to generalize ideas from that conversation, plus a growing interest I had in Vandermonde matrices and matrices used in convolutions. I chose to use permutation matrices associated with a connected graph for those reasons and was pleasantly surprised that I was able to derive the DFT and all of its properties in full, just using this permutation matrix and spectral theory, for an arbitrary $n$ x $n$ dimensional case.* \n",
    "\n",
    "Now consider the standard basis vectors $\\mathbf e_j$, where $j \\in \\{1, 2, ..., n-1, n\\}$ -- i.e. column slices of the identity matrix, shown below:\n",
    "\n",
    "$\\bigg[\\begin{array}{c|c|c|c}\n",
    "\\mathbf e_1 & \\mathbf  e_2 &\\cdots &\\mathbf  e_n\n",
    "\\end{array}\\bigg] = \\mathbf I$\n",
    "\n",
    "Each one of these vectors is a valid starting position for having a 'presence' on exactly one node of the graph. With no loss of generality, we could choose just one the standard basis vectors to be our starting point -- e.g. set $\\mathbf e_j := \\mathbf e_1$.  However, we'll keep the notation $\\mathbf e_j$, though the reader may decide to select a specific standard basis vector if helpful.\n",
    "\n",
    "Since we have a connected graph and can only be on one position at a time as we iterate though, we know that $\\mathbf P^k \\mathbf e_j \\perp \\mathbf P^r \\mathbf e_j$,\n",
    "\n",
    "for natural numbers $r$, $k$, $\\in \\{1, 2, ..., n-2, n-1\\}$, where $r \\neq k$\n",
    "\n",
    "Thus we can collect each location in the graph in an $n$ x $n$ matrix as below:\n",
    "\n",
    "$\\bigg[\\begin{array}{c|c|c|c|c}\n",
    "\\mathbf P^0\\mathbf e_j & \\mathbf P^1\\mathbf e_j & \\mathbf P^2\\mathbf e_j &\\cdots & \\mathbf P^{n-1}\\mathbf e_j\n",
    "\\end{array}\\bigg] = \\bigg[\\begin{array}{c|c|c|c|c}\n",
    "\\mathbf V \\mathbf \\Lambda^0 \\mathbf V^H\\mathbf e_j & \\mathbf V \\mathbf \\Lambda^1 \\mathbf V^H\\mathbf e_j & \\mathbf V \\mathbf \\Lambda^2 \\mathbf V^H\\mathbf e_j &\\cdots & \\mathbf V \\mathbf \\Lambda^{n-1} \\mathbf V^H\\mathbf e_j\n",
    "\\end{array}\\bigg]$\n",
    "\n",
    "\n",
    "\n",
    "$\\bigg[\\begin{array}{c|c|c|c|c}\n",
    "\\mathbf P^0\\mathbf e_j & \\mathbf P^1\\mathbf e_j & \\mathbf P^2\\mathbf e_j &\\cdots & \\mathbf P^{n-1}\\mathbf e_j\n",
    "\\end{array}\\bigg] = \\mathbf V \\bigg[\\begin{array}{c|c|c|c|c}\n",
    "\\mathbf \\Lambda^0 \\mathbf V^H\\mathbf e_j & \\mathbf \\Lambda^1 \\mathbf V^H\\mathbf e_j & \\mathbf \\Lambda^2 \\mathbf V^H\\mathbf e_j &\\cdots & \\mathbf \\Lambda^{n-1} \\mathbf V^H\\mathbf e_j\n",
    "\\end{array}\\bigg]$\n",
    "\n",
    "Now left multiply each side by full rank, unitary matrix $\\mathbf V^H$, and for notational simplity, let $\\mathbf y := \\mathbf V^H\\mathbf e_j$\n",
    "\n",
    "\n",
    "$\\mathbf V^H \\bigg[\\begin{array}{c|c|c|c|c}\n",
    "\\mathbf P^0\\mathbf e_j & \\mathbf P^1\\mathbf e_j & \\mathbf P^2\\mathbf e_j &\\cdots & \\mathbf P^{n-1}\\mathbf e_j\n",
    "\\end{array}\\bigg] = \\bigg[\\begin{array}{c|c|c|c|c}\n",
    "\\mathbf \\Lambda^0 \\mathbf y & \\mathbf \\Lambda^1 \\mathbf y & \\mathbf \\Lambda^2 \\mathbf y &\\cdots & \\mathbf \\Lambda^{n-1} \\mathbf y\n",
    "\\end{array}\\bigg]$\n",
    "\n",
    "For each column vector on the right hand side, we have $\\mathbf \\Lambda^m \\mathbf y$.  In various forms this can be written as \n",
    "\n",
    "$\\mathbf \\Lambda^m \\mathbf y =  \\mathbf \\Lambda^m \\big(\\mathbf{Diag}\\big(\\mathbf y\\big)\\mathbf 1\\big) = \\mathbf{Diag}\\big(\\mathbf y\\big) \\mathbf \\Lambda^m \\mathbf 1 =\n",
    "\\mathbf{Diag}\\big(\\mathbf y\\big)\\big(\\mathbf \\Lambda^m \\mathbf 1\\big)$\n",
    "\n",
    "Thus we can say: \n",
    "\n",
    "$\\bigg[\\begin{array}{c|c|c|c|c}\n",
    "\\mathbf \\Lambda^0 \\mathbf y & \\mathbf \\Lambda^1 \\mathbf y & \\mathbf \\Lambda^2 \\mathbf y &\\cdots & \\mathbf \\Lambda^{n-1} \\mathbf y\n",
    "\\end{array}\\bigg] = \\bigg[\\begin{array}{c|c|c|c|c}\n",
    "\\mathbf{Diag}\\big(\\mathbf y\\big)\\mathbf \\Lambda^0 \\mathbf 1 & \\mathbf{Diag}\\big(\\mathbf y\\big)\\mathbf \\Lambda^1 \\mathbf 1 & \\mathbf{Diag}\\big(\\mathbf y\\big)\\mathbf \\Lambda^2 \\mathbf 1 &\\cdots & \\mathbf{Diag}\\big(\\mathbf y\\big)\\mathbf \\Lambda^{n-1} \\mathbf 1\n",
    "\\end{array}\\bigg] $\n",
    "\n",
    "\n",
    "\n",
    "$\\bigg[\\begin{array}{c|c|c|c|c}\n",
    "\\mathbf \\Lambda^0 \\mathbf y & \\mathbf \\Lambda^1 \\mathbf y & \\mathbf \\Lambda^2 \\mathbf y &\\cdots & \\mathbf \\Lambda^{n-1} \\mathbf y\n",
    "\\end{array}\\bigg] = \\mathbf{Diag}\\big(\\mathbf y\\big)\\bigg[\\begin{array}{c|c|c|c|c}\n",
    "\\mathbf \\Lambda^0 \\mathbf 1 & \\mathbf \\Lambda^1 \\mathbf 1 & \\mathbf \\Lambda^2 \\mathbf 1 &\\cdots & \\mathbf \\Lambda^{n-1} \\mathbf 1\n",
    "\\end{array}\\bigg] $\n",
    "\n",
    "\n",
    "We make this substitution and see: \n",
    "\n",
    "$\\mathbf V^H \\bigg[\\begin{array}{c|c|c|c|c}\n",
    "\\mathbf P^0\\mathbf e_j & \\mathbf P^1\\mathbf e_j & \\mathbf P^2\\mathbf e_j &\\cdots & \\mathbf P^{n-1}\\mathbf e_j\n",
    "\\end{array}\\bigg] = \\mathbf{Diag}\\big(\\mathbf y\\big)\\bigg[\\begin{array}{c|c|c|c|c}\n",
    "\\mathbf \\Lambda^0 \\mathbf 1 & \\mathbf \\Lambda^1 \\mathbf 1 & \\mathbf \\Lambda^2 \\mathbf 1 &\\cdots & \\mathbf \\Lambda^{n-1} \\mathbf 1\n",
    "\\end{array}\\bigg] $\n",
    "\n",
    "From here we may notice that since the left hand side is full rank, the right hand side must be full rank as well. \n",
    "\n",
    "Actually, we know consideraably more than this -- i.e. we know that the left hand side is unitary. \n",
    "\n",
    "where we have $\\mathbf X = f(\\mathbf e_j) = \\bigg[\\begin{array}{c|c|c|c|c}\n",
    "\\mathbf P^0\\mathbf e_j & \\mathbf P^1\\mathbf e_j & \\mathbf P^2\\mathbf e_j &\\cdots & \\mathbf P^{n-1}\\mathbf e_j\n",
    "\\end{array}\\bigg]$\n",
    "\n",
    "In general $\\mathbf X = f(\\mathbf s)$, is called a **circulant matrix**.  For now we confine ourselves to the case where $\\mathbf s := \\mathbf e_j$, though we'll loosen up this restriction at the end of this writeup. \n",
    "\n",
    "earlier we noted that:\n",
    "\n",
    "$\\mathbf P^k \\mathbf e_j \\perp \\mathbf P^r \\mathbf e_j$\n",
    "\n",
    "\n",
    "and of course \n",
    "\n",
    "$\\big \\Vert \\mathbf P^m \\mathbf e_j\\big \\Vert_2^2 = \\big(\\mathbf P^m \\mathbf e_j\\big)^H\\big(\\mathbf P^m \\mathbf e_j\\big) = \\mathbf e_j^H \\big(\\mathbf P^m\\big)^H  \\mathbf P^m \\mathbf e_j = \\mathbf e_j^H \\mathbf I \\mathbf e_j = \\mathbf e_j^H \\mathbf e_j = 1$\n",
    "\n",
    "Thus each column in $\\mathbf X$ is mutually orthonormal -- and $\\mathbf U$ is $n$ x $n$ so it is a (real valued) unitary matrix. \n",
    "\n",
    "--\n",
    "\n",
    "From here we see\n",
    "\n",
    "$\\big(\\mathbf V^H \\mathbf X\\big)^H \\mathbf V^H \\mathbf X = \\mathbf X^H \\big(\\mathbf V \\mathbf V^H\\big) \\mathbf X = \\mathbf X^H \\mathbf X = \\mathbf I$\n",
    "\n",
    "So we know that the left hand side in unitary.  This means that the right handside must be unitary as well. \n",
    "\n",
    "Since the right hand side is unitary, that means it must be non-singular.  \n",
    "\n",
    "Note that with respect to determinants, we could say:\n",
    "\n",
    "$ \\Big \\vert Det\\Big(\\mathbf{Diag} \\big(\\mathbf y\\big)\\Big)\\Big \\vert*\\Big \\vert Det\\Big( \\bigg[\\begin{array}{c|c|c|c|c}\n",
    "\\mathbf \\Lambda^0 \\mathbf 1 & \\mathbf \\Lambda^1 \\mathbf 1 & \\mathbf \\Lambda^2 \\mathbf 1 &\\cdots & \\mathbf \\Lambda^{n-1} \\mathbf 1\n",
    "\\end{array}\\bigg]\\Big) \\Big \\vert = 1$\n",
    "\n",
    "Thus \n",
    "\n",
    "$Det\\Big( \\bigg[\\begin{array}{c|c|c|c|c}\n",
    "\\mathbf \\Lambda^0 \\mathbf 1 & \\mathbf \\Lambda^1 \\mathbf 1 & \\mathbf \\Lambda^2 \\mathbf 1 &\\cdots & \\mathbf \\Lambda^{n-1} \\mathbf 1\n",
    "\\end{array}\\bigg]\\Big) \\neq 0$\n",
    "\n",
    "Finally, we 'unpack' this matrix and see that\n",
    "\n",
    "$\\bigg[\\begin{array}{c|c|c|c|c}\n",
    "\\mathbf \\Lambda^0 \\mathbf 1 & \\mathbf \\Lambda^1 \\mathbf 1 & \\mathbf \\Lambda^2 \\mathbf 1 &\\cdots & \\mathbf \\Lambda^{n-1} \\mathbf 1\n",
    "\\end{array}\\bigg]=\\begin{bmatrix}\n",
    "1 & \\lambda_1 & \\lambda_1^2 & \\dots  & \\lambda_1^{n-1}\\\\ \n",
    "1 & \\lambda_2 & \\lambda_2^2 & \\dots &  \\lambda_2^{n-1} \\\\ \n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\\\ \n",
    "1 & \\lambda_{n} & \\lambda_{n}^{2} & \\dots  & \\lambda_{n}^{n-1}\n",
    "\\end{bmatrix}$\n",
    "\n",
    "This is the Vandermonde matrix, which is non-singular **iff**  each $\\lambda_i$ is unique.  Thus we conclude that each $\\lambda_i$ for our Permutation matrix of a connected graph must be unique.\n",
    "\n",
    "**claim:**\n",
    "\n",
    "$\\bigg[\\begin{array}{c|c|c|c|c}\n",
    "\\mathbf \\Lambda^0 \\mathbf 1 & \\mathbf \\Lambda^1 \\mathbf 1 & \\mathbf \\Lambda^2 \\mathbf 1 &\\cdots & \\mathbf \\Lambda^{n-1} \\mathbf 1\n",
    "\\end{array}\\bigg]^H \\bigg[\\begin{array}{c|c|c|c|c}\n",
    "\\mathbf \\Lambda^0 \\mathbf 1 & \\mathbf \\Lambda^1 \\mathbf 1 & \\mathbf \\Lambda^2 \\mathbf 1 &\\cdots & \\mathbf \\Lambda^{n-1} \\mathbf 1\n",
    "\\end{array}\\bigg] = n \\mathbf I$\n",
    "\n",
    "That is, the columns in the above matrix are mutually orthogonal (aka have an inner product of zero), and subject to some normalizing scalar constant, we know that the matrix is unitary. \n",
    "\n",
    "**proof:**  \n",
    "First notice that each column has a squared L2 norm of $n$\n",
    "\n",
    "for $m = \\{0, 1, 2, ..., n-1\\}$\n",
    "\n",
    "$\\big(\\mathbf \\Lambda^m \\mathbf 1\\big)^H \\mathbf \\Lambda^m \\mathbf 1 = \\mathbf 1^H \\big(\\mathbf \\Lambda^m\\big)^H \\mathbf \\Lambda^m \\mathbf 1 = \\mathbf 1^H \\big(\\mathbf \\Lambda^m\\big)^{-1} \\mathbf \\Lambda^m \\mathbf 1 = \\mathbf 1^H \\big(\\mathbf I\\big) \\mathbf 1 = trace \\big(\\mathbf I\\big)  = n$ \n",
    "\n",
    "note that when we say $\\mathbf 1^H \\big(\\mathbf I\\big) \\mathbf 1 = trace \\big(\\mathbf I\\big)$, we notice first that $\\mathbf 1^H \\big(\\mathbf Z\\big) \\mathbf 1$, means to sum up all entries in some operator $\\mathbf Z$, and if $\\mathbf Z$ is a diagonal matrix, then this is equivalent to just summing the entries along the diagonal of $\\mathbf Z$ which is equal to the trace of $\\mathbf Z$.\n",
    "\n",
    "Next we want to prove that the inner product of any column $j$ with some other column $\\neq j$, is zero.\n",
    "\n",
    "Thus we are interested in the cases of\n",
    "\n",
    "$\\big(\\mathbf \\Lambda^r \\mathbf 1\\big)^H \\mathbf \\Lambda^k \\mathbf 1$ \n",
    "\n",
    "for all natural numbers $k$, $r$, *first* where $0\\leq r \\lt k \\leq n-1$ and *second* where $0\\leq  k \\lt r \\leq n-1$.\n",
    "\n",
    "First we observe the $r \\lt k$ case:\n",
    "\n",
    "$\\big(\\mathbf \\Lambda^r \\mathbf 1\\big)^H \\mathbf \\Lambda^k \\mathbf 1 = \\mathbf 1^H \\big(\\mathbf \\Lambda^r\\big)^H \\mathbf \\Lambda^{k} \\mathbf 1 = \\mathbf 1^H \\Big(\\big( \\mathbf \\Lambda^{-r} \\big) \\mathbf \\Lambda^{k}\\Big) \\mathbf 1 = \\mathbf 1^H \\Big( \\mathbf \\Lambda^{k - r}\\Big) \\mathbf 1 = trace\\big(\\mathbf \\Lambda^{k - r}\\big) $ \n",
    "\n",
    "since $k \\gt r$, we know $0 \\lt k - r \\leq n-1$.  Put differently $(k - r) \\%n \\neq 0$\n",
    "\n",
    "Thus $\\big(\\mathbf \\Lambda^r \\mathbf 1\\big)^H \\mathbf \\Lambda^k \\mathbf 1 = trace\\big(\\mathbf \\Lambda^{k - r}\\big) = trace\\big(\\mathbf Q^H \\mathbf P^{k - r} \\mathbf Q\\big) = trace\\big(\\mathbf Q \\mathbf Q^H \\mathbf P^{k - r}\\big) = trace\\big(\\mathbf P^{k - r}\\big) = 0$\n",
    "\n",
    "note that inner products are symmetric -- except for complex conjugation-- so in the case of an inner product equal to zero, we have \n",
    "\n",
    "$\\Big(\\big(\\mathbf \\Lambda^r \\mathbf 1\\big)^H \\mathbf \\Lambda^k \\mathbf 1\\Big)^H = trace\\big(\\mathbf \\Lambda^{k - r}\\big)^H  = 0^H = 0$\n",
    "\n",
    "which covers the second case.\n",
    "\n",
    "\n",
    "Thus all columns in\n",
    "\n",
    "$\\bigg[\\begin{array}{c|c|c|c|c}\n",
    "\\mathbf \\Lambda^0 \\mathbf 1 & \\mathbf \\Lambda^1 \\mathbf 1 & \\mathbf \\Lambda^2 \\mathbf 1 &\\cdots & \\mathbf \\Lambda^{n-1} \\mathbf 1\n",
    "\\end{array}\\bigg]$\n",
    "\n",
    "have a squared length of $n$ and are mutually orthgonal.\n",
    "\n",
    "Hence we can say:\n",
    "\n",
    "$\\frac{1}{\\sqrt n} \\bigg[\\begin{array}{c|c|c|c|c}\n",
    "\\mathbf \\Lambda^0 \\mathbf 1 & \\mathbf \\Lambda^1 \\mathbf 1 & \\mathbf \\Lambda^2 \\mathbf 1 &\\cdots & \\mathbf \\Lambda^{n-1} \\mathbf 1\n",
    "\\end{array}\\bigg] = \\mathbf F$\n",
    "\n",
    "is a unitary matrix.  In fact this matrix $\\mathbf F$ is the discrete Fourier transform matrix.  \n",
    "\n",
    "*note: in some cases, we may use the the conjugate transpose of this matrix, or another variant, as the DFT.  This is ultimately just a book-keeping adjustment*\n",
    "\n",
    "\n",
    "\n",
    "# Claim: \n",
    "# The DFT Matrix is the collection of eigenvectors for a circulant matrix\n",
    "\n",
    "We say that this circulant matrix is given by $\\mathbf X$  \n",
    "\n",
    "\n",
    "When we look at \n",
    "\n",
    "$\\mathbf V^H \\bigg[\\begin{array}{c|c|c|c|c}\n",
    "\\mathbf P^0\\mathbf e_j & \\mathbf P^1\\mathbf e_j & \\mathbf P^2\\mathbf e_j &\\cdots & \\mathbf P^{n-1}\\mathbf e_j\n",
    "\\end{array}\\bigg] = \\mathbf{Diag}\\big(\\mathbf y\\big) \\bigg[\\begin{array}{c|c|c|c|c}\n",
    "\\mathbf \\Lambda^0 \\mathbf 1 & \\mathbf \\Lambda^1 \\mathbf 1 & \\mathbf \\Lambda^2 \\mathbf 1 &\\cdots & \\mathbf \\Lambda^{n-1} \\mathbf 1\n",
    "\\end{array}\\bigg]$\n",
    "\n",
    "we can re-write this as\n",
    "\n",
    "$\\mathbf V^H \\bigg[\\begin{array}{c|c|c|c|c}\n",
    "\\mathbf P^0\\mathbf e_j & \\mathbf P^1\\mathbf e_j & \\mathbf P^2\\mathbf e_j &\\cdots & \\mathbf P^{n-1}\\mathbf e_j\n",
    "\\end{array}\\bigg] = \\sqrt(n)\\mathbf{Diag}\\big(\\mathbf y\\big) \\frac{1}{\\sqrt(n)}\\bigg[\\begin{array}{c|c|c|c|c}\n",
    "\\mathbf \\Lambda^0 \\mathbf 1 & \\mathbf \\Lambda^1 \\mathbf 1 & \\mathbf \\Lambda^2 \\mathbf 1 &\\cdots & \\mathbf \\Lambda^{n-1} \\mathbf 1\n",
    "\\end{array}\\bigg] = \\sqrt(n) \\mathbf{Diag}\\big(\\mathbf y\\big) \\mathbf F$\n",
    "\n",
    "we can recongize that this is a form of the singular value decompostion on our matrix $\\mathbf X$ (so long as we relax the constraint that the diagonal matrix is real-valued, non-negative).  That is, we have \n",
    "\n",
    "$\\mathbf X = \\bigg[\\begin{array}{c|c|c|c|c}\n",
    "\\mathbf P^0\\mathbf e_j & \\mathbf P^1\\mathbf e_j & \\mathbf P^2\\mathbf e_j &\\cdots & \\mathbf P^{n-1}\\mathbf e_j\n",
    "\\end{array}\\bigg] = \\mathbf V \\Big(\\sqrt(n) \\mathbf{Diag} \\big(\\mathbf y\\big)\\Big)\\mathbf F$\n",
    "\n",
    "In the above case, $\\mathbf X$ is decomposed into unitary matrix $\\mathbf V$ times a diagonal matrix times unitary matrix $\\mathbf F$.  \n",
    "\n",
    "\n",
    "In this case, $\\mathbf X$ is itself unitary and per the note in \"Schurs_Inequality.ipynb\", that means that $\\mathbf X$ is normal.  Since $\\mathbf X$ is normal, this means that our Singular Value Decomposition in fact gives us an eigenvalue decomposition.  Put differently we can set our left singular and right singular vectors to be equal, and allocate everything else to the middle diagonal matrix.  \n",
    "\n",
    "$\\mathbf V := \\mathbf F^H $\n",
    "\n",
    "$\\mathbf X = \\mathbf F^H \\mathbf D_j \\mathbf F$\n",
    "\n",
    "or equivalently\n",
    "\n",
    "$\\mathbf F \\mathbf X \\mathbf F^H =  \\mathbf D_j $\n",
    "\n",
    "Thus in the above we can say $\\mathbf X$ is unitarily similar to a diagonal matrix $\\mathbf D_j$ with $\\mathbf F$ containing the eigenvectors.  \n",
    "\n",
    "Which is another way of saying that our unitary Vandermonde matrix $\\mathbf F$ **contains the mutually orthonormal collection of eigenvectors for** $\\mathbf X$.  \n",
    "\n",
    "This immediately motivates the question-- what if $\\mathbf X$ was a function of permuting some different, arbitrary vector $\\mathbf s$ i.e. if $\\mathbf X = f(\\mathbf s)$ -- could we still say $\\mathbf X$ is unitarily similar to a diagonal matrix with $\\mathbf F$ containing the eigenvectors?  The answer is yes, though it takes a little bit more work to show it.\n",
    "\n",
    "\n",
    "\n",
    "# Short form proof\n",
    "\n",
    "for a quick take, consider that \n",
    "\n",
    "$\\mathbf F \\big(f(\\mathbf e_j)\\big) \\mathbf F^H = \\mathbf D_j$\n",
    "\n",
    "where $\\mathbf D_j$ denotes some diagonal matrix similar to our function applied on the jth standard basis vector.\n",
    "\n",
    "The standard basis vectors form a basis so we can write $\\mathbf s$ in terms of them:  \n",
    "$\\mathbf s = \\gamma_1 \\mathbf e_1 + \\gamma_2 \\mathbf e_2 + ... + \\gamma_n \\mathbf e_n  = \\big(\\Sigma_{j=1}^{n} \\gamma_j \\mathbf e_j\\big)$\n",
    "\n",
    "Then, using linearity we can say \n",
    "\n",
    "$\\mathbf X = f(\\mathbf s) = f(\\gamma_1 \\mathbf e_1 + \\gamma_2 \\mathbf e_2 + ... + \\gamma_n \\mathbf e_n) = f(\\gamma_1 \\mathbf e_1) + f(\\gamma_2 \\mathbf e_2) + ... + (\\gamma_n \\mathbf e_n)$\n",
    "\n",
    "left multiply each side by $\\mathbf F$ and right multiply each side by $\\mathbf F^H$, and we get\n",
    "\n",
    "$\\mathbf F \\big(\\mathbf X\\big) \\mathbf F^H = \\mathbf F \\big(f(\\mathbf s)\\big) \\mathbf F^H= \\mathbf F \\big( f(\\gamma_1 \\mathbf e_1)\\big)\\mathbf F^H + \\mathbf F\\big(f(\\gamma_2 \\mathbf e_2)\\big)\\mathbf F^H + ... + \\mathbf F\\big((\\gamma_n \\mathbf e_n)\\big)\\mathbf F^H = \\gamma_1 \\mathbf D_1 + \\gamma_2 \\mathbf D_2 + ... + \\gamma_n \\mathbf D_n$\n",
    "\n",
    "The sum of a sequence of diagonal matrices is a diagonal matrix, hence we can can say that using $\\mathbf F$, we find that $\\mathbf X$ is unitarily similar to a diagonal matrix. \n",
    "\n",
    "\n",
    "# Begin Long Form Proof\n",
    "\n",
    "To begin, notice that by linearity\n",
    "\n",
    "$f(\\mathbf e_1) + f(\\mathbf e_2) = f(\\mathbf e_1 + \\mathbf e_2)$  \n",
    "\n",
    "Written in terms of a matrix, this is:  \n",
    "\n",
    "$\\bigg[\\begin{array}{c|c|c|c}\n",
    "\\mathbf P^0\\mathbf e_1 & \\mathbf P^1\\mathbf e_1 & \\cdots & \\mathbf P^{n-1}\\mathbf e_1\n",
    "\\end{array}\\bigg] + \\bigg[\\begin{array}{c|c|c|c}\n",
    "\\mathbf P^0\\mathbf e_2 & \\mathbf P^1\\mathbf e_2 & \\cdots & \\mathbf P^{n-1}\\mathbf e_2\n",
    "\\end{array}\\bigg] = \\bigg[\\begin{array}{c|c|c|c}\n",
    "\\mathbf P^0 \\mathbf e_1 +  \\mathbf P^0 \\mathbf e_2 & \\mathbf P^1 \\mathbf e_1 + \\mathbf P^1 \\mathbf e_2 & \\cdots & \\mathbf P^{n-1} \\mathbf e_1 + \\mathbf P^{n-1} \\mathbf e_2 \n",
    "\\end{array}\\bigg]$\n",
    "\n",
    "which we can restate as \n",
    "\n",
    "$\\bigg[\\begin{array}{c|c|c|c}\n",
    "\\mathbf P^0\\mathbf e_1 & \\mathbf P^1\\mathbf e_1 & \\cdots & \\mathbf P^{n-1}\\mathbf e_1\n",
    "\\end{array}\\bigg] + \\bigg[\\begin{array}{c|c|c|c}\n",
    "\\mathbf P^0\\mathbf e_2 & \\mathbf P^1\\mathbf e_2 & \\cdots & \\mathbf P^{n-1}\\mathbf e_2\n",
    "\\end{array}\\bigg] = \\bigg[\\begin{array}{c|c|c|c}\n",
    "\\mathbf P^0\\big(\\mathbf e_1 +  \\mathbf e_2\\big) & \\mathbf P^1 \\big(\\mathbf e_1 + \\mathbf e_2\\big) & \\cdots & \\mathbf P^{n-1}\\big(\\mathbf e_1 + \\mathbf e_2 \\big)\n",
    "\\end{array}\\bigg]$\n",
    "\n",
    "\n",
    "If we left multiply by $\\mathbf F$, what we get is\n",
    "\n",
    "\n",
    "$\\mathbf F \\bigg[\\begin{array}{c|c|c|c}\n",
    "\\mathbf P^0\\mathbf e_1 & \\mathbf P^1\\mathbf e_1 & \\cdots & \\mathbf P^{n-1}\\mathbf e_1\n",
    "\\end{array}\\bigg] + \\mathbf F \\bigg[\\begin{array}{c|c|c|c}\n",
    "\\mathbf P^0\\mathbf e_2 & \\mathbf P^1\\mathbf e_2 &\\cdots & \\mathbf P^{n-1}\\mathbf e_2\n",
    "\\end{array}\\bigg]= \\mathbf D_1\\mathbf F + \\mathbf D_2\\mathbf F$\n",
    "\n",
    "$= \\big(\\mathbf D_1 + \\mathbf D_2\\big) \\mathbf F = \\mathbf F \\bigg[\\begin{array}{c|c|c|c}\n",
    "\\mathbf P^0\\big(\\mathbf e_1 +  \\mathbf e_2\\big) & \\mathbf P^1 \\big(\\mathbf e_1 + \\mathbf e_2\\big) &\\cdots & \\mathbf P^{n-1}\\big(\\mathbf e_1 + \\mathbf e_2 \\big)\n",
    "\\end{array}\\bigg]$\n",
    "\n",
    "\n",
    "and to further generalize this, notice that if we added all of the standard basis vectors, we'd get the ones vector.    Where $\\mathbf D_j$ is a diagonal matrix similar to the permutation matrix given by $f(\\mathbf e_j)$.\n",
    "We can write this as:  \n",
    "\n",
    "$\\mathbf {11}^H = \\Sigma_{j=1}^{n}\\Big(\\bigg[\\begin{array}{c|c|c|c}\n",
    "\\mathbf P^0\\mathbf e_j & \\mathbf P^1\\mathbf e_j & \\cdots & \\mathbf P^{n-1}\\mathbf e_j\n",
    "\\end{array}\\bigg]\\Big) = \\bigg[\\begin{array}{c|c|c|c}\n",
    "\\mathbf P^0\\big(\\Sigma_{j=1}^{n} \\mathbf e_j \\big) & \\mathbf P^1 \\big(\\Sigma_{j=1}^{n} \\mathbf e_j \\big) & \\cdots & \\mathbf P^{n-1}\\big(\\Sigma_{j=1}^{n} \\mathbf e_j \\big)\n",
    "\\end{array}\\bigg]$  \n",
    "\n",
    "and if we left multiply by $\\mathbf F$, we get\n",
    "\n",
    "$\\mathbf F \\big(\\mathbf {11}^H \\big) = \\Sigma_{j=1}^{n}\\Big(\\mathbf F\\bigg[\\begin{array}{c|c|c|c}\n",
    "\\mathbf P^0\\mathbf e_j & \\mathbf P^1\\mathbf e_j & \\cdots & \\mathbf P^{n-1}\\mathbf e_j\n",
    "\\end{array}\\bigg]\\Big) = \\Sigma_{j=1}^{n} \\big(\\mathbf D_j \\mathbf F\\big) = \\big(\\Sigma_{j=1}^{n}\\mathbf D_j\\big) \\mathbf F$  \n",
    "\n",
    "From here, consider what would happen if we instead decided to scale each standard basis vector, $\\mathbf e_j$, by some arbitrary amount, $\\gamma_j$, giving us the following expression:  \n",
    "\n",
    "$\\Sigma_{j=1}^{n}\\Big(\\gamma_j \\bigg[\\begin{array}{c|c|c|c}\n",
    "\\mathbf P^0\\mathbf e_j & \\mathbf P^1\\mathbf e_j & \\cdots & \\mathbf P^{n-1} \\mathbf e_j\n",
    "\\end{array}\\bigg]\\Big) = \\Sigma_{j=1}^{n}\\Big(\\bigg[\\begin{array}{c|c|c|c}\n",
    "\\mathbf P^0\\big(\\gamma_j \\mathbf e_j\\big) & \\mathbf P^1\\big(\\gamma_j \\mathbf e_j\\big) & \\cdots & \\mathbf P^{n-1} \\big(\\gamma_j  \\mathbf e_j\\big)\n",
    "\\end{array}\\bigg]\\Big)$\n",
    "\n",
    "which can be restated as  \n",
    "\n",
    "$\\Sigma_{j=1}^{n}\\Big(\\gamma_j \\bigg[\\begin{array}{c|c|c|c}\n",
    "\\mathbf P^0\\mathbf e_j & \\mathbf P^1\\mathbf e_j & \\cdots & \\mathbf P^{n-1} \\mathbf e_j\n",
    "\\end{array}\\bigg]\\Big)  = \\bigg[\\begin{array}{c|c|c|c|c}\n",
    "\\mathbf P^0\\big(\\Sigma_{j=1}^{n} \\gamma_j \\mathbf e_j \\big) & \\mathbf P^1 \\big(\\Sigma_{j=1}^{n}\\gamma_j \\mathbf e_j \\big) & \\cdots & \\mathbf P^{n-1}\\big(\\Sigma_{j=1}^{n} \\gamma_j \\mathbf e_j \\big)\n",
    "\\end{array}\\bigg]$ \n",
    "\n",
    "again, left multiply this expression by $\\mathbf F$ and we see\n",
    "\n",
    "$\\mathbf F\\bigg[\\begin{array}{c|c|c|c|c}\n",
    "\\mathbf P^0\\big(\\Sigma_{j=1}^{n} \\gamma_j \\mathbf e_j \\big) & \\mathbf P^1 \\big(\\Sigma_{j=1}^{n}\\gamma_j \\mathbf e_j \\big) & \\cdots & \\mathbf P^{n-1}\\big(\\Sigma_{j=1}^{n} \\gamma_j \\mathbf e_j \\big)\n",
    "\\end{array}\\bigg] = \\Sigma_{j=1}^{n}\\Big(\\gamma_j \\mathbf F\\bigg[\\begin{array}{c|c|c|c}\n",
    "\\mathbf P^0\\mathbf e_j & \\mathbf P^1\\mathbf e_j & \\cdots & \\mathbf P^{n-1} \\mathbf e_j\n",
    "\\end{array}\\bigg]\\Big)$\n",
    "\n",
    "from here notice  \n",
    "\n",
    "$ \\Sigma_{j=1}^{n}\\Big(\\gamma_j \\mathbf F\\bigg[\\begin{array}{c|c|c|c}\n",
    "\\mathbf P^0\\mathbf e_j & \\mathbf P^1\\mathbf e_j & \\cdots & \\mathbf P^{n-1} \\mathbf e_j\n",
    "\\end{array}\\bigg]\\Big) = \\Sigma_{j=1}^{n}\\gamma_j\\Big( \\mathbf F\\bigg[\\begin{array}{c|c|c|c}\n",
    "\\mathbf P^0\\mathbf e_j & \\mathbf P^1\\mathbf e_j & \\cdots & \\mathbf P^{n-1} \\mathbf e_j\n",
    "\\end{array}\\bigg]\\Big) = \\Sigma_{j=1}^{n} \\mathbf \\gamma_j \\big(\\mathbf D_j \\mathbf F\\big) = \\big(\\Sigma_{j=1}^{n}\\mathbf \\gamma_j \\mathbf D_j\\big) \\mathbf F$\n",
    "\n",
    "Thus we say\n",
    "\n",
    "$\\mathbf F\\bigg[\\begin{array}{c|c|c|c|c}\n",
    "\\mathbf P^0\\big(\\Sigma_{j=1}^{n} \\gamma_j \\mathbf e_j \\big) & \\mathbf P^1 \\big(\\Sigma_{j=1}^{n}\\gamma_j \\mathbf e_j \\big) & \\cdots & \\mathbf P^{n-1}\\big(\\Sigma_{j=1}^{n} \\gamma_j \\mathbf e_j \\big)\n",
    "\\end{array}\\bigg] = \\big(\\Sigma_{j=1}^{n}\\mathbf \\gamma_j \\mathbf D_j\\big) \\mathbf F$\n",
    "\n",
    "\n",
    "Right multiply each side by $\\mathbf F^H$:  \n",
    "\n",
    "$\\mathbf F\\bigg[\\begin{array}{c|c|c|c|c}\n",
    "\\mathbf P^0\\big(\\Sigma_{j=1}^{n} \\gamma_j \\mathbf e_j \\big) & \\mathbf P^1 \\big(\\Sigma_{j=1}^{n}\\gamma_j \\mathbf e_j \\big) & \\cdots & \\mathbf P^{n-1}\\big(\\Sigma_{j=1}^{n} \\gamma_j \\mathbf e_j \\big)\n",
    "\\end{array}\\bigg] \\mathbf F^H = \\big(\\Sigma_{j=1}^{n}\\mathbf \\gamma_j \\mathbf D_j\\big) \\mathbf F \\mathbf F^H  = \\big(\\Sigma_{j=1}^{n}\\mathbf \\gamma_j \\mathbf D_j\\big) $\n",
    "\n",
    "Since the sum of a finite sequence of $n$ x $n$ diagonal matrices is itself a diagonal matrix, this tells us that our matrix is unitarily similar to a diagonal matrix, and the mutually orthonormal eigenvectors are contained in $\\mathbf F$ (or technically, the right eigenvectors are contained as columns in $\\mathbf F^H$ -- which again, is just a small bookkeeping adjustment).  \n",
    "\n",
    "Now consider the general case where $\\mathbf X = f(\\mathbf s)$.  This looks quite formidable -- the circulant matrix is given by: \n",
    "\n",
    "$\\mathbf X = \\bigg[\\begin{array}{c|c|c|c}\n",
    "\\mathbf P^0\\mathbf s & \\mathbf P^1\\mathbf s & \\cdots & \\mathbf P^{n-1}\\mathbf s\n",
    "\\end{array}\\bigg]  = \\begin{bmatrix}\n",
    "s_0 & s_{n-1} & s_{n-2} & \\dots & s_2 & s_1 \\\\ \n",
    "s_1 & s_0 & s_{n-1} & \\dots & s_3 & s_2 \\\\ \n",
    "s_2 & s_1 & s_0 & \\dots & s_4 & s_3 \\\\\n",
    "\\vdots & \\vdots  & \\vdots &\\ddots & \\vdots & \\vdots\\\\ \n",
    "s_{n-2} & s_{n-3} & s_{n-4} & \\dots & s_0  & s_{n-1} \\\\ \n",
    "s_{n-1} & s_{n-2}  & s_{n-3} & \\dots & s_1 &  s_0\n",
    "\\end{bmatrix}$\n",
    "\n",
    "But, we simply need to recall that the standard basis vectors in fact form a basis, so we can uniquely write $\\mathbf s$ in terms of them. \n",
    "\n",
    "$\\mathbf s = \\gamma_1 \\mathbf e_1 + \\gamma_2 \\mathbf e_2 + ... + \\gamma_n \\mathbf e_n  = \\big(\\Sigma_{j=1}^{n} \\gamma_j \\mathbf e_j\\big)$\n",
    "\n",
    "Thus we have \n",
    "\n",
    "\n",
    "$\\mathbf X = \\bigg[\\begin{array}{c|c|c|c}\n",
    "\\mathbf P^0\\mathbf s & \\mathbf P^1\\mathbf s & \\cdots & \\mathbf P^{n-1}\\mathbf s\n",
    "\\end{array}\\bigg] = \\bigg[\\begin{array}{c|c|c|c|c}\n",
    "\\mathbf P^0\\big(\\Sigma_{j=1}^{n} \\gamma_j \\mathbf e_j \\big) & \\mathbf P^1 \\big(\\Sigma_{j=1}^{n}\\gamma_j \\mathbf e_j \\big) & \\cdots & \\mathbf P^{n-1}\\big(\\Sigma_{j=1}^{n} \\gamma_j \\mathbf e_j \\big)\n",
    "\\end{array}\\bigg]$\n",
    "\n",
    "left multiply each side by $\\mathbf F$ and right multiply each side by $\\mathbf F^H$, and we get \n",
    "\n",
    "$\\mathbf F \\big(\\mathbf X\\big) \\mathbf F^H = \\mathbf F \\bigg[\\begin{array}{c|c|c|c|c}\n",
    "\\mathbf P^0\\big(\\Sigma_{j=1}^{n} \\gamma_j \\mathbf e_j \\big) & \\mathbf P^1 \\big(\\Sigma_{j=1}^{n}\\gamma_j \\mathbf e_j \\big) & \\cdots & \\mathbf P^{n-1}\\big(\\Sigma_{j=1}^{n} \\gamma_j \\mathbf e_j \\big)\n",
    "\\end{array}\\bigg] \\mathbf F^H = \\big(\\Sigma_{j=1}^{n}\\mathbf \\gamma_j \\mathbf D_j\\big)$\n",
    "\n",
    "\n",
    "which states that $\\mathbf X = f(\\mathbf s)$ is unitarily similar to a diagonal matrix, with vectors in $\\mathbf F$ forming the basis of mutually orthonormal eigenvectors.  This completes the proof that a circulant matrix $\\mathbf X$ is unitarily diagonalizable via the \"help\" of $\\mathbf F$. \n",
    "\n",
    "\n",
    "# End Long Form Proof\n",
    "\n",
    "- - - - - \n",
    "# But what the heck do the components of the DFT look like? \n",
    "\n",
    "\n",
    "When we consider that (a) each $\\lambda_i$, contained in position $\\mathbf \\Lambda_{i,i}$, is distinct and also that (b) each $\\lambda_i^n - 1 = 0$\n",
    "\n",
    "as a reminder: this is because (a) the associated Vandermonde matrix is non-singular, and (b) $\\mathbf \\Lambda^n = \\mathbf Q^H \\mathbf P^n \\mathbf Q = \\mathbf Q^H \\mathbf I \\mathbf Q = \\mathbf I $, hence each diagonal element raised to the nth power equals one.  \n",
    "\n",
    "We know that $\\lambda_1 = 1$, because $\\mathbf {P1} = \\mathbf 1$.  From here we can say, $\\lambda_1$ has polor coordinate (1, $2\\pi \\frac{(1 - 1) }{n}$) which is to say it has magnitude 1, and an angle of $0 \\pi$ i.e. it is all real valued = 1.  \n",
    "\n",
    "$\\lambda_2$ has polar coordinate of (1 , $2\\pi\\frac{(2-1)}{n} $)  \n",
    "$\\lambda_3$ has polar coordinate of (1,  $2\\pi\\frac{(3-1)}{n} $)  \n",
    "$\\vdots$  \n",
    "$\\lambda_{n-1}$ has polar coordinate of (1, $2\\pi\\frac{(n-1 -1)}{n} $)  \n",
    "$\\lambda_n$ has polar coordinate of (1, $2\\pi\\frac{(n-1)}{n}$).  \n",
    "\n",
    "There is a variant of the Pidgeon Hole principle here: we have have $n$ $\\lambda_j$'s, each of which must be unique, and there are only $n$ unique nth roots of unity$^{(1)}$ -- hence each nth root has one and only one $\\lambda_j$ \"in\" it.  (This Wolfram alpha link is worth visiting, for its nice graphic: http://mathworld.wolfram.com/RootofUnity.html )\n",
    "\n",
    "\n",
    "\n",
    "Thus **the Vandermonde matrix in the following form is unitary**:  (due to GitHub $\\LaTeX$ rendering issues, the below formula has been inserted as an image)\n",
    "\n",
    "\n",
    "![F_components](images/unitary_vandermondF_components.gif)\n",
    "\n",
    "when each $\\lambda_j$ has polar coordinate of (1, $2\\pi\\frac{(j-1)}{n} $)\n",
    "- - - - -\n",
    "\n",
    "$^{(1)}$ **Side note: How do we know there are exactly n roots of unity?** \n",
    "\n",
    "The reader may wonder how we know that there are \"only $n$ unique nth roots of unity\" available for us to choose from, for any natural number $n$.  One way to support this claim comes from using the fundamental theorem of algebra, which is rather high powered machinery that is not introduced or proved anywhere in this posting.  \n",
    "\n",
    "The other approach is self contained and comes from using Vandermonde matrices.  Consider a degree $n$ polynomial (specifically the polynomial we are interested in is $\\lambda_i^n - 1$, but any degree $n$ polynomial --that isn't the zero polynomial-- is valid here). Such a polynomial would have the following Vandermonde matrix associated with it:\n",
    "\n",
    "\n",
    "$\\mathbf S = \\begin{bmatrix}\n",
    "1 & s_1 & s_1^2 & \\dots  & s_1^{n-1} &s_1^{n} \\\\ \n",
    "1 & s_2 & s_2^2 & \\dots &  s_2^{n-1} & s_2^{n} \\\\ \n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\ \n",
    "1 & s_{n} & s_{n}^{2} & \\dots  & s_{n}^{n-1} & s_{n}^{n} \\\\\n",
    "1 & s_{n+1} & s_{n+1}^{2} & \\dots  & s_{n+1}^{n-1} & s_{n+1}^{n}\n",
    "\\end{bmatrix}$\n",
    "\n",
    "That is, we evaluate our polynomial at $s_i$ where $i = \\{1, 2, ... , n, n+1\\}$.  The polynomial has coefficients associated with it, which are given in $\\mathbf t$.  When we evaluate the polynomial at each $s_i$ we get the resulting value at each $b_i$.  Setting this up as an equation:   \n",
    "\n",
    "$\\begin{bmatrix}\n",
    "1 & s_1 & s_1^2 & \\dots  & s_1^{n-1} &s_1^{n} \\\\ \n",
    "1 & s_2 & s_2^2 & \\dots &  s_2^{n-1} & s_2^{n} \\\\ \n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\ \n",
    "1 & s_{n} & s_{n}^{2} & \\dots  & s_{n}^{n-1} & s_{n}^{n} \\\\\n",
    "1 & s_{n+1} & s_{n+1}^{2} & \\dots  & s_{n+1}^{n-1} & s_{n+1}^{n}\n",
    "\\end{bmatrix}\\begin{bmatrix}\n",
    "t_0\\\\ \n",
    "t_1\\\\ \n",
    "\\vdots\\\\ \n",
    "t_{n-1}\\\\ \n",
    "t_{n}\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "b_1\\\\ \n",
    "b_2\\\\ \n",
    "\\vdots\\\\ \n",
    "b_{n}\\\\ \n",
    "b_{n+1}\n",
    "\\end{bmatrix}$\n",
    "\n",
    "or more succinctly, \n",
    "\n",
    "$\\mathbf{St} = \\mathbf b$ \n",
    "\n",
    "**For a contradiction:** assume that each of the $n+1$ data points is a distinct root of the polynomial.  Then every resulting value in $\\mathbf b$ is zero, which reduces this to:\n",
    "\n",
    "$\\mathbf{St} = \\mathbf 0$.  However since each $s_i$ is distinct, the Vandermonde matrix is invertible, which gives us \n",
    "\n",
    "$\\mathbf S^{-1} \\mathbf{St} = \\mathbf t = \\mathbf S^{-1} \\mathbf 0 = \\mathbf 0$   \n",
    "\n",
    "thus \n",
    "\n",
    "$\\mathbf t = \\mathbf 0$   \n",
    "\n",
    "Since every coefficient is zero, we in fact have the zero polynomial -- which is a contradiction.  However, if at least one of the $s_j$ is not a root (i.e. $\\mathbf b \\neq \\mathbf 0$), then $\\mathbf t \\neq \\mathbf 0$ and hence we may still have a degree $n$ polynomial.  This gives us an upper bound which tells us that a degree $n$ polynomial can have at most $n$ distinct roots.   \n",
    "\n",
    "Now, for our DFT matrix $\\mathbf F$, we are using eigenvalues from the connected graph permutation matrix and they have a constraint given by $\\lambda^n - 1 = 0$.  Put differently we are looking for roots of a degree $n$ polynomial, where the polynomial is $\\lambda^n - 1$.  These roots are called roots of unity.  Per the above, we upper bound the number of unique roots as being $\\leq n$.  Now our matrix $\\mathbf F$ is a unitary Vandermonde Matrix, which means it is non-singular, thus we determine that each $\\lambda_k$ for $k = \\{1, 2, ..., n-1, n \\}$ must be distinct. This means there must be $\\geq n$ distinct roots of unity.  Since our upper bound and lower bound are equal, we have a sandwich and conclude that there are **exactly** $n$ unique roots of unity.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "# Below is a simple example of an explicit use of the circulant matrix for discrete convolutions in probability\n",
    "\n",
    "there are numerous opportunities to further optimize this\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x vs y\n",
      "[ 0.02  0.21  0.22  0.4   0.15]  vs  [ 0.27  0.11  0.16  0.2   0.26] \n",
      "\n",
      "[[ 0.27  0.    0.    0.    0.    0.    0.26  0.2   0.16  0.11]\n",
      " [ 0.11  0.27  0.    0.    0.    0.    0.    0.26  0.2   0.16]\n",
      " [ 0.16  0.11  0.27  0.    0.    0.    0.    0.    0.26  0.2 ]\n",
      " [ 0.2   0.16  0.11  0.27  0.    0.    0.    0.    0.    0.26]\n",
      " [ 0.26  0.2   0.16  0.11  0.27  0.    0.    0.    0.    0.  ]\n",
      " [ 0.    0.26  0.2   0.16  0.11  0.27  0.    0.    0.    0.  ]\n",
      " [ 0.    0.    0.26  0.2   0.16  0.11  0.27  0.    0.    0.  ]\n",
      " [ 0.    0.    0.    0.26  0.2   0.16  0.11  0.27  0.    0.  ]\n",
      " [ 0.    0.    0.    0.    0.26  0.2   0.16  0.11  0.27  0.  ]\n",
      " [ 0.    0.    0.    0.    0.    0.26  0.2   0.16  0.11  0.27]]\n",
      " \n",
      "# | probability    | difference between for loops and use of circulant matrix\n",
      "0 | 0.00480728004211 | 0.0\n",
      "1 | 0.060396279743 | 0.0\n",
      "2 | 0.0863612459009 | 0.0\n",
      "3 | 0.169574082205 | 2.77555756156e-17\n",
      "4 | 0.167181042318 | 2.77555756156e-17\n",
      "5 | 0.178404865542 | 0.0\n",
      "6 | 0.161314980263 | 0.0\n",
      "7 | 0.133136275169 | 0.0\n",
      "8 | 0.0388239488173 | 0.0\n",
      "9 | 0.0 | 0.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(precision = 2, linewidth=180)\n",
    "\n",
    "# setup\n",
    "\n",
    "# simple (PMF) distributions for some peculiar experiment,\n",
    "# that can return a certain number of \"heads\"\n",
    "# the number of heads = [0, 1, 2, 3, 4], \n",
    "# an associated PMFs for x and y\n",
    "\n",
    "x = np.random.random(5)\n",
    "x = x / x.sum() # normalize\n",
    "\n",
    "y = np.random.random(5)\n",
    "y = y / y.sum() # normalize\n",
    "\n",
    "print(\"x vs y\")\n",
    "print( x, \" vs \", y, \"\\n\")\n",
    "\n",
    "m = x.shape[0] * 2\n",
    "z = np.zeros(m)\n",
    "circulant_mat = np.zeros((m,m))\n",
    "\n",
    "# direct convolution is done below, \n",
    "# and the circulant matrix is populated while doing this\n",
    "for i in range(m):\n",
    "    for idx in range(x.shape[0]):\n",
    "        jdx = i - idx\n",
    "        if jdx >= y.shape[0]  or jdx < 0:\n",
    "            # simple setup with the non-negative distribution, though inefficient\n",
    "            continue\n",
    "        z[i] += x[idx] * y[jdx]\n",
    "        circulant_mat[i,idx] = y[jdx]\n",
    "  \n",
    "padded_x = np.zeros(m)\n",
    "# just some extra zeros in the padding to accomdoate \n",
    "# the higher order polynomial so to speak\n",
    "\n",
    "for i in range(x.shape[0]):\n",
    "    padded_x[i] += x[i]\n",
    "\n",
    "# mathematically, the below has no impact \n",
    "# (as these entries get scaled by the padded zeros on padded_x), \n",
    "# but it finished off the circulant structure associated w/ our convolution\n",
    "# It is important to know this exists\n",
    "\n",
    "for row_idx in range(m):\n",
    "    if np.isclose(circulant_mat[row_idx, 0], 0):\n",
    "        continue\n",
    "    else:\n",
    "        for j in range(x.shape[0],m):\n",
    "            circulant_mat[(row_idx + j) % m , j] = circulant_mat[row_idx, 0]\n",
    "\n",
    "print(circulant_mat)\n",
    "\n",
    "newz = circulant_mat @ padded_x \n",
    "# the alternative, direct, way of calculating z is via the use of this circulant matrix\n",
    "\n",
    "print(\" \")\n",
    "print(\"# | probability    | difference between for loops and use of circulant matrix\")\n",
    "for i in range(m):\n",
    "    print(i, \"|\", z[i], \"|\", z[i] - newz[i])\n",
    "\n",
    "# of course there is lots of room to optimize calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Full cycle trace relations and nilpotent matrices\n",
    "\n",
    "**claim:**  \n",
    "for $\\mathbf B \\in \\mathbb C^{n x n}$ , \n",
    "\n",
    "if $trace\\big(\\mathbf B^r\\big) = 0$, for $r = \\{1, 2, ... ,n-1, n \\}$ every eigenvalue, $\\lambda_i$, of $\\mathbf B$ is equal to zero, i.e.  $\\lambda_i = 0$ for $i = \\{1, 2, ... ,n-1, n \\}$ .  \n",
    "\n",
    "\n",
    "**comment:**  \n",
    "since the trace gives the sum of the eigenvalues and any complex matrix is similar to an upper triangular matrix, it is clearly true that if all eigenvalues are zero, then the trace will be zero for $\\mathbf B^r$ for any natural number $r$ -- including the case where $1 \\leq r \\leq n$ . What is not immediately clear is that this is an **iff**.\n",
    "\n",
    "In the derivation of the DFT, we used $trace\\big(\\mathbf P^r\\big) = 0$, for $r = \\{1, 2, ... ,n-1 \\}$, yet our matrix $\\mathbf P$ had all eigenvalues with magnitude $= 1$.  Extending the range of $r$ to also includes $n$ radically changes things and makes all eigenvalues have magnitude $= 0$.  \n",
    "\n",
    "**proof:**  \n",
    "start by constructing the Vandermonde matrix:\n",
    "\n",
    "\n",
    "$\\mathbf W = \\begin{bmatrix}\n",
    "1 & \\lambda_1 & \\lambda_1^2 & \\dots  & \\lambda_1^{n-1}\\\\ \n",
    "1 & \\lambda_2 & \\lambda_2^2 & \\dots &  \\lambda_2^{n-1} \\\\ \n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\\\ \n",
    "1 & \\lambda_{n} & \\lambda_{n}^{2} & \\dots  & \\lambda_{n}^{n-1}\n",
    "\\end{bmatrix}$\n",
    "\n",
    "\n",
    "We want to reflect our constraint as\n",
    "\n",
    "$\\mathbf 1^H \\mathbf {\\Lambda W} = \\mathbf 0^H$\n",
    "\n",
    "i.e. as\n",
    "\n",
    "$\\mathbf 1^H \\begin{bmatrix}\n",
    "\\lambda_1 & \\lambda_1^2 & \\dots  & \\lambda_1^{n-1} & \\lambda_1^{n}\\\\ \n",
    "\\lambda_2 & \\lambda_2^2 & \\dots &  \\lambda_2^{n-1}& \\lambda_2^{n} \\\\ \n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\\\ \n",
    "\\lambda_{n} & \\lambda_{n}^{2} & \\dots  & \\lambda_{n}^{n-1}& \\lambda_n^{n}\n",
    "\\end{bmatrix} = \\mathbf 0^H$\n",
    "\n",
    "\n",
    "But we aren't sure if there are any eigenvalues equal to zero in $\\mathbf \\Lambda$ so we need to remove them first.  Why? First: if any eigenvalues are zero, the left side of the equation is not invertible.  Second, we are interested in the trace relations and we know that any eigenvalues of zero have no impact on the trace calculations, hence they may safely be removed.  \n",
    "\n",
    "*The contradiction kicks in at this stage*\n",
    "\n",
    "We remove all eigenvalues equal to zero and have an $m$ x $m$ matrix for some natural number $m$, where $ m \\leq n$.  Assume $m \\geq 2$, i.e. that some non-zero eigenvalues exist that satisfy our stated trace constraint.  \n",
    "\n",
    "- - - \n",
    "(*Two bookkeeping notes that may be skipped:* First: after removing all zero eigenvalues, $m \\neq 1$ -- because if $m=1$, then $trace\\big(\\mathbf B\\big) = \\lambda_1 = 0$ and the sole remaining eigenvalue $\\lambda_1 \\neq 0$ hence $m$ cannot be equal to one.  Second: we make the adjustment so that $\\mathbf 1$ and $\\mathbf 0$ are $m$ x $1$ column vectors.)  \n",
    "\n",
    "Our equation becomes: \n",
    "\n",
    "$\\mathbf 1^H \\begin{bmatrix}\n",
    "\\lambda_1 & \\lambda_1^2 & \\dots  & \\lambda_1^{m-1} & \\lambda_1^{m}\\\\ \n",
    "\\lambda_2 & \\lambda_2^2 & \\dots &  \\lambda_2^{m-1}& \\lambda_2^{m} \\\\ \n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\\\ \n",
    "\\lambda_{m} & \\lambda_{m}^{2} & \\dots  & \\lambda_{m}^{m-1}& \\lambda_m^{m}\n",
    "\\end{bmatrix} = \\mathbf 0^H$\n",
    "\n",
    "we re-write the above as \n",
    "\n",
    "$\\mathbf y^H \\begin{bmatrix}\n",
    "\\lambda_1 & \\lambda_1^2 & \\dots  & \\lambda_1^{m-1} & \\lambda_1^{m}\\\\ \n",
    "\\lambda_2 & \\lambda_2^2 & \\dots &  \\lambda_2^{m-1}& \\lambda_2^{m} \\\\ \n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\\\ \n",
    "\\lambda_{m} & \\lambda_{m}^{2} & \\dots  & \\lambda_{m}^{m-1}& \\lambda_m^{m}\n",
    "\\end{bmatrix} = \\mathbf 0^H$\n",
    "\n",
    "where $\\mathbf y = \\mathbf 1$.  It is important to note the identity: $\\mathbf y^H \\mathbf 1 = m$.\n",
    "\n",
    "Now we further prune our adjusted Vandermonde matrix to only include unique eigenvalues.  Thus we keep the $k$ unique eigenvalues where $2 \\leq k \\leq m$, and we adjust $\\mathbf y$ so that the trace math is identical. (Again note that $k \\neq 1$, because if so then there is only one unique eigenvalue $\\lambda_1$ and thus $\\frac{1}{m} trace \\big(\\mathbf B\\big) = \\lambda_1 = 0$, but we know that $\\lambda_1 \\neq 0$.)  \n",
    "\n",
    "For example, if all eigenvalues were unique except $\\lambda_m = \\lambda_{m-1}$ we'd remove the mth row and mth column from our adjusted Vandermonde matrix, and now $\\mathbf y$ would be an $m-1$ x $1$ column vector (as would the zero vector), where we have \n",
    "\n",
    "$\\mathbf y = \\begin{bmatrix}\n",
    "1\\\\ \n",
    "1\\\\ \n",
    "\\vdots\\\\ \n",
    "1 \\\\\n",
    "2\n",
    "\\end{bmatrix}$\n",
    "\n",
    "**Put differently, at this stage $y_i$ has algebraic multiplicity for each unique non-zero eigenvalue $\\lambda_i$.**\n",
    "\n",
    "The underlying math with respect to traces is the same, and we still have the key identity $\\mathbf y^H \\mathbf 1 = m$.\n",
    "\n",
    "our equation is thus:   \n",
    "\n",
    "$\\mathbf y^H \\begin{bmatrix}\n",
    "\\lambda_1 & \\lambda_1^2 & \\dots  & \\lambda_1^{k-1} & \\lambda_1^{k}\\\\ \n",
    "\\lambda_2 & \\lambda_2^2 & \\dots &  \\lambda_2^{k-1}& \\lambda_2^{k} \\\\ \n",
    "\\vdots & \\vdots & \\ddots & \\vdots & \\vdots & \\\\ \n",
    "\\lambda_{k} & \\lambda_{k}^{2} & \\dots  & \\lambda_{k}^{k-1}& \\lambda_k^{k}\n",
    "\\end{bmatrix} = \\mathbf 0^H$\n",
    "\n",
    "This matrix has each $\\lambda_i \\neq 0$ and each $\\lambda_i$ is unique. We can factor out a diagonal matrix $\\mathbf D$ if we'd like.  Thus we have \n",
    "\n",
    "$\\mathbf y^H \\mathbf D \\begin{bmatrix}\n",
    "1 & \\lambda_1 & \\lambda_1^2 & \\dots  & \\lambda_1^{k-1} \\\\ \n",
    "1 & \\lambda_2 & \\lambda_2^2 & \\dots &  \\lambda_2^{k-1} \\\\ \n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\\\ \n",
    "1& \\lambda_{k} & \\lambda_{k}^{2} & \\dots  & \\lambda_{k}^{k-1}\n",
    "\\end{bmatrix} = \\mathbf 0^H$\n",
    "\n",
    "letting $\\mathbf K$ be our adjusted Vandermonde matrix in this equation, i.e. $\\mathbf K = \\bigg[\\begin{array}{c|c|c|c|c}\n",
    "\\mathbf D^0 \\mathbf 1 & \\mathbf D^1 \\mathbf 1 & \\mathbf D^2 \\mathbf 1 &\\cdots & \\mathbf D^{k-1} \\mathbf 1\n",
    "\\end{array}\\bigg]$\n",
    "\n",
    "we have \n",
    "\n",
    "$\\mathbf y^H \\mathbf D \\mathbf K = \\mathbf 0^H$\n",
    "\n",
    "Because all $\\lambda_i$'s are unique, $\\mathbf K$ is non-singular and so must be $\\mathbf D$ (it is diagonal with no zero eigenvalues).  We right multiply both sides of our equation by the inverse of $\\mathbf K$ and this gives us \n",
    "\n",
    "\n",
    "$\\mathbf y^H \\mathbf D = \\mathbf y^H \\mathbf D \\mathbf {KK}^{-1} = \\mathbf 0^H \\mathbf K^{-1} = \\mathbf 0^H$ \n",
    "\n",
    "now right multiply both sides by $\\mathbf D^{-1}$, and we have \n",
    "\n",
    "$\\mathbf y^H = \\mathbf y^H \\mathbf D \\mathbf D^{-1} =  \\mathbf 0^H \\mathbf D^{-1} = \\mathbf 0^H$ \n",
    "\n",
    "This tells us that $\\mathbf y^H = \\mathbf 0^H$.  Yet this is a contradiction, because \n",
    "\n",
    "$\\mathbf y^H \\mathbf 1 = m \\neq \\mathbf 0^H \\mathbf 1 = 0$\n",
    "\n",
    "\n",
    "hence we know that $m \\ngeq 2$, and as mentioned earlier $m \\neq 1$.  Thus $m = 0$.  Put differently, $\\mathbf K$ does not exist (i.e. it must be a $0$ x $0$ matrix).  This proves the claim that all eigenvalues of $\\mathbf B$ must be equal to zero if \n",
    "\n",
    "$trace\\big(\\mathbf B^r\\big) = 0$, for $r = \\{1, 2, ... ,n-1, n \\}$ \n",
    "\n",
    "- - - -\n",
    "- - - -\n",
    "- - - -\n",
    "**alternative approach:** if the reader feels the above contradiction to be unsatifying, consider instead the following setup:\n",
    "\n",
    "\n",
    "$\\mathbf y^H \\mathbf D \\begin{bmatrix}\n",
    "1 & \\lambda_1 & \\lambda_1^2 & \\dots  & \\lambda_1^{k-1} & \\lambda_1^{k} \\\\ \n",
    "1 & \\lambda_2 & \\lambda_2^2 & \\dots &  \\lambda_2^{k-1} & \\lambda_2^{k} \\\\ \n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\\\ \n",
    "1& \\lambda_{k} & \\lambda_{k}^{2} & \\dots  & \\lambda_{k}^{k-1} & \\lambda_k^{k} \\\\\n",
    "1& 0 & 0 & \\dots  & 0 & 0\n",
    "\\end{bmatrix} = \\mathbf 0^H$\n",
    "\n",
    "Here we have $k + 1$ rows in our Vandermonde matrix -- where the eigenvalue of zero is contained in the final row. $\\mathbf D$ now is a $k+1 $ x $k+1$ diagonal matrix that has a zero in its bottom right corner, and $\\mathbf y$ has the algebraic multiplicity for each of the $k+1$ unique eigenvalues (inclusive of the eigenvalue equal to zero, which is given by $y_{k+1}$).  We know that $\\mathbf B$ has $n$ eigenvalues thus $\\mathbf y^H \\mathbf 1 = n$.\n",
    "\n",
    "Our Vandermonde Matrix is invertible so we multiply both sides on the right by its inverse, giving us \n",
    "\n",
    "$\\mathbf y^H \\mathbf D = \\mathbf 0^H$\n",
    "\n",
    "or equivalently\n",
    "\n",
    "$\\mathbf D^H \\mathbf y = \\mathbf 0$\n",
    "\n",
    "now we notice that $\\mathbf D$ is singular, with all non-zero entries along its diagonal except the entry in the bottom right corner.  However the above equation tells us that \n",
    "\n",
    "for $i = \\{1, 2, ..., k\\}$\n",
    "\n",
    "$y_i \\bar{\\lambda_i} = 0$\n",
    "\n",
    "\n",
    "we observe that this is also true in the $k+1$ case:  $y_{k+1} \\bar{\\lambda_{k+1}} = 0$\n",
    "\n",
    "First we deal with $i = \\{1, 2, ..., k\\}$ noticing that $\\bar{\\lambda_i} \\neq 0$\n",
    "\n",
    "$y_i \\bar{\\lambda_i} = 0$\n",
    "\n",
    "divide both sides by $\\bar{\\lambda_i}$ and see that \n",
    "\n",
    "$y_i = 0$\n",
    "\n",
    "Finally for $y_{k+1}$, we have  \n",
    "\n",
    "$y_{k+1} \\bar{\\lambda_{k+1}} = y_{k+1} 0 = 0$\n",
    "\n",
    "but we also have the constraint $n = \\mathbf y^H \\mathbf 1 = \\mathbf 1^H \\mathbf y = \\big(y_0 + y_1 + ... y_{k-1} + y_k \\big) + y_{k+1} = \\big(0 \\big) + y_{k+1} $ \n",
    "\n",
    "hence we see that $y_{k+1} = n$.  Put differently, all of the eigenvalues for $\\mathbf B$ are zero -- i.e. $\\mathbf B$ is nilpotent.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# extension: \n",
    "**for some some $m$ x $n$ matrix ** $\\mathbf G$ **and some $n$ x $m$ matrix ** $\\mathbf H$, then $\\mathbf {GH}$ and $\\mathbf {HG}$ have the same non-zero eigenvalues (in terms of algebraic multiplicity).\n",
    "\n",
    "first notice \n",
    "\n",
    "$trace\\big((\\mathbf {GH})\\big) = trace\\big((\\mathbf {HG})\\big)$\n",
    "\n",
    "via the cyclic property of the trace.  Now in general, we can say\n",
    "\n",
    "for $r = \\{2, ... , n\\}$\n",
    "\n",
    "$trace\\big((\\mathbf {GH})^r\\big) = trace\\big(\\mathbf{GH} (\\mathbf G\\mathbf H)^{r-1}\\big) = trace\\big(\\mathbf{H} (\\mathbf G\\mathbf H)^{r-1} \\mathbf G\\big) = trace\\big((\\mathbf {HG})^r\\big)$\n",
    "\n",
    "thus we have \n",
    "\n",
    "$trace\\big((\\mathbf {GH})^r\\big) = trace\\big((\\mathbf {HG})^r\\big)$ \n",
    "\n",
    "for $r = \\{1, 2, ... , n\\}$\n",
    "\n",
    "At this point, people will frequently notice that $r$ can be any natural number (i.e. it need not be capped at $n$), then import Newton's Idenities and determine that the non-zero eigenvalues (and their algebraic multiplicities) must be the same.  While this approach is concise, Netwon's Identities are fairly high powered machinery.  Here is another approach: \n",
    "\n",
    "for each unique eigenvector not in the nullspace of $\\big(\\mathbf{HG}\\big)$\n",
    "\n",
    "$\\big(\\mathbf H \\mathbf G\\big) \\mathbf x = \\lambda \\mathbf x$\n",
    "\n",
    "Now left multiply by $\\mathbf G$\n",
    "\n",
    "$\\mathbf G \\mathbf H \\mathbf G \\mathbf x = \\mathbf G \\lambda \\mathbf x = \\lambda \\mathbf G \\mathbf x$\n",
    "\n",
    "$\\big(\\mathbf G \\mathbf H\\big) \\big(\\mathbf G \\mathbf x\\big) = \\lambda \\big(\\mathbf G \\mathbf x\\big)$\n",
    "\n",
    "so $\\{\\lambda, \\mathbf x\\}$ is the eigenpair of $\\mathbf {HG}$ and $\\{ \\lambda, (\\mathbf {Gx})\\}$ is the eigenpair for $\\mathbf{GH}$.  This holds for all $\\lambda \\neq 0$.  (The issue that comes up when $\\lambda =0$ is that we run into issues with having the zero vector as an eigenvector in the second eigenpair, which is not allowed.)\n",
    "\n",
    "Further, if we wanted to be extra thorough, we could also do a \"backward pass\" and say: \n",
    "\n",
    "for each unique eigenvector not in the nullspace of $\\big(\\mathbf{GH}\\big)$\n",
    "\n",
    "$\\big(\\mathbf{GH}\\big) \\mathbf v = \\lambda \\mathbf v$\n",
    "\n",
    "Now left multiply by $\\mathbf H$\n",
    "\n",
    "$\\mathbf {HGHv} = \\mathbf {H} \\lambda \\mathbf{v} = \\lambda \\mathbf H \\mathbf v$\n",
    "\n",
    "$\\big(\\mathbf H \\mathbf G\\big) \\big(\\mathbf H \\mathbf v\\big) = \\lambda \\big(\\mathbf H \\mathbf v\\big)$\n",
    "\n",
    "so $\\{\\lambda, \\mathbf v\\}$ is the eigenpair of $\\mathbf {GH}$ and $\\{ \\lambda, (\\mathbf {Hv})\\}$ is the eigenpair for $\\mathbf{HG}$.  This holds for all $\\lambda \\neq 0$. \n",
    "\n",
    "\n",
    "At this point we have enumerated all unique eigenvectors for $\\mathbf{GH}$ and $\\mathbf{HG}$ that are not in their respective nullspaces and have found the same eigenvalues between $\\mathbf{GH}$ and $\\mathbf {HG}$.  Since we have enumerated all unique eigenvectors not in the nullspace, we must have enumerated all unique $\\lambda \\neq 0$. Why? The vectors associated wiht unique eigenvalues must be linearly independent and hence eigenvector $i$ cannot be 'the same as' some other eigenvector $j$ unless $\\lambda_i = \\lambda_j$.  Put differently, if $\\lambda_i \\neq \\lambda_j$ then we know that the associated eigenvectors are not linearly dependent -- i.e. in an inner product space like $\\mathbb C$ this means eigenvector i must have some component that is completely independent (read: orthogonal) from eigenvector j.  Put another way, iterating through the complete collection of unique eigenvectors outside (the nullspace) does not mean we will find multiple unique eigenvalues -- it merely means that if unique eigenvalues exist outside the nullspace, we will see them during this process.  For more information see the proof earlier in this posting called \"Application of Vandermonde Matrices: Proof of Linear Independence of Eigenvectors associated with Unique Eigenvalues.\"\n",
    "\n",
    "Now to confirm that each unique non-zero eigenvalue has the same algebraic multiplicity, we use the fact that $trace\\big((\\mathbf {GH})^r\\big) = trace\\big((\\mathbf {HG})^r\\big)$, and in a setup very similar to that in the above *\"Full cycle trace relations and nilpotent matrices\"*, **we collect each unique non-zero eigenvalue in a diagonal matrix, $\\mathbf D$**, and thus $\\mathbf D$ has the following eigenvalues $\\{\\lambda_1, \\lambda_2, ..., \\lambda_k\\}$ \n",
    "For avoidance of doubt, $k$ is some natural number where $1 \\leq k \\leq min(m,n)$.\n",
    "\n",
    "We then set up the $k$ x $k$ Vandermonde matrix $\\mathbf K$\n",
    "\n",
    "$\\mathbf K = \\bigg[\\begin{array}{c|c|c|c|c}\n",
    "\\mathbf D^0 \\mathbf 1 & \\mathbf D^1 \\mathbf 1 & \\mathbf D^2 \\mathbf 1 &\\cdots & \\mathbf D^{k-1} \\mathbf 1\n",
    "\\end{array}\\bigg]$\n",
    "\n",
    "To model our trace relations, we collect the algebraic multiplicities of $(\\mathbf {GH})^r$ in $\\mathbf y$ and place it on the left hand side of the equation.  We collect the algebraic multiplicities of $(\\mathbf {HG})^r$ and place them in $\\mathbf z$ on the right hand side of the equation.  \n",
    "\n",
    "$\\mathbf y^H \\mathbf D \\mathbf K = \\mathbf z^H \\mathbf D \\mathbf K $\n",
    "\n",
    "In the special case where the trace is zero for $r = \\{1, 2, ... , n\\}$, we know that all eigenvalues are equal to zero, based on the preceding proof.  (This special case cannot occur of course, since by design we have only included $\\lambda_i \\neq 0$ in our $\\mathbf D$ and $\\mathbf K$.)    If the trace is not equal zero for a \"full cycle\",  then the above equation exists and we may say \n",
    "\n",
    "$\\mathbf y^H \\mathbf D \\mathbf K = \\mathbf z^H \\mathbf D \\mathbf K $\n",
    "\n",
    "$\\mathbf y^H \\big(\\mathbf D \\mathbf K\\big)\\big(\\mathbf D \\mathbf K\\big)^{-1} = \\mathbf y^H \\mathbf D \\mathbf {KK}^{-1} \\mathbf D ^{-1} = \\mathbf z^H \\mathbf D \\mathbf {KK}^{-1} \\mathbf D ^{-1} = \\mathbf z^H \\big(\\mathbf D \\mathbf K\\big)\\big(\\mathbf D \\mathbf K\\big)^{-1}$\n",
    "\n",
    "$\\mathbf y^H = \\mathbf z^H $\n",
    "\n",
    "Thus $\\mathbf y = \\mathbf z$, and we know that the algebraic multiplicity of each unique non-zero eigenvalue of $\\mathbf{GH}$ equals its algebraic multiplicity in $\\mathbf{HG}$ (and vice versa).  Hence we say that $\\mathbf{GH}$ and  $\\mathbf{HG}$ have the same non-zero eigenvalues.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Cayley Hamilton \n",
    "\n",
    "**claim**: each operator, $\\mathbf B \\in \\mathbb C^{n x n}$ obeys its characteristic polynomial.  i.e. \n",
    "\n",
    "$c_0 \\mathbf I + c_1 \\mathbf B + c_2 \\mathbf B^2 + ... + c_{n-1}\\mathbf B^{n-1} + c_{n}\\mathbf B^n = c_0 \\mathbf I +  \\Sigma_{r=1}^{n} c_r \\mathbf B^r = \\bigg[\\begin{array}{c|c|c|c}\n",
    "  \\mathbf 0 & \\mathbf 0 &\\cdots & \\mathbf 0\n",
    "\\end{array}\\bigg]$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**proof: non-defective case, where $\\mathbf B$ has $n$ linearly independent eigenvectors.**\n",
    "\n",
    "We know that each eigenvalue is a root to the characteristic polynomial.  \n",
    "\n",
    "Put differently, we know that for $k = \\{1, 2, ..., n-1, n\\}$, we have an eigenpair of $\\mathbf x_k, \\lambda_k$\n",
    "\n",
    "$c_0 +  \\Sigma_{r=1}^{n} c_r \\lambda_k^r = 0$\n",
    "\n",
    "\n",
    "we can multiply this by $\\mathbf x_k$:\n",
    "\n",
    "$\\big(c_0 +  \\Sigma_{r=1}^{n} c_r \\lambda_k^r \\big) \\mathbf x_k = \\mathbf 0$\n",
    "\n",
    "\n",
    "or equivalently\n",
    "\n",
    "$\\big(c_0 \\mathbf I  + \\Sigma_{r=1}^{n} c_r \\mathbf B^r \\big) \\mathbf x_k = \\mathbf 0$\n",
    "\n",
    "\n",
    "\n",
    "Now let's collect these $n$ relationships in a system of equations:\n",
    "\n",
    "\n",
    "$\\big(c_0 \\mathbf I + \\Sigma_{r=1}^{n} c_r \\mathbf B^r \\big) \\bigg[\\begin{array}{c|c|c|c}\n",
    "\\mathbf x_1 & \\mathbf x_2 &\\cdots & \\mathbf x_n\n",
    "\\end{array}\\bigg] = \\big(c_0 \\mathbf I + \\Sigma_{r=1}^{n} c_r \\mathbf B^r \\big)\\mathbf X = \\bigg[\\begin{array}{c|c|c|c}\n",
    "  \\mathbf 0 & \\mathbf 0 &\\cdots & \\mathbf 0\n",
    "\\end{array}\\bigg]$\n",
    "\n",
    "because the eigenvectors are stated to be linearly independent, we multiply each side on the right by $\\mathbf X^{-1}$ seeing that\n",
    "\n",
    "\n",
    "$\\big(c_0 \\mathbf I + \\Sigma_{r=1}^{n} c_r \\mathbf B^r \\big) = \\bigg[\\begin{array}{c|c|c|c}\n",
    "  \\mathbf 0 & \\mathbf 0 &\\cdots & \\mathbf 0\n",
    "\\end{array}\\bigg]$\n",
    "\n",
    "which proves that $\\mathbf B$ follows its characteristic polynomial at least in the case where its eigenvectors form a basis.\n",
    "\n",
    "**proof: defective case:**  \n",
    "\n",
    "a sketch of the proof covers two things\n",
    "\n",
    "First, it is clear from Schur decomposition that any matrix is unitarily similar to an upper triangular matrix\n",
    "\n",
    "$\\mathbf Q^H \\mathbf{BQ} = \\mathbf T$ or \n",
    "\n",
    "$\\mathbf{B} = \\mathbf {QTQ}^H$\n",
    "\n",
    "\n",
    "The eigenvalues of $\\mathbf T$ obey their characteristic polynomial, hence the characteristic polynomial of $\\mathbf B$ or equivalently $\\mathbf T$, must be a nilpotent matrix.  However Cayley Hamilton makes a stronger claim that in fact it is not just any nilpotent matrix, but the zero matrix.  \n",
    "\n",
    "*A key takeway from this, however, is that if a matrix was not nilpotent (i.e. it has at least one non-zero eigenvalue), and it becomes nilpotent after applying some other matrix's characteristic polynomial, then that means your matrix has roots to that other matrices characteristic polynomial -- i.e. your non-zero eigenvalues are non-zero eigenvalues for that other matrix. *\n",
    "\n",
    "an outline of the analysis approach is that:\n",
    "\n",
    "we can find an upper triangular matrix $\\mathbf R$ where all entries are identical to $\\mathbf T$ except diagonal elements are perturbed by small enough $\\delta$, $\\delta^2$, $\\delta^3$, and so on as needed for all duplicate eigenvalues.  Afterward, we have \n",
    "\n",
    "$\\big \\Vert \\mathbf{T} - \\mathbf R \\big \\Vert_F^2 \\lt \\epsilon$\n",
    "\n",
    "for any $\\epsilon \\gt 0$ \n",
    "\n",
    "where $\\mathbf C = \\mathbf{QRQ}^H$\n",
    "\n",
    "But now each eigenvalue is unique and per the proof near the top of this posting $\\mathbf C$  is now diagonalizable aka non-defective, and the earlier part of this cell -- i.e. the proof that all diagonalizable matrices obey Cayley Hamilton-- may be used. (There is a tecnical nit that by perturbing the eigenvalues, we have changed the characteristic polynomial, but this change is $O(\\delta)$ and becomes arbitrarily small as $\\delta \\to 0$).  \n",
    "\n",
    "Thus we may say, up to any arbitrary level of precision we can approximate $\\mathbf B$ or $\\mathbf T$ and find that those approximations all obey Cayley Hamilton, hence $\\mathbf B$ obeys Cayley Hamilton as well. \n",
    "\n",
    "*note: there are purely algebraic proofs of Cayley Hamilton for defective matrices that do not require limits / analysis.  The analysis view is more intuitive, but requires some heavier duty machinery to be fully rigorous.  A purely algebraic proof is pending.  In any case these different approaches are complementary.*  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We now combine two cells: \"Full cycle trace relations and nilpotent matrices\" and the above \"Cayley Hamilton\" proofs.  \n",
    "\n",
    "**claim:**  \n",
    "\n",
    "If for $m$ x $m$ matrix $\\mathbf X$ and $n$ x $n$ matrix $\\mathbf Y$:\n",
    "\n",
    "$trace\\big(\\mathbf X^k\\big)$ = $trace\\big(\\mathbf Y^k \\big)$  \n",
    "for natural numbers $k = \\{1, 2, 3, ...,\\}$, then they have the same non-zero eigenvalues (with same algebraic multiplicity for each non-zero eigenvalue).\n",
    "\n",
    "\n",
    "**commentary: ** This may be useful in cases where perhaps we know the traces, or even the eigenvalues, of $\\mathbf X$ and want to make inferences about $\\mathbf Y$.  \n",
    "\n",
    "**proof:**\n",
    "\n",
    "for convenience notice that, if for $r = \\{1, 2, 3, ... , max(m,n)\\}$\n",
    "\n",
    "$trace\\big(\\mathbf X^r\\big) = trace\\big(\\mathbf Y^r\\big)  = 0 $, then both $\\mathbf X$ and $\\mathbf Y$ are nilpotent.  \n",
    "\n",
    "*The rest of the writeup assumes that they are not nilpotent matrices.*\n",
    "\n",
    "Now suppose we know the eigenvalues of $\\mathbf X$, and in particular the non-zero eigenvalues of $\\mathbf X$.  Then we know the characteristic polynomial, $p$ and use Cayley Hamilton to see the below mapping:  \n",
    "\n",
    "$p\\big(\\mathbf X\\big) = c_0 \\mathbf I + c_1 \\mathbf X + c_2 \\mathbf X^2 + ... + c_{n-1}\\mathbf X^{n-1} + c_{n}\\mathbf X^n = c_0 \\mathbf I +  \\Sigma_{j=1}^{n} c_j \\mathbf X^j = \\bigg[\\begin{array}{c|c|c|c}\n",
    "  \\mathbf 0 & \\mathbf 0 &\\cdots & \\mathbf 0\n",
    "\\end{array}\\bigg]$\n",
    "\n",
    "We right multiply the above by $\\mathbf X$\n",
    "\n",
    "$p\\big(\\mathbf X\\big)\\mathbf X = c_0 \\mathbf X + c_1 \\mathbf X^2 + c_2 \\mathbf X^3 + ... + c_{n-1}\\mathbf X^{n} + c_{n}\\mathbf X^{n+1} =\\bigg[\\begin{array}{c|c|c|c}\n",
    "  \\mathbf 0 & \\mathbf 0 &\\cdots & \\mathbf 0\n",
    "\\end{array}\\bigg] \\mathbf X = \\bigg[\\begin{array}{c|c|c|c}\n",
    "  \\mathbf 0 & \\mathbf 0 &\\cdots & \\mathbf 0\n",
    "\\end{array}\\bigg]$\n",
    "\n",
    "Notice that if we square the above, we have \n",
    "\n",
    "$\\big(p\\big(\\mathbf X\\big)\\mathbf X\\big)^2 = \\big(c_0 \\mathbf X + c_1 \\mathbf X^2 + c_2 \\mathbf X^3 + ... + c_{n-1}\\mathbf X^{n} + c_{n}\\mathbf X^{n+1}\\big)^2 = \\big(\\Sigma_{j=1}^{n+1} (c_{j-1})\\mathbf X^j\\big) \\big(\\Sigma_{j=1}^{n+1} (c_{j-1})\\mathbf X^j\\big) = \\Sigma_i \\gamma_i \\mathbf X^i = \\bigg[\\begin{array}{c|c|c|c}\n",
    "  \\mathbf 0 & \\mathbf 0 &\\cdots & \\mathbf 0\n",
    "\\end{array}\\bigg]$\n",
    "\n",
    "where $\\gamma_i$ is the appropriate scalar that comes from multiplying out the respective $c_j$ term.  We will return to this momentarily.\n",
    "\n",
    "Taking the trace, we see:\n",
    "\n",
    "$trace\\big(p\\big(\\mathbf X\\big)\\mathbf X \\big) = c_0 trace\\big(\\mathbf X\\big) + c_1 trace\\big(\\mathbf X^2\\big) + c_2 trace\\big(\\mathbf X^3\\big) + ... + c_{n-1} trace\\big(\\mathbf X^{n}\\big) + c_{n} trace\\big(\\mathbf X^{n+1}\\big) = 0$\n",
    "\n",
    "but this is equivalent to \n",
    "\n",
    "$trace\\big(p\\big(\\mathbf Y\\big)\\mathbf Y \\big) = c_0 trace\\big(\\mathbf Y\\big) + c_1 trace\\big(\\mathbf Y^2\\big) + c_2 trace\\big(\\mathbf Y^3\\big) + ... + c_{n-1} trace\\big(\\mathbf Y^{n}\\big) + c_{n} trace\\big(\\mathbf Y^{n+1}\\big) = 0$\n",
    "\n",
    "\n",
    "and more generally, we see that \n",
    "\n",
    "for $r = \\{1, 2, 3, ... , max(m,n)\\}$\n",
    "\n",
    "$trace\\Big( \\big(p\\big(\\mathbf X\\big)\\mathbf X \\big)\\big)^r\\Big)= trace\\Big(\\big(\\Sigma_{j=1}^{n+1} (c_{j-1})\\mathbf X^j\\big)^r\\Big) = trace\\big(\\Sigma_i \\gamma_i \\mathbf X^i\\big) =\\Sigma_i \\gamma_i trace\\big(\\mathbf X^i\\big) = 0$\n",
    "\n",
    "where again, $\\gamma_i$ indicates the scalar result of multiplying the relevant $c_j$ terms.  We then recall that for each term in this finite series, for the relevant natural numbers $i$, \n",
    "\n",
    "$trace\\big(\\mathbf X^i\\big) = trace\\big(\\mathbf Y^i\\big)$  \n",
    "\n",
    "and scaling this by some $\\gamma_i$,\n",
    "\n",
    "$\\gamma_i trace\\big(\\mathbf X^i\\big) = \\gamma_i trace\\big(\\mathbf Y^i\\big)$  \n",
    "\n",
    "hence \n",
    "\n",
    "$0 = \\Sigma_i \\gamma_i trace\\big(\\mathbf X^i\\big) = \\Sigma_i \\gamma_itrace\\big(\\mathbf Y^i\\big)$  \n",
    "\n",
    "We then conclude that for $r = \\{1, 2, 3, ... , max(m,n)\\}$\n",
    "\n",
    "$trace\\Big( \\big(p\\big(\\mathbf X\\big)\\mathbf X \\big)\\big)^r\\Big) = trace\\Big( \\big(p\\big(\\mathbf Y\\big)\\mathbf Y \\big)\\big)^r\\Big) = 0$  \n",
    "\n",
    "We now know that the matrix given by $\\Big(p\\big(\\mathbf Y\\big)\\mathbf Y\\Big)$ is nilpotent.  Recalling that $p\\big(\\mathbf Y\\big)$ is just a finite series of $ \\mathbf Y^k$ with particular scalars applied for each appropriate $k$, we do Schur Decomposition and see  \n",
    "\n",
    "$ p\\big(\\mathbf Y\\big) = \\mathbf {QUQ}^H $ and $\\mathbf Y = \\mathbf {QRQ}^H$, then \n",
    "\n",
    "$\\Big(p\\big(\\mathbf Y\\big)\\mathbf Y\\Big) = \\Big(\\mathbf {QUQ}^H \\mathbf {QRQ}^H \\Big) = \\mathbf {QURQ}^H$\n",
    "\n",
    "since $\\mathbf Y$ is not nilpotent (i.e. $\\mathbf R$ is not nilpotent) but $\\big(\\mathbf{UR}\\big)$ is nilpotent, this tells us that $\\mathbf U$ is strictly upper triangular -- i.e. $\\mathbf U$ is nilpotent, which means that the matrix given by $p\\big(\\mathbf Y\\big)$ is nilpotent.  \n",
    "\n",
    "We thus see that all non-zero diagonal elements of $\\mathbf R$ -- aka the all non-zero eigenvalues of $\\mathbf Y$ obey the characteristic polynomial given by $p$.  (If we wanted, we could take this one steup further and reconize that if $p\\big(\\mathbf Y\\big)$ is nilpotent, then by Cayley Hamilton it is in fact the zero matrix, though this is not needed here so we don't pursue it.)  \n",
    "\n",
    "At a bare minimum, the above shows that the set of unique non zero eigenvalues of $\\mathbf Y \\subset $ unique non zero eigenvalues of $\\mathbf X$\n",
    "\n",
    "- - - -\n",
    "*Here are two different approaches now to finish off the proof*   \n",
    "- - - -\n",
    "\n",
    "\n",
    "**(1)**  \n",
    "\n",
    "Do the exact same argument used above, except swap $\\mathbf X$ for $\\mathbf Y$. \n",
    "\n",
    "(In many programming languages we would say:  \n",
    "(a) $\\mathbf X, \\mathbf Y := \\mathbf Y, \\mathbf X$  \n",
    "(b) call on argument used above, once.\n",
    ")  \n",
    "\n",
    "At a minimum, doing that shows that the set of unique non zero eigenvalues of $\\mathbf X \\subset $ unique non zero eigenvalues of $\\mathbf Y$\n",
    "\n",
    "hence with respect to unique non-zero eigenvalues we have $\\lambda\\big(\\mathbf X\\big) \\subset \\lambda\\big(\\mathbf Y\\big)$ and from before, with respect to nonzero eigenvalues, we have $\\lambda\\big(\\mathbf Y\\big) \\subset \\lambda\\big(\\mathbf X\\big)$ which proves that with respect to unique non-zero eigenvalues $\\lambda\\big(\\mathbf Y\\big) = \\lambda\\big(\\mathbf X\\big)$.  \n",
    "\n",
    "As before we collect these unique non-zero eigenvalues in a diagonal matrix $\\mathbf D$.  There are $t$ non-zero eigenvalues, and $\\mathbf D$ is $t$ x $t$.  \n",
    "\n",
    "Collect the algebraic multiplicities for these unique nonzero eigenvalues of $\\mathbf X$ in $\\mathbf a_x$ and collect the algebraic multiplicities for the unique nonzero eigenvalues of $\\mathbf Y$ in $\\mathbf a_y$.  (As reminder, because they are algebraic multiplicities, each entry in $\\mathbf a_x$ and $\\mathbf a_y$ must be an integer $\\geq 1$.)\n",
    "\n",
    "Thus we have \n",
    "$\\mathbf W = \\bigg[\\begin{array}{c|c|c|c|c}\n",
    "\\mathbf D^0 \\mathbf 1 & \\mathbf D^1 \\mathbf 1 & \\mathbf D^2 \\mathbf 1 &\\cdots & \\mathbf D^{t-1} \\mathbf 1\n",
    "\\end{array}\\bigg]$\n",
    "\n",
    "and we show that these traces are equal with the following expression:  \n",
    "\n",
    "\n",
    "$\\mathbf a_x^H \\mathbf D \\mathbf W = \\mathbf a_y^H \\mathbf D \\mathbf W $  \n",
    "$\\mathbf a_x^H \\big(\\mathbf D \\mathbf W\\big)\\big(\\mathbf D \\mathbf W\\big)^{-1} = \\mathbf a_x^H \\mathbf D \\mathbf W \\mathbf W^{-1} \\mathbf D^{-1} = \\mathbf a_x^H = \\mathbf a_y^H = \\mathbf a_y^H \\mathbf D \\mathbf W \\mathbf W^{-1} \\mathbf D^{-1}  = \\mathbf a_y^H  \\big(\\mathbf D \\mathbf W\\big)\\big(\\mathbf D \\mathbf W\\big)^{-1} $  \n",
    "\n",
    "hence $\\mathbf a_x^H = \\mathbf a_y^H$\n",
    "\n",
    "and equivalently: $\\mathbf a_x = \\mathbf a_y$\n",
    "\n",
    "and we see that $\\mathbf X$ and $\\mathbf Y$ not only have the same unique non-zero eigenvalues, but that each one of those unique non-zero eigenvalues has the same algebraic multiplicity.\n",
    "- - - -\n",
    "*alternative approach to finish the proof:  *  \n",
    "**(2)**\n",
    "\n",
    "In this case, we only know that with respect to unique non-zero eigenvalues, we have \n",
    "\n",
    "$\\lambda\\big(\\mathbf Y\\big) \\subset \\lambda\\big(\\mathbf X\\big)$\n",
    "\n",
    "we suppose that $\\lambda\\big(\\mathbf Y\\big) \\neq \\lambda\\big(\\mathbf X\\big)$, i.e. that $\\mathbf X$ has some unique non-zero eigenvalues that $\\mathbf Y$ doesn't have.  Thus we assume that $\\mathbf Y$ has $k$ unique non-zero eigenvalues and $\\mathbf X$ has $r$ unique non-zero eigenvalues, where $1 \\leq k \\lt r$.\n",
    "\n",
    "and we collect the algebraic multiplicities for the $k$ unique eigenvalues $\\mathbf Y$ in $\\mathbf a_y$\n",
    "\n",
    "We create a short, fat $k$ x $r$ Vandermonde matrix for $\\mathbf Y$, below \n",
    "\n",
    "$\\mathbf W_{\\mathbf Y} = \\begin{bmatrix}\n",
    "1 & \\lambda_1 & \\lambda_1^2 & \\dots  & \\lambda_1^{r-1}\\\\ \n",
    "1 & \\lambda_2 & \\lambda_2^2 & \\dots &  \\lambda_2^{r-1} \\\\ \n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\\\ \n",
    "1 & \\lambda_{k} & \\lambda_{k}^{2} & \\dots  & \\lambda_{k}^{r-1}\n",
    "\\end{bmatrix}= \\bigg[\\begin{array}{c|c|c|c|c}\n",
    "\\mathbf D^0 \\mathbf 1 & \\mathbf D^1 \\mathbf 1 & \\mathbf D^2 \\mathbf 1 &\\cdots & \\mathbf D^{r-1} \\mathbf 1\n",
    "\\end{array}\\bigg]$\n",
    "\n",
    "$rank\\big(\\mathbf W_Y\\big) = k$ \n",
    "\n",
    "now we setup the trace relation as \n",
    "\n",
    "$\\big(\\mathbf a_Y\\big) \\mathbf D \\mathbf W_{\\mathbf Y} = \\big(\\mathbf 1^H Diag\\big(\\mathbf a_Y\\big)\\big) \\mathbf D \\mathbf W_Y = \\begin{bmatrix} trace\\big(\\mathbf Y\\big) & trace\\big(\\mathbf Y^2\\big)  & trace\\big(\\mathbf Y^3\\big) & \\cdots & trace\\big(\\mathbf Y^r\\big) \\end{bmatrix}$  \n",
    "\n",
    "from here we build this out to an $r$ x $r$ matrix, where we have \n",
    "\n",
    "$\\mathbf H = \\Bigg[\\begin{matrix}\n",
    "trace\\big(\\mathbf Y\\big)  & trace\\big(\\mathbf Y^2\\big)  & trace\\big(\\mathbf Y^3\\big)  &\\cdots  &trace\\big(\\mathbf Y^r\\big) \\\\ \n",
    "trace\\big(\\mathbf Y^2\\big)  & trace\\big(\\mathbf Y^3\\big)  & trace\\big(\\mathbf Y^4\\big)  & \\cdots & trace\\big(\\mathbf Y^{r+1}\\big) \\\\ \n",
    "trace\\big(\\mathbf Y^3\\big) &  trace\\big(\\mathbf Y^4\\big)& trace\\big(\\mathbf Y^5\\big)  & \\cdots &trace\\big(\\mathbf Y^{r+2}\\big) \\\\ \n",
    "\\vdots & \\vdots & \\vdots &  \\ddots & \\vdots \\\\ \n",
    "trace\\big(\\mathbf Y^r\\big) & trace\\big(\\mathbf Y^{r+1}\\big) &trace\\big(\\mathbf Y^{r+2}\\big)  & \\cdots & trace\\big(\\mathbf Y^{2r}\\big)\n",
    "\\end{matrix}\\Bigg]$  \n",
    "\n",
    "note: While there may be some LaTeX rendering issues, it is clear that the above matrix $\\mathbf H$ is a Hankel matrix.  It is symmetric, but if any of the entries are complex, it is not Hermitian. From here, notice that while the first row is given by\n",
    "\n",
    "$\\begin{bmatrix}\n",
    "1 & 1 & 1 & \\cdots  &1 \n",
    "\\end{bmatrix}Diag\\big(\\mathbf a_Y\\big) \\mathbf D \\mathbf W_{\\mathbf Y}$\n",
    "\n",
    "the second row is given by \n",
    "\n",
    "$\\begin{bmatrix}\n",
    "\\lambda_1 & \\lambda_2 & \\lambda_3 & \\cdots  &\\lambda_k\n",
    "\\end{bmatrix}Diag\\big(\\mathbf a_Y\\big) \\mathbf D \\mathbf W_{\\mathbf Y}$\n",
    "\n",
    "the third row is given by \n",
    "\n",
    "$\\begin{bmatrix}\n",
    "\\lambda_1^2 & \\lambda_2^2 & \\lambda_3^2 & \\cdots  &\\lambda_k^2 \\end{bmatrix}Diag\\big(\\mathbf a_Y\\big) \\mathbf D \\mathbf W_{\\mathbf Y}$\n",
    "\n",
    "... and the final, rth row is given by \n",
    "\n",
    "$\\begin{bmatrix}\n",
    "\\lambda_1^{r-1} & \\lambda_2^{r-1} & \\lambda_3^{r-1} & \\cdots  &\\lambda_k^{r-1} \\end{bmatrix}Diag\\big(\\mathbf a_Y\\big) \\mathbf D \\mathbf W_{\\mathbf Y}$\n",
    "\n",
    "thus we have\n",
    "\n",
    "$\\mathbf W_{\\mathbf Y}^T Diag\\big(\\mathbf a_Y\\big) \\mathbf D \\mathbf W_{\\mathbf Y} = \\mathbf H$\n",
    "\n",
    "notice that the above $\\mathbf W^T \\neq \\mathbf W^H$ except in the special case where all $\\lambda_i$'s are real.  The above matrix is square and it is *not* full rank. It is at most rank $k$ because $rank\\big(\\mathbf W_Y\\big) = k$, and as a reminder we have assumed $k \\lt r$.  \n",
    "\n",
    "Put differently $det\\big(\\mathbf H\\big) = 0$.  \n",
    "\n",
    "If we work through the exact same calculations, for $\\mathbf X$, we find that \n",
    "\n",
    "$\\mathbf W_{\\mathbf X}^T Diag\\big(\\mathbf a_X\\big) \\mathbf \\Lambda \\mathbf W_{\\mathbf X} = \\mathbf H$\n",
    "\n",
    "where $ \\mathbf \\Lambda$ has the same diagonal elements as $\\mathbf D$ for $\\lambda_{i,i}$ for $i = \\{1,2, ..., k\\}$, and has the additional unique, non-zero eigenvalues that we've assumed in $\\lambda_{i,i}$ for $i = \\{k + 1,k + 2, ..., r\\}$. \n",
    "\n",
    "\n",
    "$\\mathbf W_{\\mathbf X}$ is an $r$ x $r$ matrix with each of those $r$ unique non-zero eigenvalues in a Vandermonde Matrix, i.e. \n",
    "\n",
    "$\\mathbf W_{\\mathbf X} = \\bigg[\\begin{array}{c|c|c|c|c}\n",
    "\\mathbf \\Lambda^0 \\mathbf 1 & \\mathbf\\Lambda^1 \\mathbf 1 & \\mathbf\\Lambda^2 \\mathbf 1 &\\cdots & \\mathbf \\Lambda^{r-1} \\mathbf 1\n",
    "\\end{array}\\bigg]$\n",
    "\n",
    "\n",
    "further notice that $a_{X, i}$ for $i = \\{1, 2, ...., r\\} \\geq 1$, hence $Diag\\big(\\mathbf a_X\\big)$  is invertible. And since each eigenvalue in $\\mathbf W_{\\mathbf X}$ is unique, the square matrix given by $\\mathbf W_{\\mathbf X}$ is full rank.  \n",
    "\n",
    "Thus we have \n",
    "\n",
    "$det\\big(\\mathbf W_{\\mathbf X}^T Diag\\big(\\mathbf a_X\\big) \\mathbf \\Lambda \\mathbf W_{\\mathbf X}\\big) = det\\big(\\mathbf H \\big) \\neq 0 = det\\big(\\mathbf H \\big) = det\\big(\\mathbf W_{\\mathbf X}^T Diag\\big(\\mathbf a_X\\big) \\mathbf \\Lambda \\mathbf W_{\\mathbf X}\\big) $\n",
    "\n",
    "which is a contradiction.  Or equivalently\n",
    "\n",
    "$rank\\big(\\mathbf W_{\\mathbf X}^T Diag\\big(\\mathbf a_X\\big) \\mathbf \\Lambda \\mathbf W_{\\mathbf X}\\big) = rank\\big(\\mathbf H \\big) = r = rank\\big(\\mathbf H \\big) = rank\\big(\\mathbf W_{\\mathbf X}^T Diag\\big(\\mathbf a_X\\big) \\mathbf \\Lambda \\mathbf W_{\\mathbf X}\\big) \\leq k$\n",
    "\n",
    "Because we have assumed that $k \\lt r$ (or equivalently $r \\gt k$), it cannot be the case that $r \\leq k$, an obvious contradiction.  Notice that even if we swapped $\\mathbf X$ and $\\mathbf Y$ ala the finish for alternative *(1)*, the equality can hold **iff** $k = r$.  \n",
    "\n",
    "Thus we observe that that the number of unique non-zero eigenvalues for $\\mathbf Y$, given by $k$ must be equal to the number of unique non-zero eigenvalues of $\\mathbf X$, given by $r$.  \n",
    "\n",
    "as in *(1)*, we recongize that $\\mathbf Y$ has $k$ unique non-zero eigenvalues, which are contained in the set of unique non-zero eigenvalues of $\\mathbf X$ which contains $r$ elements (i.e. unique non-zero eigenvalues), and we now know that $k = r$, thus we can set up our Vandermonde Matrix $\\mathbf W$ such that \n",
    "\n",
    "$\\mathbf W_{\\mathbf Y} = \\mathbf W_{\\mathbf X} = \\mathbf W$\n",
    "\n",
    "we collect the algebraic multiplicities for unique non-zero eigenvalues of $\\mathbf X$ in $\\mathbf a_x$ and the respective eigenvalues in for $\\mathbf Y$ in $\\mathbf a_y$, and we have the below equation:    \n",
    "\n",
    "$\\mathbf a_x^H \\mathbf D \\mathbf W = \\mathbf a_y^H \\mathbf D \\mathbf W $  \n",
    "$\\mathbf a_x^H \\big(\\mathbf D \\mathbf W\\big)\\big(\\mathbf D \\mathbf W\\big)^{-1} = \\mathbf a_y^H  \\big(\\mathbf D \\mathbf W\\big)\\big(\\mathbf D \\mathbf W\\big)^{-1} $  \n",
    "\n",
    "hence $\\mathbf a_x^H = \\mathbf a_y^H$\n",
    "\n",
    "and equivalently: $\\mathbf a_x = \\mathbf a_y$\n",
    "\n",
    "\n",
    "Thus, if $trace\\big(\\mathbf X^k\\big)$ = $trace\\big(\\mathbf Y^k \\big)$  for natural numbers $k = \\{1, 2, 3, ...,\\}$, then $\\mathbf X$ and $\\mathbf Y$ have the same non-zero eigenvalues (with same algebraic multiplicity for each non-zero eigenvalue).\n",
    "\n",
    "**remark:**\n",
    "\n",
    "*(2)* took a lot more lines than *(1)*, why bother?  \n",
    "\n",
    "First, proving the same thing from two (or more) different perspective is frequently instructive, helping intuition, and is a great way to flush out 'bugs' in one's logic.  \n",
    "\n",
    "Second, it may be useful to take a step back and realize that if $\\mathbf X$ and $\\mathbf Y$ are nilpotent, then we handled the entire proof at time zero.  \n",
    "\n",
    "Otherwise using if they are not nilpotent, *(2)* is in some sense simpler.  \n",
    "\n",
    "When using *(2)* with non-nilpotent matrices, in all cases we may find some matrix $\\mathbf Z$ that is not defective (i.e. even with repeated eigenvalues, we choose eigenvectors that form a basis), where $trace\\big(\\mathbf Z^k\\big) = trace\\big(\\mathbf X^k\\big)$ for $k =\\{1,2,3, ...\\}$, and as before $ trace\\big(\\mathbf X^k\\big) = trace\\big(\\mathbf Y^k\\big)$.  \n",
    "\n",
    "Specifically, we can then call on the *easy* Cayley Hamilton proof that applies for a non-defective square matrix like $\\mathbf Z$, and use proof *(2)* to show that $\\mathbf X$ has the same non-zero eigenvalues, with the same multiplicities, as $\\mathbf Z$.  We can then repeat this process and see that $\\mathbf Y$ has the same non-zero eigenvalues, with the same multiplicities as $\\mathbf Z$.  Thus $\\mathbf X$ has the same non-zero eigenvalues, with the same multiplicities as $\\mathbf Y$.  \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [py35]",
   "language": "python",
   "name": "Python [py35]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
