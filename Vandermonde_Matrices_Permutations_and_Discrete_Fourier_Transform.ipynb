{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motivating Background: figuring out a function from data points\n",
    "\n",
    "Quite often in basic machine learning applications -- say with linear regression -- we gather $n$ samples of data and look to fit a model to it.  Note: we often have *a lot* of data, and in fact n can be any natural number.  For illustrative purposes, we start with the case of n = 5. \n",
    "\n",
    "Note that we typically also have multiple different features in our data, but *the goal of this posting is to strip down ideas to their very core*, so we consider the one feature case.  Also note that in machine learning we may use notation like $\\mathbf {Xw} = \\mathbf y$, where we solve for the weights in $\\mathbf w$.  However, this posting uses the typical Linear Algebra setup of $\\mathbf{Ax} = \\mathbf b$, where we are interested in solving for $\\mathbf x$.  \n",
    "\n",
    "So initially we may just have the equation\n",
    "\n",
    "$\\mathbf{Ax} = \\begin{bmatrix}\n",
    "a_1\\\\ \n",
    "a_2\\\\ \n",
    "a_3\\\\ \n",
    "a_4\\\\ \n",
    "a_5\n",
    "\\end{bmatrix} \\begin{bmatrix}\n",
    "x_1\\\\ \n",
    "\\end{bmatrix} = \\mathbf b$\n",
    "\n",
    "**this original 'data' matrix will also be written as **\n",
    "\n",
    "$\\mathbf a = \\begin{bmatrix}\n",
    "a_1\\\\ \n",
    "a_2\\\\ \n",
    "a_3\\\\ \n",
    "a_4\\\\ \n",
    "a_5\n",
    "\\end{bmatrix}$\n",
    "\n",
    "Note that when we gather real world data there is noise in the data, so we would be *extremely* surprised if any of the entries in $\\mathbf a$ are duplicates.  So, unless otherwise noted assume that each entry in $a_i$ is unique. Since there is only one column, the column rank of $\\mathbf A$ is one, and the column rank = row rank, thus we know that the row rank = 1. \n",
    "\n",
    "Then we decide to insert a bias /affine translation piece (in index position zero -- to use notation from Caltech's \"Learning From Data\").  \n",
    "\n",
    "Thus we end up with the following equation\n",
    "\n",
    "$\\mathbf{Ax} = \\begin{bmatrix}\n",
    "1 & a_1\\\\ \n",
    "1 & a_2\\\\ \n",
    "1 & a_3\\\\ \n",
    "1 & a_4\\\\ \n",
    "1 & a_5\n",
    "\\end{bmatrix} \\begin{bmatrix}\n",
    "x_0\\\\\n",
    "x_1\\\\ \n",
    "\\end{bmatrix} = x_0 \\mathbf 1 + x_1 \\mathbf a = \\mathbf b$\n",
    "\n",
    "Column 0 of $\\mathbf A$ is the ones vector, also denoted as $\\mathbf 1$.  \n",
    "\n",
    "At this point we know that $\\mathbf A$ still has full column rank (i.e. rank = 2) -- if this wasn't the case, this would imply that we could scale column 0 to get column 1 (i.e. everything in column 1 would have to be identical).   \n",
    "\n",
    "From here we may simply decide to do least squares and solve (which we always can do when we have full column rank, and $\\mathbf A $ has m rows and n columns, where $m \\geq n$).  \n",
    "\n",
    "Or we may decide to map this to a higher dimensional space that has a quadratic term.  \n",
    "\n",
    "$\\mathbf{Ax} = \\begin{bmatrix}\n",
    "1 & a_1 & a_1^2\\\\ \n",
    "1 & a_2 & a_2^2\\\\ \n",
    "1 & a_3 & a_3^2\\\\ \n",
    "1 & a_4 & a_4^2\\\\ \n",
    "1 & a_5 & a_5^2\n",
    "\\end{bmatrix} \\begin{bmatrix}\n",
    "x_0\\\\\n",
    "x_1\\\\ \n",
    "x_2\\\\\n",
    "\\end{bmatrix} = \\mathbf b$\n",
    "\n",
    "\n",
    "At this point we may just do least squares and solve.  But that requires $\\mathbf A$ to have full column rank.  How do we know that $\\mathbf A$ has full column rank?  An intuitive way to think about it is that squaring each $a_i$ to get column 2 is not a linear transformation, so we would not expect it to be linear combination of prior columns.  \n",
    "\n",
    "$\\mathbf a \\circ \\mathbf a \\neq \\gamma_0 \\mathbf 1 + \\gamma_1 \\mathbf a$\n",
    "\n",
    "where $\\circ$ denotes the Hadamard product.  And by earlier argument, we know $\\mathbf a \\neq \\gamma_0 \\mathbf 1$, hence each column is linearly independent.  There is another (more mathemetically exact) way to verify linear independence of these columns -- which comes from the Vandermonde Matrix, and we will address this shortly.  \n",
    "\n",
    "We may however decide we want an even higher dimensional space for our data, so we add a cubic term:\n",
    "\n",
    "$\\mathbf{Ax} = \\begin{bmatrix}\n",
    "1 & a_1 & a_1^2 & a_1^3\\\\ \n",
    "1 & a_2 & a_2^2 & a_2^3\\\\ \n",
    "1 & a_3 & a_3^2 & a_3^3\\\\ \n",
    "1 & a_4 & a_4^2 & a_4^3\\\\ \n",
    "1 & a_5 & a_5^2 & a_5^3\n",
    "\\end{bmatrix} \\begin{bmatrix}\n",
    "x_0\\\\\n",
    "x_1\\\\ \n",
    "x_2\\\\\n",
    "x_3\\\\\n",
    "\\end{bmatrix} = \\mathbf b$\n",
    "\n",
    "Again we may be confident that the columns are linearly independent because our new column -- cubing $\\mathbf a$ is not a linear transformation (or alternatively, using the hadamard product is not a linear transformation), so we write: \n",
    "\n",
    "$\\mathbf a \\circ \\mathbf a \\circ \\mathbf a \\neq \\gamma_0 \\mathbf 1 + \\gamma_1 \\mathbf a + \\gamma_2 \\big(\\mathbf a \\circ \\mathbf a\\big)$\n",
    "\n",
    "And if the above is *still* not enough, we may add a term to the fourth power:\n",
    "\n",
    "$\\mathbf{Ax} = \\begin{bmatrix}\n",
    "1 & a_1 & a_1^2 & a_1^3 & a_1^4\\\\ \n",
    "1 & a_2 & a_2^2 & a_2^3 & a_2^4\\\\ \n",
    "1 & a_3 & a_3^2 & a_3^3 & a_3^4\\\\ \n",
    "1 & a_4 & a_4^2 & a_4^3 & a_4^4\\\\ \n",
    "1 & a_5 & a_5^2 & a_5^3 & a_5^4\n",
    "\\end{bmatrix} \\begin{bmatrix}\n",
    "x_0\\\\\n",
    "x_1\\\\ \n",
    "x_2\\\\\n",
    "x_3\\\\\n",
    "x_4\\\\\n",
    "\\end{bmatrix} = \\mathbf b$\n",
    "\n",
    "Again quite confident that the above has full column rank because \n",
    "\n",
    "$\\mathbf a \\circ \\mathbf a \\circ \\mathbf a \\circ \\mathbf a \\neq \\gamma_0 \\mathbf 1 + \\gamma_1 \\mathbf a + \\gamma_2 \\big(\\mathbf a \\circ \\mathbf a\\big) + \\gamma_3 \\big(\\mathbf a \\circ \\mathbf a \\circ \\mathbf a \\big)$\n",
    "\n",
    "We may be tempted to go to an even higher dimensional space at this point, but this requires considerable justification.  Notice that $\\mathbf A$ is a square matrix now, and as we've argued, it has full column rank -- which means it also has full row rank.  Thus we can be sure to solve the above equation for a single, exact solution, where $\\mathbf x = \\mathbf A^{-1}\\mathbf b$.  If we were to go to a higher dimensional space we would be entering the world of an underdetermined system of equations -- see postings titled \"Underdetermined_System_of_Equations.ipynb\" for the L2 norm oriented solution, and \"underdetermined_regression_minimize_L1_norm.ipynb\" for the L1 norm oriented solution.  Since we can already be certain of solving for a single exact solution in this problem, we will stop mapping to higher dimensions here.  \n",
    "\n",
    "In the above equation of $\\mathbf{Ax} = \\mathbf b$, the square $\\mathbf A$ is a Vandermonde matrix.  Technical note: some texts say that $\\mathbf A$ is the Vandermonde matrix, while others say $\\mathbf A^T$ is the Vandermonde matrix.  The calculation of the determinant is identical, and for other properties, a mere small book-keeping adjustment is required.\n",
    "  \n",
    "Note that the Vandermonde matrix is well studied, has special fast matrix vector multiplication (i.e. $\\lt O(n^2)$) algorithms associated with it -- and a very special type of Vandermonde matrix is the Discrete Fourier Transform matrix.  The Vandermonde matrix  also has some very interesting properties for thinking about eigenvalues. \n",
    "\n",
    "\n",
    "There is another, more exacting way to verify that $\\mathbf A$ is full rank.  Let's look at the determinant of $\\mathbf A^T$.  There are a few different ways to prove this.  Sergei Winitzki had an interesting proof using wedge products -- that I may revisit at some point in the future.  \n",
    "\n",
    "\n",
    "# Begin Look at Vandermonde Matrices\n",
    "\n",
    "For some real valued Vandermonde matrix $\\mathbf A$, or it's transpose, we can say the following:\n",
    "\n",
    "(note the book-keeping required to evaluate this as a complex matrix, is just a very small alteration)\n",
    "\n",
    "\n",
    "$\\mathbf A^T  = \\begin{bmatrix}\n",
    "1 & 1 & 1 & \\dots & 1 & 1\\\\ \n",
    "a_1 & a_2 & a_3 & \\dots & a_{n-1} & a_n \\\\ \n",
    "a_1^2 & a_2^2 & a_3^2 & \\dots & a_{n-1}^2 & a_{n}^2\\\\ \n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\ \n",
    "a_{1}^{n-2} & a_{2}^{n-2} & a_{3}^{n-2} & \\dots & a_{n-1}^{n-2} & a_{n}^{n-2}\\\\\n",
    "a_{1}^{n-1} & a_{2}^{n-1} & a_{3}^{n-1} & \\dots & a_{n-1}^{n-1} & a_{n}^{n-1}\n",
    "\\end{bmatrix} $\n",
    "\n",
    "For the now,  I'll just notice that there is a rather obvious 'pattern' to these Vandermonde matrices, so we'll do the proof using mathematical induction, which takes advantage of this pattern / progression in polynomial terms.  \n",
    "\n",
    "\n",
    "\n",
    "**claim**: \n",
    "\n",
    "for natural number $n \\geq 2$ where $\\mathbf A \\in \\mathbb R^{n x n}$, and $\\mathbf A$ is a Vandermonde matrix, \n",
    "\n",
    "$det \\big(\\mathbf A \\big) = det \\big(\\mathbf A^T \\big) = \\prod_{1 \\leq i \\lt j \\leq n} (a_j - a_i)$\n",
    "\n",
    "*Base Case:* \n",
    "\n",
    "$n = 2$\n",
    "\n",
    "$\\mathbf A^T = \\begin{bmatrix}\n",
    "1 & 1\\\\ \n",
    "a_1 & a_2\n",
    "\\end{bmatrix}$\n",
    "\n",
    "$det \\big(\\mathbf A^T \\big) = (a_2 - a_1) = \\prod_{1 \\leq i \\lt j \\leq n} (a_j - a_i)$\n",
    "\n",
    "*sneak peak:*  \n",
    "if we follow the row operation procedure used during the inductive case, what we'd have is:\n",
    "\n",
    "$det \\big(\\mathbf A^T \\big) = det\\Big(\\begin{bmatrix}\n",
    "1 & 1\\\\ \n",
    "0 & (a_2 - a_1)\n",
    "\\end{bmatrix}\\Big) = 1*(a_2 - a_1)$\n",
    "\n",
    "\n",
    "*Inductive Case:*\n",
    "\n",
    "For $n \\gt 2$, assume formula is true where $\\mathbf C \\in \\mathbb R^{(n-1) x (n -1)}$\n",
    "\n",
    "i.e. assume true where \n",
    "\n",
    "$\\mathbf C = \\begin{bmatrix}\n",
    "1 & 1 & 1 & \\dots & 1\\\\ \n",
    "a_1 & a_2 & a_3 & \\dots & a_{n-1}\\\\ \n",
    "a_1^2 & a_2^2 & a_3^2 & \\dots & a_{n-1}^2\\\\ \n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots\\\\ \n",
    "a_{1}^{n-2} & a_{2}^{n-2} & a_{3}^{n-2} & \\dots & a_{n-1}^{n-2}\n",
    "\\end{bmatrix}$\n",
    "\n",
    "Note that we call this submatrix $\\mathbf C$ -- it will make a reappearance shortly!\n",
    "\n",
    "\n",
    "We need to show that the formula holds true where dimension of $\\mathbf A$ is $n$ x $n$. Thus consider the case where:\n",
    "\n",
    "$\\mathbf A^T  = \\begin{bmatrix}\n",
    "1 & 1 & 1 & \\dots & 1 & 1\\\\ \n",
    "a_1 & a_2 & a_3 & \\dots & a_{n-1} & a_n \\\\ \n",
    "a_1^2 & a_2^2 & a_3^2 & \\dots & a_{n-1}^2 & a_{n}^2\\\\ \n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\ \n",
    "a_{1}^{n-2} & a_{2}^{n-2} & a_{3}^{n-2} & \\dots & a_{n-1}^{n-2} & a_{n}^{n-2}\\\\\n",
    "a_{1}^{n-1} & a_{2}^{n-1} & a_{3}^{n-1} & \\dots & a_{n-1}^{n-1} & a_{n}^{n-1}\n",
    "\\end{bmatrix} $\n",
    "\n",
    "**Procedure:**\n",
    "subtract $a_1$ times the $i - 1$ row from the ith row, for  $0 \\lt i \\leq n$ **starting from the bottom of the matrix and working our way up** (i.e. the operations / subproblems do not overlap in this regard).  \n",
    "\n",
    "- - - - - \n",
    "**Justification:**\n",
    "\n",
    "First, the reason we'd like to do this is because we see an obvious pattern in the polynomial progression in each column of $\\mathbf A^T$.  Thus by following this procedure, we can zero out all entries in the zeroth column of $\\mathbf A^T$ except, the 1 located in the top left (i.e. in $a_{0,0}$).  This will allow us to, in effect, reduce our problem to the n - 1 x n - 1 dimensional case.  \n",
    "\n",
    "Also recall that the determinant of $\\mathbf A^T$ is equivalent to the determinant of $\\mathbf A$. Thus the above procedure is equivalent to subtracting a scaled version of column 0 of the original $\\mathbf A$ from column 1, and a scaled version of column 1 in the original $\\mathbf A$ from column 2, and so on.  These are standard operations that are well understood to not change the calculated determinant over any legal field. \n",
    "\n",
    "Since, your author particularly likes Gram–Schmidt and orthgonality, there is an additional more visual interpretation that can be used over inner product spaces (i.e. real or complex fields).  Consider that $\\mathbf A = \\mathbf{QR}$, thus $det \\big(\\mathbf A \\big) = det \\big(\\mathbf{QR} \\big) = det \\big(\\mathbf{Q} \\big)det \\big(\\mathbf{R} \\big)$.  Notice that these column operations will have no impact on $\\mathbf Q$, and will only change the value of entries above the diagonal in $\\mathbf R$, thus there is no change in $det \\big(\\mathbf{Q} \\big)$ or $det \\big(\\mathbf{R} \\big)$ (which is given by the product of its diagonal entries).  This means there is no change in $det \\big(\\mathbf{A} \\big)$.  \n",
    "\n",
    "\n",
    "- - - - - \n",
    "\n",
    "$ = \\begin{bmatrix}\n",
    "1 & 1 & 1 & \\dots & 1 & 1\\\\ \n",
    "0 & a_2 - a_1 & a_3 - a_1 & \\dots & a_{n-1} - a_1 & a_n - a_1 \\\\ \n",
    "0 & a_2^2 - a_1 a_2 & a_3^2 - a_1 a_3 & \\dots & a_{n-1}^2 - a_1 a_{n-1} & a_{n}^2 - a_1 a_{n}\\\\ \n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\ \n",
    "0 & a_{2}^{n-2} - a_1 a_{2}^{n-3} & a_{3}^{n-2} - a_1 a_{3}^{n-3} & \\dots & a_{n-1}^{n-2} - a_1 a_{n-1}^{n-3} & a_{n}^{n-2} - a_1 a_{n}^{n-3}\\\\\n",
    "0 & a_{2}^{n-1} - a_1 a_2^{n-2} & a_{3}^{n-1} - a_1 a_3^{n-2}& \\dots & a_{n-1}^{n-1} -  a_1 a_{n-1}^{n-2}& a_{n}^{n-1} - a_1 a_{n}^{n-1}\n",
    "\\end{bmatrix} $\n",
    "\n",
    "$ = \\begin{bmatrix}\n",
    "1 & 1 & 1 & \\dots & 1 & 1\\\\ \n",
    "0 & (a_2 - a_1) 1 & (a_3 - a_1)1 & \\dots & (a_{n-1} - a_1) 1 & (a_n - a_1) 1 \\\\ \n",
    "0 & (a_2 - a_1) a_2 & (a_3 - a_1) a_3 & \\dots & (a_{n-1} - a_1) a_{n-1} & (a_n - a_1) a_{n}\\\\ \n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\ \n",
    "0 & (a_2 - a_1)a_{2}^{n-3} & (a_3 - a_1)a_{3}^{n-3} & \\dots & (a_{n-1} - a_1)a_{n-1}^{n-3} & (a_n - a_1)a_{n}^{n-3}\\\\\n",
    "0 & (a_2 - a_1)a_{2}^{n-2} & (a_3 - a_1)a_{3}^{n-2} & \\dots & (a_{n-1} - a_1)a_{n-1}^{n-2} & (a_n - a_1)a_{n}^{n-2} \n",
    "\\end{bmatrix}  $\n",
    "\n",
    "we can rewrite this as \n",
    "\n",
    "$= \\begin{bmatrix}\n",
    "1 & \\mathbf 1^T\\\\ \n",
    "\\mathbf 0 & \\mathbf{CD}\n",
    "\\end{bmatrix}$\n",
    "\n",
    "\n",
    "where \n",
    "\n",
    "$\\mathbf D = Diag\\Big(\\begin{bmatrix}\n",
    "a_2 & a_3 & a_4 & \\dots & a_n\n",
    "\\end{bmatrix}^T \\Big) - a_1 \\mathbf I =    \\begin{bmatrix}\n",
    "(a_2-a_1) & 0 &  0& \\dots & 0\\\\ \n",
    "0 & (a_3 - a_1) &0  &\\dots  &0 \\\\ \n",
    "0 & 0 & (a_4 - a_1) & \\dots & 0\\\\ \n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\ \n",
    "0 & 0 & 0 & \\dots & (a_n - a_1)\n",
    "\\end{bmatrix}$\n",
    "\n",
    "Note that $\\begin{bmatrix}\n",
    "1 & \\mathbf 1^T\\\\ \\mathbf 0 & \\mathbf{CD} \\end{bmatrix} - \\lambda \\begin{bmatrix}\n",
    "1 & \\mathbf 0^T\\\\ \\mathbf 0 & \\mathbf{I} \\end{bmatrix} = \\begin{bmatrix}\n",
    "1 - \\lambda & \\mathbf 1^T\\\\ \\mathbf 0 & \\mathbf{CD } - \\mathbf \\lambda \\mathbf I \\end{bmatrix}$, which is not invertible when $\\lambda := 1$ (because the left most column is all zeros).  \n",
    "\n",
    "Hence we know that there is an eigenvalue of 1, given by the top left diagonal entry, associated with $\\begin{bmatrix}\n",
    "1 & \\mathbf 1^T\\\\ \n",
    "\\mathbf 0 & \\mathbf{CD}\n",
    "\\end{bmatrix}$. We'll call this $\\lambda_1$ -- for the first eigenvalue of the \"MatrixAfterRowOperations\".  \n",
    "\n",
    "Thus the determinant can be written as \n",
    "\n",
    "$det\\big(\\mathbf A^T \\big) = det\\big(MatrixAfterRowOperations\\big) = (\\lambda_1) * (\\lambda_2  * \\lambda_3 * ... * \\lambda_n\\big) = (1) * \\det\\big(\\mathbf{CD}\\big) = \\det\\big(\\mathbf{C}\\big) \\det\\big(\\mathbf{D}\\big)$\n",
    "\n",
    "\n",
    "\n",
    "- - - - -\n",
    "\n",
    "**begin interlude** \n",
    "\n",
    "The fact that \n",
    "$det\\big(\\begin{bmatrix}\n",
    "1 & \\mathbf *\\\\ \n",
    "\\mathbf 0 & \\mathbf{Z}\n",
    "\\end{bmatrix}\\big) = 1 * det\\big(\\mathbf{Z}\\big)$\n",
    "\n",
    "is well understood via properities of block matrices over many fields.  However, as is often the case, there is an additional interpretation over inner product spaces that makes use of orthogonality.  **This interlude is a bit overkill and may safely be skipped**.\n",
    "\n",
    "Another way to think about this, is we can borrow from the Schur Decomposition $\\mathbf X = \\mathbf V \\mathbf R \\mathbf V^{H}$ where $\\mathbf V$ is unitary and $\\mathbf R$ is upper triangular.  Equivalently, $ \\mathbf V^H \\mathbf X  \\mathbf V = \\mathbf R$.  Also, we know the  eigenvector associated with $\\lambda_1$ (which is $\\begin{bmatrix}1 \\\\ \\mathbf 0 \\\\ \\end{bmatrix}$) can be chosen to be the left most column of $\\mathbf V$.  Since all columns in $\\mathbf V$ are mutually orthonormal, and hence all other columns must have a zero in the upper-most position.  Writing this out, and working through the blocked multiplication we get the following:\n",
    "\n",
    "(note that $^H$ denotes conjugate transpose -- and of course if the values are real, then this acts like a regular transpose operation)\n",
    "\n",
    "$\\mathbf V^{H} \\begin{bmatrix}\n",
    "1 & \\mathbf 1^H\\\\ \n",
    "\\mathbf 0 & \\mathbf{CD}\n",
    "\\end{bmatrix} \\mathbf V = \\mathbf R$\n",
    "\n",
    "$\\mathbf V^{H} \\begin{bmatrix}\n",
    "1 & \\mathbf 1^H\\\\ \n",
    "\\mathbf 0 & \\mathbf{CD}\n",
    "\\end{bmatrix} \\mathbf V = \\begin{bmatrix}1 & \\mathbf 0^H \\\\ \n",
    "\\mathbf 0 & \\mathbf{Q}\n",
    "\\end{bmatrix}^H \\begin{bmatrix}\n",
    "1 & \\mathbf 1^H\\\\ \n",
    "\\mathbf 0 & \\mathbf{CD} \\end{bmatrix} \\begin{bmatrix}\n",
    "1 & \\mathbf 0^H \\\\ \n",
    "\\mathbf 0 & \\mathbf{Q}\n",
    "\\end{bmatrix}= \\begin{bmatrix}1 & \\mathbf 0^H \\\\ \n",
    "\\mathbf 0 & \\mathbf Q^H\n",
    "\\end{bmatrix} \\Big(\\begin{bmatrix}\n",
    "1 & \\mathbf 1^H\\\\ \n",
    "\\mathbf 0 & \\mathbf{CD} \\end{bmatrix} \\begin{bmatrix}\n",
    "1 & \\mathbf 0^H \\\\ \n",
    "\\mathbf 0 & \\mathbf{Q}\n",
    "\\end{bmatrix}\\Big) $\n",
    "\n",
    "\n",
    "$\\mathbf V^{H} \\begin{bmatrix}\n",
    "1 & \\mathbf 1^H\\\\ \n",
    "\\mathbf 0 & \\mathbf{CD}\n",
    "\\end{bmatrix} \\mathbf V =  \\begin{bmatrix} 1 & \\mathbf 0^H \\\\ \n",
    "\\mathbf 0 & \\mathbf Q^H\n",
    "\\end{bmatrix} \\Big(\\begin{bmatrix}1 & \\mathbf 1^H \\mathbf Q \\\\ \n",
    "\\mathbf 0 & \\mathbf{CDQ}\n",
    "\\end{bmatrix}\\Big) = \\begin{bmatrix}1 & \\mathbf 1^H \\mathbf Q \\\\ \n",
    "\\mathbf 0 & \\mathbf{Q}^H\\mathbf{ CDQ}\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "1 & \\mathbf 1^H \\mathbf Q \\\\ \n",
    "\\mathbf 0 & \\mathbf{T}\n",
    "\\end{bmatrix}= \\mathbf R$\n",
    "\n",
    "Thus we know that the determinant we want comes from a similar matrix $\\mathbf R$, who's determinant is the product of its eigenvalues (which are along its diagonal).  We further know that this is equal to $1 * det\\big(\\mathbf T\\big) = 1 *det\\big(\\mathbf Q^H \\mathbf{CDQ}\\big) = det\\big(\\mathbf{Q}^H\\big) det\\big(\\mathbf C\\big)\\det(\\mathbf D\\big) det\\big(\\mathbf Q\\big) = det\\big(\\mathbf C\\big)det\\big(\\mathbf D\\big)$, via the fact that upper triangular matrix $\\mathbf T = \\mathbf Q^H \\mathbf{CDQ}$, then applying multiplicative properties of determinants (and perhaps noticing that $\\mathbf{CD}$ is similar to $\\mathbf T$).  \n",
    "\n",
    "Thus $det\\Big(\\begin{bmatrix}\n",
    "1 & \\mathbf 1^H\\\\ \n",
    "\\mathbf 0 & \\mathbf{CD}\n",
    "\\end{bmatrix}\\Big) = det\\Big(\\begin{bmatrix}\n",
    "1 & \\mathbf *\\\\ \n",
    "\\mathbf 0 & \\mathbf{CD}\n",
    "\\end{bmatrix}\\Big) = det\\big(\\mathbf{CD}\\big) = det\\big(\\mathbf{C}\\big) det\\big(\\mathbf{D}\\big)$\n",
    "\n",
    "**end interlude**\n",
    "- - - - -\n",
    "We know that \n",
    "\n",
    "$\\det\\big(\\mathbf{D}\\big) = (a_2-a_1) * (a_3 - a_1) * ... * (a_n - a_1)$\n",
    "\n",
    "because the determininant of a diagonal matrix is the product of its diagonal entries (i.e. its eigenvalues)  \n",
    "\n",
    "and \n",
    "\n",
    "$det \\big(\\mathbf C \\big) = \\prod_{1 \\leq i \\lt j \\leq n-1} (a_j - a_i)$ \n",
    "\n",
    "by inductive hypothesis.  \n",
    "Thus we can say \n",
    "\n",
    "$ det\\big(\\mathbf A^T \\big) = \\big(\\prod_{1 \\leq i \\lt j \\leq n-1} (a_j - a_i)\\big) \\big((a_2-a_1) * (a_3 - a_1) * ... * (a_n - a_1)\\big) = \\prod_{1 \\leq i \\lt j \\leq n} (a_j - a_i)$\n",
    "\n",
    "And the induction is proved.  \n",
    "\n",
    "Finally, we note that $det \\big(\\mathbf A \\big) = det \\big(\\mathbf A^T \\big)$ because $\\mathbf A$ and $\\mathbf A^T$ have the same characteristic polynomials (or equivalently, they have the same eigenvalues). We have thus proved the determinant formula for $\\mathbf A$.  \n",
    "\n",
    "(Technical note: if $\\mathbf A \\in \\mathbb C^{n x n}$ then the above results still hold with respect to the magnitude of the determinant of $\\mathbf A$.  This includes the very important special case of whether or not $\\big\\vert det\\big(\\mathbf A\\big)\\big\\vert = 0$ --i.e. whether or not $\\mathbf A^{-1}$ exists.  However, with respect to the exact determinant, it would be more proper to state that $det\\big(\\mathbf A\\big) = conjugate\\Big(\\det\\big(\\mathbf A^H\\big)\\Big)$. \n",
    "- - - -\n",
    "\n",
    "This gives us another way to confirm that our Vandermonde Matrix is full rank.  We know that a square, finite dimensional matrix is singular iff it has a determinant of 0.  We then see that \n",
    "\n",
    "$\\det \\big(\\mathbf A\\big) = \\big(\\prod_{1 \\leq i \\lt j \\leq n} (a_j - a_i)\\big) = 0$ iff there is some $a_j = a_i$ where $i \\neq j$.  \n",
    "\n",
    "This of course is another way of saying that our Vandermonde Matrix is not full rank if some entry in our 'original' matrix of \n",
    "\n",
    "$\\mathbf a = \\begin{bmatrix}\n",
    "a_1\\\\ \n",
    "a_2\\\\ \n",
    "a_3\\\\ \n",
    "a_4\\\\ \n",
    "a_5\n",
    "\\end{bmatrix}$\n",
    "\n",
    "was not unique.  \n",
    "\n",
    "\n",
    "- - - -\n",
    "It is worth highlighting that if for some reason we did not like to explicitly use determinants, we could instead just repeatedly, and recursively apply the above procedure as a type of Gaussian Elimination, and in the end we would get have transformed $\\mathbf{A}^T$ into the below Row Echelon form: \n",
    "\n",
    "$\\begin{bmatrix}\n",
    "1 & 1 & 1 &  \\dots & 1 & 1\\\\\n",
    "0 &(a_2-a_1) & 1 &   \\dots & 1 & 1\\\\ \n",
    "0& 0 & (a_3 - a_1)(a_3 - a_2)  &\\dots &1 &1 \\\\ \n",
    "\\vdots &\\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\ \n",
    "0&0 & 0 & \\dots & \\big(\\prod_{1 \\leq i \\lt n-1} (a_{n-1} - a_i)\\big) & 1\\\\ \n",
    "0& 0 & 0 & \\dots & 0 & \\big(\\prod_{1 \\leq i \\lt n} (a_{n} - a_i)\\big)\n",
    "\\end{bmatrix}\\mathbf x = \\mathbf b$\n",
    "\n",
    "(Of course, we can immediately notice that the determinant formula can be recovered by multiplying the diagonal elements of the above matrix.)\n",
    "\n",
    "It is instructive to realize that we can solve for an exact $\\mathbf x$ so long as we don't have any zeros on the diagonal of our above upper triangular /row echelon matrix.  We notice that this is the case only if and only if all $a_i$ are unique.\n",
    "\n",
    "- - - -\n",
    "\n",
    "\n",
    "\n",
    "Furthermore, notice that this determinant formula gives us a proof that we have full column rank in any thinner (i.e. more rows than columns) version of our Vandermonde matrix.  E.g. consider the case of \n",
    "\n",
    "\n",
    "$\\mathbf{A} = \\begin{bmatrix}\n",
    "1 & a_1 & a_1^2 & a_1^3\\\\ \n",
    "1 & a_2 & a_2^2 & a_2^3\\\\ \n",
    "1 & a_3 & a_3^2 & a_3^3\\\\ \n",
    "1 & a_4 & a_4^2 & a_4^3\\\\ \n",
    "1 & a_5 & a_5^2 & a_5^3\n",
    "\\end{bmatrix}$\n",
    "\n",
    "\n",
    "These columns must be linearly independent, so long as each $a_i \\neq a_j$ where $i \\neq j$.  If that was not the case, then appending additional columns until square (i.e. append $\\mathbf a \\circ \\mathbf a \\circ \\mathbf a \\circ \\mathbf a$) would mean that \n",
    "\n",
    "$\\mathbf A = \\begin{bmatrix}\n",
    "1 & a_1 & a_1^2 & a_1^3 & a_1^4\\\\ \n",
    "1 & a_2 & a_2^2 & a_2^3 & a_2^4\\\\ \n",
    "1 & a_3 & a_3^2 & a_3^3 & a_3^4\\\\ \n",
    "1 & a_4 & a_4^2 & a_4^3 & a_4^4\\\\ \n",
    "1 & a_5 & a_5^2 & a_5^3 & a_5^4\n",
    "\\end{bmatrix} $\n",
    "\n",
    "could not have full column rank either.  Yet we know this matrix is full rank via our determinant formula (again so long as each $a_i$ is unique) thus we know that the columns of any smaller  \"long and skinny\" version of this matrix must also be linearly independent.\n",
    "\n",
    "Also, when each $a_i$ is unique, since we know that our Vandermonde matrix is full rank, we know that each of its rows is linearly independent.  If for some reason we had a 'short and fat' version of the above matrix, like:\n",
    "\n",
    "$\\mathbf A = \\begin{bmatrix}\n",
    "1 & a_1 & a_1^2 & a_1^3 & a_1^4\\\\ \n",
    "1 & a_2 & a_2^2 & a_2^3 & a_2^4\\\\ \n",
    "1 & a_3 & a_3^2 & a_3^3 & a_3^4\\\\ \n",
    "\\end{bmatrix} $\n",
    "\n",
    "we would know that it is full row rank -- i.e. each of its rows are linearly independent.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Implication: A degree n-1 polynomial is completely given by n uniqe data points**\n",
    "\n",
    "Assuming there is no noise in the data -- or numeric precision issues-- the Vandermonde matrix, $\\mathbf A$, allows you to solve for the unique values in some polynomial with coefficients of \n",
    "\n",
    "\n",
    "$x_0 * 1 + x_1 a + x_2 a^2 + x_3 a^3 + x_4 a^4 = b$\n",
    "\n",
    "\n",
    "- - - - -\n",
    "$\\mathbf{Ax} = \\begin{bmatrix}\n",
    "1 & a_1 & a_1^2 & a_1^3 & a_1^4\\\\ \n",
    "1 & a_2 & a_2^2 & a_2^3 & a_2^4\\\\ \n",
    "1 & a_3 & a_3^2 & a_3^3 & a_3^4\\\\ \n",
    "1 & a_4 & a_4^2 & a_4^3 & a_4^4\\\\ \n",
    "1 & a_5 & a_5^2 & a_5^3 & a_5^4\n",
    "\\end{bmatrix} \\begin{bmatrix}\n",
    "x_0\\\\\n",
    "x_1\\\\ \n",
    "x_2\\\\\n",
    "x_3\\\\\n",
    "x_4\\\\\n",
    "\\end{bmatrix} = \\mathbf b$\n",
    "- - - - -\n",
    "\n",
    "The next extension is perhaps a bit more interesting.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extension: two ways to think about polynomials** \n",
    "\n",
    "Knowing that we can exactly specify a degree $n-1$ polynomial with $n$ distinct data points leads us to wonder:\n",
    "\n",
    "is it 'better' to think about polynomials with respect to the coefficients or the data points?  In the above vector form -- the question becomes is it better to think about the polynomial in terms of $\\mathbf x$ or $\\mathbf b$? \n",
    "\n",
    "The answer is-- it depends.  To directly evaluate a function is much quicker when we know $\\mathbf x$.  But as it turns out, when we want to multiply or convolve polynomials, it is considerably faster to know their point values contained in $\\mathbf b$.  \n",
    "\n",
    "And since the Vandermonde matrix is so helpful for encapsulating all of our knowledge about a polynomial, a natural question is -- what if we wanted to make multiplying $\\mathbf A^{-1} \\mathbf b$ to get $\\mathbf x$ at least as easy as just multiplying $\\mathbf{Ax}$ to get $\\mathbf b$?  The clear answer would mean finding a way so that you don't have to explicitly invert $\\mathbf A$.  This can be done most easily if $\\mathbf A$ is unitary (i.e. orthogonal albeit in a complex inner product space), hence $\\mathbf A^H = \\mathbf A^{-1}$.  If $\\mathbf A$ is unitary, this directly leads us to the Discrete Fourier Transform.  (And from there to the Fast Fourier Transform which is widely regarded as one of the top 10 algorithms of the last 100 years.)\n",
    "\n",
    "But first, let's work through a couple of important related ideas where we can apply Vandermonde matrices: (a) square matrices that have unique eigenvalues must be diagonalizable and (b) some interesting cyclic and structural properties underlying Permutation matrices. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "*small formatting note*: in bold face LaTeX, the capital A, $\\mathbf A$, looks very similar to the capital Lambda, given by $\\mathbf \\Lambda$, which is a diagonal matrix with eigenvalues $\\lambda_k$, along the diagonal.  The rest of this posting will stop using capital A for an operator, accordingly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Application of Vandermonde Matrices: \n",
    "# Proof of Linear Independence of Eigenvectors associated with Unique Eigenvalues\n",
    "\n",
    "This proves that if a square (finite dimensional) matrix --aka an operator --has all eigenvalues that are unique, then the eigenvectors must be linearly independent.  Put differently, this proves that such an operator is diagonalizable.  \n",
    "\n",
    "The typical argument for linear indepdence is in fact a touch shorter than this and does not need Vandermonde matrices -- however it relies on a contradiction that is not particularly satisfying.  The following proof -- adapted from Winitzki's *Linear Algebra via Exterior Products* is direct -- and to your author-- very intuitive.   \n",
    "\n",
    "Consider $\\mathbf B \\in \\mathbb C^{n x n}$ matrix, which has n unique eigenvalues -- i.e. $\\lambda_1 \\neq \\lambda_2 \\neq ... \\neq \\lambda_n$.  \n",
    "\n",
    "When looking for linear indepenence, \n",
    "\n",
    "$\\gamma_1 \\mathbf v_1 + \\gamma_2 \\mathbf v_2 + ... + \\gamma_n \\mathbf v_n = \\mathbf 0$  \n",
    "\n",
    "we can say that **the eigenvectors are linearly independent iff** $\\gamma_1 = \\gamma_2 = ... = \\gamma_n = 0$\n",
    "\n",
    "Further, for $k = \\{1, 2, ..., n\\}$, we know that  \n",
    "$\\mathbf v_k  = \\mathbf v_k$  \n",
    "$\\mathbf B \\mathbf v_k = \\lambda_k \\mathbf v_k$  \n",
    "$\\mathbf B \\mathbf B \\mathbf v_k = \\mathbf B^2 \\mathbf v_k = \\lambda_k^2 \\mathbf v_k$  \n",
    "$\\vdots $  \n",
    "\n",
    "$\\mathbf B^{n-1} \\mathbf v_k = \\lambda_k^{n-1} \\mathbf v_k$  \n",
    "\n",
    "\n",
    "Thus we can take our original linear independence test,\n",
    "\n",
    "$\\gamma_1 \\mathbf v_1 + \\gamma_2 \\mathbf v_2 + ... + \\gamma_n \\mathbf v_n = \\mathbf 0$  \n",
    "\n",
    "and left multiply by $\\mathbf B^r$ and get the following equalities, as well: \n",
    "\n",
    "$\\mathbf B^r \\mathbf 0 = \\mathbf 0 =  \\mathbf B^r \\big(\\gamma_1 \\mathbf v_1 + \\gamma_2 \\mathbf v_2 + ... + \\gamma_n \\mathbf v_n\\big) =  \\lambda_1^r \\gamma_1 \\mathbf v_1 + \\lambda_2^r  \\gamma_2 \\mathbf v_2 + ... + \\lambda_n^r \\gamma_n \\mathbf v_n $  \n",
    "\n",
    "for $r = \\{1, 2, ..., n-1\\}$\n",
    "- - - -\n",
    "\n",
    "Now let's collect these $n$ relationships in a system of equations:\n",
    "\n",
    "\n",
    "$\\bigg[\\begin{array}{c|c|c|c}\n",
    "\\gamma_1 \\mathbf v_1 & \\gamma_2 \\mathbf v_2 &\\cdots & \\gamma_n \\mathbf v_n\n",
    "\\end{array}\\bigg] \\mathbf W = \\bigg[\\begin{array}{c|c|c|c}\n",
    "  \\mathbf 0 & \\mathbf 0 &\\cdots & \\mathbf 0\n",
    "\\end{array}\\bigg]$\n",
    "\n",
    "\n",
    "where \n",
    "\n",
    "$\\mathbf W = \\begin{bmatrix}\n",
    "1 & \\lambda_1 & \\lambda_1^2 & \\dots  & \\lambda_1^{n-1}\\\\ \n",
    "1 & \\lambda_2 & \\lambda_2^2 & \\dots &  \\lambda_2^{n-1} \\\\ \n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\\\ \n",
    "1 & \\lambda_{n} & \\lambda_{n}^{2} & \\dots  & \\lambda_{n}^{n-1}\n",
    "\\end{bmatrix}$\n",
    "\n",
    "\n",
    "Notice that $\\mathbf W$ is a Vandermonde matrix. Since $\\lambda_i \\neq \\lambda_k$ if $i \\neq k$, we know that $det \\big(\\mathbf W\\big) \\neq 0$, and thus $\\mathbf W^{-1}$ exists as a unique operator.  We multiply each term on the right by $\\mathbf W^{-1}$.  \n",
    "\n",
    "$\\bigg[\\begin{array}{c|c|c|c}\n",
    "\\mathbf \\gamma_1 \\mathbf v_1 & \\gamma_2 \\mathbf v_2 &\\cdots & \\gamma_n \\mathbf v_n\n",
    "\\end{array}\\bigg]\n",
    " \\mathbf W \\mathbf W^{-1}= \\bigg[\\begin{array}{c|c|c|c}\n",
    "\\gamma_1 \\mathbf v_1 & \\gamma_2 \\mathbf v_2 &\\cdots & \\gamma_n \\mathbf v_n\n",
    "\\end{array}\\bigg]\\mathbf I = \\bigg[\\begin{array}{c|c|c|c}\n",
    "  \\mathbf 0 & \\mathbf 0 &\\cdots & \\mathbf 0\n",
    "\\end{array}\\bigg] \\mathbf W^{-1} = \\bigg[\\begin{array}{c|c|c|c}\n",
    "  \\mathbf 0 & \\mathbf 0 &\\cdots & \\mathbf 0\n",
    "\\end{array}\\bigg]$  \n",
    "\n",
    "\n",
    "Thus we know that \n",
    "\n",
    "$\\bigg[\\begin{array}{c|c|c|c}\n",
    "\\mathbf \\gamma_1 \\mathbf v_1 & \\gamma_2 \\mathbf v_2 &\\cdots & \\gamma_n \\mathbf v_n\n",
    "\\end{array}\\bigg] = \\bigg[\\begin{array}{c|c|c|c}\n",
    "  \\mathbf 0 & \\mathbf 0 &\\cdots & \\mathbf 0\n",
    "\\end{array}\\bigg]$\n",
    "\n",
    "By definition each eigenvector $\\mathbf v_k \\neq \\mathbf 0$.  This means that each scalar $\\gamma_k = 0$.  Each eigenvector has thus been proven to be linearly independent in the case where eigenvalues are unique.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Permutation Matrices and Periodic Behavior (introducting DFT)\n",
    "\n",
    "Consider an $n$ x $n$ permutation matrix $\\mathbf P$.  Note that this matrix is real valued where each column has all zeros, and a single 1.  \n",
    "\n",
    "We'll use the $^H$ to denote conjugate transpose (even though the matrix is entirely real valued), as some imaginary numbers will creep in later.\n",
    "\n",
    "\n",
    "It is easy to verify that the permutation matrix is unitary (a special case infact which is orthogonal), i.e. that \n",
    "\n",
    "$\\mathbf P^H \\mathbf P = \\mathbf I$\n",
    "\n",
    "because, by construction, each column in a permutation matrix has all zeros, except a single 1, and the Permutation matrix is full rank -- hence each column must be orthogonal.  \n",
    "\n",
    "Further, as mentioned in \"Schurs_Inequality.ipynb\", such a matrix can be diagonalized where   \n",
    "\n",
    "$\\mathbf P = \\mathbf{V\\Lambda V}^H$\n",
    "\n",
    "where $\\mathbf V$ is unitary, each eigenvalue $\\lambda_i$ is contained along the diagonal of $\\mathbf \\Lambda$ and is on the unit circle.  \n",
    "\n",
    "notice that for any permtuation matrix: \n",
    "\n",
    "$\\mathbf {P1} = \\mathbf 1$\n",
    "\n",
    "Hence such a permutation matrix has $\\lambda_1 = 1$ (i.e. a permutation matrix is stochastic -- in fact doubly so).  \n",
    "\n",
    "Because $\\mathbf P$ has all zeros, except a single 1 in each column (or equivalently, in each row), it can be interpretted as a special kind of Adjacency Matrix for a directed graph. \n",
    "- - - - \n",
    "Of particular interest is **the permutation matrix that relates to a connected graph** (i.e. where each node is reachable from each other node) with $n$ nodes.  One example, where $n = 6$ is the below:\n",
    "\n",
    "<table style=\"background-color: white\"></table><tr><td><table><tr><td><img src='images/permutation_graph_matrix.gif'style=\"width: 100%; background-color: white;\"></td><td><img src='images/permutation_matrix_graph.png'style=\"width: 50%; background-color: white;\" ></td></tr></table>\n",
    "\n",
    "\n",
    "\n",
    "*Claim:*  \n",
    "For a permutation matrix associated with a connected graph (i.e. where each node may be visited from each other node), the time it takes to repeat a visit to a node is $n$ iterations. \n",
    "\n",
    "*Proof:*  \n",
    "since each node in the directed graph has an outbound connection to only one other node, and there are $n$ nodes total, if a cycle can occur in $\\leq n - 1$ iterations, then the number of nodes you can reach from the starting node (including itself) is $\\leq n - 1$ nodes, and hence the graph is not connected -- a contradiction. (And for avoidance of doubt, if it took $\\geq n + 1$ iterations to visit the starting node, then that would mean after $n$ iterations, you've visited (at least)  one of the $n-1$ nodes, other than the starting one, more than once which means there is a cycle in the graph $\\leq n-1$ nodes, which is a contradiction, as outlined above.  This second part in many ways is not needed-- we have many other tools to deal with linear dependence after n iterations.  The point is that this type of graph, by construction, has periodicity equal to its number of nodes -- i.e. all cycles take n iterations.)\n",
    "\n",
    "\n",
    "Thus we can say that $\\mathbf P^ 0 = \\mathbf P^n = \\mathbf I$. So $trace\\big(\\mathbf P^0\\big) = trace\\big(\\mathbf P^n\\big) = trace\\big(\\mathbf I\\big) = n$.  \n",
    "\n",
    "However, the diagonal entries of $\\mathbf P^k$ are all zero for $k \\in \\{1, 2, ..., n-2, n-1\\}$. Thus we have:\n",
    "\n",
    "$trace\\big(\\mathbf P^k\\big) = 0$\n",
    "\n",
    "*note: the reader may wonder why I chose a permutation matrix associated with a connected graph -- this post was supposed to be about Vandermonde matrices! The core reasons are simple -- it is a special type of unitary (or orthogonal) matrx, it has a simple visual representation via graph theory, and its trace is extremely easy to compute.  On top of that there is a messier, related reason -- I was inspired by the n iterations cycle that is implied in the proof of linear independence of eigenvectors associated with unique eigenvalues which used a Vandermonde matrix, and I had recently used a connected graph 3 x 3 permutation matrix (and its eigenvalues) to explain to someone the that 3 complex numbers on the unit circle must be equidistant when there is a constraint they all sum to zero  -- I knew that first eigenvalue had to be one because the matrix is stochastic, I knew the trace was 0, and I knew that for a real valued matrix, complex eigenvalues numbers come in conjugate pairs and hence in the 3 x 3 case they had to be evenly spaced in the unit circle.  In many respects this posting grew out of my attempt to generalize ideas from that conversation, plus a growing interest I had in Vandermonde matrices and matrices used in convolutions. I chose to use permutation matrices associated with a connected graph for those reasons and was pleasantly surprised that I was able to derive the DFT and all of its properties in full, just using this permutation matrix and spectral theory, for an arbitrary $n$ x $n$ dimensional case.* \n",
    "\n",
    "Now consider the standard basis vectors $\\mathbf e_j$, where $j \\in \\{1, 2, ..., n-1, n\\}$ -- i.e. column slices of the identity matrix, shown below:\n",
    "\n",
    "$\\bigg[\\begin{array}{c|c|c|c}\n",
    "\\mathbf e_1 & \\mathbf  e_2 &\\cdots &\\mathbf  e_n\n",
    "\\end{array}\\bigg] = \\mathbf I$\n",
    "\n",
    "Each one of these vectors is a valid starting position for having a 'presence' on exactly one node of the graph. With no loss of generality, we could choose just one the standard basis vectors to be our starting point -- e.g. set $\\mathbf e_j := \\mathbf e_1$.  However, we'll keep the notation $\\mathbf e_j$, though the reader may decide to select a specific standard basis vector if helpful.\n",
    "\n",
    "Since we have a connected graph and can only be on one position at a time as we iterate though, we know that $\\mathbf P^k \\mathbf e_j \\perp \\mathbf P^r \\mathbf e_j$,\n",
    "\n",
    "for natural numbers $r$, $k$, $\\in \\{1, 2, ..., n-2, n-1\\}$, where $r \\neq k$\n",
    "\n",
    "Thus we can collect each location in the graph in an $n$ x $n$ matrix as below:\n",
    "\n",
    "$\\bigg[\\begin{array}{c|c|c|c|c}\n",
    "\\mathbf P^0\\mathbf e_j & \\mathbf P^1\\mathbf e_j & \\mathbf P^2\\mathbf e_j &\\cdots & \\mathbf P^{n-1}\\mathbf e_j\n",
    "\\end{array}\\bigg] = \\bigg[\\begin{array}{c|c|c|c|c}\n",
    "\\mathbf V \\mathbf \\Lambda^0 \\mathbf V^H\\mathbf e_j & \\mathbf V \\mathbf \\Lambda^1 \\mathbf V^H\\mathbf e_j & \\mathbf V \\mathbf \\Lambda^2 \\mathbf V^H\\mathbf e_j &\\cdots & \\mathbf V \\mathbf \\Lambda^{n-1} \\mathbf V^H\\mathbf e_j\n",
    "\\end{array}\\bigg]$\n",
    "\n",
    "\n",
    "\n",
    "$\\bigg[\\begin{array}{c|c|c|c|c}\n",
    "\\mathbf P^0\\mathbf e_j & \\mathbf P^1\\mathbf e_j & \\mathbf P^2\\mathbf e_j &\\cdots & \\mathbf P^{n-1}\\mathbf e_j\n",
    "\\end{array}\\bigg] = \\mathbf V \\bigg[\\begin{array}{c|c|c|c|c}\n",
    "\\mathbf \\Lambda^0 \\mathbf V^H\\mathbf e_j & \\mathbf \\Lambda^1 \\mathbf V^H\\mathbf e_j & \\mathbf \\Lambda^2 \\mathbf V^H\\mathbf e_j &\\cdots & \\mathbf \\Lambda^{n-1} \\mathbf V^H\\mathbf e_j\n",
    "\\end{array}\\bigg]$\n",
    "\n",
    "Now left multiply each side by full rank, unitary matrix $\\mathbf V^H$, and for notational simplity, let $\\mathbf y := \\mathbf V^H\\mathbf e_j$\n",
    "\n",
    "\n",
    "$\\mathbf V^H \\bigg[\\begin{array}{c|c|c|c|c}\n",
    "\\mathbf P^0\\mathbf e_j & \\mathbf P^1\\mathbf e_j & \\mathbf P^2\\mathbf e_j &\\cdots & \\mathbf P^{n-1}\\mathbf e_j\n",
    "\\end{array}\\bigg] = \\bigg[\\begin{array}{c|c|c|c|c}\n",
    "\\mathbf \\Lambda^0 \\mathbf y & \\mathbf \\Lambda^1 \\mathbf y & \\mathbf \\Lambda^2 \\mathbf y &\\cdots & \\mathbf \\Lambda^{n-1} \\mathbf y\n",
    "\\end{array}\\bigg]$\n",
    "\n",
    "For each column vector on the right hand side, we have $\\mathbf \\Lambda^m \\mathbf y$.  In various forms this can be written as \n",
    "\n",
    "$\\mathbf \\Lambda^m \\mathbf y =  \\mathbf \\Lambda^m \\big(\\mathbf{Diag}\\big(\\mathbf y\\big)\\mathbf 1\\big) = \\mathbf{Diag}\\big(\\mathbf y\\big) \\mathbf \\Lambda^m \\mathbf 1 =\n",
    "\\mathbf{Diag}\\big(\\mathbf y\\big)\\big(\\mathbf \\Lambda^m \\mathbf 1\\big)$\n",
    "\n",
    "Thus we can say: \n",
    "\n",
    "$\\bigg[\\begin{array}{c|c|c|c|c}\n",
    "\\mathbf \\Lambda^0 \\mathbf y & \\mathbf \\Lambda^1 \\mathbf y & \\mathbf \\Lambda^2 \\mathbf y &\\cdots & \\mathbf \\Lambda^{n-1} \\mathbf y\n",
    "\\end{array}\\bigg] = \\bigg[\\begin{array}{c|c|c|c|c}\n",
    "\\mathbf{Diag}\\big(\\mathbf y\\big)\\mathbf \\Lambda^0 \\mathbf 1 & \\mathbf{Diag}\\big(\\mathbf y\\big)\\mathbf \\Lambda^1 \\mathbf 1 & \\mathbf{Diag}\\big(\\mathbf y\\big)\\mathbf \\Lambda^2 \\mathbf 1 &\\cdots & \\mathbf{Diag}\\big(\\mathbf y\\big)\\mathbf \\Lambda^{n-1} \\mathbf 1\n",
    "\\end{array}\\bigg] $\n",
    "\n",
    "\n",
    "\n",
    "$\\bigg[\\begin{array}{c|c|c|c|c}\n",
    "\\mathbf \\Lambda^0 \\mathbf y & \\mathbf \\Lambda^1 \\mathbf y & \\mathbf \\Lambda^2 \\mathbf y &\\cdots & \\mathbf \\Lambda^{n-1} \\mathbf y\n",
    "\\end{array}\\bigg] = \\mathbf{Diag}\\big(\\mathbf y\\big)\\bigg[\\begin{array}{c|c|c|c|c}\n",
    "\\mathbf \\Lambda^0 \\mathbf 1 & \\mathbf \\Lambda^1 \\mathbf 1 & \\mathbf \\Lambda^2 \\mathbf 1 &\\cdots & \\mathbf \\Lambda^{n-1} \\mathbf 1\n",
    "\\end{array}\\bigg] $\n",
    "\n",
    "\n",
    "We make this substitution and see: \n",
    "\n",
    "$\\mathbf V^H \\bigg[\\begin{array}{c|c|c|c|c}\n",
    "\\mathbf P^0\\mathbf e_j & \\mathbf P^1\\mathbf e_j & \\mathbf P^2\\mathbf e_j &\\cdots & \\mathbf P^{n-1}\\mathbf e_j\n",
    "\\end{array}\\bigg] = \\mathbf{Diag}\\big(\\mathbf y\\big)\\bigg[\\begin{array}{c|c|c|c|c}\n",
    "\\mathbf \\Lambda^0 \\mathbf 1 & \\mathbf \\Lambda^1 \\mathbf 1 & \\mathbf \\Lambda^2 \\mathbf 1 &\\cdots & \\mathbf \\Lambda^{n-1} \\mathbf 1\n",
    "\\end{array}\\bigg] $\n",
    "\n",
    "From here we may notice that since the left hand side is full rank, the right hand side must be full rank as well. \n",
    "\n",
    "Actually, we know consideraably more than this -- i.e. we know that the left hand side is unitary. \n",
    "\n",
    "where we have $\\mathbf X = f(\\mathbf e_j) = \\bigg[\\begin{array}{c|c|c|c|c}\n",
    "\\mathbf P^0\\mathbf e_j & \\mathbf P^1\\mathbf e_j & \\mathbf P^2\\mathbf e_j &\\cdots & \\mathbf P^{n-1}\\mathbf e_j\n",
    "\\end{array}\\bigg]$\n",
    "\n",
    "In general $\\mathbf X = f(\\mathbf s)$, is called a **circulant matrix**.  For now we confine ourselves to the case where $\\mathbf s := \\mathbf e_j$, though we'll loosen up this restriction at the end of this writeup. \n",
    "\n",
    "earlier we noted that:\n",
    "\n",
    "$\\mathbf P^k \\mathbf e_j \\perp \\mathbf P^r \\mathbf e_j$\n",
    "\n",
    "\n",
    "and of course \n",
    "\n",
    "$\\big \\Vert \\mathbf P^m \\mathbf e_j\\big \\Vert_2^2 = \\big(\\mathbf P^m \\mathbf e_j\\big)^H\\big(\\mathbf P^m \\mathbf e_j\\big) = \\mathbf e_j^H \\big(\\mathbf P^m\\big)^H  \\mathbf P^m \\mathbf e_j = \\mathbf e_j^H \\mathbf I \\mathbf e_j = \\mathbf e_j^H \\mathbf e_j = 1$\n",
    "\n",
    "Thus each column in $\\mathbf X$ is mutually orthonormal -- and $\\mathbf U$ is $n$ x $n$ so it is a (real valued) unitary matrix. \n",
    "\n",
    "--\n",
    "\n",
    "From here we see\n",
    "\n",
    "$\\big(\\mathbf V^H \\mathbf X\\big)^H \\mathbf V^H \\mathbf X = \\mathbf X^H \\big(\\mathbf V \\mathbf V^H\\big) \\mathbf X = \\mathbf X^H \\mathbf X = \\mathbf I$\n",
    "\n",
    "So we know that the left hand side in unitary.  This means that the right handside must be unitary as well. \n",
    "\n",
    "Since the right hand side is unitary, that means it must be non-singular.  \n",
    "\n",
    "Note that with respect to determinants, we could say:\n",
    "\n",
    "$ \\Big \\vert Det\\Big(\\mathbf{Diag} \\big(\\mathbf y\\big)\\Big)\\Big \\vert*\\Big \\vert Det\\Big( \\bigg[\\begin{array}{c|c|c|c|c}\n",
    "\\mathbf \\Lambda^0 \\mathbf 1 & \\mathbf \\Lambda^1 \\mathbf 1 & \\mathbf \\Lambda^2 \\mathbf 1 &\\cdots & \\mathbf \\Lambda^{n-1} \\mathbf 1\n",
    "\\end{array}\\bigg]\\Big) \\Big \\vert = 1$\n",
    "\n",
    "Thus \n",
    "\n",
    "$Det\\Big( \\bigg[\\begin{array}{c|c|c|c|c}\n",
    "\\mathbf \\Lambda^0 \\mathbf 1 & \\mathbf \\Lambda^1 \\mathbf 1 & \\mathbf \\Lambda^2 \\mathbf 1 &\\cdots & \\mathbf \\Lambda^{n-1} \\mathbf 1\n",
    "\\end{array}\\bigg]\\Big) \\neq 0$\n",
    "\n",
    "Finally, we 'unpack' this matrix and see that\n",
    "\n",
    "$\\bigg[\\begin{array}{c|c|c|c|c}\n",
    "\\mathbf \\Lambda^0 \\mathbf 1 & \\mathbf \\Lambda^1 \\mathbf 1 & \\mathbf \\Lambda^2 \\mathbf 1 &\\cdots & \\mathbf \\Lambda^{n-1} \\mathbf 1\n",
    "\\end{array}\\bigg]=\\begin{bmatrix}\n",
    "1 & \\lambda_1 & \\lambda_1^2 & \\dots  & \\lambda_1^{n-1}\\\\ \n",
    "1 & \\lambda_2 & \\lambda_2^2 & \\dots &  \\lambda_2^{n-1} \\\\ \n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\\\ \n",
    "1 & \\lambda_{n} & \\lambda_{n}^{2} & \\dots  & \\lambda_{n}^{n-1}\n",
    "\\end{bmatrix}$\n",
    "\n",
    "This is the Vandermonde matrix, which is non-singular **iff**  each $\\lambda_i$ is unique.  Thus we conclude that each $\\lambda_i$ for our Permutation matrix of a connected graph must be unique.\n",
    "\n",
    "**claim:**\n",
    "\n",
    "$\\bigg[\\begin{array}{c|c|c|c|c}\n",
    "\\mathbf \\Lambda^0 \\mathbf 1 & \\mathbf \\Lambda^1 \\mathbf 1 & \\mathbf \\Lambda^2 \\mathbf 1 &\\cdots & \\mathbf \\Lambda^{n-1} \\mathbf 1\n",
    "\\end{array}\\bigg]^H \\bigg[\\begin{array}{c|c|c|c|c}\n",
    "\\mathbf \\Lambda^0 \\mathbf 1 & \\mathbf \\Lambda^1 \\mathbf 1 & \\mathbf \\Lambda^2 \\mathbf 1 &\\cdots & \\mathbf \\Lambda^{n-1} \\mathbf 1\n",
    "\\end{array}\\bigg] = n \\mathbf I$\n",
    "\n",
    "That is, the columns in the above matrix are mutually orthogonal (aka have an inner product of zero), and subject to some normalizing scalar constant, we know that the matrix is unitary. \n",
    "\n",
    "**proof:**  \n",
    "First notice that each column has a squared L2 norm of $n$\n",
    "\n",
    "for $m = \\{0, 1, 2, ..., n-1\\}$\n",
    "\n",
    "$\\big(\\mathbf \\Lambda^m \\mathbf 1\\big)^H \\mathbf \\Lambda^m \\mathbf 1 = \\mathbf 1^H \\big(\\mathbf \\Lambda^m\\big)^H \\mathbf \\Lambda^m \\mathbf 1 = \\mathbf 1^H \\big(\\mathbf \\Lambda^m\\big)^{-1} \\mathbf \\Lambda^m \\mathbf 1 = \\mathbf 1^H \\big(\\mathbf I\\big) \\mathbf 1 = trace \\big(\\mathbf I\\big)  = n$ \n",
    "\n",
    "note that when we say $\\mathbf 1^H \\big(\\mathbf I\\big) \\mathbf 1 = trace \\big(\\mathbf I\\big)$, we notice first that $\\mathbf 1^H \\big(\\mathbf Z\\big) \\mathbf 1$, means to sum up all entries in some operator $\\mathbf Z$, and if $\\mathbf Z$ is a diagonal matrix, then this is equivalent to just summing the entries along the diagonal of $\\mathbf Z$ which is equal to the trace of $\\mathbf Z$.\n",
    "\n",
    "Next we want to prove that the inner product of any column $j$ with some other column $\\neq j$, is zero.\n",
    "\n",
    "Thus we are interested in the cases of\n",
    "\n",
    "$\\big(\\mathbf \\Lambda^r \\mathbf 1\\big)^H \\mathbf \\Lambda^k \\mathbf 1$ \n",
    "\n",
    "for all natural numbers $k$, $r$, *first* where $0\\leq r \\lt k \\leq n-1$ and *second* where $0\\leq  k \\lt r \\leq n-1$.\n",
    "\n",
    "First we observe the $r \\lt k$ case:\n",
    "\n",
    "$\\big(\\mathbf \\Lambda^r \\mathbf 1\\big)^H \\mathbf \\Lambda^k \\mathbf 1 = \\mathbf 1^H \\big(\\mathbf \\Lambda^r\\big)^H \\mathbf \\Lambda^{k} \\mathbf 1 = \\mathbf 1^H \\Big(\\big( \\mathbf \\Lambda^{-r} \\big) \\mathbf \\Lambda^{k}\\Big) \\mathbf 1 = \\mathbf 1^H \\Big( \\mathbf \\Lambda^{k - r}\\Big) \\mathbf 1 = trace\\big(\\mathbf \\Lambda^{k - r}\\big) $ \n",
    "\n",
    "since $k \\gt r$, we know $0 \\lt k - r \\leq n-1$.  Put differently $(k - r) \\%n \\neq 0$\n",
    "\n",
    "Thus $\\big(\\mathbf \\Lambda^r \\mathbf 1\\big)^H \\mathbf \\Lambda^k \\mathbf 1 = trace\\big(\\mathbf \\Lambda^{k - r}\\big) = trace\\big(\\mathbf Q^H \\mathbf P^{k - r} \\mathbf Q\\big) = trace\\big(\\mathbf Q \\mathbf Q^H \\mathbf P^{k - r}\\big) = trace\\big(\\mathbf P^{k - r}\\big) = 0$\n",
    "\n",
    "note that inner products are symmetric -- except for complex conjugation-- so in the case of an inner product equal to zero, we have \n",
    "\n",
    "$\\Big(\\big(\\mathbf \\Lambda^r \\mathbf 1\\big)^H \\mathbf \\Lambda^k \\mathbf 1\\Big)^H = trace\\big(\\mathbf \\Lambda^{k - r}\\big)^H  = 0^H = 0$\n",
    "\n",
    "which covers the second case.\n",
    "\n",
    "\n",
    "Thus all columns in\n",
    "\n",
    "$\\bigg[\\begin{array}{c|c|c|c|c}\n",
    "\\mathbf \\Lambda^0 \\mathbf 1 & \\mathbf \\Lambda^1 \\mathbf 1 & \\mathbf \\Lambda^2 \\mathbf 1 &\\cdots & \\mathbf \\Lambda^{n-1} \\mathbf 1\n",
    "\\end{array}\\bigg]$\n",
    "\n",
    "have a squared length of $n$ and are mutually orthgonal.\n",
    "\n",
    "Hence we can say:\n",
    "\n",
    "$\\frac{1}{\\sqrt n} \\bigg[\\begin{array}{c|c|c|c|c}\n",
    "\\mathbf \\Lambda^0 \\mathbf 1 & \\mathbf \\Lambda^1 \\mathbf 1 & \\mathbf \\Lambda^2 \\mathbf 1 &\\cdots & \\mathbf \\Lambda^{n-1} \\mathbf 1\n",
    "\\end{array}\\bigg] = \\mathbf F$\n",
    "\n",
    "is a unitary matrix.  In fact this matrix $\\mathbf F$ is the discrete Fourier transform matrix.  \n",
    "\n",
    "*note: in some cases, we may use the the conjugate transpose of this matrix, or another variant, as the DFT.  This is ultimately just a book-keeping adjustment*\n",
    "\n",
    "\n",
    "\n",
    "# Claim: \n",
    "# The DFT Matrix is the collection of eigenvectors for a circulant matrix\n",
    "\n",
    "We say that this circulant matrix is given by $\\mathbf X$  \n",
    "\n",
    "\n",
    "When we look at \n",
    "\n",
    "$\\mathbf V^H \\bigg[\\begin{array}{c|c|c|c|c}\n",
    "\\mathbf P^0\\mathbf e_j & \\mathbf P^1\\mathbf e_j & \\mathbf P^2\\mathbf e_j &\\cdots & \\mathbf P^{n-1}\\mathbf e_j\n",
    "\\end{array}\\bigg] = \\mathbf{Diag}\\big(\\mathbf y\\big) \\bigg[\\begin{array}{c|c|c|c|c}\n",
    "\\mathbf \\Lambda^0 \\mathbf 1 & \\mathbf \\Lambda^1 \\mathbf 1 & \\mathbf \\Lambda^2 \\mathbf 1 &\\cdots & \\mathbf \\Lambda^{n-1} \\mathbf 1\n",
    "\\end{array}\\bigg]$\n",
    "\n",
    "we can re-write this as\n",
    "\n",
    "$\\mathbf V^H \\bigg[\\begin{array}{c|c|c|c|c}\n",
    "\\mathbf P^0\\mathbf e_j & \\mathbf P^1\\mathbf e_j & \\mathbf P^2\\mathbf e_j &\\cdots & \\mathbf P^{n-1}\\mathbf e_j\n",
    "\\end{array}\\bigg] = \\sqrt(n)\\mathbf{Diag}\\big(\\mathbf y\\big) \\frac{1}{\\sqrt(n)}\\bigg[\\begin{array}{c|c|c|c|c}\n",
    "\\mathbf \\Lambda^0 \\mathbf 1 & \\mathbf \\Lambda^1 \\mathbf 1 & \\mathbf \\Lambda^2 \\mathbf 1 &\\cdots & \\mathbf \\Lambda^{n-1} \\mathbf 1\n",
    "\\end{array}\\bigg] = \\sqrt(n) \\mathbf{Diag}\\big(\\mathbf y\\big) \\mathbf F$\n",
    "\n",
    "we can recongize that this is a form of the singular value decompostion on our matrix $\\mathbf X$ (so long as we relax the constraint that the diagonal matrix is real-valued, non-negative).  That is, we have \n",
    "\n",
    "$\\mathbf X = \\bigg[\\begin{array}{c|c|c|c|c}\n",
    "\\mathbf P^0\\mathbf e_j & \\mathbf P^1\\mathbf e_j & \\mathbf P^2\\mathbf e_j &\\cdots & \\mathbf P^{n-1}\\mathbf e_j\n",
    "\\end{array}\\bigg] = \\mathbf V \\Big(\\sqrt(n) \\mathbf{Diag} \\big(\\mathbf y\\big)\\Big)\\mathbf F$\n",
    "\n",
    "In the above case, $\\mathbf X$ is decomposed into unitary matrix $\\mathbf V$ times a diagonal matrix times unitary matrix $\\mathbf F$.  \n",
    "\n",
    "\n",
    "In this case, $\\mathbf X$ is itself unitary and per the note in \"Schurs_Inequality.ipynb\", that means that $\\mathbf X$ is normal.  Since $\\mathbf X$ is normal, this means that our Singular Value Decomposition in fact gives us an eigenvalue decomposition.  Put differently we can set our left singular and right singular vectors to be equal, and allocate everything else to the middle diagonal matrix.  \n",
    "\n",
    "$\\mathbf V := \\mathbf F^H $\n",
    "\n",
    "$\\mathbf X = \\mathbf F^H \\mathbf D_j \\mathbf F$\n",
    "\n",
    "or equivalently\n",
    "\n",
    "$\\mathbf F \\mathbf X \\mathbf F^H =  \\mathbf D_j $\n",
    "\n",
    "Thus in the above we can say $\\mathbf X$ is unitarily similar to a diagonal matrix $\\mathbf D_j$ with $\\mathbf F$ containing the eigenvectors.  \n",
    "\n",
    "Which is another way of saying that our unitary Vandermonde matrix $\\mathbf F$ **contains the mutually orthonormal collection of eigenvectors for** $\\mathbf X$.  \n",
    "\n",
    "This immediately motivates the question-- what if $\\mathbf X$ was a function of permuting some different, arbitrary vector $\\mathbf s$ i.e. if $\\mathbf X = f(\\mathbf s)$ -- could we still say $\\mathbf X$ is unitarily similar to a diagonal matrix with $\\mathbf F$ containing the eigenvectors?  The answer is yes, though it takes a little bit more work to show it.\n",
    "\n",
    "\n",
    "\n",
    "# Short form proof\n",
    "\n",
    "for a quick take, consider that \n",
    "\n",
    "$\\mathbf F \\big(f(\\mathbf e_j)\\big) \\mathbf F^H = \\mathbf D_j$\n",
    "\n",
    "where $\\mathbf D_j$ denotes some diagonal matrix similar to our function applied on the jth standard basis vector.\n",
    "\n",
    "The standard basis vectors form a basis so we can write $\\mathbf s$ in terms of them:  \n",
    "$\\mathbf s = \\gamma_1 \\mathbf e_1 + \\gamma_2 \\mathbf e_2 + ... + \\gamma_n \\mathbf e_n  = \\big(\\Sigma_{j=1}^{n} \\gamma_j \\mathbf e_j\\big)$\n",
    "\n",
    "Then, using linearity we can say \n",
    "\n",
    "$\\mathbf X = f(\\mathbf s) = f(\\gamma_1 \\mathbf e_1 + \\gamma_2 \\mathbf e_2 + ... + \\gamma_n \\mathbf e_n) = f(\\gamma_1 \\mathbf e_1) + f(\\gamma_2 \\mathbf e_2) + ... + (\\gamma_n \\mathbf e_n)$\n",
    "\n",
    "left multiply each side by $\\mathbf F$ and right multiply each side by $\\mathbf F^H$, and we get\n",
    "\n",
    "$\\mathbf F \\big(\\mathbf X\\big) \\mathbf F^H = \\mathbf F \\big(f(\\mathbf s)\\big) \\mathbf F^H= \\mathbf F \\big( f(\\gamma_1 \\mathbf e_1)\\big)\\mathbf F^H + \\mathbf F\\big(f(\\gamma_2 \\mathbf e_2)\\big)\\mathbf F^H + ... + \\mathbf F\\big((\\gamma_n \\mathbf e_n)\\big)\\mathbf F^H = \\gamma_1 \\mathbf D_1 + \\gamma_2 \\mathbf D_2 + ... + \\gamma_n \\mathbf D_n$\n",
    "\n",
    "The sum of a sequence of diagonal matrices is a diagonal matrix, hence we can can say that using $\\mathbf F$, we find that $\\mathbf X$ is unitarily similar to a diagonal matrix. \n",
    "\n",
    "\n",
    "# Begin Long Form Proof\n",
    "\n",
    "To begin, notice that by linearity\n",
    "\n",
    "$f(\\mathbf e_1) + f(\\mathbf e_2) = f(\\mathbf e_1 + \\mathbf e_2)$  \n",
    "\n",
    "Written in terms of a matrix, this is:  \n",
    "\n",
    "$\\bigg[\\begin{array}{c|c|c|c}\n",
    "\\mathbf P^0\\mathbf e_1 & \\mathbf P^1\\mathbf e_1 & \\cdots & \\mathbf P^{n-1}\\mathbf e_1\n",
    "\\end{array}\\bigg] + \\bigg[\\begin{array}{c|c|c|c}\n",
    "\\mathbf P^0\\mathbf e_2 & \\mathbf P^1\\mathbf e_2 & \\cdots & \\mathbf P^{n-1}\\mathbf e_2\n",
    "\\end{array}\\bigg] = \\bigg[\\begin{array}{c|c|c|c}\n",
    "\\mathbf P^0 \\mathbf e_1 +  \\mathbf P^0 \\mathbf e_2 & \\mathbf P^1 \\mathbf e_1 + \\mathbf P^1 \\mathbf e_2 & \\cdots & \\mathbf P^{n-1} \\mathbf e_1 + \\mathbf P^{n-1} \\mathbf e_2 \n",
    "\\end{array}\\bigg]$\n",
    "\n",
    "which we can restate as \n",
    "\n",
    "$\\bigg[\\begin{array}{c|c|c|c}\n",
    "\\mathbf P^0\\mathbf e_1 & \\mathbf P^1\\mathbf e_1 & \\cdots & \\mathbf P^{n-1}\\mathbf e_1\n",
    "\\end{array}\\bigg] + \\bigg[\\begin{array}{c|c|c|c}\n",
    "\\mathbf P^0\\mathbf e_2 & \\mathbf P^1\\mathbf e_2 & \\cdots & \\mathbf P^{n-1}\\mathbf e_2\n",
    "\\end{array}\\bigg] = \\bigg[\\begin{array}{c|c|c|c}\n",
    "\\mathbf P^0\\big(\\mathbf e_1 +  \\mathbf e_2\\big) & \\mathbf P^1 \\big(\\mathbf e_1 + \\mathbf e_2\\big) & \\cdots & \\mathbf P^{n-1}\\big(\\mathbf e_1 + \\mathbf e_2 \\big)\n",
    "\\end{array}\\bigg]$\n",
    "\n",
    "\n",
    "If we left multiply by $\\mathbf F$, what we get is\n",
    "\n",
    "\n",
    "$\\mathbf F \\bigg[\\begin{array}{c|c|c|c}\n",
    "\\mathbf P^0\\mathbf e_1 & \\mathbf P^1\\mathbf e_1 & \\cdots & \\mathbf P^{n-1}\\mathbf e_1\n",
    "\\end{array}\\bigg] + \\mathbf F \\bigg[\\begin{array}{c|c|c|c}\n",
    "\\mathbf P^0\\mathbf e_2 & \\mathbf P^1\\mathbf e_2 &\\cdots & \\mathbf P^{n-1}\\mathbf e_2\n",
    "\\end{array}\\bigg]= \\mathbf D_1\\mathbf F + \\mathbf D_2\\mathbf F$\n",
    "\n",
    "$= \\big(\\mathbf D_1 + \\mathbf D_2\\big) \\mathbf F = \\mathbf F \\bigg[\\begin{array}{c|c|c|c}\n",
    "\\mathbf P^0\\big(\\mathbf e_1 +  \\mathbf e_2\\big) & \\mathbf P^1 \\big(\\mathbf e_1 + \\mathbf e_2\\big) &\\cdots & \\mathbf P^{n-1}\\big(\\mathbf e_1 + \\mathbf e_2 \\big)\n",
    "\\end{array}\\bigg]$\n",
    "\n",
    "\n",
    "and to further generalize this, notice that if we added all of the standard basis vectors, we'd get the ones vector.    Where $\\mathbf D_j$ is a diagonal matrix similar to the permutation matrix given by $f(\\mathbf e_j)$.\n",
    "We can write this as:  \n",
    "\n",
    "$\\mathbf {11}^H = \\Sigma_{j=1}^{n}\\Big(\\bigg[\\begin{array}{c|c|c|c}\n",
    "\\mathbf P^0\\mathbf e_j & \\mathbf P^1\\mathbf e_j & \\cdots & \\mathbf P^{n-1}\\mathbf e_j\n",
    "\\end{array}\\bigg]\\Big) = \\bigg[\\begin{array}{c|c|c|c}\n",
    "\\mathbf P^0\\big(\\Sigma_{j=1}^{n} \\mathbf e_j \\big) & \\mathbf P^1 \\big(\\Sigma_{j=1}^{n} \\mathbf e_j \\big) & \\cdots & \\mathbf P^{n-1}\\big(\\Sigma_{j=1}^{n} \\mathbf e_j \\big)\n",
    "\\end{array}\\bigg]$  \n",
    "\n",
    "and if we left multiply by $\\mathbf F$, we get\n",
    "\n",
    "$\\mathbf F \\big(\\mathbf {11}^H \\big) = \\Sigma_{j=1}^{n}\\Big(\\mathbf F\\bigg[\\begin{array}{c|c|c|c}\n",
    "\\mathbf P^0\\mathbf e_j & \\mathbf P^1\\mathbf e_j & \\cdots & \\mathbf P^{n-1}\\mathbf e_j\n",
    "\\end{array}\\bigg]\\Big) = \\Sigma_{j=1}^{n} \\big(\\mathbf D_j \\mathbf F\\big) = \\big(\\Sigma_{j=1}^{n}\\mathbf D_j\\big) \\mathbf F$  \n",
    "\n",
    "From here, consider what would happen if we instead decided to scale each standard basis vector, $\\mathbf e_j$, by some arbitrary amount, $\\gamma_j$, giving us the following expression:  \n",
    "\n",
    "$\\Sigma_{j=1}^{n}\\Big(\\gamma_j \\bigg[\\begin{array}{c|c|c|c}\n",
    "\\mathbf P^0\\mathbf e_j & \\mathbf P^1\\mathbf e_j & \\cdots & \\mathbf P^{n-1} \\mathbf e_j\n",
    "\\end{array}\\bigg]\\Big) = \\Sigma_{j=1}^{n}\\Big(\\bigg[\\begin{array}{c|c|c|c}\n",
    "\\mathbf P^0\\big(\\gamma_j \\mathbf e_j\\big) & \\mathbf P^1\\big(\\gamma_j \\mathbf e_j\\big) & \\cdots & \\mathbf P^{n-1} \\big(\\gamma_j  \\mathbf e_j\\big)\n",
    "\\end{array}\\bigg]\\Big)$\n",
    "\n",
    "which can be restated as  \n",
    "\n",
    "$\\Sigma_{j=1}^{n}\\Big(\\gamma_j \\bigg[\\begin{array}{c|c|c|c}\n",
    "\\mathbf P^0\\mathbf e_j & \\mathbf P^1\\mathbf e_j & \\cdots & \\mathbf P^{n-1} \\mathbf e_j\n",
    "\\end{array}\\bigg]\\Big)  = \\bigg[\\begin{array}{c|c|c|c|c}\n",
    "\\mathbf P^0\\big(\\Sigma_{j=1}^{n} \\gamma_j \\mathbf e_j \\big) & \\mathbf P^1 \\big(\\Sigma_{j=1}^{n}\\gamma_j \\mathbf e_j \\big) & \\cdots & \\mathbf P^{n-1}\\big(\\Sigma_{j=1}^{n} \\gamma_j \\mathbf e_j \\big)\n",
    "\\end{array}\\bigg]$ \n",
    "\n",
    "again, left multiply this expression by $\\mathbf F$ and we see\n",
    "\n",
    "$\\mathbf F\\bigg[\\begin{array}{c|c|c|c|c}\n",
    "\\mathbf P^0\\big(\\Sigma_{j=1}^{n} \\gamma_j \\mathbf e_j \\big) & \\mathbf P^1 \\big(\\Sigma_{j=1}^{n}\\gamma_j \\mathbf e_j \\big) & \\cdots & \\mathbf P^{n-1}\\big(\\Sigma_{j=1}^{n} \\gamma_j \\mathbf e_j \\big)\n",
    "\\end{array}\\bigg] = \\Sigma_{j=1}^{n}\\Big(\\gamma_j \\mathbf F\\bigg[\\begin{array}{c|c|c|c}\n",
    "\\mathbf P^0\\mathbf e_j & \\mathbf P^1\\mathbf e_j & \\cdots & \\mathbf P^{n-1} \\mathbf e_j\n",
    "\\end{array}\\bigg]\\Big)$\n",
    "\n",
    "from here notice  \n",
    "\n",
    "$ \\Sigma_{j=1}^{n}\\Big(\\gamma_j \\mathbf F\\bigg[\\begin{array}{c|c|c|c}\n",
    "\\mathbf P^0\\mathbf e_j & \\mathbf P^1\\mathbf e_j & \\cdots & \\mathbf P^{n-1} \\mathbf e_j\n",
    "\\end{array}\\bigg]\\Big) = \\Sigma_{j=1}^{n}\\gamma_j\\Big( \\mathbf F\\bigg[\\begin{array}{c|c|c|c}\n",
    "\\mathbf P^0\\mathbf e_j & \\mathbf P^1\\mathbf e_j & \\cdots & \\mathbf P^{n-1} \\mathbf e_j\n",
    "\\end{array}\\bigg]\\Big) = \\Sigma_{j=1}^{n} \\mathbf \\gamma_j \\big(\\mathbf D_j \\mathbf F\\big) = \\big(\\Sigma_{j=1}^{n}\\mathbf \\gamma_j \\mathbf D_j\\big) \\mathbf F$\n",
    "\n",
    "Thus we say\n",
    "\n",
    "$\\mathbf F\\bigg[\\begin{array}{c|c|c|c|c}\n",
    "\\mathbf P^0\\big(\\Sigma_{j=1}^{n} \\gamma_j \\mathbf e_j \\big) & \\mathbf P^1 \\big(\\Sigma_{j=1}^{n}\\gamma_j \\mathbf e_j \\big) & \\cdots & \\mathbf P^{n-1}\\big(\\Sigma_{j=1}^{n} \\gamma_j \\mathbf e_j \\big)\n",
    "\\end{array}\\bigg] = \\big(\\Sigma_{j=1}^{n}\\mathbf \\gamma_j \\mathbf D_j\\big) \\mathbf F$\n",
    "\n",
    "\n",
    "Right multiply each side by $\\mathbf F^H$:  \n",
    "\n",
    "$\\mathbf F\\bigg[\\begin{array}{c|c|c|c|c}\n",
    "\\mathbf P^0\\big(\\Sigma_{j=1}^{n} \\gamma_j \\mathbf e_j \\big) & \\mathbf P^1 \\big(\\Sigma_{j=1}^{n}\\gamma_j \\mathbf e_j \\big) & \\cdots & \\mathbf P^{n-1}\\big(\\Sigma_{j=1}^{n} \\gamma_j \\mathbf e_j \\big)\n",
    "\\end{array}\\bigg] \\mathbf F^H = \\big(\\Sigma_{j=1}^{n}\\mathbf \\gamma_j \\mathbf D_j\\big) \\mathbf F \\mathbf F^H  = \\big(\\Sigma_{j=1}^{n}\\mathbf \\gamma_j \\mathbf D_j\\big) $\n",
    "\n",
    "Since the sum of a finite sequence of $n$ x $n$ diagonal matrices is itself a diagonal matrix, this tells us that our matrix is unitarily similar to a diagonal matrix, and the mutually orthonormal eigenvectors are contained in $\\mathbf F$ (or technically, the right eigenvectors are contained as columns in $\\mathbf F^H$ -- which again, is just a small bookkeeping adjustment).  \n",
    "\n",
    "Now consider the general case where $\\mathbf X = f(\\mathbf s)$.  This looks quite formidable -- the circulant matrix is given by: \n",
    "\n",
    "$\\mathbf X = \\bigg[\\begin{array}{c|c|c|c}\n",
    "\\mathbf P^0\\mathbf s & \\mathbf P^1\\mathbf s & \\cdots & \\mathbf P^{n-1}\\mathbf s\n",
    "\\end{array}\\bigg]  = \\begin{bmatrix}\n",
    "s_0 & s_{n-1} & s_{n-2} & \\dots & s_2 & s_1 \\\\ \n",
    "s_1 & s_0 & s_{n-1} & \\dots & s_3 & s_2 \\\\ \n",
    "s_2 & s_1 & s_0 & \\dots & s_4 & s_3 \\\\\n",
    "\\vdots & \\vdots  & \\vdots &\\ddots & \\vdots & \\vdots\\\\ \n",
    "s_{n-2} & s_{n-3} & s_{n-4} & \\dots & s_0  & s_{n-1} \\\\ \n",
    "s_{n-1} & s_{n-2}  & s_{n-3} & \\dots & s_1 &  s_0\n",
    "\\end{bmatrix}$\n",
    "\n",
    "But, we simply need to recall that the standard basis vectors in fact form a basis, so we can uniquely write $\\mathbf s$ in terms of them. \n",
    "\n",
    "$\\mathbf s = \\gamma_1 \\mathbf e_1 + \\gamma_2 \\mathbf e_2 + ... + \\gamma_n \\mathbf e_n  = \\big(\\Sigma_{j=1}^{n} \\gamma_j \\mathbf e_j\\big)$\n",
    "\n",
    "Thus we have \n",
    "\n",
    "\n",
    "$\\mathbf X = \\bigg[\\begin{array}{c|c|c|c}\n",
    "\\mathbf P^0\\mathbf s & \\mathbf P^1\\mathbf s & \\cdots & \\mathbf P^{n-1}\\mathbf s\n",
    "\\end{array}\\bigg] = \\bigg[\\begin{array}{c|c|c|c|c}\n",
    "\\mathbf P^0\\big(\\Sigma_{j=1}^{n} \\gamma_j \\mathbf e_j \\big) & \\mathbf P^1 \\big(\\Sigma_{j=1}^{n}\\gamma_j \\mathbf e_j \\big) & \\cdots & \\mathbf P^{n-1}\\big(\\Sigma_{j=1}^{n} \\gamma_j \\mathbf e_j \\big)\n",
    "\\end{array}\\bigg]$\n",
    "\n",
    "left multiply each side by $\\mathbf F$ and right multiply each side by $\\mathbf F^H$, and we get \n",
    "\n",
    "$\\mathbf F \\big(\\mathbf X\\big) \\mathbf F^H = \\mathbf F \\bigg[\\begin{array}{c|c|c|c|c}\n",
    "\\mathbf P^0\\big(\\Sigma_{j=1}^{n} \\gamma_j \\mathbf e_j \\big) & \\mathbf P^1 \\big(\\Sigma_{j=1}^{n}\\gamma_j \\mathbf e_j \\big) & \\cdots & \\mathbf P^{n-1}\\big(\\Sigma_{j=1}^{n} \\gamma_j \\mathbf e_j \\big)\n",
    "\\end{array}\\bigg] \\mathbf F^H = \\big(\\Sigma_{j=1}^{n}\\mathbf \\gamma_j \\mathbf D_j\\big)$\n",
    "\n",
    "\n",
    "which states that $\\mathbf X = f(\\mathbf s)$ is unitarily similar to a diagonal matrix, with vectors in $\\mathbf F$ forming the basis of mutually orthonormal eigenvectors.  This completes the proof that a circulant matrix $\\mathbf X$ is unitarily diagonalizable via the \"help\" of $\\mathbf F$. \n",
    "\n",
    "\n",
    "# End Long Form Proof\n",
    "\n",
    "- - - - - \n",
    "# But what the heck do the components of the DFT look like? \n",
    "\n",
    "\n",
    "When we consider that (a) each $\\lambda_i$, contained in position $\\mathbf \\Lambda_{i,i}$, is distinct and also that (b) each $\\lambda_i^n - 1 = 0$\n",
    "\n",
    "as a reminder: this is because (a) the associated Vandermonde matrix is non-singular, and (b) $\\mathbf \\Lambda^n = \\mathbf Q^H \\mathbf P^n \\mathbf Q = \\mathbf Q^H \\mathbf I \\mathbf Q = \\mathbf I $, hence each diagonal element raised to the nth power equals one.  \n",
    "\n",
    "We know that $\\lambda_1 = 1$, because $\\mathbf {P1} = \\mathbf 1$.  From here we can say, $\\lambda_1$ has polor coordinate (1, $2\\pi \\frac{(1 - 1) }{n}$) which is to say it has magnitude 1, and an angle of $0 \\pi$ i.e. it is all real valued = 1.  \n",
    "\n",
    "$\\lambda_2$ has polar coordinate of (1 , $2\\pi\\frac{(2-1)}{n} $)  \n",
    "$\\lambda_3$ has polar coordinate of (1,  $2\\pi\\frac{(3-1)}{n} $)  \n",
    "$\\vdots$  \n",
    "$\\lambda_{n-1}$ has polar coordinate of (1, $2\\pi\\frac{(n-1 -1)}{n} $)  \n",
    "$\\lambda_n$ has polar coordinate of (1, $2\\pi\\frac{(n-1)}{n}$).  \n",
    "\n",
    "There is a variant of the Pidgeon Hole principle here: we have have $n$ $\\lambda_j$'s, each of which must be unique, and there are only $n$ unique nth roots of unity$^{(1)}$ -- hence each nth root has one and only one $\\lambda_j$ \"in\" it.  (This Wolfram alpha link is worth visiting, for its nice graphic: http://mathworld.wolfram.com/RootofUnity.html )\n",
    "\n",
    "\n",
    "\n",
    "Thus **the Vandermonde matrix in the following form is unitary**:  (due to GitHub $\\LaTeX$ rendering issues, the below formula has been inserted as an image)\n",
    "\n",
    "\n",
    "![F_components](images/unitary_vandermondF_components.gif)\n",
    "\n",
    "when each $\\lambda_j$ has polar coordinate of (1, $2\\pi\\frac{(j-1)}{n} $)\n",
    "- - - - -\n",
    "\n",
    "$^{(1)}$ **Side note: How do we know there are exactly n roots of unity?** \n",
    "\n",
    "The reader may wonder how we know that there are \"only $n$ unique nth roots of unity\" available for us to choose from, for any natural number $n$.  One way to support this claim comes from using the fundamental theorem of algebra, which is rather high powered machinery that is not introduced or proved anywhere in this posting.  \n",
    "\n",
    "The other approach is self contained and comes from using Vandermonde matrices.  Consider a degree $n$ polynomial (specifically the polynomial we are interested in is $\\lambda_i^n - 1$, but any degree $n$ polynomial --that isn't the zero polynomial-- is valid here). Such a polynomial would have the following Vandermonde matrix associated with it:\n",
    "\n",
    "\n",
    "$\\mathbf S = \\begin{bmatrix}\n",
    "1 & s_1 & s_1^2 & \\dots  & s_1^{n-1} &s_1^{n} \\\\ \n",
    "1 & s_2 & s_2^2 & \\dots &  s_2^{n-1} & s_2^{n} \\\\ \n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\ \n",
    "1 & s_{n} & s_{n}^{2} & \\dots  & s_{n}^{n-1} & s_{n}^{n} \\\\\n",
    "1 & s_{n+1} & s_{n+1}^{2} & \\dots  & s_{n+1}^{n-1} & s_{n+1}^{n}\n",
    "\\end{bmatrix}$\n",
    "\n",
    "That is, we evaluate our polynomial at $s_i$ where $i = \\{1, 2, ... , n, n+1\\}$.  The polynomial has coefficients associated with it, which are given in $\\mathbf t$.  When we evaluate the polynomial at each $s_i$ we get the resulting value at each $b_i$.  Setting this up as an equation:   \n",
    "\n",
    "$\\begin{bmatrix}\n",
    "1 & s_1 & s_1^2 & \\dots  & s_1^{n-1} &s_1^{n} \\\\ \n",
    "1 & s_2 & s_2^2 & \\dots &  s_2^{n-1} & s_2^{n} \\\\ \n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\ \n",
    "1 & s_{n} & s_{n}^{2} & \\dots  & s_{n}^{n-1} & s_{n}^{n} \\\\\n",
    "1 & s_{n+1} & s_{n+1}^{2} & \\dots  & s_{n+1}^{n-1} & s_{n+1}^{n}\n",
    "\\end{bmatrix}\\begin{bmatrix}\n",
    "t_0\\\\ \n",
    "t_1\\\\ \n",
    "\\vdots\\\\ \n",
    "t_{n-1}\\\\ \n",
    "t_{n}\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "b_1\\\\ \n",
    "b_2\\\\ \n",
    "\\vdots\\\\ \n",
    "b_{n}\\\\ \n",
    "b_{n+1}\n",
    "\\end{bmatrix}$\n",
    "\n",
    "or more succinctly, \n",
    "\n",
    "$\\mathbf{St} = \\mathbf b$ \n",
    "\n",
    "**For a contradiction:** assume that each of the $n+1$ data points is a distinct root of the polynomial.  Then every resulting value in $\\mathbf b$ is zero, which reduces this to:\n",
    "\n",
    "$\\mathbf{St} = \\mathbf 0$.  However since each $s_i$ is distinct, the Vandermonde matrix is invertible, which gives us \n",
    "\n",
    "$\\mathbf S^{-1} \\mathbf{St} = \\mathbf t = \\mathbf S^{-1} \\mathbf 0 = \\mathbf 0$   \n",
    "\n",
    "thus \n",
    "\n",
    "$\\mathbf t = \\mathbf 0$   \n",
    "\n",
    "Since every coefficient is zero, we in fact have the zero polynomial -- which is a contradiction.  However, if at least one of the $s_j$ is not a root (i.e. $\\mathbf b \\neq \\mathbf 0$), then $\\mathbf t \\neq \\mathbf 0$ and hence we may still have a degree $n$ polynomial.  This gives us an upper bound which tells us that a degree $n$ polynomial can have at most $n$ distinct roots.   \n",
    "\n",
    "Now, for our DFT matrix $\\mathbf F$, we are using eigenvalues from the connected graph permutation matrix and they have a constraint given by $\\lambda^n - 1 = 0$.  Put differently we are looking for roots of a degree $n$ polynomial, where the polynomial is $\\lambda^n - 1$.  These roots are called roots of unity.  Per the above, we upper bound the number of unique roots as being $\\leq n$.  Now our matrix $\\mathbf F$ is a unitary Vandermonde Matrix, which means it is non-singular, thus we determine that each $\\lambda_k$ for $k = \\{1, 2, ..., n-1, n \\}$ must be distinct. This means there must be $\\geq n$ distinct roots of unity.  Since our upper bound and lower bound are equal, we have a sandwich and conclude that there are **exactly** $n$ unique roots of unity.  \n",
    "\n",
    "** For an simple example of using a circulant matrix for convolutions of discrete probability distributions, see \n",
    "**\n",
    "https://github.com/DerekB7/LinearAlgebra/blob/master/circulant_convolution_example.ipynb  \n",
    "\n",
    "The code was originally part of this file, but it caused problems in formatting / rendering this file in Github, and hence is treated separately\n",
    "\n",
    "# Full cycle trace relations and nilpotent matrices\n",
    "\n",
    "**claim:**  \n",
    "for $\\mathbf B \\in \\mathbb C^{n x n}$ , \n",
    "\n",
    "if $trace\\big(\\mathbf B^r\\big) = 0$, for $r = \\{1, 2, ... ,n-1, n \\}$ every eigenvalue, $\\lambda_i$, of $\\mathbf B$ is equal to zero, i.e.  $\\lambda_i = 0$ for $i = \\{1, 2, ... ,n-1, n \\}$ .  \n",
    "\n",
    "\n",
    "**comment:**  \n",
    "since the trace gives the sum of the eigenvalues and any complex matrix is similar to an upper triangular matrix, it is clearly true that if all eigenvalues are zero, then the trace will be zero for $\\mathbf B^r$ for any natural number $r$ -- including the case where $1 \\leq r \\leq n$ . What is not immediately clear is that this is an **iff**.\n",
    "\n",
    "In the derivation of the DFT, we used $trace\\big(\\mathbf P^r\\big) = 0$, for $r = \\{1, 2, ... ,n-1 \\}$, yet our matrix $\\mathbf P$ had all eigenvalues with magnitude $= 1$.  Extending the range of $r$ to also includes $n$ radically changes things and makes all eigenvalues have magnitude $= 0$.  \n",
    "\n",
    "Note that the posting called \"Fun_with_Trace_and_Quadratic_Forms_and_CauchySchwarz.ipynb\" proved that a nilpotent $n$ x $n$ matrix must have all eigenvalues equal to zero.  \n",
    "\n",
    "**proof:**  \n",
    "start by constructing the Vandermonde matrix:\n",
    "\n",
    "\n",
    "$\\mathbf W = \\begin{bmatrix}\n",
    "1 & \\lambda_1 & \\lambda_1^2 & \\dots  & \\lambda_1^{n-1}\\\\ \n",
    "1 & \\lambda_2 & \\lambda_2^2 & \\dots &  \\lambda_2^{n-1} \\\\ \n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\\\ \n",
    "1 & \\lambda_{n} & \\lambda_{n}^{2} & \\dots  & \\lambda_{n}^{n-1}\n",
    "\\end{bmatrix}$\n",
    "\n",
    "\n",
    "We want to reflect our constraint as\n",
    "\n",
    "$\\mathbf 1^H \\mathbf {\\Lambda W} = \\mathbf 0^H$\n",
    "\n",
    "i.e. as\n",
    "\n",
    "$\\mathbf 1^H \\begin{bmatrix}\n",
    "\\lambda_1 & \\lambda_1^2 & \\dots  & \\lambda_1^{n-1} & \\lambda_1^{n}\\\\ \n",
    "\\lambda_2 & \\lambda_2^2 & \\dots &  \\lambda_2^{n-1}& \\lambda_2^{n} \\\\ \n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\\\ \n",
    "\\lambda_{n} & \\lambda_{n}^{2} & \\dots  & \\lambda_{n}^{n-1}& \\lambda_n^{n}\n",
    "\\end{bmatrix} = \\mathbf 0^H$\n",
    "\n",
    "\n",
    "But we aren't sure if there are any eigenvalues equal to zero in $\\mathbf \\Lambda$ so we need to remove them first.  Why? First: if any eigenvalues are zero, the left side of the equation is not invertible.  Second, we are interested in the trace relations and we know that any eigenvalues of zero have no impact on the trace calculations, hence they may safely be removed.  \n",
    "\n",
    "*The contradiction kicks in at this stage*\n",
    "\n",
    "We remove all eigenvalues equal to zero and have an $m$ x $m$ matrix for some natural number $m$, where $ m \\leq n$.  Assume $m \\geq 2$, i.e. that some non-zero eigenvalues exist that satisfy our stated trace constraint.  \n",
    "\n",
    "- - - \n",
    "(*Two bookkeeping notes that may be skipped:* First: after removing all zero eigenvalues, $m \\neq 1$ -- because if $m=1$, then $trace\\big(\\mathbf B\\big) = \\lambda_1 = 0$ and the sole remaining eigenvalue $\\lambda_1 \\neq 0$ hence $m$ cannot be equal to one.  Second: we make the adjustment so that $\\mathbf 1$ and $\\mathbf 0$ are $m$ x $1$ column vectors.)  \n",
    "\n",
    "Our equation becomes: \n",
    "\n",
    "$\\mathbf 1^H \\begin{bmatrix}\n",
    "\\lambda_1 & \\lambda_1^2 & \\dots  & \\lambda_1^{m-1} & \\lambda_1^{m}\\\\ \n",
    "\\lambda_2 & \\lambda_2^2 & \\dots &  \\lambda_2^{m-1}& \\lambda_2^{m} \\\\ \n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\\\ \n",
    "\\lambda_{m} & \\lambda_{m}^{2} & \\dots  & \\lambda_{m}^{m-1}& \\lambda_m^{m}\n",
    "\\end{bmatrix} = \\mathbf 0^H$\n",
    "\n",
    "we re-write the above as \n",
    "\n",
    "$\\mathbf y^H \\begin{bmatrix}\n",
    "\\lambda_1 & \\lambda_1^2 & \\dots  & \\lambda_1^{m-1} & \\lambda_1^{m}\\\\ \n",
    "\\lambda_2 & \\lambda_2^2 & \\dots &  \\lambda_2^{m-1}& \\lambda_2^{m} \\\\ \n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\\\ \n",
    "\\lambda_{m} & \\lambda_{m}^{2} & \\dots  & \\lambda_{m}^{m-1}& \\lambda_m^{m}\n",
    "\\end{bmatrix} = \\mathbf 0^H$\n",
    "\n",
    "where $\\mathbf y = \\mathbf 1$.  It is important to note the identity: $\\mathbf y^H \\mathbf 1 = m$.\n",
    "\n",
    "Now we further prune our adjusted Vandermonde matrix to only include unique eigenvalues.  Thus we keep the $k$ unique eigenvalues where $2 \\leq k \\leq m$, and we adjust $\\mathbf y$ so that the trace math is identical. (Again note that $k \\neq 1$, because if so then there is only one unique eigenvalue $\\lambda_1$ and thus $\\frac{1}{m} trace \\big(\\mathbf B\\big) = \\lambda_1 = 0$, but we know that $\\lambda_1 \\neq 0$.)  \n",
    "\n",
    "For example, if all eigenvalues were unique except $\\lambda_m = \\lambda_{m-1}$ we'd remove the mth row and mth column from our adjusted Vandermonde matrix, and now $\\mathbf y$ would be an $m-1$ x $1$ column vector (as would the zero vector), where we have \n",
    "\n",
    "$\\mathbf y = \\begin{bmatrix}\n",
    "1\\\\ \n",
    "1\\\\ \n",
    "\\vdots\\\\ \n",
    "1 \\\\\n",
    "2\n",
    "\\end{bmatrix}$\n",
    "\n",
    "**Put differently, at this stage $y_i$ has algebraic multiplicity for each unique non-zero eigenvalue $\\lambda_i$.**\n",
    "\n",
    "The underlying math with respect to traces is the same, and we still have the key identity $\\mathbf y^H \\mathbf 1 = m$.\n",
    "\n",
    "our equation is thus:   \n",
    "\n",
    "$\\mathbf y^H \\begin{bmatrix}\n",
    "\\lambda_1 & \\lambda_1^2 & \\dots  & \\lambda_1^{k-1} & \\lambda_1^{k}\\\\ \n",
    "\\lambda_2 & \\lambda_2^2 & \\dots &  \\lambda_2^{k-1}& \\lambda_2^{k} \\\\ \n",
    "\\vdots & \\vdots & \\ddots & \\vdots & \\vdots & \\\\ \n",
    "\\lambda_{k} & \\lambda_{k}^{2} & \\dots  & \\lambda_{k}^{k-1}& \\lambda_k^{k}\n",
    "\\end{bmatrix} = \\mathbf 0^H$\n",
    "\n",
    "This matrix has each $\\lambda_i \\neq 0$ and each $\\lambda_i$ is unique. We can factor out a diagonal matrix $\\mathbf D$ if we'd like.  Thus we have \n",
    "\n",
    "$\\mathbf y^H \\mathbf D \\begin{bmatrix}\n",
    "1 & \\lambda_1 & \\lambda_1^2 & \\dots  & \\lambda_1^{k-1} \\\\ \n",
    "1 & \\lambda_2 & \\lambda_2^2 & \\dots &  \\lambda_2^{k-1} \\\\ \n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\\\ \n",
    "1& \\lambda_{k} & \\lambda_{k}^{2} & \\dots  & \\lambda_{k}^{k-1}\n",
    "\\end{bmatrix} = \\mathbf 0^H$\n",
    "\n",
    "letting $\\mathbf K$ be our adjusted Vandermonde matrix in this equation, i.e. $\\mathbf K = \\bigg[\\begin{array}{c|c|c|c|c}\n",
    "\\mathbf D^0 \\mathbf 1 & \\mathbf D^1 \\mathbf 1 & \\mathbf D^2 \\mathbf 1 &\\cdots & \\mathbf D^{k-1} \\mathbf 1\n",
    "\\end{array}\\bigg]$\n",
    "\n",
    "we have \n",
    "\n",
    "$\\mathbf y^H \\mathbf D \\mathbf K = \\mathbf 0^H$\n",
    "\n",
    "Because all $\\lambda_i$'s are unique, $\\mathbf K$ is non-singular and so must be $\\mathbf D$ (it is diagonal with no zero eigenvalues).  We right multiply both sides of our equation by the inverse of $\\mathbf K$ and this gives us \n",
    "\n",
    "\n",
    "$\\mathbf y^H \\mathbf D = \\mathbf y^H \\mathbf D \\mathbf {KK}^{-1} = \\mathbf 0^H \\mathbf K^{-1} = \\mathbf 0^H$ \n",
    "\n",
    "now right multiply both sides by $\\mathbf D^{-1}$, and we have \n",
    "\n",
    "$\\mathbf y^H = \\mathbf y^H \\mathbf D \\mathbf D^{-1} =  \\mathbf 0^H \\mathbf D^{-1} = \\mathbf 0^H$ \n",
    "\n",
    "This tells us that $\\mathbf y^H = \\mathbf 0^H$.  Yet this is a contradiction, because \n",
    "\n",
    "$\\mathbf y^H \\mathbf 1 = m \\neq \\mathbf 0^H \\mathbf 1 = 0$\n",
    "\n",
    "\n",
    "hence we know that $m \\ngeq 2$, and as mentioned earlier $m \\neq 1$.  Thus $m = 0$.  Put differently, $\\mathbf K$ does not exist (i.e. it must be a $0$ x $0$ matrix).  This proves the claim that all eigenvalues of $\\mathbf B$ must be equal to zero if \n",
    "\n",
    "$trace\\big(\\mathbf B^r\\big) = 0$, for $r = \\{1, 2, ... ,n-1, n \\}$ \n",
    "\n",
    "- - - -\n",
    "- - - -\n",
    "- - - -\n",
    "**alternative approach:** if the reader feels the above contradiction to be unsatifying, consider instead the following setup:\n",
    "\n",
    "\n",
    "$\\mathbf y^H \\mathbf D \\begin{bmatrix}\n",
    "1 & \\lambda_1 & \\lambda_1^2 & \\dots  & \\lambda_1^{k-1} & \\lambda_1^{k} \\\\ \n",
    "1 & \\lambda_2 & \\lambda_2^2 & \\dots &  \\lambda_2^{k-1} & \\lambda_2^{k} \\\\ \n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\\\ \n",
    "1& \\lambda_{k} & \\lambda_{k}^{2} & \\dots  & \\lambda_{k}^{k-1} & \\lambda_k^{k} \\\\\n",
    "1& 0 & 0 & \\dots  & 0 & 0\n",
    "\\end{bmatrix} = \\mathbf 0^H$\n",
    "\n",
    "Here we have $k + 1$ rows in our Vandermonde matrix -- where the eigenvalue of zero is contained in the final row. $\\mathbf D$ now is a $k+1 $ x $k+1$ diagonal matrix that has a zero in its bottom right corner, and $\\mathbf y$ has the algebraic multiplicity for each of the $k+1$ unique eigenvalues (inclusive of the eigenvalue equal to zero, which is given by $y_{k+1}$).  We know that $\\mathbf B$ has $n$ eigenvalues thus $\\mathbf y^H \\mathbf 1 = n$.\n",
    "\n",
    "Our Vandermonde Matrix is invertible so we multiply both sides on the right by its inverse, giving us \n",
    "\n",
    "$\\mathbf y^H \\mathbf D = \\mathbf 0^H$\n",
    "\n",
    "or equivalently\n",
    "\n",
    "$\\mathbf D^H \\mathbf y = \\mathbf 0$\n",
    "\n",
    "now we notice that $\\mathbf D$ is singular, with all non-zero entries along its diagonal except the entry in the bottom right corner.  However the above equation tells us that \n",
    "\n",
    "for $i = \\{1, 2, ..., k\\}$\n",
    "\n",
    "$y_i \\bar{\\lambda_i} = 0$\n",
    "\n",
    "\n",
    "we observe that this is also true in the $k+1$ case:  $y_{k+1} \\bar{\\lambda_{k+1}} = 0$\n",
    "\n",
    "First we deal with $i = \\{1, 2, ..., k\\}$ noticing that $\\bar{\\lambda_i} \\neq 0$\n",
    "\n",
    "$y_i \\bar{\\lambda_i} = 0$\n",
    "\n",
    "divide both sides by $\\bar{\\lambda_i}$ and see that \n",
    "\n",
    "$y_i = 0$\n",
    "\n",
    "Finally for $y_{k+1}$, we have  \n",
    "\n",
    "$y_{k+1} \\bar{\\lambda_{k+1}} = y_{k+1} 0 = 0$\n",
    "\n",
    "but we also have the constraint $n = \\mathbf y^H \\mathbf 1 = \\mathbf 1^H \\mathbf y = \\big(y_0 + y_1 + ... y_{k-1} + y_k \\big) + y_{k+1} = \\big(0 \\big) + y_{k+1} $ \n",
    "\n",
    "hence we see that $y_{k+1} = n$.  Put differently, all of the eigenvalues for $\\mathbf B$ are zero -- i.e. $\\mathbf B$ is nilpotent.\n",
    "\n",
    "*Book-keeping note: in either case, we've determined that* $\\mathbf B$* is nilpotent (i.e. all of its eigenvalues are zero), but only considered traces of powers up to k.  From here we simply mention that for k + 1, k +2, ... , n, we know that* $trace\\big(\\mathbf B^k\\big)= \\Sigma_{i=1}^n \\lambda_i^k = \\Sigma_{i=1}^n 0^k = 0$\n",
    "\n",
    "# Cayley Hamilton \n",
    "\n",
    "**claim**: each operator, $\\mathbf B \\in \\mathbb C^{n x n}$ obeys its characteristic polynomial.  i.e. \n",
    "\n",
    "$c_0 \\mathbf I + c_1 \\mathbf B + c_2 \\mathbf B^2 + ... + c_{n-1}\\mathbf B^{n-1} + c_{n}\\mathbf B^n = c_0 \\mathbf I +  \\Sigma_{r=1}^{n} c_r \\mathbf B^r = \\bigg[\\begin{array}{c|c|c|c}\n",
    "  \\mathbf 0 & \\mathbf 0 &\\cdots & \\mathbf 0\n",
    "\\end{array}\\bigg]$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**proof: non-defective case, where $\\mathbf B$ has $n$ linearly independent eigenvectors.**\n",
    "\n",
    "We know that each eigenvalue is a root to the characteristic polynomial.  \n",
    "\n",
    "Put differently, we know that for $k = \\{1, 2, ..., n-1, n\\}$, we have an eigenpair of $\\mathbf x_k, \\lambda_k$\n",
    "\n",
    "$c_0 +  \\Sigma_{r=1}^{n} c_r \\lambda_k^r = 0$\n",
    "\n",
    "\n",
    "we can multiply this by $\\mathbf x_k$:\n",
    "\n",
    "$\\big(c_0 +  \\Sigma_{r=1}^{n} c_r \\lambda_k^r \\big) \\mathbf x_k = \\mathbf 0$\n",
    "\n",
    "\n",
    "or equivalently\n",
    "\n",
    "$\\big(c_0 \\mathbf I  + \\Sigma_{r=1}^{n} c_r \\mathbf B^r \\big) \\mathbf x_k = \\mathbf 0$\n",
    "\n",
    "\n",
    "\n",
    "Now let's collect these $n$ relationships in a system of equations:\n",
    "\n",
    "\n",
    "$\\big(c_0 \\mathbf I + \\Sigma_{r=1}^{n} c_r \\mathbf B^r \\big) \\bigg[\\begin{array}{c|c|c|c}\n",
    "\\mathbf x_1 & \\mathbf x_2 &\\cdots & \\mathbf x_n\n",
    "\\end{array}\\bigg] = \\big(c_0 \\mathbf I + \\Sigma_{r=1}^{n} c_r \\mathbf B^r \\big)\\mathbf X = \\bigg[\\begin{array}{c|c|c|c}\n",
    "  \\mathbf 0 & \\mathbf 0 &\\cdots & \\mathbf 0\n",
    "\\end{array}\\bigg]$\n",
    "\n",
    "because the eigenvectors are stated to be linearly independent, we multiply each side on the right by $\\mathbf X^{-1}$ seeing that\n",
    "\n",
    "\n",
    "$\\big(c_0 \\mathbf I + \\Sigma_{r=1}^{n} c_r \\mathbf B^r \\big) = \\bigg[\\begin{array}{c|c|c|c}\n",
    "  \\mathbf 0 & \\mathbf 0 &\\cdots & \\mathbf 0\n",
    "\\end{array}\\bigg]$\n",
    "\n",
    "which proves that $\\mathbf B$ follows its characteristic polynomial at least in the case where its eigenvectors form a basis.\n",
    "\n",
    "- - - -\n",
    "*begin alternative proof: non-defective case* \n",
    "\n",
    "an alternative aproach, which foreshadows the algebraic approach to the defective case, is to factor the characteristic polynomial applied to $\\mathbf B$\n",
    "\n",
    "\n",
    "$c_0 \\mathbf I + c_1 \\mathbf B + c_2 \\mathbf B^2 + ... + c_{n-1}\\mathbf B^{n-1} + c_{n}\\mathbf B^n = c_0 \\mathbf I +  \\Sigma_{r=1}^{n} c_r \\mathbf B^r  = \\big(\\mathbf B - \\lambda_{1} \\mathbf I\\big)\\big(\\mathbf B - \\lambda_{2} \\mathbf I\\big)...\\big(\\mathbf B - \\lambda_{n-1} \\mathbf I\\big)\\big(\\mathbf B - \\lambda_n \\mathbf I\\big) $\n",
    "\n",
    "Because $\\mathbf B$ is not defective, we diagonalize it, as shown below\n",
    "\n",
    "$\\mathbf B = \\mathbf{PD}\\mathbf P^{-1}$\n",
    "\n",
    "$ c_0 \\mathbf I +  \\Sigma_{r=1}^{n} c_r \\mathbf B^r = \\big(\\mathbf{PD}\\mathbf P^{-1} - \\lambda_{1} \\mathbf I\\big)\\big(\\mathbf{PD}\\mathbf P^{-1} - \\lambda_{2} \\mathbf I\\big)...\\big(\\mathbf{PD}\\mathbf P^{-1} - \\lambda_{n-1} \\mathbf I\\big)\\big(\\mathbf{PD}\\mathbf P^{-1} - \\lambda_n \\mathbf I\\big) $\n",
    "\n",
    "$ c_0 \\mathbf I +  \\Sigma_{r=1}^{n} c_r \\mathbf B^r = \\big(\\mathbf{PD}\\mathbf P^{-1} - \\lambda_{1} \\mathbf P \\mathbf I \\mathbf P^{-1}\\big)\\big(\\mathbf{PD}\\mathbf P^{-1} - \\lambda_{2} \\mathbf P \\mathbf I \\mathbf P^{-1}\\big)...\\big(\\mathbf{PD}\\mathbf P^{-1} - \\lambda_{n-1} \\mathbf P\\mathbf I\\mathbf P^{-1}\\big)\\big(\\mathbf{PD}\\mathbf P^{-1} - \\lambda_n \\mathbf P\\mathbf I\\mathbf P^{-1}\\big)$\n",
    "\n",
    "$ c_0 \\mathbf I +  \\Sigma_{r=1}^{n} c_r \\mathbf B^r = \\big(\\mathbf{P}\\big(\\mathbf D - \\lambda_{1}\\mathbf I\\big) \\mathbf P^{-1}\\big)\\big(\\mathbf{P}\\big(\\mathbf D - \\lambda_{2}\\mathbf I\\big)\\mathbf P^{-1} \\big)...\\big(\\mathbf{P}\\big(\\mathbf D - \\lambda_{n-1}\\mathbf I\\big)\\mathbf P^{-1}\\big)\\big(\\mathbf{P}\\big(\\mathbf D - \\lambda_{n}\\mathbf I\\big)\\mathbf P^{-1}\\big)$\n",
    "\n",
    "\n",
    "$ c_0 \\mathbf I +  \\Sigma_{r=1}^{n} c_r \\mathbf B^r = \\mathbf P\\Big(\\big(\\mathbf D - \\lambda_{1}\\mathbf I\\big)\\big(\\mathbf D - \\lambda_{2}\\mathbf I\\big)...(\\mathbf D - \\lambda_{n-1}\\mathbf I\\big)(\\mathbf D - \\lambda_{n}\\mathbf I\\big)\\Big)\\mathbf P^{-1}$\n",
    "\n",
    "$ c_0 \\mathbf I +  \\Sigma_{r=1}^{n} c_r \\mathbf B^r = \\mathbf P\\Big(Diag\\big(\\mathbf 0\\big)\\Big)\\mathbf P^{-1} = \\bigg[\\begin{array}{c|c|c|c}\n",
    "  \\mathbf 0 & \\mathbf 0 &\\cdots & \\mathbf 0\n",
    "\\end{array}\\bigg]$\n",
    "\n",
    "*end alternative proof: non-defective case* \n",
    "- - - -\n",
    "\n",
    "**proof: defective case:**  \n",
    "\n",
    "a sketch of the analysis proof covers two things\n",
    "\n",
    "First, it is clear from Schur decomposition that any $n$ x $n$ matrix in $\\mathbb C$ is unitarily similar to an upper triangular matrix\n",
    "\n",
    "$\\mathbf Q^H \\mathbf{BQ} = \\mathbf T$ or \n",
    "\n",
    "$\\mathbf{B} = \\mathbf {QTQ}^H$\n",
    "\n",
    "\n",
    "The eigenvalues of $\\mathbf T$ obey their characteristic polynomial, hence the characteristic polynomial of $\\mathbf B$ or equivalently $\\mathbf T$, must be a nilpotent matrix.  However Cayley Hamilton makes a stronger claim that in fact it is not just any nilpotent matrix, but the zero matrix.  \n",
    "\n",
    "*A key takeway from this, however, is that if a matrix was not nilpotent (i.e. it has at least one non-zero eigenvalue), and it becomes nilpotent after applying some other matrix's characteristic polynomial, then that means your matrix has roots to that other matrices characteristic polynomial -- i.e. your non-zero eigenvalues are non-zero eigenvalues for that other matrix. *\n",
    "\n",
    "an outline of the analysis approach is that:\n",
    "\n",
    "we can find an upper triangular matrix $\\mathbf R$ where all entries are identical to $\\mathbf T$ except diagonal elements are perturbed by small enough $\\delta$, $\\delta^2$, $\\delta^3$, and so on as needed for all duplicate eigenvalues.  Afterward, we have \n",
    "\n",
    "$\\big \\Vert \\mathbf{T} - \\mathbf R \\big \\Vert_F^2 \\lt \\epsilon$\n",
    "\n",
    "for any $\\epsilon \\gt 0$ \n",
    "\n",
    "where $\\mathbf C = \\mathbf{QRQ}^H$\n",
    "\n",
    "But now each eigenvalue is unique and per the proof near the top of this posting $\\mathbf C$  is now diagonalizable aka non-defective, and the earlier part of this cell -- i.e. the proof that all diagonalizable matrices obey Cayley Hamilton-- may be used. (There is a tecnical nit that by perturbing the eigenvalues, we have changed the characteristic polynomial, but this change is $O(\\delta)$ and becomes arbitrarily small as $\\delta \\to 0$).  \n",
    "\n",
    "Thus we may say, up to any arbitrary level of precision we can approximate $\\mathbf B$ or $\\mathbf T$ and find that those approximations all obey Cayley Hamilton, hence $\\mathbf B$ obeys Cayley Hamilton as well. \n",
    "\n",
    "*note: there are purely algebraic proofs of Cayley Hamilton for defective matrices that do not require limits / analysis.  The analysis view is quite intuitive, but requires some heavy duty machinery to be fully rigorous.  In any case these different approaches are complementary.*  \n",
    "\n",
    "**A purely algebraic proof is below.**  \n",
    "\n",
    "** Special structures in multiplying with Diagonal matrices and Triangular matrices**\n",
    "\n",
    "Matrix multiplication is associative but generally does not commute.  However in certain special cases it does.  In particular, when you have an $n$ x $n$ matrix $\\mathbf B$ and scaled form of the identity matrix, multiplication does commute. \n",
    "\n",
    "e.g. \n",
    "\n",
    "$\\big(\\gamma_1 \\mathbf B - \\lambda_1 \\mathbf I\\big)\\big(\\gamma_2 \\mathbf B - \\lambda_2 \\mathbf I\\big) = \\big(\\gamma_2 \\gamma_1 \\mathbf B^2 - (\\lambda_2 \\gamma_1 + \\lambda_1 \\gamma_2) \\mathbf B + \\lambda_1 \\lambda_2 \\mathbf I\\big) = \\big(\\gamma_2 \\mathbf B - \\lambda_2 \\mathbf I\\big)\\big(\\gamma_1 \\mathbf B - \\lambda_1 \\mathbf I\\big)$\n",
    "\n",
    "which we may verify by inspection.\n",
    "\n",
    "More generally, when we multiply this out over k terms\n",
    "\n",
    "$\\big(\\gamma_1 \\mathbf B - \\lambda_1 \\mathbf I\\big)\\big(\\gamma_2 \\mathbf B - \\lambda_2 \\mathbf I\\big)\\big(\\gamma_3 \\mathbf B - \\lambda_3 \\mathbf I\\big)...\\big(\\gamma_k \\mathbf B - \\lambda_k \\mathbf I\\big)$\n",
    "\n",
    "we may permute the ordering any way we like an get the same result.  \n",
    "\n",
    "Put differently, we may swap any two adjacent terms, where \n",
    "\n",
    "$\\big(\\gamma_{r-1} \\mathbf B - \\lambda_{r-1} \\mathbf I\\big)\\big(\\gamma_r \\mathbf B - \\lambda_r \\mathbf I\\big) = \\big(\\gamma_r \\mathbf B - \\lambda_r \\mathbf I\\big)\\big(\\gamma_{r-1} \\mathbf B - \\lambda_{r-1} \\mathbf I\\big)$\n",
    "\n",
    "for $r = \\{2, 3, 4, ... , k\\}$\n",
    "\n",
    "and using associativity, we see that the product is unchanged.  I.e. \n",
    "\n",
    "$\\Big(\\big(\\gamma_1 \\mathbf B - \\lambda_1 \\mathbf I\\big)\\big(\\gamma_2 \\mathbf B - \\lambda_2 \\mathbf I\\big)... \\Big(\\big(\\gamma_{r-1} \\mathbf B - \\lambda_{r-1} \\mathbf I\\big)\\big(\\gamma_r \\mathbf B - \\lambda_r \\mathbf I\\big)\\Big)\\Big)\\Big(\\big(\\gamma_{r+1} \\mathbf B - \\lambda_{r+1} \\mathbf I\\big) ...\\big(\\gamma_k \\mathbf B - \\lambda_k \\mathbf I\\big)\\Big)$\n",
    "\n",
    "is equivalent to \n",
    "\n",
    "$\\Big(\\big(\\gamma_1 \\mathbf B - \\lambda_1 \\mathbf I\\big)\\big(\\gamma_2 \\mathbf B - \\lambda_2 \\mathbf I\\big)... \\Big(\\big(\\gamma_{r} \\mathbf B - \\lambda_{r} \\mathbf I\\big)\\big(\\gamma_{r-1} \\mathbf B - \\lambda_{r-1} \\mathbf I\\big)\\Big)\\Big)\\Big(\\big(\\gamma_{r+1} \\mathbf B - \\lambda_{r+1} \\mathbf I\\big) ...\\big(\\gamma_k \\mathbf B - \\lambda_k \\mathbf I\\big)\\Big)$\n",
    "\n",
    "\n",
    "and we may repeatedly use this to move any term from any starting position $r$ to any ending position $j$.  This is equivalent to permuting a deck of cards (slowly) by deciding which card we want at the top of the deck and doing pairwise swaps until it is there.  Then deciding which card we want in the 2nd to top place and doing pairwise swaps until it is there, and so on.  (Note: in the language of permuting symmetric groups on $k$ elements, this pairwise swap would be called a \"basic transposition\", a term that sometimes comes up when discussing determinants, though the use of 'transpose' with a different meaning is a bit unfortunate here.)\n",
    "\n",
    "**Triangular Matrices**\n",
    "\n",
    "Upper triangular matrices are particularly easy to work with and may be interpretted as a directed graph without cycles -- except there may be a very special cycle-- i.e. a self-loop-- if there are non-zero eigenvalues.  (The same arguments may be made with lower triangular matrices, but the convention is to use upper triangular, and that is what this posting does.)  \n",
    "\n",
    "The standard basis vectors, are particularly easy to use when making arguments with upper triangular matrices. \n",
    "\n",
    "$\\bigg[\\begin{array}{c|c|c|c}\n",
    "  \\mathbf e_1 & \\mathbf e_2 &\\cdots & \\mathbf e_n\n",
    "\\end{array}\\bigg] = \\mathbf I$\n",
    "\n",
    "For instance: Consider the case where we have a nilpotent, i.e. strictly upper triangular matrix, $\\mathbf T$.  In this case we know that \n",
    "\n",
    "$\\mathbf {T e}_1 = \\mathbf 0$\n",
    "\n",
    "and we know that \n",
    "\n",
    "$\\mathbf {T e}_2 = t_{1,2} \\mathbf e_1$\n",
    "\n",
    "hence $\\mathbf  T^2 \\mathbf  e_2 = \\mathbf {TT e}_2 = t_{1,2} \\mathbf T\\mathbf e_1 = \\mathbf 0$\n",
    "\n",
    "and the same idea occurs with \n",
    "\n",
    "$\\mathbf  T^3 \\mathbf  e_3 = \\mathbf {TTT e}_3 =  t_{1,3}\\mathbf T \\big(\\mathbf T \\mathbf e_1\\big) +  t_{2,3}\\mathbf T^2\\mathbf e_2 = \\mathbf 0$\n",
    "\n",
    "and in general for any $\\mathbf e_k$, we have \n",
    "\n",
    "$\\mathbf  T \\mathbf  e_k =   t_{1,k}\\mathbf e_1 + t_{2,k}\\mathbf e_2 + ... +  t_{k-1,k}\\mathbf e_{k-1}$\n",
    "\n",
    "Repeated left multiplication by $\\mathbf T$ generates smaller and smaller subproblems.  I.e. if we left multiply the above by $\\mathbf T$ again, we get a linear combination of $\\{\\mathbf e_1, \\mathbf e_2, ... , \\mathbf e_{k-2}\\}$ And hence we can repeatedly left multiply any $\\mathbf  e_k$ by $\\mathbf T$ for $k - 2$ times, and at worse we recover a linear combination of $\\mathbf e_2$ and $\\mathbf e_1$, which we have already shown becomes the zero vector after at most two left multiplications of $\\mathbf T$.  \n",
    "\n",
    "Hence we conclude that $\\mathbf T^k \\mathbf e_k = \\mathbf 0$ \n",
    "\n",
    "Now, the only difference between the general upper triangular case and the strictly upper triangular case is the possibility of self-loops in the former.  \n",
    "\n",
    "*That is, we now assume* $\\mathbf T$ *is not necesarily nilpotent.  Instead, it is unitarily similar to *$\\mathbf B$, via Schur Decomposition.  I.e. $\\mathbf Q^H \\mathbf {BQ} = \\mathbf T$\n",
    "\n",
    "Then we now have \n",
    "\n",
    "$\\mathbf  T \\mathbf  e_k =   t_{1,k}\\mathbf e_1 + t_{2,k}\\mathbf e_2 + ... +  t_{k-1,k}\\mathbf e_{k-1} +  t_{k,k}\\mathbf e_{k} = t_{1,k}\\mathbf e_1 + t_{2,k}\\mathbf e_2 + ... +  t_{k-1,k}\\mathbf e_{k-1} +  \\lambda_k \\mathbf e_{k}$\n",
    "\n",
    "And thus we see the role of the non-zero eigenvalue in diagonal position $k$.  \n",
    "\n",
    "If we take \n",
    "\n",
    "$\\big(\\mathbf T - \\lambda_n \\mathbf I\\big)$, we can be sure that it has a zero in its bottom right (i.e. nth) diagonal position.  \n",
    "\n",
    "Thus we have \n",
    "\n",
    "$\\big(\\mathbf T - \\lambda_n \\mathbf I\\big)\\mathbf e_n = t_{1,k}\\mathbf e_1 + t_{2,k}\\mathbf e_2 + ... +  t_{k-1,k}\\mathbf e_{k-1} = \\Sigma_{i = 1}^{n-1} t_{i, n} \\mathbf e_i$\n",
    "\n",
    "Now suppose we left multiply this by \n",
    "\n",
    "$\\big(\\mathbf T - \\lambda_{n-1} \\mathbf I\\big)$\n",
    "\n",
    "we then have \n",
    "\n",
    "$\\big(\\mathbf T - \\lambda_{n-1} \\mathbf I\\big)\\big(\\mathbf T - \\lambda_n \\mathbf I\\big)\\mathbf e_n = \\big(\\mathbf T - \\lambda_{n-1} \\mathbf I\\big) \\Sigma_{i = 1}^{n-1} t_{i, n} \\mathbf e_i = \\Sigma_{i = 1}^{n-1}  \\Big(t_{i, n} \\big(\\mathbf T - \\lambda_{n-1} \\mathbf I\\big) \\mathbf e_i\\Big)= \\Sigma_{i = 1}^{n-2} \\gamma_i \\mathbf e_i $\n",
    "\n",
    "where $\\gamma_i$ denotes the appropriate scalar -- whose value we are not particularly interested in.  We verify the right hand side by noting that at worst, $\\big(\\mathbf T - \\lambda_{n-1} \\mathbf I\\big)$ has a non-zero eigenvalue for each $\\mathbf e_r$ where $r \\lt n-1$, and $\\big(\\mathbf T - \\lambda_{n-1} \\mathbf I\\big)$ has a zero eigenvalue in the n-1 spot, so $\\big(\\mathbf T - \\lambda_{n-1} \\mathbf I\\big)\\mathbf e_{n-1}$ returns a linear combination of $\\{\\mathbf e_1, \\mathbf e_2, ..., \\mathbf e_{n-2}\\}$ and $\\big(\\mathbf T - \\lambda_{n-1} \\mathbf I\\big)\\mathbf e_{r}$ returns a linear combination that at most has elements in $\\{\\mathbf e_1, \\mathbf e_2, ..., \\mathbf e_{n-2}\\}$.\n",
    "\n",
    "We now left multiply by $\\big(\\mathbf T - \\lambda_{n-2} \\mathbf I\\big)$ and see \n",
    "\n",
    "$\\big(\\mathbf T - \\lambda_{n-2} \\mathbf I\\big)\\big(\\mathbf T - \\lambda_{n-1} \\mathbf I\\big)\\big(\\mathbf T - \\lambda_n \\mathbf I\\big)\\mathbf e_n = \\Sigma_{i = 1}^{n-3} \\gamma_i \\mathbf e_i$\n",
    "\n",
    "And we continue this process until we have \n",
    "\n",
    "$\\big(\\mathbf T - \\lambda_{2} \\mathbf I\\big)\\big(\\mathbf T - \\lambda_{3} \\mathbf I\\big)...\\big(\\mathbf T - \\lambda_{n-1} \\mathbf I\\big)\\big(\\mathbf T - \\lambda_n \\mathbf I\\big)\\mathbf e_n = \\Sigma_{i = 1}^{1} \\gamma_i \\mathbf e_i = \\gamma_1 \\mathbf e_1$\n",
    "\n",
    "One more appropriate left multiplication gives us\n",
    "\n",
    "$\\big(\\mathbf T - \\lambda_{1} \\mathbf I\\big)\\big(\\mathbf T - \\lambda_{2} \\mathbf I\\big)\\big(\\mathbf T - \\lambda_{3} \\mathbf I\\big)...\\big(\\mathbf T - \\lambda_{n-1} \\mathbf I\\big)\\big(\\mathbf T - \\lambda_n \\mathbf I\\big)\\mathbf e_n = \\gamma_1 \\big(\\mathbf T - \\lambda_{1} \\mathbf I\\big) \\mathbf e_1 = \\mathbf 0$\n",
    "\n",
    "The left hand side has $n$ terms in it that we multiply by $\\mathbf e_n$ and get the zero vector.  \n",
    "\n",
    "It naturally follows that we can use *any* $\\mathbf e_k$ on the left hand side and get the zero vector.  There are a few ways to argue this.  The easiest, perhaps, is to remember that these multiplications commute so we may choose the ordering as we please.  \n",
    "\n",
    "Hence if $\\mathbf e_k \\neq \\mathbf e_n$ we simply reorder the equation, (using, in effect, a cyclic property that we get for free with commutativity):\n",
    "\n",
    "i.e. we know that \n",
    "\n",
    "$\\big(\\mathbf T - \\lambda_{k+1} \\mathbf I\\big)...\\big(\\mathbf T - \\lambda_n \\mathbf I\\big)\\big(\\mathbf T - \\lambda_{1} \\mathbf I\\big)...\\big(\\mathbf T - \\lambda_{k-1} \\mathbf I\\big)\\big(\\mathbf T - \\lambda_k \\mathbf I\\big) = \\big(\\mathbf T - \\lambda_{1} \\mathbf I\\big)\\big(\\mathbf T - \\lambda_{2}\\mathbf I\\big)...\\big(\\mathbf T - \\lambda_n \\mathbf I\\big)$\n",
    "\n",
    "Thus we have \n",
    "\n",
    "$\\Big(\\big(\\mathbf T - \\lambda_{k+1} \\mathbf I\\big)...\\big(\\mathbf T - \\lambda_n \\mathbf I\\big)\\big(\\mathbf T - \\lambda_{1} \\mathbf I\\big)\\big(\\mathbf T - \\lambda_{2} \\mathbf I\\big)...\\big(\\mathbf T - \\lambda_{k-1} \\mathbf I\\big)\\big(\\mathbf T - \\lambda_k \\mathbf I\\big)\\Big)\\mathbf e_k$\n",
    "\n",
    "now making further use of associativity  \n",
    "\n",
    "$\\Big(\\big(\\mathbf T - \\lambda_{k+1} \\mathbf I\\big)...\\big(\\mathbf T - \\lambda_n \\mathbf I\\big)\\Big)\\Big(\\big(\\mathbf T - \\lambda_{1} \\mathbf I\\big)\\big(\\mathbf T - \\lambda_{2} \\mathbf I\\big)...\\big(\\mathbf T - \\lambda_{k-1} \\mathbf I\\big)\\big(\\mathbf T - \\lambda_k \\mathbf I\\big)\\mathbf e_k\\Big)  = \\Big(\\big(\\mathbf T - \\lambda_{k+1} \\mathbf I\\big)...\\big(\\mathbf T - \\lambda_n \\mathbf I\\big)\\Big)\\mathbf 0 = \\mathbf 0$\n",
    "\n",
    "collect these relationships for each $\\mathbf e_k$ in matrix form, and we find \n",
    "\n",
    "$\\Big(\\big(\\mathbf T - \\lambda_{1} \\mathbf I\\big)\\big(\\mathbf T - \\lambda_{2} \\mathbf I\\big)\\big(\\mathbf T - \\lambda_{3} \\mathbf I\\big)...\\big(\\mathbf T - \\lambda_{n-1} \\mathbf I\\big)\\big(\\mathbf T - \\lambda_n \\mathbf I\\big)\\Big)  \\bigg[\\begin{array}{c|c|c|c}\n",
    "  \\mathbf e_1 & \\mathbf e_2 &\\cdots & \\mathbf e_n\n",
    "\\end{array}\\bigg]  = \\bigg[\\begin{array}{c|c|c|c}\n",
    "  \\mathbf 0 & \\mathbf 0 &\\cdots & \\mathbf 0\n",
    "\\end{array}\\bigg]$\n",
    "\n",
    "\n",
    "normally this posting would say to invert and right multiply by the full rank matrix we formed with $\\mathbf e_k$'s, however we simply recall that \n",
    "\n",
    "$\\bigg[\\begin{array}{c|c|c|c}\n",
    "  \\mathbf e_1 & \\mathbf e_2 &\\cdots & \\mathbf e_n\n",
    "\\end{array}\\bigg] = \\mathbf I$\n",
    "\n",
    "Thus we have \n",
    "\n",
    "$\\big(\\mathbf T - \\lambda_{1} \\mathbf I\\big)\\big(\\mathbf T - \\lambda_{2} \\mathbf I\\big)\\big(\\mathbf T - \\lambda_{3} \\mathbf I\\big)...\\big(\\mathbf T - \\lambda_{n-1} \\mathbf I\\big)\\big(\\mathbf T - \\lambda_n \\mathbf I\\big)   \n",
    " = \\bigg[\\begin{array}{c|c|c|c}\n",
    "  \\mathbf 0 & \\mathbf 0 &\\cdots & \\mathbf 0\n",
    "\\end{array}\\bigg]$\n",
    "\n",
    "\n",
    "From here we may notice that the factorization of \n",
    "\n",
    "$\\big(\\mathbf T - \\lambda_{1} \\mathbf I\\big)\\big(\\mathbf T - \\lambda_{2} \\mathbf I\\big)\\big(\\mathbf T - \\lambda_{3} \\mathbf I\\big)...\\big(\\mathbf T - \\lambda_{n-1} \\mathbf I\\big)\\big(\\mathbf T - \\lambda_n \\mathbf I\\big)$\n",
    "\n",
    "is just factoring the characteristic polynomial of $\\mathbf B$ (or equivalently $\\mathbf T$, since they are similar), thus \n",
    "\n",
    "$\\big(\\mathbf T - \\lambda_{1} \\mathbf I\\big)\\big(\\mathbf T - \\lambda_{2} \\mathbf I\\big)...\\big(\\mathbf T - \\lambda_{n-1} \\mathbf I\\big)\\big(\\mathbf T - \\lambda_n \\mathbf I\\big) = c_0 \\mathbf I + c_1 \\mathbf T + c_2 \\mathbf T^2 + ... + c_{n-1}\\mathbf T^{n-1} + c_{n}\\mathbf T^n = \\bigg[\\begin{array}{c|c|c|c}\n",
    "  \\mathbf 0 & \\mathbf 0 &\\cdots & \\mathbf 0\n",
    "\\end{array}\\bigg]$\n",
    "\n",
    "recalling that, $\\mathbf B = \\mathbf {QTQ}^H$\n",
    "\n",
    "we left multiply each side of the above by $\\mathbf Q$ and right multiply each side of the above by $\\mathbf Q^H$\n",
    "\n",
    "$\\mathbf Q\\big(c_0 \\mathbf I + c_1 \\mathbf T + c_2 \\mathbf T^2 + ... + c_{n-1}\\mathbf T^{n-1} + c_{n}\\mathbf T^n\\big)\\mathbf Q^H = \\mathbf Q\\bigg[\\begin{array}{c|c|c|c}\n",
    "  \\mathbf 0 & \\mathbf 0 &\\cdots & \\mathbf 0\n",
    "\\end{array}\\bigg] \\mathbf Q^H = \\bigg[\\begin{array}{c|c|c|c}\n",
    "  \\mathbf 0 & \\mathbf 0 &\\cdots & \\mathbf 0\n",
    "\\end{array}\\bigg]$\n",
    "\n",
    "$c_0 \\mathbf Q \\mathbf I \\mathbf Q^H + c_1 \\mathbf Q\\mathbf T\\mathbf Q^H + c_2 \\mathbf Q\\mathbf T^2\\mathbf Q^H + ... + c_{n-1}\\mathbf Q\\mathbf T^{n-1}\\mathbf Q^H + c_{n}\\mathbf Q\\mathbf T^n \\mathbf Q^H = \\bigg[\\begin{array}{c|c|c|c}\n",
    "  \\mathbf 0 & \\mathbf 0 &\\cdots & \\mathbf 0\n",
    "\\end{array}\\bigg]$\n",
    "\n",
    "and finally:\n",
    "\n",
    "$c_0 \\mathbf I + c_1 \\mathbf B + c_2 \\mathbf B^2 + ... + c_{n-1}\\mathbf B^{n-1} + c_{n}\\mathbf B^n = c_0 \\mathbf I +  \\Sigma_{r=1}^{n} c_r \\mathbf B^r = \\bigg[\\begin{array}{c|c|c|c}\n",
    "  \\mathbf 0 & \\mathbf 0 &\\cdots & \\mathbf 0\n",
    "\\end{array}\\bigg]$\n",
    "\n",
    "This is the long-form algebraic proof of Cayley Hamilton when $\\mathbf B$ is defective.  \n",
    "- - - -\n",
    "\n",
    "# The traces of a matrix over large enough k iterations, uniquely characterize the non-zero eigenvalues\n",
    "\n",
    "We now combine two different results: \"Full cycle trace relations and nilpotent matrices\" and the above \"Cayley Hamilton\" proofs.  \n",
    "\n",
    "**claim:**  \n",
    "\n",
    "If for $n$ x $n$ matrix $\\mathbf X$ and $m$ x $m$ matrix $\\mathbf Y$:\n",
    "\n",
    "$trace\\big(\\mathbf X^k\\big)$ = $trace\\big(\\mathbf Y^k \\big)$  for natural numbers $k = \\{1, 2, 3, ...\\}$,  \n",
    "\n",
    "then $\\mathbf X$ and $\\mathbf Y$ have the same non-zero eigenvalues (with same algebraic multiplicities).\n",
    "\n",
    "\n",
    "**commentary: ** \n",
    "\n",
    "There is an alternative proof which shows a slightly stronger claim using only Vandermonde Matrices after this one. Strictly speaking this claim only requires the traces to be equivalent for $k = \\{1, 2, 3, ..., \\big(max(m,n) + 1\\big)^2\\}$, i.e. the number of iterations is approximately the size of the bigger dimension, squared, whereas in the alternative proof, the number of iterations is required to be only approx twice the size of the bigger dimension (I.e. two full 'cycles').  Your author belives that with considerably more work the claim could be made while requiring only one cycle instead of two cycle, though the tighter claim would require considerably more machinery and not be as intuitive.  Hence in a manner like teaching Kosaraju's algorithm instead of Tarjan's for Strongly Connected Components, the 2 cycle approach is emphasized as it is considerably more intuitive than the one cycle approach. Of course, both approaches are big Oh of one cycle.  (That said, an illustrative sketch of the one cycle approach is given at the end of this posting, under the heading of \"Enter the Companion Matrix\")\n",
    "\n",
    "The idea of this proof is that the trace of a matrix raised to repeated powers gives a 'signatures' that uniquely specifies the non-zero eigenvalues.  The idea may be useful in cases where perhaps we know the traces, or even the eigenvalues, of $\\mathbf X$ and want to make inferences about $\\mathbf Y$.  \n",
    "\n",
    "One interpretation underpinning this is that there is a way to recreat a characteristic polynomial purely from traces of matrix powers -- so of course the trace of repeated matrix powers 'gives away' the eigenvalues.  An alternative interpretation comes in the analogy with the method of moments in probability -- if we have $E[X^k]$ for $k = \\{1, 2, 3, ...,\\}$, then the underlying probability distribution is uniquely specified.  Notice that $\\frac{1}{m} trace\\big(\\mathbf X^k\\big)$ is analogous to $E[X^k]$ (though there is the question as to whether we want to us the m total eigenvalues in $\\frac{1}{m}$, or perhaps instead just use $\\frac{1}{s}$ where $s:= n - z$, where we have z eigenvalues equal to zero.  In any case the analogoy with methods of moments is quite interesting.  \n",
    "\n",
    "*Subsequent note: My second approach, including collecting 2n - 1 iterations of the \"expected values\" in a Hankel matrix, seems awfully similar to that used in certain cases for the \"Hamburger Moment Problem\", https://en.wikipedia.org/wiki/Hamburger_moment_problem#Uniqueness_of_solutions .  It is unfortunate, but perhaps not surprising that my 'discoveries' seem to have been done before.*\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**proof:**\n",
    "\n",
    "for convenience notice that, if for $r = \\{1, 2, 3, ... , max(m,n)\\}$\n",
    "\n",
    "$trace\\big(\\mathbf X^r\\big) = trace\\big(\\mathbf Y^r\\big)  = 0 $, then both $\\mathbf X$ and $\\mathbf Y$ are nilpotent.  \n",
    "\n",
    "*The rest of the writeup assumes that they are not nilpotent matrices.*\n",
    "\n",
    "Now suppose we know the eigenvalues of $\\mathbf X$, and in particular the non-zero eigenvalues of $\\mathbf X$.  Then we know the characteristic polynomial, $p$ and use Cayley Hamilton to see the below mapping:  \n",
    "\n",
    "$p\\big(\\mathbf X\\big) = c_0 \\mathbf I + c_1 \\mathbf X + c_2 \\mathbf X^2 + ... + c_{n-1}\\mathbf X^{n-1} + c_{n}\\mathbf X^n = c_0 \\mathbf I +  \\Sigma_{j=1}^{n} c_j \\mathbf X^j = \\bigg[\\begin{array}{c|c|c|c}\n",
    "  \\mathbf 0 & \\mathbf 0 &\\cdots & \\mathbf 0\n",
    "\\end{array}\\bigg]$\n",
    "\n",
    "We right multiply the above by $\\mathbf X$\n",
    "\n",
    "$p\\big(\\mathbf X\\big)\\mathbf X = c_0 \\mathbf X + c_1 \\mathbf X^2 + c_2 \\mathbf X^3 + ... + c_{n-1}\\mathbf X^{n} + c_{n}\\mathbf X^{n+1} =\\bigg[\\begin{array}{c|c|c|c}\n",
    "  \\mathbf 0 & \\mathbf 0 &\\cdots & \\mathbf 0\n",
    "\\end{array}\\bigg] \\mathbf X = \\bigg[\\begin{array}{c|c|c|c}\n",
    "  \\mathbf 0 & \\mathbf 0 &\\cdots & \\mathbf 0\n",
    "\\end{array}\\bigg]$\n",
    "\n",
    "Notice that if we square the above, we have \n",
    "\n",
    "$\\big(p\\big(\\mathbf X\\big)\\mathbf X\\big)^2 = \\big(c_0 \\mathbf X + c_1 \\mathbf X^2 + c_2 \\mathbf X^3 + ... + c_{n-1}\\mathbf X^{n} + c_{n}\\mathbf X^{n+1}\\big)^2 = \\bigg[\\begin{array}{c|c|c|c}\n",
    "  \\mathbf 0 & \\mathbf 0 &\\cdots & \\mathbf 0\n",
    "\\end{array}\\bigg] $\n",
    "\n",
    "$\\big(p\\big(\\mathbf X\\big)\\mathbf X\\big)^2 = c_0^2 \\mathbf X^2 + c_1^2 \\mathbf X^4 + c_2^2 \\mathbf X^6 + ... + c_{n-1}^2\\mathbf X^{2n} + c_{n}^2\\mathbf X^{2n+2} + 2c_0 c_1 \\mathbf X^3 + 2c_1 c_2 \\mathbf X^5 + ... + 2c_{n-1}c_{n}  \\mathbf X^{2n+1}= \\bigg[\\begin{array}{c|c|c|c}\n",
    "  \\mathbf 0 & \\mathbf 0 &\\cdots & \\mathbf 0\n",
    "\\end{array}\\bigg]$\n",
    "\n",
    "or equivalently\n",
    "\n",
    "\n",
    "$\\big(p\\big(\\mathbf X\\big)\\mathbf X\\big)^2 = \\big(\\Sigma_{j=1}^{n+1} (c_{j-1})\\mathbf X^j\\big) \\big(\\Sigma_{j=1}^{n+1} (c_{j-1})\\mathbf X^j\\big) = \\Sigma_i \\gamma_i \\mathbf X^i = \\bigg[\\begin{array}{c|c|c|c}\n",
    "  \\mathbf 0 & \\mathbf 0 &\\cdots & \\mathbf 0\n",
    "\\end{array}\\bigg]$\n",
    "\n",
    "where $\\gamma_i$ is the appropriate scalar that comes from multiplying out the respective $c_j$ terms.  We will return to this momentarily.\n",
    "\n",
    "Taking the trace of $\\big(p\\big(\\mathbf X\\big)\\mathbf X \\big)$, we see:  \n",
    "\n",
    "$trace\\big(p\\big(\\mathbf X\\big)\\mathbf X \\big) = c_0 trace\\big(\\mathbf X\\big) + c_1 trace\\big(\\mathbf X^2\\big) + c_2 trace\\big(\\mathbf X^3\\big) + ... + c_{n-1} trace\\big(\\mathbf X^{n}\\big) + c_{n} trace\\big(\\mathbf X^{n+1}\\big) = 0$\n",
    "\n",
    "but this is equivalent to \n",
    "\n",
    "$trace\\big(p\\big(\\mathbf Y\\big)\\mathbf Y \\big) = c_0 trace\\big(\\mathbf Y\\big) + c_1 trace\\big(\\mathbf Y^2\\big) + c_2 trace\\big(\\mathbf Y^3\\big) + ... + c_{n-1} trace\\big(\\mathbf Y^{n}\\big) + c_{n} trace\\big(\\mathbf Y^{n+1}\\big) = 0$\n",
    "\n",
    "\n",
    "and more generally, we see that \n",
    "\n",
    "for $r = \\{1, 2, 3, ... , max(m,n)\\}$\n",
    "\n",
    "$trace\\Big( \\big(p\\big(\\mathbf X\\big)\\mathbf X \\big)\\big)^r\\Big)= trace\\Big(\\big(\\Sigma_{j=1}^{n+1} (c_{j-1})\\mathbf X^j\\big)^r\\Big) = trace\\big(\\Sigma_i \\gamma_i \\mathbf X^i\\big) =\\Sigma_i \\gamma_i trace\\big(\\mathbf X^i\\big) = 0$\n",
    "\n",
    "where again, $\\gamma_i$ indicates the scalar result of multiplying the relevant $c_j$ terms.  We then recall that for each term in this finite series, for the relevant natural numbers $i$, \n",
    "\n",
    "$trace\\big(\\mathbf X^i\\big) = trace\\big(\\mathbf Y^i\\big)$  \n",
    "\n",
    "and scaling this by some $\\gamma_i$,\n",
    "\n",
    "$\\gamma_i trace\\big(\\mathbf X^i\\big) = \\gamma_i trace\\big(\\mathbf Y^i\\big)$  \n",
    "\n",
    "hence \n",
    "\n",
    "$0 = \\Sigma_i \\gamma_i trace\\big(\\mathbf X^i\\big) = \\Sigma_i \\gamma_itrace\\big(\\mathbf Y^i\\big)$  \n",
    "\n",
    "We then conclude that for $r = \\{1, 2, 3, ... , max(m,n)\\}$\n",
    "\n",
    "$trace\\Big( \\big(p\\big(\\mathbf X\\big)\\mathbf X \\big)\\big)^r\\Big) = trace\\Big( \\big(p\\big(\\mathbf Y\\big)\\mathbf Y \\big)\\big)^r\\Big) = 0$  \n",
    "\n",
    "We now know that the matrix given by $\\Big(p\\big(\\mathbf Y\\big)\\mathbf Y\\Big)$ is nilpotent.  Recalling that $p\\big(\\mathbf Y\\big)$ is just a finite series of $ \\mathbf Y^k$ with particular scalars applied for each appropriate $k$, we do Schur Decomposition and see  \n",
    "\n",
    "$ p\\big(\\mathbf Y\\big) = \\mathbf {QTQ}^H $ and $\\mathbf Y = \\mathbf {QRQ}^H$, then \n",
    "\n",
    "$\\Big(p\\big(\\mathbf Y\\big)\\mathbf Y\\Big) = \\Big(\\mathbf {QTQ}^H \\mathbf {QRQ}^H \\Big) = \\mathbf {QTRQ}^H$\n",
    "\n",
    "since $\\mathbf Y$ is not nilpotent (i.e. $\\mathbf R$ is not nilpotent) but $\\big(\\mathbf{TR}\\big)$ is nilpotent, this tells us that $\\mathbf T$ is strictly upper triangular -- i.e. $\\mathbf T$ is nilpotent, which means that the matrix given by $p\\big(\\mathbf Y\\big)$ is nilpotent.  \n",
    "\n",
    "We thus see that all non-zero diagonal elements of $\\mathbf R$ -- aka the all non-zero eigenvalues of $\\mathbf Y$ obey the characteristic polynomial given by $p$. \n",
    "\n",
    "At a bare minimum, the above shows that the set of unique non zero eigenvalues of $\\mathbf Y$ is a subset of the set of unique non zero eigenvalues of $\\mathbf X$.  We denote this as $\\lambda\\big(\\mathbf Y\\big) \\subset \\lambda\\big(\\mathbf X\\big)$. \n",
    "\n",
    "Now, do the exact same argument used above, except swap $\\mathbf X$ for $\\mathbf Y$. \n",
    "\n",
    "(In many programming languages we would say:  \n",
    "(a) $\\mathbf X, \\mathbf Y := \\mathbf Y, \\mathbf X$  \n",
    "(b) call on argument used above, once.\n",
    ")  \n",
    "\n",
    "At a minimum, doing that shows that the set of unique non zero eigenvalues of $\\mathbf X$ is a subset of the unique non zero eigenvalues of $\\mathbf Y$.  \n",
    "\n",
    "hence with respect to unique non-zero eigenvalues we have $\\lambda\\big(\\mathbf X\\big) \\subset \\lambda\\big(\\mathbf Y\\big)$ and from before, with respect to unique nonzero eigenvalues, we have $\\lambda\\big(\\mathbf Y\\big) \\subset \\lambda\\big(\\mathbf X\\big)$ which proves that with respect to unique non-zero eigenvalues $\\lambda\\big(\\mathbf Y\\big) = \\lambda\\big(\\mathbf X\\big)$.  \n",
    "\n",
    "As in the nilpotence by trace proof, we now collect these unique non-zero eigenvalues in a diagonal matrix $\\mathbf D$.  There are $t$ non-zero eigenvalues, and $\\mathbf D$ is $t$ x $t$.  \n",
    "\n",
    "Collect the algebraic multiplicities for these unique nonzero eigenvalues of $\\mathbf X$ in $\\mathbf a_x$ and collect the algebraic multiplicities for the unique nonzero eigenvalues of $\\mathbf Y$ in $\\mathbf a_y$.  (As reminder, because they are algebraic multiplicities, each entry in $\\mathbf a_x$ and $\\mathbf a_y$ must be an integer $\\geq 1$.)\n",
    "\n",
    "Thus we have \n",
    "$\\mathbf W = \\bigg[\\begin{array}{c|c|c|c|c}\n",
    "\\mathbf D^0 \\mathbf 1 & \\mathbf D^1 \\mathbf 1 & \\mathbf D^2 \\mathbf 1 &\\cdots & \\mathbf D^{t-1} \\mathbf 1\n",
    "\\end{array}\\bigg]$\n",
    "\n",
    "and we show that these traces are equal with the following expression:  \n",
    "\n",
    "\n",
    "$\\mathbf a_x^H \\mathbf D \\mathbf W = \\mathbf a_y^H \\mathbf D \\mathbf W $  \n",
    "$\\mathbf a_x^H \\big(\\mathbf D \\mathbf W\\big)\\big(\\mathbf D \\mathbf W\\big)^{-1} = \\mathbf a_x^H \\mathbf D \\mathbf W \\mathbf W^{-1} \\mathbf D^{-1} = \\mathbf a_x^H = \\mathbf a_y^H = \\mathbf a_y^H \\mathbf D \\mathbf W \\mathbf W^{-1} \\mathbf D^{-1}  = \\mathbf a_y^H  \\big(\\mathbf D \\mathbf W\\big)\\big(\\mathbf D \\mathbf W\\big)^{-1} $  \n",
    "\n",
    "hence $\\mathbf a_x^H = \\mathbf a_y^H$\n",
    "\n",
    "and equivalently: $\\mathbf a_x = \\mathbf a_y$\n",
    "\n",
    "and we see that $\\mathbf X$ and $\\mathbf Y$ not only have the same unique non-zero eigenvalues, but that each one of those unique non-zero eigenvalues has the same algebraic multiplicity.\n",
    "- - - -\n",
    "# Alternative proof that a large enough number of traces uniquely gives the eigenvalues of a matrix\n",
    "\n",
    "**claim: over a Complex (or real) field, for n x n matrix** $\\mathbf Y$ **and n x n matrix** $\\mathbf X$, if:\n",
    "\n",
    "$trace\\big(\\mathbf X^k \\big) = trace \\big(\\mathbf Y^k\\big)$\n",
    "\n",
    "for $k = \\{1, 2, 3,... , 2n-1\\}$,\n",
    "\n",
    "then $\\mathbf X$ and $\\mathbf Y$ have the same non-zero eigenvalues, with same algebraic multiplicities.  And because they have the same dimension, then they have the same number of non-zero eigenvalues as well.  \n",
    "\n",
    "**proof:**\n",
    "\n",
    "*The proof progresses in three stages.  First we prove that these matrices must have the same number of unique non-zero eigenvalues (cardinality of sets of non-zero eigenvalues).  Then we prove that these non-zero eigenvalues must be the same for both $\\mathbf X$ and $\\mathbf Y$.  Finally we prove that these unique non-zero eigenvalues has the same algebraic multiplicity in each case.  Since the matrices are the same dimension, this means they have the same number of eigenvalues equal to zero as well.  After this is done, we offer a couple of extensions / generalization. *  \n",
    "\n",
    "**part one**\n",
    "\n",
    "We start by supposing that $\\mathbf X$ has more unique non-zero eigenvalues than $\\mathbf Y$.  \n",
    "\n",
    "Thus we assume that $\\mathbf Y$ has $q$ unique non-zero eigenvalues and $\\mathbf X$ has $r$ unique non-zero eigenvalues, where $1 \\leq q \\lt r$.  Note that if $q = 0$, then $\\mathbf Y$ would be nilpotent (see earlier cell in this posting on nilpotence and full cycle trace relations), and $\\mathbf X$ would have to be nilpotent as well.  Thus we are interested in $1 \\leq q$.  \n",
    "\n",
    "We collect the algebraic multiplicities for the $q$ unique non-zero eigenvalues $\\mathbf Y$ in $\\mathbf a_y$.  As a reminder each entry in $\\mathbf a_y$ (and $\\mathbf a_x$) is a natural number $\\geq 1$.  \n",
    "\n",
    "We create a short, fat $q$ x $r$ Vandermonde matrix for $\\mathbf Y$, below \n",
    "\n",
    "$\\mathbf W_{\\mathbf Y} = \\begin{bmatrix}\n",
    "1 & \\lambda_1 & \\lambda_1^2 & \\dots  & \\lambda_1^{r-1}\\\\ \n",
    "1 & \\lambda_2 & \\lambda_2^2 & \\dots &  \\lambda_2^{r-1} \\\\ \n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\\\ \n",
    "1 & \\lambda_{q} & \\lambda_{q}^{2} & \\dots  & \\lambda_{q}^{r-1}\n",
    "\\end{bmatrix}= \\bigg[\\begin{array}{c|c|c|c|c}\n",
    "\\mathbf D^0 \\mathbf 1 & \\mathbf D^1 \\mathbf 1 & \\mathbf D^2 \\mathbf 1 &\\cdots & \\mathbf D^{r-1} \\mathbf 1\n",
    "\\end{array}\\bigg]$\n",
    "\n",
    "$rank\\big(\\mathbf W_{\\mathbf Y}\\big) = q$ \n",
    "\n",
    "now we setup the trace relation as \n",
    "\n",
    "$\\big(\\mathbf a_y^H\\big) \\mathbf D \\mathbf W_{\\mathbf Y} = \\big(\\mathbf 1^H Diag\\big(\\mathbf a_y^H \\big)\\big) \\mathbf D \\mathbf W_{\\mathbf Y} =\\big(\\mathbf 1^H Diag\\big(\\mathbf a_y\\big)\\big) \\mathbf D \\mathbf W_{\\mathbf Y} = \\begin{bmatrix} trace\\big(\\mathbf Y\\big) & trace\\big(\\mathbf Y^2\\big)  & trace\\big(\\mathbf Y^3\\big) & \\cdots & trace\\big(\\mathbf Y^r\\big) \\end{bmatrix}$  \n",
    "\n",
    "from here we build this out to an $r$ x $r$ matrix, where we have \n",
    "\n",
    "$\\mathbf H = \\mathbf H(r) = \\Bigg[\\begin{matrix}\n",
    "trace\\big(\\mathbf Y\\big)  & trace\\big(\\mathbf Y^2\\big)  & trace\\big(\\mathbf Y^3\\big)  &\\cdots  &trace\\big(\\mathbf Y^r\\big) \\\\ \n",
    "trace\\big(\\mathbf Y^2\\big)  & trace\\big(\\mathbf Y^3\\big)  & trace\\big(\\mathbf Y^4\\big)  & \\cdots & trace\\big(\\mathbf Y^{r+1}\\big) \\\\ \n",
    "trace\\big(\\mathbf Y^3\\big) &  trace\\big(\\mathbf Y^4\\big)& trace\\big(\\mathbf Y^5\\big)  & \\cdots &trace\\big(\\mathbf Y^{r+2}\\big) \\\\ \n",
    "\\vdots & \\vdots & \\vdots &  \\ddots & \\vdots \\\\ \n",
    "trace\\big(\\mathbf Y^r\\big) & trace\\big(\\mathbf Y^{r+1}\\big) &trace\\big(\\mathbf Y^{r+2}\\big)  & \\cdots & trace\\big(\\mathbf Y^{2r-1}\\big)\n",
    "\\end{matrix}\\Bigg]$  \n",
    "\n",
    "note: While there may be some LaTeX rendering issues, it is clear that the above matrix $\\mathbf H$ is a Hankel matrix.  It is symmetric, but if any of the entries are complex, it is not Hermitian. From here, notice that while the first row is given by\n",
    "\n",
    "$\\begin{bmatrix}\n",
    "1 & 1 & 1 & \\cdots  &1 \n",
    "\\end{bmatrix}Diag\\big(\\mathbf a_y\\big) \\mathbf D \\mathbf W_{\\mathbf Y}$\n",
    "\n",
    "the second row is given by \n",
    "\n",
    "$\\begin{bmatrix}\n",
    "\\lambda_1 & \\lambda_2 & \\lambda_3 & \\cdots  &\\lambda_q\n",
    "\\end{bmatrix}Diag\\big(\\mathbf a_y\\big) \\mathbf D \\mathbf W_{\\mathbf Y}$\n",
    "\n",
    "the third row is given by \n",
    "\n",
    "$\\begin{bmatrix}\n",
    "\\lambda_1^2 & \\lambda_2^2 & \\lambda_3^2 & \\cdots  &\\lambda_q^2 \\end{bmatrix}Diag\\big(\\mathbf a_y\\big) \\mathbf D \\mathbf W_{\\mathbf Y}$\n",
    "\n",
    "... and the final, rth row is given by \n",
    "\n",
    "$\\begin{bmatrix}\n",
    "\\lambda_1^{r-1} & \\lambda_2^{r-1} & \\lambda_3^{r-1} & \\cdots  &\\lambda_q^{r-1} \\end{bmatrix}Diag\\big(\\mathbf a_y\\big) \\mathbf D \\mathbf W_{\\mathbf Y}$\n",
    "\n",
    "thus we have the following matrix\n",
    "\n",
    "$\\big(\\mathbf W_{\\mathbf Y}^T Diag\\big(\\mathbf a_y\\big) \\mathbf D \\mathbf W_{\\mathbf Y}\\big) = \\mathbf H$\n",
    "\n",
    "notice that the above $\\mathbf W^T \\neq \\mathbf W^H$ except in the special case where all $\\lambda_i$'s are real.  The above matrix is square and it is *not* full rank. It is at most rank $q$ because $rank\\big(\\mathbf W_{\\mathbf Y}\\big) = q$, and as a reminder we have assumed $q \\lt r$.  \n",
    "\n",
    "Put differently $det\\big(\\mathbf H\\big) = 0$.  \n",
    "\n",
    "If we work through the exact same calculations, for $\\mathbf X$, we find that \n",
    "\n",
    "$\\mathbf W_{\\mathbf X}^T Diag\\big(\\mathbf a_y\\big) \\mathbf \\Lambda \\mathbf W_{\\mathbf X} = \\mathbf H$\n",
    "\n",
    "Note that $ \\mathbf \\Lambda$ has $r$ entries along its diagonal, i.e. $\\lambda_{i}$ for $i = \\{1, 2, ..., r\\}$. \n",
    "\n",
    "$\\mathbf W_{\\mathbf X}$ is an $r$ x $r$ matrix with each of those $r$ unique non-zero eigenvalues in a Vandermonde Matrix, i.e. \n",
    "\n",
    "$\\mathbf W_{\\mathbf X} = \\bigg[\\begin{array}{c|c|c|c|c}\n",
    "\\mathbf \\Lambda^0 \\mathbf 1 & \\mathbf\\Lambda^1 \\mathbf 1 & \\mathbf\\Lambda^2 \\mathbf 1 &\\cdots & \\mathbf \\Lambda^{r-1} \\mathbf 1\n",
    "\\end{array}\\bigg]$\n",
    "\n",
    "further notice that $a_{x, i}$ for $i = \\{1, 2, ...., r\\} \\geq 1$, hence $Diag\\big(\\mathbf a_x\\big)$  is invertible. And since each eigenvalue in $\\mathbf W_{\\mathbf X}$ is unique, the square matrix given by $\\mathbf W_{\\mathbf X}$ is full rank.  \n",
    "\n",
    "Thus we have \n",
    "\n",
    "$det\\big(\\mathbf W_{\\mathbf X}^T Diag\\big(\\mathbf a_x\\big) \\mathbf \\Lambda \\mathbf W_{\\mathbf X}\\big) = det\\big(\\mathbf H \\big) \\neq 0 = det\\big(\\mathbf H \\big) = det\\big(\\mathbf W_{\\mathbf Y}^T Diag\\big(\\mathbf a_y\\big) \\mathbf D \\mathbf W_{\\mathbf Y}\\big) $\n",
    "\n",
    "which is a contradiction.  Or equivalently\n",
    "\n",
    "$rank\\big(\\mathbf W_{\\mathbf X}^T Diag\\big(\\mathbf a_x\\big) \\mathbf \\Lambda \\mathbf W_{\\mathbf X}\\big) = rank\\big(\\mathbf H \\big) = r = rank\\big(\\mathbf H \\big) = rank\\big(\\mathbf W_{\\mathbf Y}^T Diag\\big(\\mathbf a_y\\big) \\mathbf D \\mathbf W_{\\mathbf Y}\\big) \\leq q$\n",
    "\n",
    "This is a contradiction, because we have assumed that $q \\lt r$, so it cannot be the case that $r \\leq q$.  \n",
    "\n",
    "We repeat the argument and suppose that $q \\gt r$, \n",
    "\n",
    "Thus $\\mathbf W_{\\mathbf Y}$ is a $q$ x $q$ Vandermonde matrix for $\\mathbf Y$, and $\\mathbf W_{\\mathbf X}$ is a short fat $r$ x $q$ Vandermonde matrix for $\\mathbf X$\n",
    "\n",
    "and $\\mathbf H = \\mathbf H(q)$\n",
    "\n",
    "And again we have\n",
    "\n",
    "$det\\big(\\mathbf W_{\\mathbf X}^T Diag\\big(\\mathbf a_x\\big) \\mathbf \\Lambda \\mathbf W_{\\mathbf X}\\big) = det\\big(\\mathbf H \\big) = 0 \\neq det\\big(\\mathbf H \\big) = det\\big(\\mathbf W_{\\mathbf Y}^T Diag\\big(\\mathbf a_y\\big) \\mathbf D \\mathbf W_{\\mathbf Y}\\big) $\n",
    "\n",
    "Which again is a contradiction.   Which means it cannot be the case that $q \\neq r$.  \n",
    "\n",
    "Notice that if both sides are composed solely of square matrices of dimension $r$, the relation of course can be true if $\\mathbf a_y =\\mathbf a_x$ and $\\mathbf D = \\mathbf \\Lambda$ (and allowing any permutation of the ordering of the eigenvalues contained in either of the diagonal matrices).  But it also seems possible that other configurations could exist as well.  The next step is to prove that that the unique non-zero eigenvalue that are contained on the diagonal elements of $\\mathbf D$ and $\\mathbf \\Lambda$ must be the same.  \n",
    "\n",
    "To conclude part 1, we have observed that the the number of unique non-zero eigenvalues for $\\mathbf Y$, given by $q$ must be equal to the number of unique non-zero eigenvalues of $\\mathbf X$, given by $r$.  \n",
    "\n",
    "**part 2** \n",
    "\n",
    "Thus $\\mathbf Y$ and $\\mathbf X$ each have $r$ unique non-zero eigenvalues.  We now need to prove these non-zero eigenvalues are the same.  \n",
    "\n",
    "To prove the first thing, we iterate through each unique non-zero eigenvalue for $\\mathbf X$, given in $\\{\\lambda_1, \\lambda_2, ..., \\lambda_r\\}$, or equivalently: $\\lambda_i$ for $i = \\{1, 2, ...., r\\}$ and consider whether or not there is at least least one case where \n",
    "\n",
    "$\\big(\\mathbf Y - \\lambda_i \\mathbf I\\big)$ is not a singular matrix but $\\big(\\mathbf X - \\lambda_i \\mathbf I\\big)$ is of course singular. \n",
    "\n",
    "Put differently, we are going to prove that $\\lambda_i$ is in fact an eigenvalue for $\\mathbf X$ and for $\\mathbf Y$ for $i = \\{1,2, 3,..., r\\}$.  Then we will have proved that the set of $r$ unique eigenvalues of $\\mathbf X$ is equivalent to set of $r$ unique non-zero eigenvalues contained in $\\mathbf Y$.  \n",
    "\n",
    "*begin note on multiplication*\n",
    "\n",
    "$\\big(\\mathbf X - \\lambda_i \\mathbf I\\big)^k = (-1)^k \\lambda_i ^k \\mathbf I + \\Sigma_{j=0}^{k-1} \\big(\\lambda_i^j (-1)^j \\binom{k}{j} \\mathbf X^{k-j}\\big)$\n",
    "\n",
    "by the binomial theorem, and the fact that a scaled version of the identity matrix commutes with any other matrix\n",
    "\n",
    "thus we have \n",
    "\n",
    "$trace\\Big(\\big(\\mathbf X - \\lambda_i \\mathbf I\\big)^k\\Big) = trace\\Big((-1)^k \\lambda_i ^k \\mathbf I + \\Sigma_{j=0}^{k-1} \\big(\\lambda_i^j (-1)^j \\binom{k}{j} \\mathbf X^{k-j}\\big)\\Big) = (-1)^k \\lambda_i^k trace\\big(\\mathbf I\\big) + \\Sigma_{j=0}^{k-1} \\lambda_i^j (-1)^j \\binom{k}{j} trace\\big(\\mathbf X^{k-j}\\big)$\n",
    "\n",
    "then notice\n",
    "\n",
    "$(-1)^k \\lambda_i ^k trace\\big(\\mathbf I\\big) + \\Sigma_{j=0}^{k-1} \\lambda_i^j (-1)^j \\binom{k}{j} trace\\big(\\mathbf X^{k-j}\\big) = (-1)^k \\lambda_i ^k trace\\big(\\mathbf I\\big) + \\Sigma_{j=0}^{k-1} \\lambda_i^j (-1)^j \\binom{k}{j} trace\\big(\\mathbf Y^{k-j}\\big) = trace\\Big(\\big(\\mathbf Y - \\lambda_i \\mathbf I\\big)^k \\Big)$\n",
    "\n",
    "thus\n",
    "\n",
    "$trace\\Big(\\big(\\mathbf X - \\lambda_i \\mathbf I\\big)^k\\Big) = trace\\Big(\\big(\\mathbf Y - \\lambda_i \\mathbf I\\big)^k\\Big)$\n",
    "\n",
    "for $k = \\{1, 2, ..., 2n-1\\}$\n",
    "\n",
    "*end note on multiplication*  \n",
    "\n",
    "As before we model out these relationships in Vandermonde matrices and form a Hankel matrix.  \n",
    "\n",
    "This time, a touch of special handling is needed.  \n",
    "\n",
    "If $\\mathbf X$ is singular, then we collect all of its  unique eigenvalues *including* the eigenvalue = zero in an $r+1$ x $r+1$ diagonal matrix and call this $\\mathbf \\Lambda$. If $\\mathbf X$ is non-singular, then $\\mathbf \\Lambda$ is $r$ x $r$.  \n",
    "\n",
    "The same apples for $\\mathbf Y$ and its unique eigenvalues, including that of zero, if applicable, are collected in $\\mathbf D$.  \n",
    "\n",
    "It is a bit awkward that the dimensions may not line up, but the argument is the same, just more verbose. \n",
    "\n",
    "As a reminder, in all cases $\\mathbf a_x$ and $\\mathbf a_y$ contain the algebraic multiplicities of each unique eigenvalue associated with $\\mathbf X$ and $\\mathbf Y$.  These multiplicities are *always* natural numbers $\\geq 1$.  Hence $Diag\\big(\\mathbf a_x\\big)$ and $Diag\\big(\\mathbf a_y\\big)$ are both always invertible.  \n",
    "\n",
    "**First**  \n",
    "assume both digaonal matrices containing eigenvalues for $\\mathbf X$ and $\\mathbf Y$ are each $r$ x $r$.\n",
    "\n",
    "$\\mathbf W_{\\mathbf X} = \\bigg[\\begin{array}{c|c|c|c|c}\n",
    " \\mathbf 1 & \\big(\\mathbf \\Lambda - \\lambda_i\\mathbf I\\big)^1 \\mathbf 1 & \\big(\\mathbf \\Lambda - \\lambda_i\\mathbf I\\big)^2 \\mathbf 1 &\\cdots & \\big(\\mathbf \\Lambda - \\lambda_i\\mathbf I\\big)^{r-1} \\mathbf 1\n",
    "\\end{array}\\bigg]$\n",
    "\n",
    "\n",
    "$\\mathbf W_{\\mathbf Y} = \\bigg[\\begin{array}{c|c|c|c|c}\n",
    " \\mathbf 1 & \\big(\\mathbf D - \\lambda_i\\mathbf I\\big)^1 \\mathbf 1 & \\big(\\mathbf D - \\lambda_i\\mathbf I\\big)^2 \\mathbf 1 &\\cdots & \\big(\\mathbf D - \\lambda_i\\mathbf I\\big)^{r-1} \\mathbf 1\n",
    "\\end{array}\\bigg]$\n",
    "\n",
    "\n",
    "Again, since we know $\\mathbf X$ and $\\mathbf Y$ have the same number of unique non-zero eigenvalues ($r$ of them, to be exact), then $\\mathbf X$ and $\\mathbf Y$ have the same eigenvalues if and only if for every $\\lambda_i$, $  \\big(\\mathbf Y - \\lambda_i \\mathbf I\\big)$ and $\\big(\\mathbf X - \\lambda_i \\mathbf I\\big)$ are both singular (i.e. in effect subracting $\\lambda_i \\mathbf I$ zeroes out at least one eigenvalue from $\\mathbf X$ by design and it also does so for $\\mathbf Y$ in all cases if they have the same set of unique non-zero eigenvalues)\n",
    "\n",
    "We are again interested in the trace relations, collected in the form of a Hankel matrix, this time given as, $\\mathbf H\\big(r\\big)$  \n",
    "\n",
    "$\\mathbf H = \\mathbf H\\big(r\\big) = \\Bigg[\\begin{matrix}\n",
    "trace\\big(\\big(\\mathbf X - \\lambda_i \\mathbf I \\big)^1\\big)  & trace\\big(\\big(\\mathbf X - \\lambda_i \\mathbf I\\big)^2\\big)  & trace\\big(\\big(\\mathbf X - \\lambda_i \\mathbf I\\big)^3 \\big)  &\\cdots  &trace\\big( \\big(\\mathbf X - \\lambda_i\\mathbf I\\big)^r\\big) \\\\ \n",
    "trace\\big(\\big(\\mathbf X - \\lambda_i \\mathbf I \\big)^2\\big)  & trace\\big(\\big(\\mathbf X - \\lambda_i \\mathbf I\\big)^3\\big)  & trace\\big(\\big(\\mathbf X - \\lambda_i \\mathbf I\\big)^4 \\big)  &\\cdots  &trace\\big( \\big(\\mathbf X - \\lambda_i \\mathbf I \\big)^{r+1}\\big) \\\\ \n",
    "\\vdots & \\vdots & \\vdots &  \\ddots & \\vdots \\\\ \n",
    "trace\\big(\\big(\\mathbf X - \\lambda_i \\mathbf I \\big)^r\\big)  & trace\\big(\\big(\\mathbf X - \\lambda_i \\mathbf I\\big)^{r+1}\\big)  & trace\\big(\\big(\\mathbf X - \\lambda_i \\mathbf I\\big)^{r+2} \\big)  &\\cdots  &trace\\big( \\big(\\mathbf X - \\lambda_i\\mathbf I\\big)^{2r-1}\\big) \\end{matrix}\\Bigg]$  \n",
    "\n",
    "We summarize this relationship as:  \n",
    "\n",
    "$\\mathbf W_{\\mathbf Y}^T Diag\\big(\\mathbf a_y\\big)\\big( \\mathbf D - \\lambda_i \\mathbf I\\big) \\mathbf W_{\\mathbf Y} = \\mathbf H = \\mathbf W_{\\mathbf X}^T Diag\\big(\\mathbf a_x\\big)\\big( \\mathbf \\Lambda - \\lambda_i\\mathbf I\\big)\\mathbf W_{\\mathbf X} $\n",
    "\n",
    "Notice that by construction the right hand side is singular aka has determinant = 0.  (Again, the simplest reason is that $\\big( \\mathbf \\Lambda -\\lambda_i\\big)$ is a diagonal matrix with a zero on its diagonal.)\n",
    "\n",
    "Thus $\\mathbf H$ must be rank deficient, aka $det\\big(\\mathbf H\\big) = 0$ which means that the left hand side must be rank deficient as well.  Everything on the left hand side is square and hence easy to work with.  As always, $Diag\\big(\\mathbf a_y\\big)$ is invertible, and our Vandermonde matrix $\\mathbf W_{\\mathbf Y}$ is invertible, and so is its transpose,  thus, the left hand side, is singular if and only if $\\big(\\mathbf D - \\lambda_i\\mathbf I\\big)$ has a zero along its diagonal. This occurs if and only if $\\lambda_i$ is an eigenvalue of $\\mathbf Y$, and thus we conlcude that $\\lambda_i$ is an eigenvalue for $\\mathbf Y$.  \n",
    "\n",
    "**Second**  \n",
    "assume the diagonal matrix $\\mathbf \\Lambda$ containing the unique eigenvalues for $\\mathbf X$ is $r+1$ x $r + 1$ and the same for $\\mathbf Y$ (i.e. $\\mathbf D$ is $r+1$ x $r + 1$ as well.)  \n",
    "\n",
    "\n",
    "$\\mathbf W_{\\mathbf X} = \\bigg[\\begin{array}{c|c|c|c|c}\n",
    " \\mathbf 1 & \\big(\\mathbf \\Lambda - \\lambda_i\\mathbf I\\big)^1 \\mathbf 1 & \\big(\\mathbf \\Lambda - \\lambda_i\\mathbf I\\big)^2 \\mathbf 1 &\\cdots & \\big(\\mathbf \\Lambda - \\lambda_i\\mathbf I\\big)^{r} \\mathbf 1\n",
    "\\end{array}\\bigg]$\n",
    "\n",
    "\n",
    "$\\mathbf W_{\\mathbf Y} = \\bigg[\\begin{array}{c|c|c|c|c}\n",
    " \\mathbf 1 & \\big(\\mathbf D - \\lambda_i\\mathbf I\\big)^1 \\mathbf 1 & \\big(\\mathbf D - \\lambda_i\\mathbf I\\big)^2 \\mathbf 1 &\\cdots & \\big(\\mathbf D - \\lambda_i\\mathbf I\\big)^{r} \\mathbf 1\n",
    "\\end{array}\\bigg]$\n",
    "\n",
    "$\\mathbf H = \\mathbf H\\big(r +1\\big)$\n",
    "\n",
    "$\\mathbf H = \\mathbf H\\big(r + 1\\big) = \\Bigg[\\begin{matrix}\n",
    "trace\\big(\\big(\\mathbf X - \\lambda_i \\mathbf I \\big)^1\\big)  & trace\\big(\\big(\\mathbf X - \\lambda_i \\mathbf I\\big)^2\\big)  & trace\\big(\\big(\\mathbf X - \\lambda_i \\mathbf I\\big)^3 \\big)  &\\cdots  &trace\\big( \\big(\\mathbf X - \\lambda_i\\mathbf I \\big)^{r+1}\\big) \\\\ \n",
    "trace\\big(\\big(\\mathbf X - \\lambda_i \\mathbf I \\big)^2\\big)  & trace\\big(\\big(\\mathbf X - \\lambda_i \\mathbf I\\big)^3\\big)  & trace\\big(\\big(\\mathbf X - \\lambda_i \\mathbf I\\big)^4 \\big)  &\\cdots  &trace\\big( \\big(\\mathbf X - \\lambda_i\\mathbf I\\big)^{r+2}\\big)\\\\ \n",
    "\\vdots & \\vdots & \\vdots &  \\ddots & \\vdots \\\\ \n",
    "trace\\big(\\big(\\mathbf X - \\lambda_i \\mathbf I \\big)^r\\big)  & trace\\big(\\big(\\mathbf X - \\lambda_i \\mathbf I\\big)^{r+1}\\big)  & trace\\big(\\big(\\mathbf X - \\lambda_i \\mathbf I\\big)^{r+2} \\big)  &\\cdots  &trace\\big( \\big(\\mathbf X - \\lambda_i\\mathbf I \\big)^{2r}\\big) \\\\ \n",
    "trace\\big(\\big(\\mathbf X - \\lambda_i \\mathbf I\\big)^{r+1}\\big) & trace\\big(\\big(\\mathbf X - \\lambda_i \\mathbf I\\big)^{r+2} \\big) & trace\\big(\\big(\\mathbf X - \\lambda_i \\mathbf I\\big)^{r+3} \\big)  &\\cdots  &trace\\big( \\big(\\mathbf X - \\lambda_i\\mathbf I\\big)^{2r+1}\\big) \n",
    "\\end{matrix}\\Bigg]$  \n",
    "\n",
    "\n",
    "\n",
    "$\\mathbf W_{\\mathbf Y}^T Diag\\big(\\mathbf a_y\\big)\\big( \\mathbf D - \\lambda_i \\mathbf I\\big) \\mathbf W_{\\mathbf Y} = \\mathbf H = \\mathbf W_{\\mathbf X}^T Diag\\big(\\mathbf a_x\\big)\\big( \\mathbf \\Lambda -\\lambda_i\\mathbf I\\big)\\mathbf W_{\\mathbf X} $\n",
    "\n",
    "And again, the right hand side is singular aka has determinant = 0.  (Again, the simplest reason is that $\\big( \\mathbf \\Lambda -\\lambda_i\\big)$ is a diagonal matrix with a zero on its diagonal.)  So the left hand side must be as well, and thus we conclude there is a zero along the diagonal of $\\big(\\mathbf D - \\lambda_i \\mathbf I\\big)$, i.e. that $\\lambda_i$ is an eigenvalue of $\\mathbf Y$ as well.\n",
    "\n",
    "\n",
    "**Third**   \n",
    "assume the diagonal matrix containing unique eigenvalues for $\\mathbf X$ is $r$ x $r$ and the respetive one for $\\mathbf Y$'s is $r + 1$ x $r + 1$.\n",
    "\n",
    "shorter, slightly fatter  \n",
    "$\\mathbf W_{\\mathbf X} = \\bigg[\\begin{array}{c|c|c|c|c}\n",
    " \\mathbf 1 & \\big(\\mathbf \\Lambda - \\lambda_i\\mathbf I\\big)^1 \\mathbf 1 & \\big(\\mathbf \\Lambda - \\lambda_i\\mathbf I\\big)^2 \\mathbf 1 &\\cdots & \\big(\\mathbf \\Lambda - \\lambda_i\\mathbf I\\big)^{r} \\mathbf 1\n",
    "\\end{array}\\bigg]$\n",
    "\n",
    "square  \n",
    "$\\mathbf W_{\\mathbf Y} = \\bigg[\\begin{array}{c|c|c|c|c}\n",
    " \\mathbf 1 & \\big(\\mathbf D - \\lambda_i\\mathbf I\\big)^1 \\mathbf 1 & \\big(\\mathbf D - \\lambda_i\\mathbf I\\big)^2 \\mathbf 1 &\\cdots & \\big(\\mathbf D - \\lambda_i\\mathbf I\\big)^{r} \\mathbf 1\n",
    "\\end{array}\\bigg]$\n",
    "\n",
    "\n",
    "$\\mathbf H = \\mathbf H\\big( r+1\\big)$ \n",
    "i.e. the same as in the second case  \n",
    "\n",
    "$\\mathbf W_{\\mathbf Y}^T Diag\\big(\\mathbf a_y\\big)\\big( \\mathbf D - \\lambda_i \\mathbf I\\big) \\mathbf W_{\\mathbf Y} = \\mathbf H = \\mathbf W_{\\mathbf X}^T Diag\\big(\\mathbf a_x\\big)\\big( \\mathbf \\Lambda -\\lambda_i\\mathbf I\\big)\\mathbf W_{\\mathbf X} $\n",
    "\n",
    "\n",
    "Again, the right hand side is rank deficient which means that $det\\big(\\mathbf H\\big) = 0$. \n",
    "\n",
    "The left hand side thus must be rank deficient as well.  So we conclude that $\\lambda_i$ must be an eigenvalue of $\\mathbf Y$ in this case, too.  \n",
    "\n",
    "\n",
    "**Fourth**   \n",
    "assume the diagonal matrix containing unique eigenvalues for $\\mathbf X$ is $r+1$ x $r+1$ and for $\\mathbf Y$ it is $r$ x $r$.\n",
    "\n",
    "\n",
    "taller, slightly skinnier  \n",
    "$\\mathbf W_{\\mathbf X} = \\bigg[\\begin{array}{c|c|c|c|c}\n",
    " \\mathbf 1 & \\big(\\mathbf \\Lambda - \\lambda_i\\mathbf I\\big)^1 \\mathbf 1 & \\big(\\mathbf \\Lambda - \\lambda_i\\mathbf I\\big)^2 \\mathbf 1 &\\cdots & \\big(\\mathbf \\Lambda - \\lambda_i\\mathbf I\\big)^{r-1} \\mathbf 1\n",
    "\\end{array}\\bigg]$\n",
    "\n",
    "square  \n",
    "$\\mathbf W_{\\mathbf Y} = \\bigg[\\begin{array}{c|c|c|c|c}\n",
    " \\mathbf 1 & \\big(\\mathbf D - \\lambda_i\\mathbf I\\big)^1 \\mathbf 1 & \\big(\\mathbf D - \\lambda_i\\mathbf I\\big)^2 \\mathbf 1 &\\cdots & \\big(\\mathbf D - \\lambda_i\\mathbf I\\big)^{r-1} \\mathbf 1\n",
    "\\end{array}\\bigg]$\n",
    "\n",
    "\n",
    "$\\mathbf H = \\mathbf H\\big(r\\big)$\n",
    "i.e. the same as in the first case  \n",
    "\n",
    "$\\mathbf W_{\\mathbf Y}^T Diag\\big(\\mathbf a_y\\big)\\big( \\mathbf D - \\lambda_i \\mathbf I\\big) \\mathbf W_{\\mathbf Y} = \\mathbf H = \\mathbf W_{\\mathbf X}^T Diag\\big(\\mathbf a_x\\big)\\big( \\mathbf \\Lambda -\\lambda_i \\mathbf I\\big)\\mathbf W_{\\mathbf X} $\n",
    "\n",
    "\n",
    "And once again the right hand side is rank deficient which means that $det\\big(\\mathbf H\\big) = 0$. Notice that the left hand side is easy to evaluate because everything is square and the argument is the same as above.  We know that here, too, there must be a zero along the diagonal of $\\big( \\mathbf D - \\lambda_i \\mathbf I\\big)$ for each and every $\\lambda_i$. \n",
    "\n",
    "\n",
    "**final step**  \n",
    "\n",
    "In all cases we know that $\\lambda_i \\neq 0$  is an eigenvalue for $\\mathbf X$ and also for $\\mathbf Y$.  Since $\\mathbf X$ and $\\mathbf Y$ have the same number of unique non-zero eigenvalues, after enumerating each of the $r$ unique non-zero eigenvalues of $\\mathbf X$ -- i.e. $\\lambda_i$ for $i = \\{1, 2, 3, ..., r\\}$ -- we will have necessarily enumerated each of the unique non-zero eigenvalues for $\\mathbf Y$ as well. \n",
    "\n",
    "Thus we know that $\\mathbf X$ and $\\mathbf Y$ have the same set of unique non-zero eigenvalues ($r$ of them in total).  It remains for us to prove that these eigenvalues have the same algebraic multiplicities.  After proving that, it then naturally follows, since both matrices are n x n,  that they have the same number of eigenvalues equal to zero (i.e. sum of algebraic multiplicities of non-zero unique eigenvalues + algebraic multiplictity of zeros = $n$).  \n",
    "\n",
    "*To finish off the proof*  \n",
    "\n",
    "At this point we have $r$ sytems of equations for both $\\mathbf X$ and $\\mathbf Y$, that is:  \n",
    "\n",
    "$\\sum_{i=1}^{r} a_{y,i}\\lambda_{y,i}^k = \\sum_{i=1}^{r} a_{y,i}\\lambda_{x,i}^k = trace\\big(\\mathbf Y^k\\big) = trace\\big(\\mathbf X^k\\big) = \\sum_{i=1}^{r} a_{x,i}\\lambda_{y,i}^k = \\sum_{i=1}^{r} a_{x,i}\\lambda_{x,i}^k$  \n",
    "\n",
    "for $k = \\{1, 2, ..., r\\}$, recalling that $\\lambda_{y_i} = \\lambda_{x,i}$\n",
    "\n",
    "For both $\\mathbf X$ and $\\mathbf Y$ they have the same scalar result, *and* these are $r$ equations with $r$ terms are linearly indepedendent.  (Why are they linearly independent? No matter how we encode them in a Vandermonde matrix, said matrix is square with determinant $\\neq 0$, aka it is full rank.) We can 'eyeball' the above and see it is consistent if $a_{y,i} = a_{x,i}$.  Since we have $r$ terms in $r$ linearly independent equations, we know that there is one and only one solution to the above equation, and hence the 'eyeyball' one is unique.  \n",
    "\n",
    "Alternatively, we can once again explicitly use a Vandermonde matrix, as shown below.  \n",
    "\n",
    "$\\mathbf a_x^H \\mathbf D \\mathbf W_{\\mathbf X} = \\mathbf a_y^H \\mathbf D \\mathbf W_{\\mathbf X} = \\begin{bmatrix} trace\\big(\\mathbf X\\big) & trace\\big(\\mathbf X^2\\big)  & trace\\big(\\mathbf X^3\\big) & \\cdots & trace\\big(\\mathbf X^r\\big) \\end{bmatrix} $\n",
    "\n",
    "$\\mathbf a_x^H  = \\mathbf a_y^H = \\begin{bmatrix} trace\\big(\\mathbf X\\big) & trace\\big(\\mathbf X^2\\big)  & trace\\big(\\mathbf X^3\\big) & \\cdots & trace\\big(\\mathbf X^r\\big) \\end{bmatrix}\\big(\\mathbf D \\mathbf W_{\\mathbf X}\\big)^{-1}  $\n",
    "\n",
    "- - - - \n",
    "or if the reader prefers: \n",
    "\n",
    "$ \\mathbf a_y^H \\mathbf \\Lambda \\mathbf W_{\\mathbf Y} = \\mathbf a_x^H \\mathbf \\Lambda \\mathbf W_{\\mathbf Y}= \\begin{bmatrix} trace\\big(\\mathbf Y\\big) & trace\\big(\\mathbf Y^2\\big)  & trace\\big(\\mathbf Y^3\\big) & \\cdots & trace\\big(\\mathbf Y^r\\big) \\end{bmatrix} $\n",
    "\n",
    "$\\mathbf a_y^H = \\mathbf a_x^H  = \\begin{bmatrix} trace\\big(\\mathbf Y\\big) & trace\\big(\\mathbf Y^2\\big)  & trace\\big(\\mathbf Y^3\\big) & \\cdots & trace\\big(\\mathbf Y^r\\big) \\end{bmatrix}\\big(\\mathbf \\Lambda \\mathbf W_{\\mathbf Y}\\big)^{-1}$\n",
    "- - - - \n",
    "\n",
    "hence $\\mathbf a_x^H = \\mathbf a_y^H$\n",
    "\n",
    "and equivalently: $\\mathbf a_x = \\mathbf a_y$\n",
    "\n",
    "Thus we know that $\\mathbf X$ and $\\mathbf Y$ have the same non-zero eigenvalues with same algebraic multiplicities, and in fact have the same number of zeros, as well.  \n",
    "\n",
    "Thus, if $trace\\big(\\mathbf X^k\\big) = trace\\big(\\mathbf Y^k \\big)$  for natural numbers $k = \\{1, 2, 3, ..., 2n-1\\}$, then $\\mathbf X$ and $\\mathbf Y$ have the same non-zero eigenvalues (with same algebraic multiplicity for each non-zero eigenvalue).\n",
    "\n",
    "\n",
    "**extension:**\n",
    "\n",
    "consider the more general case where $trace\\big(\\mathbf X^k\\big) = trace\\big(\\mathbf Y^k \\big)$  for natural numbers $k = \\{1, 2, 3, ..., 2n-1\\}$  \n",
    "\n",
    "but $\\mathbf X$ is $m$ x $m$ and $\\mathbf Y$ is $n$ x $n$, where for simplicity, we assume $n \\gt m$.  \n",
    "\n",
    "The easiest way to extend this is if we take each of the eigenvalues (including repeats) of $\\mathbf X$, and place them in the first $m$ diagonal entries of an $n$ x $n$ zero matrix, $\\mathbf Z$. Thus $\\mathbf Z$ becomes a diagonal matrix, and if $\\mathbf X$ had $s$ non-zero (including repeats) eigenvalues, then there will be the respective $s$ non-zero entries along the diagonal of $\\mathbf Z$.  \n",
    "\n",
    "Alternatively, we *could* make $\\mathbf Z$ be equal to $\\mathbf X$, but we just append rows zeros below and columns of zeros to the right, until $\\mathbf Z$ has the same dimension as $\\mathbf Y$.  This alternative approach directly preserves the traces of $\\mathbf X$ and clearly is appending eigenvalues of zero to it as well.  The former approach of explicitly creating $\\mathbf Z$ with $\\mathbf X$'s eigenvalues is a bit easier to work so that is what we discuss below.  \n",
    "\n",
    "Thus by construction, we know that $\\mathbf X$ has the same non-zero eigenvalues as $\\mathbf Z$.  We also know that \n",
    "\n",
    "$trace\\big(\\mathbf Z^k\\big) = trace\\big(\\mathbf X^k\\big) = trace\\big(\\mathbf Y^k \\big)$  for natural numbers $k = \\{1, 2, 3, ..., 2n-1\\}$\n",
    "\n",
    "now repeat the argument used above, with respect to the same dimensioned $\\mathbf X$ and $\\mathbf Y$, except this time use it on $\\mathbf Z$ and $\\mathbf Y$.  We find that $\\mathbf Z$ and $\\mathbf Y$ have the same non-zero eigenvalues with same algebraic multiplicities, and because $\\mathbf Z$ has the same non-zero eigenvalues with same algebraic multiplicities as $\\mathbf X$, we know that $\\mathbf X$ and $\\mathbf Y$ have the same non-zero eigenvalues with same algebraic multiplicities.  \n",
    "\n",
    "\n",
    "# extension: \n",
    "**for some some $m$ x $n$ matrix ** $\\mathbf G$ **and some $n$ x $m$ matrix ** $\\mathbf H$, then $\\mathbf {GH}$ and $\\mathbf {HG}$ have the same non-zero eigenvalues (in terms of algebraic multiplicity).\n",
    "\n",
    "first notice \n",
    "\n",
    "$trace\\Big(\\big(\\mathbf {GH}\\big)\\Big) = trace\\Big(\\big(\\mathbf {HG}\\big)\\Big)$\n",
    "\n",
    "via the cyclic property of the trace.  Now in general, we can say\n",
    "\n",
    "for $r = \\{2, 3, ... \\}$\n",
    "\n",
    "$trace\\Big(\\big(\\mathbf {GH}\\big)^r\\Big) = trace\\Big(\\mathbf{GH} \\big(\\mathbf G\\mathbf H\\big)^{r-1}\\Big) = trace\\Big( \\mathbf{H} \\big(\\mathbf G\\mathbf H\\big)^{r-1} \\mathbf G\\Big) = trace\\Big(\\big(\\mathbf {HG}\\big)^r\\Big)$\n",
    "\n",
    "thus we have \n",
    "\n",
    "$trace\\Big(\\big(\\mathbf {GH}\\big)^k\\Big) = trace\\Big(\\big(\\mathbf {HG}\\big)^k\\Big)$ \n",
    "\n",
    "for $k = \\{1, 2,  3, ... \\}$\n",
    "\n",
    "We now apply either of the two preceding proofs, and know that $\\big(\\mathbf {GH}\\big)$ has the same non-zero eigenvalues (with the same algebraic multiplicties) as $\\big(\\mathbf {HG}\\big)$ \n",
    "\n",
    "\n",
    "# extension:  \n",
    "\n",
    "for some $m$ x $m$ matrix $\\mathbf Y$, we can determine the number of unique non-zero eigenvalues it has by collecting its traces in a Hankel matrix.  (Note that we can determine whether or not it has an eigenvalue of zero via the use of determinants, or Gaussian Elimination, or whatever other nullspace oriented tool.)\n",
    "\n",
    "i.e. where we have  \n",
    "\n",
    "$\\mathbf H(m) = \\Bigg[\\begin{matrix}\n",
    "trace\\big(\\mathbf Y\\big)  & trace\\big(\\mathbf Y^2\\big)  & trace\\big(\\mathbf Y^3\\big)  &\\cdots  &trace\\big(\\mathbf Y^m\\big) \\\\ \n",
    "trace\\big(\\mathbf Y^2\\big)  & trace\\big(\\mathbf Y^3\\big)  & trace\\big(\\mathbf Y^4\\big)  & \\cdots & trace\\big(\\mathbf Y^{m+1}\\big) \\\\ \n",
    "trace\\big(\\mathbf Y^3\\big) &  trace\\big(\\mathbf Y^4\\big)& trace\\big(\\mathbf Y^5\\big)  & \\cdots &trace\\big(\\mathbf Y^{m+2}\\big) \\\\ \n",
    "\\vdots & \\vdots & \\vdots &  \\ddots & \\vdots \\\\ \n",
    "trace\\big(\\mathbf Y^m\\big) & trace\\big(\\mathbf Y^{m+1}\\big) &trace\\big(\\mathbf Y^{m+2}\\big)  & \\cdots & trace\\big(\\mathbf Y^{2m-1}\\big)\n",
    "\\end{matrix}\\Bigg]$  \n",
    "\n",
    "\n",
    "$rank\\big(\\mathbf H(m)\\big) = $ number of unique non-zero eigenvalues\n",
    "\n",
    "- - - - \n",
    "# Enter the Companion Matrix \n",
    "\n",
    "Over a complex field, let $\\mathbf C$ be the $n$ x $n$ matrix callled the Companion Matrix\n",
    "\n",
    "$\\mathbf C = \\begin{bmatrix}\n",
    "0 & 0& 0&  \\cdots&  0& -c_o\\\\ \n",
    "1 & 0& 0&  \\cdots&  0& -c_1\\\\ \n",
    "0 & 1& 0&  \\cdots&  0& -c_2\\\\ \n",
    "0 & 0& 1&  \\cdots&  0& -c_3\\\\ \n",
    "\\vdots & \\vdots& \\vdots&  \\ddots&  \\vdots& \\vdots\\\\ \n",
    "0 & 0& 0&  \\cdots& 1 & -c_{n-1} \n",
    "\\end{bmatrix}$\n",
    "\n",
    "Notice that this looks *extremely* familiar -- it is the the Permutation matrix associated with a connected graph, except the final column looks rather different.  This time, in the final column, we have the characteristic polynomial (where $c_n = 1$ and is not shown.)  \n",
    "\n",
    "Now consider the kth left eigenpair $\\lambda_k$, $ \\tilde{\\mathbf w_k^T}$\n",
    "\n",
    "$ \\tilde{\\mathbf w_k^T} \\mathbf C = \\begin{bmatrix}\n",
    "w_{k,2}& w_{k,3} &  w_{k,4}& \\cdots & w_{k,n} & \\sum_{i = 1}^{n}-c_{i-1}w_{k,i}\n",
    "\\end{bmatrix}$\n",
    "\n",
    "by the definition of vector matrix multiplication, and we see\n",
    "\n",
    "$ \\tilde{\\mathbf w_k^T} \\mathbf C = \\lambda_k \\tilde{\\mathbf w_k^T} = \\begin{bmatrix}\n",
    "\\lambda_k w_{k,1} & \\lambda_k w_{k,2} &  \\lambda_k w_{k,3}& \\lambda_k w_{k,4}  & \\cdots & \\lambda_k w_{k,n}\n",
    "\\end{bmatrix}$  \n",
    "by the definition of an eigenvector\n",
    "\n",
    "hence we see\n",
    "\n",
    "$\\tilde{\\mathbf w_k^T} \\mathbf C = \\begin{bmatrix}\n",
    "w_{k,2}& w_{k,3} &  w_{k,4}& \\cdots & w_{k,n} & \\sum_{i = 1}^{n}-c_{i-1}w_{k,i}\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "\\lambda_k w_{k,1} & \\lambda_k w_{k,2} &  \\lambda_k w_{k,3}&  \\cdots & \\lambda_k w_{k,n-1} & \\lambda_k w_{k,n}\n",
    "\\end{bmatrix}$ \n",
    "\n",
    "To solve this, we start by setting $w_{k,1} := \\alpha$ and forward propagate\n",
    "\n",
    "$\\tilde{\\mathbf w_k^T} \\mathbf C = \\begin{bmatrix}\n",
    "w_{k,2}& w_{k,3} &  w_{k,4}& \\cdots & w_{k,n} & \\sum_{i = 1}^{n}-c_{i-1}w_{k,i}\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "\\lambda_k \\alpha & \\lambda_k^2 \\alpha &  \\lambda_k^3 \\alpha & \\cdots & \\lambda_k^{n-1} & ?\n",
    "\\end{bmatrix}$ \n",
    "\n",
    "\n",
    "in general $\\alpha$ may be any scalar value, except, as always special care is needed when dealing with zeros (or else we could have the case where the eigenvector is the zero vector, which is not allowed).  We may revisit the issue of $\\alpha$ not being equal to zero, later\n",
    "\n",
    "Any nonzero value of $\\alpha$ may be divided out, which gives us\n",
    "\n",
    "\n",
    "$\\tilde{\\mathbf w_k^T} \\mathbf C = \\alpha \\begin{bmatrix}\n",
    "w_{k,2}& w_{k,3} &  w_{k,4}& \\cdots & w_{k,n} & \\sum_{i = 1}^{n}-c_{i-1}w_{k,i}\n",
    "\\end{bmatrix} = \\alpha \\begin{bmatrix}\n",
    "\\lambda_k & \\lambda_k^2 &  \\lambda_k^3 & \\cdots & \\lambda_k^{n-1} & ?\n",
    "\\end{bmatrix} = \\alpha \\lambda_k \\begin{bmatrix}\n",
    "1 & \\lambda_k &  \\lambda_k^2 & \\cdots & \\lambda_k^{n-2} & ??\n",
    "\\end{bmatrix}$ \n",
    "\n",
    "\n",
    "This *too* looks awfully familiar.  So we may guess\n",
    "\n",
    "$?? = \\lambda_k w_{k,n} = \\sum_{i = 1}^{n}-c_{i-1}w_{k,i} = \\lambda_k^{n-1}$  \n",
    "\n",
    "and indeed we verify this because   \n",
    "\n",
    "*(a)  *  \n",
    "$\\lambda_k w_{n-1} = w_{k,n}$\n",
    "\n",
    "$\\lambda_k \\lambda_k^{n-2} = \\lambda_k^{n-1}$\n",
    "\n",
    "and *(b)*  \n",
    "\n",
    "$\\sum_{i = 1}^{n}-c_{i-1}w_{k,i} = c_n \\lambda_k^n - ( c_n \\lambda_k^n +\\sum_{i = 1}^{n}-c_{i-1}w_{k,i}) =   c_n \\lambda_k^n  - ( c_n \\lambda_k^n + c_1 w_{k,1} + \\sum_{i = 2}^{n}-c_{i-1}w_{k,i})$\n",
    "\n",
    "$\\sum_{i = 1}^{n}-c_{i-1}w_{k,i}  =  c_n \\lambda_k^n - ( c_n \\lambda_k^n + c_1 *1 + \\sum_{i = 2}^{n}-c_{i-1}\\lambda_k^{i-1}) =  c_n \\lambda_k^n + (0) =  \\lambda_k^n = \\lambda_k (\\lambda_k^{n-1})$\n",
    "\n",
    "\n",
    "Again, recalling our Vandermonde matrix,\n",
    "\n",
    "$\\mathbf W = \\begin{bmatrix}\n",
    "1 & \\lambda_1 & \\lambda_1^2 & \\dots  & \\lambda_1^{n-1}\\\\ \n",
    "1 & \\lambda_2 & \\lambda_2^2 & \\dots &  \\lambda_2^{n-1} \\\\ \n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\\\ \n",
    "1 & \\lambda_{n} & \\lambda_{n}^{2} & \\dots  & \\lambda_{n}^{n-1}\n",
    "\\end{bmatrix}$\n",
    "\n",
    "we collect these relationships over all $n$ eigenpairs, and see\n",
    "\n",
    "$\\mathbf{WC} = \\mathbf {\\Lambda W}$\n",
    "\n",
    "If the eigenvectors are linearly independent, then the companion matrix is diagonalizable, and hence we have:  \n",
    "\n",
    "$\\mathbf{C} = \\mathbf W^{-1}\\mathbf {\\Lambda  W}$\n",
    "\n",
    "**remarks**   \n",
    "It is interesting to note that $\\mathbf W^{-1}$ exists **iff** all $\\lambda_k$ are unique.  So much earlier on, we proved that an $n$ x $n$ matrix is always diagonalizable if all of its eigenvalues are unique (but it may be diagonalizable, despite repeated eigenvalues, for orther reasons too -- e.g. it is always the diagonalizable if the matrix is normal, or we may just be fortunate as there are non-defective matrices that have repeated eigenvalues), but $\\mathbf C$ is much more brittle and *always* defective unless all of its eigenvalues are unique.  \n",
    "\n",
    "Of course, over a complex numbers field, the Companion matrix, like any other $n$ x $n$ matrix is still similar to an upper triangular matrix.  \n",
    "\n",
    "Recalling our earlier use of the permutation matrix for a connected graph, it is interesting to think about \n",
    "\n",
    "$\\mathbf P = \\begin{bmatrix}\n",
    "0 & 0& 0&  \\cdots&  0& -c_o\\\\ \n",
    "1 & 0& 0&  \\cdots&  0& 0\\\\ \n",
    "0 & 1& 0&  \\cdots&  0& 0\\\\ \n",
    "0 & 0& 1&  \\cdots&  0& 0\\\\ \n",
    "\\vdots & \\vdots& \\vdots&  \\ddots&  \\vdots& \\vdots\\\\ \n",
    "0 & 0& 0&  \\cdots& 1 & 0 \n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "0 & 0& 0&  \\cdots&  0& 1\\\\ \n",
    "1 & 0& 0&  \\cdots&  0& 0\\\\ \n",
    "0 & 1& 0&  \\cdots&  0& 0\\\\ \n",
    "0 & 0& 1&  \\cdots&  0& 0\\\\ \n",
    "\\vdots & \\vdots& \\vdots&  \\ddots&  \\vdots& \\vdots\\\\ \n",
    "0 & 0& 0&  \\cdots& 1 & 0 \n",
    "\\end{bmatrix}$\n",
    "\n",
    "which is to say that we were looking at the characteristic polynomial of $\\lambda^n + 0 + 0 + .... + 0 + c_0 = \\lambda^n - 1$, i.e. the two matrices are the same if $c_0 = -1$ and $c_r = 0$ for $r = \\{2, 3, ..., n-1\\}$.  Given that we ultimately found distinct roots of unity as the eigenvalues, this is perhaps not a surprise.  \n",
    "\n",
    "Using the companion matrix, we now offer a high level sketch of recovering the characteristic polynomial view traces of $\\mathbf C$ over $n$ iteration.  This approach should seem quite intuitive, though the exact proof seems a bit too tedious and hence is omitted.  \n",
    "\n",
    "general idea: \n",
    "\n",
    "we've shown that $\\mathbf C$ has unique eigenvalues that are roots of the characteristic polynomial given in its right most column.  To verify that the algebraic multiplicities are intact, we'd calculate $det\\big(\\mathbf C - \\lambda \\mathbf I\\big)$, use induction with Laplace Expansion and confirm that $\\mathbf C$ itself has the characteristic polynomial given in its right most column.  \n",
    "\n",
    "From here we consider our problem where we have two $n$ x $n$ matrices, $\\mathbf X$ and $\\mathbf Y$, and we know their traces are the same over some intervale -- i.e. $trace\\big(\\mathbf X^k\\big) = trace\\big(\\mathbf Y^k\\big)$ for $k = \\{1, 2, 3, ..., 2n-1\\}$.  We've already proved that these two matrices have the same eigenvalues with same algebraic multiplicities -- i.e. the same characteristic polynomial.  \n",
    "\n",
    "We can further extend this, and endcode that characteristic polynomial in an $n$ x $n$ companion matrix. Hence \n",
    "\n",
    "$trace\\big(\\mathbf C^k\\big) = trace\\big(\\mathbf X^k\\big) = trace\\big(\\mathbf Y^k\\big)$ for $k = \\{1, 2, 3, ..., 2n-1\\}$\n",
    "\n",
    "for illustrative purposes, consider the case where $n = 5$\n",
    "\n",
    "Thus we have \n",
    "\n",
    "$\\mathbf C^1 = \\left[\\begin{matrix}0 & 0 & 0 & 0 & - c_{0}\\\\1 & 0 & 0 & 0 & - c_{1}\\\\0 & 1 & 0 & 0 & - c_{2}\\\\0 & 0 & 1 & 0 & - c_{3}\\\\0 & 0 & 0 & 1 & - c_{4}\\end{matrix}\\right]$\n",
    "\n",
    "$trace\\big(\\mathbf C^1\\big) = -c_{4} = -c_{n-1}$ \n",
    "\n",
    "which is what we'd expect, as it is possible to define the trace as the negative coeffecient of second highest term of the characteristic polynomial (in monic form).  \n",
    "\n",
    "$\\mathbf C^2 = \\left[\\begin{matrix}0 & 0 & 0 & - c_{0} & c_{0} c_{4}\\\\0 & 0 & 0 & - c_{1} & - c_{0} + c_{1} c_{4}\\\\1 & 0 & 0 & - c_{2} & - c_{1} + c_{2} c_{4}\\\\0 & 1 & 0 & - c_{3} & - c_{2} + c_{3} c_{4}\\\\0 & 0 & 1 & - c_{4} & - c_{3} + c_{4}^{2}\\end{matrix}\\right]$\n",
    "\n",
    "$trace\\big(\\mathbf C^2\\big) =- 2 c_{3} + c_{4}^{2}$\n",
    "\n",
    "notice there is one new term here and one old one.  We may extract the value for $c_3$ in terms of the the value of $c_4$ which was provided in $trace\\big(\\mathbf C^1\\big)$\n",
    "\n",
    "\n",
    "$\\mathbf C^3 = \\left[\\begin{matrix}0 & 0 & - c_{0} & c_{0} c_{4} & c_{0} c_{3} - c_{0} c_{4}^{2}\\\\0 & 0 & - c_{1} & - c_{0} + c_{1} c_{4} & c_{1} c_{3} - c_{4} \\left(- c_{0} + c_{1} c_{4}\\right)\\\\0 & 0 & - c_{2} & - c_{1} + c_{2} c_{4} & - c_{0} + c_{2} c_{3} - c_{4} \\left(- c_{1} + c_{2} c_{4}\\right)\\\\1 & 0 & - c_{3} & - c_{2} + c_{3} c_{4} & - c_{1} + c_{3}^{2} - c_{4} \\left(- c_{2} + c_{3} c_{4}\\right)\\\\0 & 1 & - c_{4} & - c_{3} + c_{4}^{2} & - c_{2} + c_{3} c_{4} - c_{4} \\left(- c_{3} + c_{4}^{2}\\right)\\end{matrix}\\right]$\n",
    "\n",
    "$trace\\big(\\mathbf C^3\\big) = - 3 c_{2} + 2 c_{3} c_{4} - c_{4} \\left(- c_{3} + c_{4}^{2}\\right)$\n",
    "\n",
    "We start to notice that while the the matrix (and even the trace expressions) are getting complicated, the sub-problems are nicely overlapping in the traces -- we can compute $trace\\big(\\mathbf C^3\\big)$ and then solve for $c_2$ since it is the only new term in here. (Also notice that each time a new term is introduced it is in a simple form -- i.e. just a real scalar -- no possibility of multiple values like in the case of having a, say, a squared term.)\n",
    "\n",
    "$\\mathbf C^4 = \\left[\\begin{matrix}0 & - c_{0} & c_{0} c_{4} & c_{0} c_{3} - c_{0} c_{4}^{2} & c_{0} c_{2} - c_{0} c_{3} c_{4} - c_{4} \\left(c_{0} c_{3} - c_{0} c_{4}^{2}\\right)\\\\0 & - c_{1} & - c_{0} + c_{1} c_{4} & c_{1} c_{3} - c_{4} \\left(- c_{0} + c_{1} c_{4}\\right) & c_{1} c_{2} - c_{3} \\left(- c_{0} + c_{1} c_{4}\\right) - c_{4} \\left(c_{1} c_{3} - c_{4} \\left(- c_{0} + c_{1} c_{4}\\right)\\right)\\\\0 & - c_{2} & - c_{1} + c_{2} c_{4} & - c_{0} + c_{2} c_{3} - c_{4} \\left(- c_{1} + c_{2} c_{4}\\right) & c_{2}^{2} - c_{3} \\left(- c_{1} + c_{2} c_{4}\\right) - c_{4} \\left(- c_{0} + c_{2} c_{3} - c_{4} \\left(- c_{1} + c_{2} c_{4}\\right)\\right)\\\\0 & - c_{3} & - c_{2} + c_{3} c_{4} & - c_{1} + c_{3}^{2} - c_{4} \\left(- c_{2} + c_{3} c_{4}\\right) & - c_{0} + c_{2} c_{3} - c_{3} \\left(- c_{2} + c_{3} c_{4}\\right) - c_{4} \\left(- c_{1} + c_{3}^{2} - c_{4} \\left(- c_{2} + c_{3} c_{4}\\right)\\right)\\\\1 & - c_{4} & - c_{3} + c_{4}^{2} & - c_{2} + c_{3} c_{4} - c_{4} \\left(- c_{3} + c_{4}^{2}\\right) & - c_{1} + c_{2} c_{4} - c_{3} \\left(- c_{3} + c_{4}^{2}\\right) - c_{4} \\left(- c_{2} + c_{3} c_{4} - c_{4} \\left(- c_{3} + c_{4}^{2}\\right)\\right)\\end{matrix}\\right]$\n",
    "\n",
    "$trace\\big(\\mathbf C^4\\big) = - 4 c_{1} + 2 c_{2} c_{4} + c_{3}^{2} - c_{3} \\left(- c_{3} + c_{4}^{2}\\right) - c_{4} \\left(- c_{2} + c_{3} c_{4}\\right) - c_{4} \\left(- c_{2} + c_{3} c_{4} - c_{4} \\left(- c_{3} + c_{4}^{2}\\right)\\right)$\n",
    "\n",
    "Again, if we computed the earlier traces (and memo-ized our results for $c_r$ for $r = \\{2, 3, 4\\}$) we can uniquely solve for $c_1$ after computing $trace\\big(\\mathbf C^4\\big)$\n",
    "\n",
    "$\\mathbf C^5 = \\left[\\begin{matrix}- c_{0} & c_{0} c_{4} & c_{0} c_{3} - c_{0} c_{4}^{2} & c_{0} c_{2} - c_{0} c_{3} c_{4} - c_{4} \\left(c_{0} c_{3} - c_{0} c_{4}^{2}\\right) & c_{0} c_{1} - c_{0} c_{2} c_{4} - c_{3} \\left(c_{0} c_{3} - c_{0} c_{4}^{2}\\right) - c_{4} \\left(c_{0} c_{2} - c_{0} c_{3} c_{4} - c_{4} \\left(c_{0} c_{3} - c_{0} c_{4}^{2}\\right)\\right)\\\\- c_{1} & - c_{0} + c_{1} c_{4} & c_{1} c_{3} - c_{4} \\left(- c_{0} + c_{1} c_{4}\\right) & c_{1} c_{2} - c_{3} \\left(- c_{0} + c_{1} c_{4}\\right) - c_{4} \\left(c_{1} c_{3} - c_{4} \\left(- c_{0} + c_{1} c_{4}\\right)\\right) & c_{1}^{2} - c_{2} \\left(- c_{0} + c_{1} c_{4}\\right) - c_{3} \\left(c_{1} c_{3} - c_{4} \\left(- c_{0} + c_{1} c_{4}\\right)\\right) - c_{4} \\left(c_{1} c_{2} - c_{3} \\left(- c_{0} + c_{1} c_{4}\\right) - c_{4} \\left(c_{1} c_{3} - c_{4} \\left(- c_{0} + c_{1} c_{4}\\right)\\right)\\right)\\\\- c_{2} & - c_{1} + c_{2} c_{4} & - c_{0} + c_{2} c_{3} - c_{4} \\left(- c_{1} + c_{2} c_{4}\\right) & c_{2}^{2} - c_{3} \\left(- c_{1} + c_{2} c_{4}\\right) - c_{4} \\left(- c_{0} + c_{2} c_{3} - c_{4} \\left(- c_{1} + c_{2} c_{4}\\right)\\right) & c_{1} c_{2} - c_{2} \\left(- c_{1} + c_{2} c_{4}\\right) - c_{3} \\left(- c_{0} + c_{2} c_{3} - c_{4} \\left(- c_{1} + c_{2} c_{4}\\right)\\right) - c_{4} \\left(c_{2}^{2} - c_{3} \\left(- c_{1} + c_{2} c_{4}\\right) - c_{4} \\left(- c_{0} + c_{2} c_{3} - c_{4} \\left(- c_{1} + c_{2} c_{4}\\right)\\right)\\right)\\\\- c_{3} & - c_{2} + c_{3} c_{4} & - c_{1} + c_{3}^{2} - c_{4} \\left(- c_{2} + c_{3} c_{4}\\right) & - c_{0} + c_{2} c_{3} - c_{3} \\left(- c_{2} + c_{3} c_{4}\\right) - c_{4} \\left(- c_{1} + c_{3}^{2} - c_{4} \\left(- c_{2} + c_{3} c_{4}\\right)\\right) & c_{1} c_{3} - c_{2} \\left(- c_{2} + c_{3} c_{4}\\right) - c_{3} \\left(- c_{1} + c_{3}^{2} - c_{4} \\left(- c_{2} + c_{3} c_{4}\\right)\\right) - c_{4} \\left(- c_{0} + c_{2} c_{3} - c_{3} \\left(- c_{2} + c_{3} c_{4}\\right) - c_{4} \\left(- c_{1} + c_{3}^{2} - c_{4} \\left(- c_{2} + c_{3} c_{4}\\right)\\right)\\right)\\\\- c_{4} & - c_{3} + c_{4}^{2} & - c_{2} + c_{3} c_{4} - c_{4} \\left(- c_{3} + c_{4}^{2}\\right) & - c_{1} + c_{2} c_{4} - c_{3} \\left(- c_{3} + c_{4}^{2}\\right) - c_{4} \\left(- c_{2} + c_{3} c_{4} - c_{4} \\left(- c_{3} + c_{4}^{2}\\right)\\right) & - c_{0} + c_{1} c_{4} - c_{2} \\left(- c_{3} + c_{4}^{2}\\right) - c_{3} \\left(- c_{2} + c_{3} c_{4} - c_{4} \\left(- c_{3} + c_{4}^{2}\\right)\\right) - c_{4} \\left(- c_{1} + c_{2} c_{4} - c_{3} \\left(- c_{3} + c_{4}^{2}\\right) - c_{4} \\left(- c_{2} + c_{3} c_{4} - c_{4} \\left(- c_{3} + c_{4}^{2}\\right)\\right)\\right)\\end{matrix}\\right]$\n",
    "\n",
    "$trace\\big(\\mathbf C^5\\big) = - 5 c_{0} + 2 c_{1} c_{4} + 2 c_{2} c_{3} - c_{2} \\left(- c_{3} + c_{4}^{2}\\right) - c_{3} \\left(- c_{2} + c_{3} c_{4}\\right) - c_{3} \\left(- c_{2} + c_{3} c_{4} - c_{4} \\left(- c_{3} + c_{4}^{2}\\right)\\right) - c_{4} \\left(- c_{1} + c_{2} c_{4}\\right) - c_{4} \\left(- c_{1} + c_{3}^{2} - c_{4} \\left(- c_{2} + c_{3} c_{4}\\right)\\right) - c_{4} \\left(- c_{1} + c_{2} c_{4} - c_{3} \\left(- c_{3} + c_{4}^{2}\\right) - c_{4} \\left(- c_{2} + c_{3} c_{4} - c_{4} \\left(- c_{3} + c_{4}^{2}\\right)\\right)\\right) $\n",
    "\n",
    "And once again the subproblems in terms of traces nicely overlap --despite a horror inducing matrix given by $\\mathbf C^5$.  We can also uniquely find $c_0$ here just from having $trace\\big(\\mathbf C^5\\big)$ as well as the results from $trace\\big(\\mathbf C^r\\big)$ for $ r = \\{1,2, 3, 4\\}$.  \n",
    "\n",
    "**What's the point?**  \n",
    "\n",
    "What the above example aimed to show, though *not prove*, is that we actually have the option of early stopping. Put differently, we knew that if we had\n",
    "\n",
    "$trace\\big(\\mathbf C^k\\big) = trace\\big(\\mathbf X^k\\big) = trace\\big(\\mathbf Y^k\\big)$ for $k = \\{1, 2, 3, ..., 2n-1\\}$\n",
    "\n",
    "then we could conlcude that all three $n$ by $n$ matrices had the same characteristic polynomial (and hence same eigenvalues with same algebraic multiplicities).  \n",
    "\n",
    "But if we are clever (and patient) enough, we can actually uniquely recover the characteristic polynomial of a matrix over just 'one cycle', i.e. \n",
    "\n",
    "$trace\\big(\\mathbf C^k\\big) = trace\\big(\\mathbf X^k\\big) = trace\\big(\\mathbf Y^k\\big)$ for $k = \\{1, 2, 3, ..., n\\}$\n",
    "\n",
    "Thus we could tighten our earlier result of showing two $n$ x $n$ matrices have same eigenvalues if they have the same trace over a certain number of iterations -- and decrease the required iterations from $2n -1 $ to  $n$.  \n",
    "\n",
    "The companion matrix was originally going to be omitted in this writeup, but because its eigenvectors are given in the Vandermonde matrix, and it gives some new insights on information from traces, it was included, though again, certain proofs have been omitted due to tedium. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [py35]",
   "language": "python",
   "name": "Python [py35]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
