{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proof of Hadamard's Inequality for Hermitian positive semi-definite matrices\n",
    "(page 274, problem 20 of Kuttler's freely available *Linear Algebra, Theory and Applications*)\n",
    "note that if you look this up under wikipedia it will be focused on something a bit different, but it will say regarding the problem of interest here:  \"Sometimes this is also known as Hadamard's inequality.\"\n",
    "\n",
    "There is some further interesting commentary on this inequality in Meyer's *Matrix Analysis* especially as it relates to geometric interpretations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**claim: for some Hermitian positive semi-definite matrix n x n** matrix, $\\mathbf A$, then $det(\\mathbf A) \\leq   \\prod_{i=1}^{n} a_{i,i}$.  \n",
    "\n",
    "\n",
    "*notational note*  because $\\mathbf A$ is Hermitian, we say that $\\mathbf A^H = \\mathbf A$.  (Some texts will instead say $\\mathbf A^* = \\mathbf A$, using $^*$ instead of $^H$ to denote the conjugate transpose.)  \n",
    "\n",
    "- - - - \n",
    "\n",
    "**commentary:**  \n",
    "\n",
    "This inequality has two diffferent interesting links with other inequalities in general math:\n",
    "\n",
    "(a) One interpretation of this problem is that it allows an intersesting extension and linkage of the $GM \\leq AM$ inequality.  Note that since all eigenvalues and diagonal entries of $\\mathbf A$ are real valued and $\\geq 0$, it seems like we may be able to use $GM \\leq AM$  here. Not knowing the Hadamard Inequality, we could upper bound the determinant (to the $\\frac{1}{n}$ power) of $\\mathbf A$ --i.e. the product of the eigenvalues-- by the arithmetic mean of the eigenvalues which is also equal to the arithmetic mean of the diagonal elements of $\\mathbf A$ (i.e. $det\\big(\\mathbf A\\big)^{\\frac{1}{n}} \\leq \\ \\frac{1}{n}trace\\big(\\mathbf A\\big)$).  What the Hadamard Inequality allows us to do is bound the determinant (to the $\\frac{1}{n}$ power) of $\\mathbf A$ by the geometric mean of the diagonal elements of $\\mathbf A$.  Since $GM \\leq AM$, we know that this gives us a tigher bound.  I.e. It allows us to say \n",
    "\n",
    "$det\\big(\\mathbf A\\big)^{\\frac{1}{n}} \\leq \\big(\\prod_{k=1}^{n} a_{k,k}\\big)^{\\frac{1}{n}} \\leq \\frac{1}{n} \\big(\\sum_{k=1}^{n} a_{k,k}\\big) = \\frac{1}{n}trace\\big(\\mathbf A\\big) = \\frac{1}{n}\\sum_{k=1}^n \\lambda_k$\n",
    "\n",
    "(b) Assume we have some square matrix $\\mathbf B$, where $\\mathbf B^H \\mathbf B := \\mathbf A$, what this inequality says, is that from a geometric perspective, if we look at the diagonal of $\\mathbf A$, we see the squared lengths of each respective column of $\\mathbf B$ in each diagonal entry of $\\mathbf A$.  The underlying geometry is that the squared magnitude of the determinant of $\\mathbf B$ --i.e. given by $det \\big(\\mathbf A\\big)$ -- cannot be any bigger than the product of squared lengths of each column of $\\mathbf B$ -- and we have equality **iff** each column of $\\mathbf B$ is orthogonal to each other --resulting in a diagonal matrix $\\mathbf A$ (or in the degenerate case where $\\mathbf B$ has at least one column of zeros and hence it doesn't have enough column vectors with positive length to even try to form a parallelepiped that matches its dimensions).\n",
    "\n",
    "- - - - \n",
    "**proof:**\n",
    "\n",
    "where we have an upper triangular matrix $\\mathbf R = \\bigg[\\begin{array}{c|c|c|c}\\mathbf r_1 & \\mathbf r_2 &\\cdots & \\mathbf r_{n}\\end{array}\\bigg]$, and using Cholesky (or $\\mathbf A = \\mathbf B^H\\mathbf B$ and doing QR factorization on $\\mathbf B$), we can factorize $\\mathbf A = \\mathbf R^H \\mathbf R$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "note that for column one  \n",
    "$\\big \\vert r_{1,1}\\big \\vert = \\big \\Vert\\mathbf r_1\\big \\Vert_2  $  \n",
    "and in general for column $j$  \n",
    "$0\\leq \\big \\vert r_{j,j}\\big \\vert \\leq \\Big(\\sum_{i=1}^n \\big \\vert r_{i,j}\\big \\vert^2\\Big)^\\frac{1}{2} = \\big \\Vert\\mathbf r_j\\big \\Vert_2  $  \n",
    "\n",
    "multiplying over this bound we have  \n",
    "$0\\leq \\prod_{j=1}^n \\big \\vert r_{j,j}\\big \\vert = \\big \\vert\\prod_{j=1}^n  r_{j,j}\\big \\vert \\leq  \\big \\Vert\\mathbf r_j\\big \\Vert_2  $  \n",
    "\n",
    "Further taking advantage of positivity and squaring, we get  \n",
    "$\\det\\Big(\\mathbf A \\Big) $  \n",
    "$=  \\det\\Big(\\mathbf R^* \\mathbf R\\Big) $  \n",
    "$= \\Big \\vert \\det\\Big(\\mathbf R^* \\mathbf R\\Big)\\Big \\vert $  \n",
    "$= \\Big \\vert \\det\\Big(\\mathbf R^*\\Big) \\det\\Big( \\mathbf R\\Big)\\Big \\vert $  \n",
    "$= \\Big \\vert \\det\\Big(\\mathbf R^*\\Big)\\Big \\vert \\Big \\vert\\det\\Big( \\mathbf R\\Big)\\Big \\vert$  \n",
    "$= \\big(\\big \\vert\\prod_{j=1}^n  r_{j,j}\\big \\vert\\big)^2 $  \n",
    "$ \\leq  \\Big(\\prod_{j=1}^n \\big \\Vert\\mathbf r_j\\big \\Vert_2\\Big)^2  $  \n",
    "$= \\prod_{j=1}^n \\big \\Vert\\mathbf r_j\\big \\Vert_2^2  $  \n",
    "$= \\prod_{j=1}^n a_{j,j}$  \n",
    "\n",
    "This *is* the Hadamard determinant inequality for Hermitian Positive (Semi)definite matrices  \n",
    "\n",
    "For *singular* $\\mathbf A$, $\\det\\Big(\\mathbf A \\Big) =0$  and each $a_{j,j} = \\Vert\\mathbf r_j\\big \\Vert_2^2 \\geq 0$ so there is equality iff some $\\mathbf r_j = 0$ (i.e. a diagonal component of $\\mathbf A$ is zero)  \n",
    "\n",
    "For a *non-singular* $\\mathbf A$ there is equality **iff** $\\mathbf A$ is diagonal.  If we think a generic way of generating Hermitian positive definite matrices, it is by taking some matrix $\\mathbf C$ with full column rank, and left multiplying by the conjugate transpose, so $\\mathbf A = \\mathbf C^*\\mathbf C$, and the Hadamard determinant inequality says that the equality case is met *iff* each column of $\\mathbf C$ is mutually orthogonal.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**extension**  \n",
    "While not strictly needed, a nice application the Hadamard Inequality is in deriving Singular Value Decomposition. We take the notebook of \"Schur's Inequality\" as background material, which unlocks the spectral theorem.  \n",
    "\n",
    "\n",
    "To derive SVD it is enough to consider a matrix $\\mathbf C$ that is tall and skinny  \n",
    "\n",
    "(i.e. $n \\text{ columns } \\leq m \\text{ rows }$)\n",
    "if this isn't the case run the below argument on the conjugate transpose of $\\mathbf C$, so we proceed by assuming WLOG that $\\mathbf C$ is tall and skinny   \n",
    "- - - - -  \n",
    "**case 1**   \n",
    "consider the case of $\\mathbf C$ with *full column rank*   \n",
    "\n",
    "Then consider $\\mathbf A = \\mathbf C^* \\mathbf C$  \n",
    "Per the spectral theorem we know we may unitarily diagonalize this so  \n",
    "$\\mathbf C^* \\mathbf C = \\mathbf A = \\mathbf {VD V}^* $  \n",
    "\n",
    "(note this means $\\mathbf V \\in \\mathbb C^{\\text{n  x  n}}$)  \n",
    "\n",
    "thus for a quadratic form we have  \n",
    "$0 \\lt \\lambda_k = \\mathbf v_k^* \\mathbf A \\mathbf v_k =\\mathbf v_k^* \\mathbf C^* \\mathbf C \\mathbf v_k = \\sigma_k^2 $  \n",
    "\n",
    "that is, by construction each $\\sigma_k$ is the (non-negative) square root of the eigenvalue $\\lambda_k$ associated with $\\mathbf A$ \n",
    "\n",
    "and in general  \n",
    "$\\mathbf C\\mathbf v_k =  \\mathbf u_k \\cdot \\sigma_k$  \n",
    "where we know $\\big \\Vert  \\mathbf u_k \\big \\Vert_2^2 = 1$ \n",
    "but we do not yet know much else about the $\\mathbf u_k$  \n",
    "\n",
    "if we collect all $n$  of these relationships in matrix form we get  \n",
    "$\\mathbf C\\mathbf V = \\mathbf U \\mathbf \\Sigma $  \n",
    "left mutliplying each side by the respective conjugate transpose, we have  \n",
    "\n",
    "$\\mathbf D = \\mathbf V^* \\mathbf C^*\\mathbf C\\mathbf V = \\mathbf  \\Sigma^*  \\mathbf U^* \\mathbf U \\mathbf \\Sigma $  \n",
    "- - - - \n",
    "note that we have   \n",
    "$ \\mathbf  \\Sigma^*  \\big(\\mathbf U^* \\mathbf U\\big) \\mathbf \\Sigma = \\mathbf \\Sigma \\begin{bmatrix}\\big \\Vert  \\mathbf u_1\\big\\Vert_2^2 & * & \\cdots & * &* \\\\ * & \\big \\Vert  \\mathbf u_2\\big\\Vert_2^2 & \\cdots &*  &* \\\\ \\vdots & \\vdots &  \\ddots & \\vdots & \\vdots \\\\  * & * & \\cdots & \\big \\Vert  \\mathbf u_{n-1}\\big\\Vert_2^2 & * \\\\ * & * & \\cdots & * & \\big \\Vert  \\mathbf u_n\\big\\Vert_2^2 \\end{bmatrix}\\mathbf  \\Sigma^*=  \\begin{bmatrix}\\sigma_1^2 \\big \\Vert  \\mathbf u_1\\big\\Vert_2^2 & * & \\cdots & * &* \\\\ * & \\sigma_2^2 \\big \\Vert  \\mathbf u_2\\big\\Vert_2^2 & \\cdots &*  &* \\\\ \\vdots & \\vdots &  \\ddots & \\vdots & \\vdots \\\\  * & * & \\cdots & \\sigma_{n-1}^2 \\big \\Vert  \\mathbf u_{n-1}\\big\\Vert_2^2 & * \\\\ * & * & \\cdots & * & \\sigma_n^2 \\big \\Vert  \\mathbf u_n\\big\\Vert_2^2 \\end{bmatrix}$  \n",
    "- - - - \n",
    "taking the determinant of each side of the above equation gives  \n",
    "$0 \\lt \\det\\big(\\mathbf D\\big) = \\prod_{k=1}^n \\lambda_k = \\prod_{k=1}^n \\sigma_k^2  = \\det\\Big( \\mathbf  \\Sigma^*  \\mathbf U^* \\mathbf U^* \\mathbf \\Sigma \\Big) \\leq \\prod_{k=1}^n \\big \\Vert \\mathbf u_k\\big \\Vert_2^2 \\cdot \\sigma_k^2 = \\prod_{k=1}^n 1 \\cdot \\sigma_k^2 = \\prod_{k=1}^n  \\sigma_k^2$  \n",
    "by the Hadamard determinant inequality.  But the determinant is non-zero and met with equality hence we see that the columns of $\\mathbf U$ must be mutually orthogonal as this *is* the equality condition of the Hadamard Inequality.  And since by construction each column of $\\mathbf U$ has length of one we know that each column is mutually orthonormal.  \n",
    "\n",
    "\n",
    "**case 2**   \n",
    "suppose that $\\mathbf C$ does not have full column rank, so  \n",
    "$0 = \\det\\big(\\mathbf D\\big) = \\det\\big(\\mathbf A\\big)= \\det\\big(\\mathbf C^*\\mathbf C\\big)$  \n",
    "\n",
    "- - - -  \n",
    "note: there is nothing to do in the case that $\\mathbf C$ is the zero matrix, so to avoid triviality we assume $\\mathbf C \\neq \\mathbf 0$  \n",
    "- - - -  \n",
    "\n",
    "re-using the above argument we *still* have  \n",
    "$\\mathbf C\\mathbf V = \\mathbf U \\mathbf \\Sigma $  \n",
    "and then  \n",
    "\n",
    "$ \\mathbf D = \\mathbf V^* \\mathbf C^*\\mathbf C\\mathbf V = \\mathbf  \\Sigma  \\mathbf U^* \\mathbf U \\mathbf \\Sigma $  \n",
    "if we add the identity matrix to each side and take determinants we get  \n",
    "$0 \\lt \\det\\big(\\mathbf I_n + \\mathbf D\\big)  = \\det\\big( \\mathbf I_n + \\mathbf V^* \\mathbf C^*\\mathbf C\\mathbf V  \\big) = \\det\\big(\\mathbf I_n + \\mathbf  \\Sigma^*  \\mathbf U^* \\mathbf U \\mathbf \\Sigma \\big) $  \n",
    " \n",
    "(where we us the fact that a positive definite matrix plus a positive semi definite matrix gives a positive definite matrix, alternatively see \"determinant_addtion_two_matrices_inequality.ipynb\")  \n",
    "\n",
    "but  \n",
    "$ \\det\\big(\\mathbf I_n + \\mathbf D\\big)  = \\prod_{k=1}^n \\big(1 + \\lambda_k\\big) = \\prod_{k=1}^n \\big(1 + \\sigma_k^2\\big) =\\det\\big(\\mathbf I_n + \\mathbf  \\Sigma^*  \\mathbf U^* \\mathbf U \\mathbf \\Sigma \\big) \\leq \\prod_{k=1}^n \\big(1 + \\big \\Vert \\mathbf u_k\\big \\Vert_2^2\\cdot \\sigma_k^2\\big) = \\prod_{k=1}^n  \\big(1+\\sigma_k^2\\big) $  \n",
    "\n",
    "which again is met with equality and the non-singular equality case of the Hadamard Inequality tells us that even if $\\mathbf C$ is column rank deficient, we *still* know that $\\mathbf U$ has mutually orthogonal columns with length 1 (i.e. mutually orthonormal)  \n",
    "\n",
    "- - - -  \n",
    "thus we arrive at SVD since  \n",
    "$\\mathbf C = \\mathbf C\\mathbf V \\mathbf V^*  = \\mathbf U \\mathbf \\Sigma\\mathbf V^* $  \n",
    "- - - - -\n",
    "*remark 0:*  \n",
    "the above proof could be streamlined by collapsing both proofs into the case where we look at $\\det\\big(\\mathbf I_n + \\mathbf D\\big)$.  However it is often most insightful to first consider some generic case (involving non-singular matrices) and then deal special singularities later.  In particular, in this particular problem most of the geometric intuition involved is associated with case 1.  \n",
    "\n",
    "*remark 1:*  \n",
    "while the Hadamard Inequality isn't strictly needed here (or conventionally used in the proof of deriving SVD) it offers a nice geometric insight, especially when $\\mathbf C$ is square.  So focusing on square and invertible $\\mathbf C$, if we look at the magnitude of the volume of $\\mathbf C$ and square it, we know if is given by the product of the eigenvalues of $\\big(\\mathbf C^*\\mathbf C\\big)$.  Now if we rotate our original matrix and look at $\\mathbf {CV}$ that will not change the magnitude of its squared volume, but $\\mathbf {CV} = \\mathbf {U\\Sigma}$ and while the $\\mathbf U$ exists mostly for book-keeping purposes, and has columns with 2 norm equal to one by construction, the *only* way it cannot distort (i.e. shrink) the magnitude of the squared volume of $\\mathbf {CV}$ is if its columns are mututally orthnormal.  Of course if $\\mathbf {CV}$ is rank deficient then it has volume of zero and hence this argument about non-distortion of volume does not make much sense.  Nevertheless we can make simple algebraic alterations (like adding the identity matrix in case 2) and recover the same underlying result that the columns of $\\mathbf U$ must be mutually orthonormal.  \n",
    "\n",
    "*remark 2:*  \n",
    "The conventions vary, but if $\\mathbf C$ is non-square (i.e. *actually* tall and skinny here) then one of the above three matrices in a singular value decomposition would be non-square.  The easiest approach to line up with the above proof is to select $\\mathbf V$ and $\\mathbf \\Sigma$ to be $\\text{n  x  n}$ , but we have $\\mathbf U$ tall and skinny itself -- i.e. $\\mathbf U \\in \\mathbb C^{\\text{m  x  n}}$.  \n",
    "\n",
    "Thus $\\mathbf U^* \\mathbf U = \\mathbf I_n$  but $\\mathbf {UU}^*$ is an m x m projector with rank $n$.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
