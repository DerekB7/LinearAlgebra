{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "proof of Hadamard's Inequality for Hermitian positive semi-definite matrices\n",
    "(page 274, problem 20 of Kuttler's freely available \"Linear Algebra, Theory and Applications\")\n",
    "note that if you look this up under wikipedia it will be focused on something a bit different, but it will say regarding the problem I'm interested in here: \"Sometimes this is also known as Hadamard's inequality.\"\n",
    "\n",
    "\n",
    "note that in effect this proof uses Cholesky, though I prefer to go at this in a methodical way and get to Cholesky via QR factorization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**claim: ** for some Hermitian positive semi-definite matrix, **n** x **n** matrix, $\\mathbf A$, then $det(\\mathbf A) \\leq   \\prod_{i=1}^{n} A_{i,i}$.  Thus $\\mathbf A^H = \\mathbf A$.  (Some texts will instead say $\\mathbf A^* = \\mathbf A$, using $*$ instead of $H$ to denote the conjugate transpose.)  That is, the product of the diagonal entries in $\\mathbf A$ is at least as large as the product of its determinant.  \n",
    "- - - -\n",
    "\n",
    "**proof: **\n",
    "Since $\\mathbf A$ is Hermitian positive semi-definite, we may find its positive square root, $\\mathbf B$ where $\\mathbf B \\mathbf B = \\mathbf A$.  Observe that $\\mathbf B$ is identical to $\\mathbf A$ except we took the positive square root of all the eigenvalues of $\\mathbf A$ to create $\\mathbf B$.  (Of course all of $\\mathbf A$'s eigs are real and non-negative by virtue of being Hermitian, positive semidefinite.)  This means that $\\mathbf B^H \\mathbf B$ = $\\mathbf A$. \n",
    "\n",
    "From here apply $\\mathbf Q \\mathbf R$ factorization to $\\mathbf B$.\n",
    "\n",
    "$\\mathbf B = \\mathbf Q \\mathbf R$  \n",
    "$\\mathbf B^H = (\\mathbf Q \\mathbf R)^H = \\mathbf R^H \\mathbf Q^H$\n",
    "\n",
    "$\\mathbf B^H \\mathbf B = \\mathbf A =  \\big(\\mathbf R^H \\mathbf Q^H \\big) \\big( \\mathbf Q \\mathbf R\\big) = \\mathbf R^H \\big( \\mathbf Q^H \\mathbf Q \\big) \\mathbf R = \\mathbf R^H \\mathbf I \\mathbf R = \\mathbf R^H \\mathbf R $\n",
    "\n",
    "Thus we have \n",
    "$\\mathbf A = \\mathbf R^H \\mathbf R$ \n",
    "\n",
    "note this is the Cholesky Factorization of $\\mathbf A$ if we assign $\\mathbf L := \\mathbf R^H$.  \n",
    "\n",
    "\n",
    "hence we can say \n",
    "$det\\big(\\mathbf A\\big) = det\\big(\\mathbf R^H \\mathbf R\\big) = det\\big(\\mathbf R^H \\big) det\\big(\\mathbf R \\big) = \\Big( \\prod_{i=1}^{n}  \\mathbf R^H_{i,i}\\Big) \\Big(\\prod_{i=1}^{n} \\mathbf R_{i,i} \\Big)  =  \\prod_{i=1}^{n} \\mathbf R^H_{i,i} \\mathbf R_{i,i}$\n",
    "\n",
    "\n",
    "Thus $det\\big(\\mathbf A\\big)$ is equivalent to the product of each diagonal entry of $\\mathbf R$ times its conjugate.  \n",
    "\n",
    "Notation gets a bit challenging here.  (For a concise conclusion to the above, simply skip to the end section that states \"A final way, succinct way of stating the above is simply\".)  But it's worth recalling that, $\\mathbf B^H \\mathbf B = \\mathbf R^H \\mathbf R = \\mathbf A$, and then that $trace(\\mathbf B^H \\mathbf B) = trace(\\mathbf R^H \\mathbf R) = trace(\\mathbf A) = \\big|\\big|\\mathbf B \\big| \\big|_{F}^2$.  Why is this the case? Because each diagonal entry in $\\mathbf A_{j,j}$ has the value of the L2 norm, squared, of the jth column of $\\mathbf B$.  So if we add up all the diagonal entries of $\\mathbf A$, we get the sum of all L2 norm, squared, of all columns of $\\mathbf B$ (which is one way to interpret the Frobenius norm, squared, of $\\mathbf B$).  \n",
    "\n",
    "Equivalently, we can say that  $\\mathbf A_{j,j}$  has the value of the L2 norm, squared, of the entire the jth column of $\\mathbf R$.  If all other entries in that column are zero, then the determinant contribution from $det(\\mathbf A) = \\prod_{j=1}^{n} \\mathbf R_{j,j} \\mathbf R^H_{j,j}$ is equal with respect to the jth column of $\\mathbf R$.  Otherwise, the contribution is strictly larger.  And where are all values are non-negative reals, a product of a finite sequence of values where one sequence is at least as big in eveyposition as the other, the former product must be at least as large as the latter.  \n",
    "\n",
    "Put differently, we can interpret $det(\\mathbf A) $ as being the product of the L2 norm squared, value of all columns in $\\mathbf R$, if we had gone in and first zero'd out all non-diagonal entries in $\\mathbf R$.  But $\\prod_{i=1}^{n} A_{i,i}$ is equal to the product of the L2 norm squared, value of all columns in $\\mathbf R$, without zeroing out anything in $\\mathbf R$.  Thus $\\prod_{i=1}^{n} \\mathbf A_{i,i} \\gt det(\\mathbf A) $, with equality only in cases where $\\mathbf R$ is diagonal, or when at least one column in $\\mathbf R$ is all zeros (and hence the determinant is zero, and so is $\\prod_{i=1}^{n} A_{i,i})$,\n",
    "- - - -\n",
    "*A final way, succinct way of stating the above is simply:*\n",
    "\n",
    "(Note the below Latex seems to get distorted and stretched when rendered on Github, but looks fine locally.  See the cell below, using latex magic in python, for a hack / alternative rendering.) \n",
    "    \n",
    "\\begin{align}\n",
    "\\mathbf A &= \\mathbf A^H \\\\\n",
    "\\mathbf x^H \\mathbf A \\mathbf x &\\geq 0\\\\\n",
    "\\mathbf A &= \\mathbf R^H \\mathbf R \\\\\n",
    "det(\\mathbf A) & = \\prod_{k=1}^{n} \\mathbf R^H_{k,k} \\mathbf R_{k,k} \\\\\n",
    "\\prod_{k=1}^{n} \\Big(A_{k,k}\\Big)& =  \\prod_{k=1}^{n}\\Big(  \\sum_{j =1}^{n}  \\mathbf R^H_{k,j} \\mathbf R_{j,k} \\Big ) = \\prod_{k=1}^{n}\\Big(   \\mathbf R^H_{k,k} \\mathbf R_{k,k} + \\sum_{j \\neq k}  \\mathbf R^H_{k,j} \\mathbf R_{j,k} \\Big ) \\\\\n",
    "where & \\\\\n",
    "\\mathbf R^H_{k,k} \\mathbf R_{k,k} &\\geq 0  \\\\\n",
    "\\sum_{j \\neq k}  \\mathbf R^H_{k,j} \\mathbf R_{j,k} &\\geq 0  \\\\\n",
    "det(\\mathbf A) &= \\prod_{k=1}^{n} \\mathbf R^H_{k,k} \\mathbf R_{k,k} \\leq  \\prod_{k=1}^{n}\\Big(   \\mathbf R^H_{k,k} \\mathbf R_{k,k} + \\sum_{j \\neq k}  \\mathbf R^H_{k,j} \\mathbf R_{j,k} \\Big )  =\\prod_{k=1}^{n} A_{k,k}\\\\\n",
    "Thus & \\\\\n",
    "det(\\mathbf A) & \\leq \\prod_{k=1}^{n} A_{k,k}\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\n",
       "\\begin{align}\n",
       "\\mathbf A &= \\mathbf A^H \\\\\n",
       "\\mathbf x^H \\mathbf A \\mathbf x &\\geq 0\\\\\n",
       "\\mathbf A &= \\mathbf R^H \\mathbf R \\\\\n",
       "det(\\mathbf A) & = \\prod_{k=1}^{n} \\mathbf R^H_{k,k} \\mathbf R_{k,k} \\\\\n",
       "\\prod_{k=1}^{n} \\Big(A_{k,k}\\Big)& =  \\prod_{k=1}^{n}\\Big(  \\sum_{j =1}^{n}  \\mathbf R^H_{k,j} \\mathbf R_{j,k} \\Big ) = \\prod_{k=1}^{n}\\Big(   \\mathbf R^H_{k,k} \\mathbf R_{k,k} + \\sum_{j \\neq k}  \\mathbf R^H_{k,j} \\mathbf R_{j,k} \\Big ) \\\\\n",
       "where & \\\\\n",
       "\\mathbf R^H_{k,k} \\mathbf R_{k,k} &\\geq 0  \\\\\n",
       "\\sum_{j \\neq k}  \\mathbf R^H_{k,j} \\mathbf R_{j,k} &\\geq 0  \\\\\n",
       "det(\\mathbf A) &= \\prod_{k=1}^{n} \\mathbf R^H_{k,k} \\mathbf R_{k,k} \\leq  \\prod_{k=1}^{n}\\Big(   \\mathbf R^H_{k,k} \\mathbf R_{k,k} + \\sum_{j \\neq k}  \\mathbf R^H_{k,j} \\mathbf R_{j,k} \\Big )  =\\prod_{k=1}^{n} A_{k,k}\\\\\n",
       "Thus & \\\\\n",
       "det(\\mathbf A) & \\leq \\prod_{k=1}^{n} A_{k,k}\n",
       "\\end{align}"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%latex\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf A &= \\mathbf A^H \\\\\n",
    "\\mathbf x^H \\mathbf A \\mathbf x &\\geq 0\\\\\n",
    "\\mathbf A &= \\mathbf R^H \\mathbf R \\\\\n",
    "det(\\mathbf A) & = \\prod_{k=1}^{n} \\mathbf R^H_{k,k} \\mathbf R_{k,k} \\\\\n",
    "\\prod_{k=1}^{n} \\Big(A_{k,k}\\Big)& =  \\prod_{k=1}^{n}\\Big(  \\sum_{j =1}^{n}  \\mathbf R^H_{k,j} \\mathbf R_{j,k} \\Big ) = \\prod_{k=1}^{n}\\Big(   \\mathbf R^H_{k,k} \\mathbf R_{k,k} + \\sum_{j \\neq k}  \\mathbf R^H_{k,j} \\mathbf R_{j,k} \\Big ) \\\\\n",
    "where & \\\\\n",
    "\\mathbf R^H_{k,k} \\mathbf R_{k,k} &\\geq 0  \\\\\n",
    "\\sum_{j \\neq k}  \\mathbf R^H_{k,j} \\mathbf R_{j,k} &\\geq 0  \\\\\n",
    "det(\\mathbf A) &= \\prod_{k=1}^{n} \\mathbf R^H_{k,k} \\mathbf R_{k,k} \\leq  \\prod_{k=1}^{n}\\Big(   \\mathbf R^H_{k,k} \\mathbf R_{k,k} + \\sum_{j \\neq k}  \\mathbf R^H_{k,j} \\mathbf R_{j,k} \\Big )  =\\prod_{k=1}^{n} A_{k,k}\\\\\n",
    "Thus & \\\\\n",
    "det(\\mathbf A) & \\leq \\prod_{k=1}^{n} A_{k,k}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [py35]",
   "language": "python",
   "name": "Python [py35]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
