{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the matrix given by $\\mathbf C = \\mathbf A + \\mathbf B$\n",
    "\n",
    "For simplicity everything is in Reals, and all matrices are $n$ x $n$.  We note that in the degenerate case of $\\mathbf C = \\mathbf 0$, the analysis in what follows can be trivially verified, and hence for simplicity, we banish the zero matrix from this writeup -- i.e. $\\mathbf C \\neq \\mathbf 0$. \n",
    "\n",
    "Now consider the problem of maximizing the value of the bilinear form:\n",
    "\n",
    "$\\mathbf x^T \\mathbf{Cy}$, subject to the constraints that $\\big \\vert \\big \\vert \\mathbf x \\big \\vert \\big \\vert_2 = 1$, and $\\big \\vert \\big \\vert \\mathbf y \\big \\vert \\big \\vert_2 = 1$.  \n",
    "\n",
    "Recall from posting \"Fun_with_Trace_and_Quadratic_Forms_and_CauchySchwarz.ipynb\" that the largest singular value of $\\mathbf C$ is $\\geq$ largest eigenvalue of $\\mathbf C$. (Also note that the ideas in here quite easily generalize to non-square matrices where eigenvalues cannot directly apply.) \n",
    "\n",
    "Here we can use the Singular Value Decomposition, to cleanly partition $\\mathbf C$, thus we have\n",
    "\n",
    "$\\mathbf C = \\bigg[\\begin{array}{c|c|c|c}\n",
    "\\mathbf u_1 & \\mathbf u_2 &\\cdots & \\mathbf u_{n}\n",
    "\\end{array}\\bigg] Diag\\Big(\\begin{bmatrix}\n",
    "\\sigma_1 & \\sigma_2  & \\cdots  & \\sigma_n\n",
    "\\end{bmatrix}^T \\Big)\n",
    " \\bigg[\\begin{array}{c|c|c|c}\n",
    "\\mathbf v_1 & \\mathbf v_2 &\\cdots & \\mathbf v_{n}\n",
    "\\end{array}\\bigg]^T$\n",
    " \n",
    "$= \\bigg[\\begin{array}{c|c|c|c}\n",
    "\\mathbf u_1 & \\mathbf u_2 &\\cdots & \\mathbf u_{n}\n",
    "\\end{array}\\bigg] \\begin{bmatrix}\n",
    "\\sigma_1 & 0 &0  &0 \\\\ \n",
    "0 & \\sigma_2& 0 & 0\\\\ \n",
    "\\vdots & \\vdots &  \\ddots & \\vdots \\\\ \n",
    "0 & 0 & 0 & \\sigma_n  \n",
    "\\end{bmatrix} \\bigg[\\begin{array}{c|c|c|c}\n",
    "\\mathbf v_1 & \\mathbf v_2 &\\cdots & \\mathbf v_{n}\n",
    "\\end{array}\\bigg]^T\n",
    "$   \n",
    "\n",
    "where we have well ordered singuglar values, so that \n",
    "\n",
    "$\\sigma_1 \\geq \\sigma_2 \\geq .... \\geq \\sigma_n \\geq 0$\n",
    "\n",
    "\n",
    "Referring back to our bilinear form that we want to maximize: \n",
    "\n",
    "$max \\big(\\mathbf x^T \\mathbf{U\\Sigma V}^T \\mathbf y\\big)$, again subject to the constraints on the length of $\\mathbf x$ and $\\mathbf y$.\n",
    "\n",
    "$max \\big(\\mathbf x^T \\mathbf U\\big) \\mathbf \\Sigma \\big(\\mathbf V^T \\mathbf y \\big)$\n",
    "\n",
    "$max \\big(\\mathbf U^T \\mathbf x\\big)^T \\mathbf \\Sigma \\big(\\mathbf V^T \\mathbf y \\big)$\n",
    "\n",
    "change of variables, where $\\mathbf w = \\mathbf U^T \\mathbf x$, and $\\mathbf z = \\mathbf V^T \\mathbf y$, recalling that $\\mathbf U$ and $\\mathbf V$ are full rank, square, orthogonal matrices -- hence are invertible and act as a type of coordinate / are length preserving.  \n",
    "\n",
    "$max \\big(\\mathbf w \\big)^T \\mathbf \\Sigma \\big(\\mathbf z \\big) = \\sigma_1 w_1 z_1 + \\sigma_2 w_2 z_2 + ... + \\sigma_n w_n z_n$\n",
    "\n",
    "The answer clearly is that you can do no better than allocate everything in $\\mathbf w$ to $w_1$, i.e. $\\mathbf w = \\mathbf e_1$ (the first unit standard vector -- i.e. first column slice of the Identity Matrix)  and the same logic applies for $\\mathbf z = \\mathbf e_1$.  \n",
    "\n",
    "A familiar exchange argument proves this.  For the moment consider the case where $\\sigma_1 \\gt \\sigma_2 \\geq ....  \\geq \\sigma_n \\geq 0$\n",
    "\n",
    "Now assume we have found an optimal solution where we have some $ 0< \\epsilon \\leq 1$ for $k = \\{2, 3, ..., n-1, n\\}$, where $\\mathbf w = \\mathbf z = (1- \\epsilon) \\mathbf e_1 + \\epsilon \\mathbf e_k$, then our payoff is given by:\n",
    "\n",
    "$(1 - \\epsilon) \\sigma_1  + \\epsilon \\sigma_k $, but we can find an even better solution as $\\epsilon \\to 0$, i.e. \n",
    "\n",
    "$(1) \\sigma_1  + 0 \\sigma_k \\gt (1 - \\epsilon) \\sigma_1  + \\epsilon \\sigma_k$ for any $ 0< \\epsilon \\leq 1$, and hence a contradiction arises.  Note that in the case where $\\sigma_1 \\geq \\sigma_2 \\geq .... \\geq \\sigma_{n-1} \\geq \\sigma_n \\geq 0$, the argument is softened up slightly and we say $(1) \\sigma_1  + 0 \\sigma_k \\geq (1 - \\epsilon) \\sigma_1  + \\epsilon \\sigma_k$, hence $\\epsilon = 0$ is the (weakly) dominant solution. \n",
    "\n",
    "There are some minor book keeping items here.  It is worth pointing out the even in the strict dominance case given by  $\\sigma_1 \\gt \\sigma_2 \\geq .... \\geq \\sigma_{n-1} \\geq \\sigma_n \\geq 0$, instead of saying $\\mathbf w := \\mathbf e_1$, we could do $\\mathbf w := -1*\\mathbf e_1 $, then as before, set $\\mathbf z := \\mathbf w$.  The multiplication by a negative scalar does not change the lengths of the vectors, and the $\\sigma_1* -1*-1 = \\sigma_1*1*1 = \\sigma_1$. This is a minor bookkeeping point and will be disregarded for the rest of this posting.  (Note that we are working in reals, so complex numbers would not be relevant here -- however if we were working in complex numbers, the argument would be that you could apply a scalar anywhere on the unit circle to $\\mathbf e_1$.)\n",
    "\n",
    "- - - -\n",
    "It is worth pointing out that if $\\mathbf w \\neq \\mathbf z$, there is deadweight loss in the payoff, so this possibility is strictly dominated.  This can be visually seen, but we can also note that since $\\mathbf \\Sigma$ has real valued, strictly non-negative entries, we could rewrite our final expression as \n",
    "\n",
    "$max \\big(\\mathbf w \\big)^T \\mathbf \\Sigma \\big(\\mathbf z \\big) = \\mathbf w ^T \\mathbf \\Sigma^\\frac{1}{2} \\mathbf \\Sigma^\\frac{1}{2} \\mathbf z =   \\big(\\mathbf \\Sigma^\\frac{1}{2}\\mathbf w\\big)^T \\big(\\mathbf \\Sigma^\\frac{1}{2} \\mathbf z \\big) = \\sigma_1 w_1 z_1 + \\sigma_2 w_2 z_2 + ... + \\sigma_n w_n z_n$\n",
    "\n",
    "From here observe that by Cauchy-Schwarz, we know $\\big(\\mathbf \\Sigma^\\frac{1}{2}\\mathbf w\\big)^T \\big(\\mathbf \\Sigma^\\frac{1}{2} \\mathbf z \\big)$ is maximized when $\\big(\\mathbf \\Sigma^\\frac{1}{2}\\mathbf w\\big) = \\alpha \\big(\\mathbf \\Sigma^\\frac{1}{2} \\mathbf z \\big)$.  Further we know that $\\alpha = 1$ because of the length restriction on $\\mathbf w$ and $\\mathbf z$.  (Of course we could set $\\alpha := -1$ and still satisfy the length restriction, but this would return a result that is *upper bounded* by zero -- whereas $\\alpha := +1$ is *lower bounded* by zero, and hence strictly preferable.)\n",
    "\n",
    "Thus to maximize the payoff we have $\\mathbf w = \\mathbf z$.   (One technical exception would appear to exist-- when a portion of $\\mathbf w$ and $\\mathbf x$ are being allocated to some $\\sigma_k = 0$, and hence there is flexibility in the nullspace, but the point remains that we can do no better than setting $\\mathbf w = \\mathbf z$, and further note that $\\sigma_1 \\gt 0$ because $\\mathbf C \\neq \\mathbf 0$, and by the earlier exchange argument, we always prefer $\\sigma_1$ to a payoff of $\\sigma_k = 0$, hence we always allocate to $\\sigma_1$ if $\\sigma_1 \\gt \\sigma_k =0$, which is to say $\\mathbf w = \\mathbf z$ because we never choose to allocate to a payof of zero.  Put differently, and reminiscent of the posting titled \"Underdetermined_System_of_Equations\", allocating to a singular value of zero in $\\mathbf \\Sigma$, costs you with respect to length, but does not contribute to the payoff we are trying to maximize, hence we'd never allocate to such a singular value)\n",
    "- - - -\n",
    "\n",
    "Using our earlier change of variables equation, we can say $\\mathbf w = \\mathbf e_1 = \\mathbf U^T \\mathbf x$, thus $\\mathbf x = \\mathbf {U e}_1= \\mathbf u_1$.  Similarly $\\mathbf {V z} = \\mathbf {V e}_1 = \\mathbf v_1 = \\mathbf y$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the beginning of the posting we stated $\\mathbf C = \\mathbf A + \\mathbf B$, and we recognized that maximizing the bilinear form\n",
    "\n",
    "$max \\big(\\mathbf x^T \\mathbf C \\mathbf y\\big) = \\sigma_1$\n",
    "\n",
    "this can be rewritten as:\n",
    "\n",
    "$max \\big(\\mathbf x_C^T \\mathbf C \\mathbf y_C\\big) = \\sigma_{C,1}$\n",
    "\n",
    "Thus consider \n",
    "\n",
    "$max \\big(\\mathbf x_C^T \\mathbf C \\mathbf y_C\\big)$ vs $max \\big(\\mathbf x_{A}^T \\mathbf A \\mathbf y_{A} + \\mathbf x_{B}^T \\mathbf B \\mathbf y_{B}\\big)$\n",
    "\n",
    "with respect to the right hand side, we can simply notice that we are able to at least get a payoff equal to $\\sigma_{C,1}$, by setting $\\mathbf x_{A} : = \\mathbf x_{B} := \\mathbf x_C$ and also $\\mathbf y_{A} : = \\mathbf y_{B} := \\mathbf y_C$\n",
    "\n",
    "this yields:  \n",
    "\n",
    "$\\sigma_{A,1} + \\sigma_{B,1} = max \\big(\\mathbf x_{A}^T \\mathbf A \\mathbf y_{A} + \\mathbf x_{B}^T \\mathbf B \\mathbf y_{B}\\big) \\geq max \\big(\\mathbf x_{C}^T \\mathbf A \\mathbf y_{C} + \\mathbf x_{C}^T \\mathbf B \\mathbf y_{C}\\big) = max\\big(\\mathbf x_{C}^T \\Big( \\mathbf A + \\mathbf B \\Big) \\mathbf y_{C}  = max\\big(\\mathbf x_C^T \\mathbf C \\mathbf y_C\\big) = \\sigma_{C,1} $\n",
    "\n",
    "hence, using this inequality and the and the logic from the top cell, we are able to say\n",
    "\n",
    "$\\sigma_{A,1} + \\sigma_{B,1} \\geq \\sigma_{C,1}$\n",
    "\n",
    "\n",
    "or in the terms of operator norms,\n",
    "\n",
    "$\\big \\vert \\big \\vert \\mathbf A \\big \\vert \\big \\vert_2 + \\big \\vert \\big \\vert \\mathbf B \\big \\vert \\big \\vert_2 \\geq \\big \\vert \\big \\vert \\Big( \\mathbf A + \\mathbf B \\Big) \\big \\vert \\big \\vert_2$\n",
    "\n",
    "which we may re-arrange to:\n",
    "\n",
    "$\\big \\vert \\big \\vert \\Big( \\mathbf A + \\mathbf B \\Big) \\big \\vert \\big \\vert_2 \\leq \\big \\vert \\big \\vert \\mathbf A \\big \\vert \\big \\vert_2 + \\big \\vert \\big \\vert \\mathbf B \\big \\vert \\big \\vert_2  $\n",
    "\n",
    "\n",
    "An interesting extension is that we may notice:\n",
    "\n",
    "$ \\big \\vert \\big \\vert vec \\big( \\mathbf C\\big)  \\big \\vert \\big \\vert_2 =  \\big \\vert \\big \\vert \\Big( vec \\big( \\mathbf A \\big) + vec \\big( \\mathbf B\\big) \\Big) \\big \\vert \\big \\vert_2 \\leq \\big \\vert \\big \\vert vec\\big(\\mathbf A\\big) \\big \\vert \\big \\vert_2 + \\big \\vert \\big \\vert vec \\big(\\mathbf B\\big) \\big \\vert \\big \\vert_2 $\n",
    "\n",
    "by the Triangle Inequality, hence we can say\n",
    "\n",
    "$ \\big \\vert \\big \\vert \\Big( \\mathbf A + \\mathbf B \\Big) \\big \\vert \\big \\vert_F \\leq \\big \\vert \\big \\vert \\mathbf A \\big \\vert \\big \\vert_F + \\big \\vert \\big \\vert \\mathbf B \\big \\vert \\big \\vert_F $\n",
    "\n",
    "alternatively written as:\n",
    "\n",
    "$ \\Big(\\sigma_{C,1}^2 + \\sigma_{C,2}^2 + ... + \\sigma_{C,n}^2\\Big)^\\frac{1}{2} \\leq \\Big(\\sigma_{A,1}^2 + \\sigma_{A,2}^2 + ... + \\sigma_{A,n}^2\\Big)^\\frac{1}{2} + \\Big(\\sigma_{B,1}^2 + \\sigma_{B,2}^2 + ... + \\sigma_{B,n}^2\\Big)^\\frac{1}{2} $\n",
    "\n",
    "\n",
    "This final inequality seems to motivate a comparison between:\n",
    "\n",
    "$\\begin{bmatrix}\n",
    "\\sigma_{1,C} \\\\ \n",
    "\\sigma_{2,C} \\\\ \n",
    "\\vdots\\\\ \n",
    "\\sigma_{n,C}\n",
    "\\end{bmatrix}$ v.s. $\\begin{bmatrix}\n",
    "\\sigma_{1,A} \\\\ \n",
    "\\sigma_{2,A} \\\\ \n",
    "\\vdots\\\\ \n",
    "\\sigma_{n,A}\n",
    "\\end{bmatrix}, \\begin{bmatrix}\n",
    "\\sigma_{1,B} \\\\ \n",
    "\\sigma_{2,B} \\\\ \n",
    "\\vdots\\\\ \n",
    "\\sigma_{n,B}\n",
    "\\end{bmatrix}$\n",
    "\n",
    "Our most recently proved inequality tells us that the $L_2$ norm of the vector constaining the Singular Values for $\\mathbf B$ plus the $L_2$ norm of the vector containing the Singular Values for $\\mathbf A$ is $\\geq$ the $L_2$ norm of the vector containing the singular values for $\\mathbf C$.  \n",
    "\n",
    "This would seem to beg a simple question -- what about the $L_1$ norm comparison? Note that all of the singular values are real, and non-negative so we can reduce the $L_1$ norm comparison to the question:\n",
    "\n",
    "is it true that (assuming square matrices)\n",
    "\n",
    "$trace\\big(\\mathbf \\Sigma_C\\big)  \\leq trace\\big(\\mathbf \\Sigma_A\\big)  + trace\\big(\\mathbf \\Sigma_B\\big) $\n",
    "\n",
    "By analogy, we might note (assuming square matrices) that the sum of the eigenvalues of $\\mathbf A$ plus the sum of the eigenvalues of $\\mathbf B$ = sum of the eigenvalues of $\\mathbf C$, because $trace\\big(\\mathbf A\\big) + trace\\big(\\mathbf B \\big) = trace\\big(\\mathbf A + \\mathbf B\\big) = trace\\big(\\mathbf C \\big)$\n",
    "\n",
    "It is an interesting analogy, though of course the eigenvalues may be negative or even complex valued.  Things are a bit different since singular values are always real non-negative -- this would seem to be a hint that the triangle inequality will come into play.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#  L1 norm for singular values\n",
    "\n",
    "*note, if the matrix represents a graph, this would refer to the 'energy' of the matrix or graph.  This is also sometimes referred to as the Nuclear Norm.* \n",
    "\n",
    "\n",
    "**to start, this is for cases of** $n$ x $n$** matrices with scalars in **$\\mathbb C$.  This is the Ky Fan inequality where k = n for the case of square matrices.  The non-square case will be dealt with after.\n",
    "\n",
    "Where   $\\mathbf C = \\mathbf A + \\mathbf B $\n",
    "\n",
    "**claim: **\n",
    "    $\\big(\\sigma_{1,C} + \\sigma_{2,C} + ... + \\sigma_{n,C} \\big) \\leq \\big(\\sigma_{1,A} + \\sigma_{2,A} + ... + \\sigma_{n,A} \\big) + \\big(\\sigma_{1,B} + \\sigma_{2,B} + ... + \\sigma_{n,B} \\big) $\n",
    "   \n",
    "i.e. the sum of the singular values on the Left Hand Side is less than or equal to the sum of the singular values on the right hand side \n",
    "\n",
    "- - - \n",
    "*technical note: *\n",
    "\n",
    "\n",
    "$\\mathbf C = \\mathbf U_C \\mathbf \\Sigma_C \\mathbf V_C^H$  \n",
    "$\\mathbf A = \\mathbf U_A \\mathbf \\Sigma_A \\mathbf V_A^H$  \n",
    "$\\mathbf B = \\mathbf U_B \\mathbf \\Sigma_B \\mathbf V_B^H$  \n",
    "\n",
    "\n",
    "as always, assume well ordering: \n",
    "\n",
    "i.e. \n",
    "\n",
    "$\\sigma_{1,C} \\geq \\sigma_{2,C}\\geq ... \\geq \\sigma_{n,C} \\geq 0$  \n",
    "$\\sigma_{1,A} \\geq \\sigma_{2,A}\\geq ... \\geq \\sigma_{n,A} \\geq 0$  \n",
    "$\\sigma_{1,B} \\geq \\sigma_{2,B}\\geq ... \\geq \\sigma_{n,B} \\geq 0$  \n",
    "\n",
    "- - - \n",
    "\n",
    "**proof: **  \n",
    "$\\mathbf C = \\mathbf A + \\mathbf B $\n",
    "\n",
    "now multiply each side by (square, full rank) unitary matrices -- $\\mathbf U_C^H$ on the left and $\\mathbf V_C$ on the right.  \n",
    "\n",
    "$\\mathbf \\Sigma_C  = \\mathbf U_C^H \\mathbf C\\mathbf V_C = \\mathbf U_C^H\\big(\\mathbf A  + \\mathbf B\\big) \\mathbf V_C = \\mathbf U_C^H\\mathbf A \\mathbf V_C + \\mathbf U_C^H\\mathbf B \\mathbf V_C$\n",
    "\n",
    "since the Left Hand Side and the Right Hand Side are equal (and square), then their traces must be equal\n",
    "\n",
    "$ trace\\big(\\mathbf \\Sigma_C\\big)= trace\\big(\\mathbf U_C^H\\mathbf A \\mathbf V_C\\big) + trace\\big(\\mathbf U_C^H\\mathbf B \\mathbf V_C\\big) \\leq  \\Big\\vert trace\\big(\\mathbf U_C^H\\mathbf A \\mathbf V_C\\big) + trace\\big(\\mathbf U_C^H\\mathbf B \\mathbf V_C\\big)\\Big\\vert \\leq \\Big\\vert trace\\big(\\mathbf U_C^H\\mathbf A \\mathbf V_C\\big)\\Big\\vert + \\Big\\vert trace\\big(\\mathbf U_C^H\\mathbf B \\mathbf V_C\\big)\\Big\\vert $\n",
    "- - - - \n",
    "*comment:* in the above envisioned case, $trace\\big(\\mathbf U_C^H\\mathbf A \\mathbf V_C\\big) + trace\\big(\\mathbf U_C^H\\mathbf B \\mathbf V_C\\big) = \\Big\\vert trace\\big(\\mathbf U_C^H\\mathbf A \\mathbf V_C\\big) + trace\\big(\\mathbf U_C^H\\mathbf B \\mathbf V_C\\big)\\Big\\vert $ however we use the looser claim that this LHS is $\\leq$ RHS for convenience.  Then we apply the apply the Triangle Inequality to get $\\Big\\vert trace\\big(\\mathbf U_C^H\\mathbf A \\mathbf V_C\\big) + trace\\big(\\mathbf U_C^H\\mathbf B \\mathbf V_C\\big)\\Big\\vert \\leq \\Big\\vert trace\\big(\\mathbf U_C^H\\mathbf A \\mathbf V_C\\big)\\Big\\vert + \\Big\\vert trace\\big(\\mathbf U_C^H\\mathbf B \\mathbf V_C\\big)\\Big\\vert $\n",
    "- - - - \n",
    "$ trace\\big(\\mathbf \\Sigma_C\\big) \\leq \\Big\\vert trace\\big(\\mathbf U_C^H\\mathbf A \\mathbf V_C\\big)\\Big\\vert + \\Big\\vert trace\\big(\\mathbf U_C^H\\mathbf B \\mathbf V_C\\big)\\Big\\vert = \\Big\\vert trace\\big(\\mathbf U_C^H \\big(\\mathbf U_A \\mathbf \\Sigma_A \\mathbf V_A^H\\big) \\mathbf V_C\\big)\\Big\\vert + \\Big\\vert trace\\big(\\mathbf U_C^H \\big(\\mathbf U_B \\mathbf \\Sigma_B \\mathbf V_B^H\\big)\\mathbf V_C\\big)\\Big\\vert  $\n",
    "\n",
    "$ trace\\big(\\mathbf \\Sigma_C\\big) \\leq \\Big\\vert trace\\big(\\mathbf U_C^H\\mathbf A \\mathbf V_C\\big)\\Big\\vert + \\Big\\vert trace\\big(\\mathbf U_C^H\\mathbf B \\mathbf V_C\\big)\\Big\\vert = \\Big \\vert trace\\big(\\mathbf V_A^H \\mathbf V_C \\mathbf U_C^H \\mathbf U_A \\mathbf \\Sigma_A \\big)\\Big\\vert + \\Big\\vert trace\\big(\\mathbf V_B^H \\mathbf V_C \\mathbf U_C^H \\mathbf U_B \\mathbf \\Sigma_B \\big)\\Big\\vert  $\n",
    "\n",
    "Let  \n",
    "$\\mathbf Q_A = \\mathbf V_A^H \\mathbf V_C \\mathbf U_C^H \\mathbf U_A$  \n",
    "$\\mathbf Q_B = \\mathbf V_B^H \\mathbf V_C \\mathbf U_C^H \\mathbf U_B$\n",
    "\n",
    "where $\\mathbf Q_A$ and $\\mathbf Q_B$ are unitary\n",
    "\n",
    "Reference proof in \"Fun_with_Trace_and_Quadratic_Forms_and_CauchySchwarz.ipynb\" under heading 'Hermitian Positive Semi Definite Trace Inequalities' which states that the magnitude of the trace of [a unitary matrix times a Hermitian Positive semi-definite matrix] is less than or equal to the trace of said Hermitian Positive semi-definite matrix.  \n",
    "\n",
    "$\\Big\\vert trace\\big(\\mathbf Q_A \\mathbf \\Sigma_A \\big)\\Big\\vert \\leq trace\\big(\\mathbf \\Sigma_A\\big)$  \n",
    "$\\Big\\vert trace\\big(\\mathbf Q_B \\mathbf \\Sigma_B \\big)\\Big\\vert \\leq trace\\big(\\mathbf \\Sigma_B\\big)$\n",
    "\n",
    "Thus we have  \n",
    "\n",
    "$ trace\\big(\\mathbf \\Sigma_C\\big) \\leq \\Big\\vert trace\\big(\\mathbf U_C^H\\mathbf A \\mathbf V_C\\big)\\Big\\vert + \\Big\\vert trace\\big(\\mathbf U_C^H\\mathbf B \\mathbf V_C\\big)\\Big\\vert = \\Big \\vert trace\\big(\\mathbf Q_A \\mathbf \\Sigma_A \\big)\\Big\\vert + \\Big\\vert trace\\big(\\mathbf Q_B \\mathbf \\Sigma_B \\big)\\Big\\vert \\leq   trace\\big(\\mathbf \\Sigma_A\\big) + trace\\big(\\mathbf \\Sigma_B\\big)$\n",
    "\n",
    "hence \n",
    "\n",
    "$\\big(\\sigma_{1,C} + \\sigma_{2,C} + ... + \\sigma_{n,C} \\big) = trace\\big(\\mathbf \\Sigma_C\\big) \\leq trace\\big(\\mathbf \\Sigma_A\\big) + trace\\big(\\mathbf \\Sigma_B\\big) = \\big(\\sigma_{1,A} + \\sigma_{2,A} + ... + \\sigma_{n,A} \\big) + \\big(\\sigma_{1,B} + \\sigma_{2,B} + ... + \\sigma_{n,B} \\big) $\n",
    "\n",
    "QED\n",
    "\n",
    "\n",
    "**extension**\n",
    "\n",
    "what if we are dealing with non-square matrices?\n",
    "\n",
    "for cases of $m$ x $n$ matrices with scalars in $\\mathbb C$, where $ m \\neq n$, but still  $\\mathbf C = \\mathbf A + \\mathbf B $\n",
    "\n",
    "\n",
    "*ugly solution: *\n",
    "\n",
    "*1)* if $m \\gt n$ append columns of zeros until all matrices are square.  note that pre augmentation\n",
    "\n",
    "where we use $\\mathbf X$ to stand in for any arbitrary matrix that is m x n (and this includes $\\mathbf A$, $\\mathbf B$, and $\\mathbf C$)\n",
    "\n",
    "$\\mathbf X = \\bigg[\\begin{array}{c|c|c|c}\n",
    "\\mathbf x_1 & \\mathbf x_2 &\\cdots & \\mathbf x_{n}\n",
    "\\end{array}\\bigg] $\n",
    "\n",
    "$\\mathbf X \\mathbf X ^H = \\mathbf x_1 \\mathbf x_1^H + \\mathbf x_2 \\mathbf x_2^H + ...  + \\mathbf x_n \\mathbf x_n^H$\n",
    "\n",
    "and after augmentation to make it square:  \n",
    "\n",
    "$\\mathbf X = \\bigg[\\begin{array}{c|c|c|c|c|c|c|c}\n",
    "\\mathbf x_1 & \\mathbf x_2 &\\cdots & \\mathbf x_{n} & \\mathbf 0 & \\mathbf 0  &\\cdots & \\mathbf 0\n",
    "\\end{array}\\bigg] $\n",
    "\n",
    "$\\mathbf X \\mathbf X ^H = \\mathbf x_1 \\mathbf x_1^H + \\mathbf x_2 \\mathbf x_2^H + ...  + \\mathbf x_n \\mathbf x_n^H + \\mathbf{00}^H + \\mathbf{00}^H + ... + \\mathbf{00}^H$\n",
    "\n",
    "and we verify that pre-agumentation and post augmentation $\\mathbf X \\mathbf X^H$ is unchanged.  Since all non-zero singular values of $\\mathbf X$ come as the positive square root of the non-zero eigenvalues of $\\mathbf X \\mathbf X^H$, then we confirm that the non-zero singular values of $\\mathbf X$ are unchanged when it is augmented to become square. Note: appending columns filled with zeros will increase the number of singular values equal to zero but the key thing is it leaves the non-zero singular values intact, and hence the value of the sum of singular values for $\\mathbf X$ does not change.\n",
    "\n",
    "Hence after appending columns of zeros until all matrices are square, w can repeat the proof above and find that \n",
    "\n",
    "$\\big(\\sigma_{1,C} + \\sigma_{2,C} + ... + \\sigma_{n,C} \\big) \\leq \\big(\\sigma_{1,A} + \\sigma_{2,A} + ... + \\sigma_{n,A} \\big) + \\big(\\sigma_{1,B} + \\sigma_{2,B} + ... + \\sigma_{n,B} \\big) $\n",
    "\n",
    "\n",
    "*2)* if $m \\lt n$, instead consider the conjugate transpose of each side, noting that taking the conjugate transpose does not alter the singular values, \n",
    "\n",
    "$\\mathbf C^H = \\mathbf A^H + \\mathbf B^H$\n",
    "\n",
    "\n",
    "and now we have more rows than columns, and may repeat the above argument, \n",
    "\n",
    "$\\big(\\sigma_{1,C^H} + \\sigma_{2,C^H} + ... + \\sigma_{n,C^H} \\big) \\leq \\big(\\sigma_{1,A^H} + \\sigma_{2,A^H} + ... + \\sigma_{n,A^H} \\big) + \\big(\\sigma_{1,B^H} + \\sigma_{2,B^H} + ... + \\sigma_{n,B^H} \\big) $\n",
    "\n",
    "or if we'd like to ignore indexing issues\n",
    "\n",
    "$\\sum_{i} \\sigma_{i,C} = \\sum_i \\sigma_{i,C^H} \\leq \\big(\\sum_i \\sigma_{i,A^H}\\big) + \\big(\\sum_i \\sigma_{i,B^H}\\big) = \\big(\\sum_i \\sigma_{i,A}\\big) + \\big(\\sum_i \\sigma_{i,B}\\big) $\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "3.15071427816 3.03046779948 \n",
      "\n",
      "\n",
      "[ 2.01728529  0.71988691  0.32518378  0.0883583 ] \n",
      "\n",
      "[ 1.93983449  0.68268476  0.30557087  0.10237768]\n",
      "[1.939834492232775, 0.68268475904870007, -0.10237767976497554, -0.30557086842981457]\n"
     ]
    }
   ],
   "source": [
    "m = 4\n",
    "\n",
    "A = np.random.random((m,m))\n",
    "sigma_x = np.linalg.svd(A, False, False)\n",
    "\n",
    "Y = 0.5*(A + A.T)\n",
    "sigma_y = np.linalg.svd(Y, False, False)\n",
    "eig_Y = sorted(np.linalg.eigvalsh(Y), reverse=True)\n",
    "\n",
    "print(np.sum(sigma_x) >= np.sum(sigma_y))\n",
    "print(np.sum(sigma_x), np.sum(sigma_y), \"\\n\\n\")\n",
    "print(sigma_x, \"\\n\")\n",
    "print(sigma_y)\n",
    "print(eig_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.31557086843\n",
      "True\n",
      "4.41299775188 3.47685417681 \n",
      "\n",
      "\n",
      "[ 2.33285615  1.03545778  0.64075465  0.40392917] \n",
      "\n",
      "[ 2.25540536  0.99825563  0.21319319  0.01      ]\n",
      "[2.2554053606625888, 0.99825562747851371, 0.21319318866483897, 0.010000000000000106]\n"
     ]
    }
   ],
   "source": [
    "alpha = abs(eig_Y[-1])+0.01\n",
    "print(alpha)\n",
    "\n",
    "sigma_x += alpha\n",
    "Y += np.identity(m) * alpha\n",
    "\n",
    "sigma_y = np.linalg.svd(Y, False, False)\n",
    "eig_Y = sorted(np.linalg.eigvalsh(Y), reverse=True)\n",
    "\n",
    "print(np.sum(sigma_x ) >= np.sum(sigma_y))\n",
    "print(np.sum(sigma_x), np.sum(sigma_y), \"\\n\\n\")\n",
    "print(sigma_x, \"\\n\")\n",
    "print(sigma_y)\n",
    "print(eig_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [py35]",
   "language": "python",
   "name": "Python [py35]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
