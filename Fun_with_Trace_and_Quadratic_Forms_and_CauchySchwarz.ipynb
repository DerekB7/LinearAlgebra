{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Proof of Cauchy Schwarz\n",
    "\n",
    "the underlying scalars are in $\\mathbb C$ and we have $n $ x $1$ vectors. \n",
    "\n",
    "consider some vector $\\mathbf x =  \\begin{bmatrix}\n",
    "x_1\\\\ \n",
    "x_2\\\\ \n",
    "\\vdots \\\\ \n",
    "x_n\n",
    "\\end{bmatrix}$\n",
    "\n",
    "and some other vector,  $\\mathbf y =  \\begin{bmatrix}\n",
    "y_1\\\\ \n",
    "y_2\\\\ \n",
    "\\vdots \\\\ \n",
    "y_n\n",
    "\\end{bmatrix}$\n",
    "\n",
    "now consider the outer product given by \n",
    "\n",
    "$\\mathbf {xy}^H =  \\begin{bmatrix}\n",
    "x_1 \\bar{y_1} & x_1 \\bar{y_2} &...& x_1 \\bar{y_n}\\\\ \n",
    "x_2 \\bar{y_1} & x_2 \\bar{y_2} & ... & x_2 \\bar{y_n} \\\\ \n",
    "\\vdots & \\vdots & \\ddots &\\vdots \\\\ \n",
    "x_n \\bar{y_1} & x_n \\bar{y_2} &.... & x_n \\bar{y_n}\n",
    "\\end{bmatrix}$\n",
    "\n",
    "\n",
    "From here, consider where we want the squared Frobenius norm of $\\mathbf {xy}^H$ \n",
    "\n",
    "$\\big\\vert\\big\\vert \\mathbf {xy}^H\\big\\vert\\big\\vert_{F}^2 = trace\\Big(\\big(\\mathbf {xy}^H\\big)^H \\big( \\mathbf {xy}^H\\big)\\Big) = trace\\Big( \\mathbf {yx}^H \\mathbf {xy}^H\\Big) = trace\\Big( \\mathbf{y}^H \\mathbf{yx}^H \\mathbf {x}\\Big) = \\mathbf {y}^H\\mathbf y \\mathbf x^H \\mathbf {x}  = \\big\\vert\\big\\vert \\mathbf{y}\\big\\vert\\big\\vert_{2}^2\\big\\vert\\big\\vert \\mathbf {x}\\big\\vert\\big\\vert_{2}^2$\n",
    "\n",
    "where the middle equalities made use of the cyclic property of the trace\n",
    "\n",
    "We conclude the proof with the following:\n",
    "\n",
    "$ \\big\\vert\\big\\vert \\mathbf{y}\\big\\vert\\big\\vert_{2}^2\\big\\vert\\big\\vert \\mathbf {x}\\big\\vert\\big\\vert_{2}^2 =  trace\\Big(\\big(\\mathbf {\\mathbf {xy}}^H\\big)^H \\big( \\mathbf {\\mathbf {xy}}^H\\big)\\Big) \n",
    " \\geq \\big \\vert trace\\Big( \\big(\\mathbf {xy}^H\\big) \\big(\\mathbf {xy}^H\\big)\\Big)\\big \\vert =  \\big \\vert trace\\Big( \\mathbf y^H \\mathbf {xy}^H\\mathbf {x}\\Big)\\big \\vert  = \\big \\vert trace\\Big( \\big(\\mathbf y^H \\mathbf {x}\\big) \\big(\\mathbf y^H \\mathbf {x}\\big)\\Big)\\big \\vert = \\big \\vert \\mathbf y^H \\mathbf x \\big \\vert^2 $\n",
    "\n",
    "hence \n",
    "\n",
    "$\\big\\vert\\big\\vert \\mathbf{y}\\big\\vert\\big\\vert_{2}^2\\big\\vert\\big\\vert \\mathbf {x}\\big\\vert\\big\\vert_{2}^2 = \\Big(\\Sigma_{i=1}^{n}\\big\\vert y_i\\big\\vert^2\\Big) \\Big(\\Sigma_{i=1}^{n}\\big\\vert x_i\\big\\vert^2\\Big)  \\geq \\big \\vert\\Big(\\Sigma_{i=1}^{n}x_i \\bar{y_i}\\Big)^2\\big \\vert= \\big \\vert \\mathbf y^H \\mathbf x \\big \\vert^2 $\n",
    "\n",
    "\n",
    "noting that \n",
    "\n",
    "$trace\\Big(\\big(\\mathbf {\\mathbf {xy}}^H\\big)^H \\big( \\mathbf {\\mathbf {xy}}^H\\big)\\Big)\n",
    "\\geq \\big \\vert trace\\Big( \\big(\\mathbf {xy}^H\\big) \\big(\\mathbf {xy}^H\\big)\\Big)\\big \\vert $\n",
    "\n",
    "via the Schur Inequality in the special case of a rank one matrix (and hence triangle inequality is *not* required here).  \n",
    "\n",
    "- - - - \n",
    "\n",
    "**extension: **  \n",
    "Suppose we we relax the requirement that an inner product is positive definite, and instead allow it to be positive semi-definite.  (Sometimes this may be referred to as a semi inner product.)\n",
    "\n",
    "\n",
    "for some Hermitian positive semi-definite $\\mathbf A$:   \n",
    "$\\langle \\mathbf x, \\mathbf y \\rangle = \\mathbf x^H \\mathbf A \\mathbf y =  \\big(\\mathbf A^{\\frac{1}{2}} \\mathbf x\\big)^H \\big(\\mathbf A^{\\frac{1}{2}}\\mathbf y\\big)$\n",
    "\n",
    "\n",
    "$\\mathbf b:= \\mathbf A^{\\frac{1}{2}} \\mathbf x$  \n",
    "$\\mathbf c:= \\mathbf A^{\\frac{1}{2}} \\mathbf y$  \n",
    "\n",
    "$\\mathbf \\langle \\mathbf x, \\mathbf y \\rangle = \\mathbf b^H \\mathbf c$\n",
    "\n",
    "and we verify that Cauchy's Inequality still holds by repeating the above argument: \n",
    "\n",
    "$\\langle \\mathbf y, \\mathbf y \\rangle \\langle \\mathbf x, \\mathbf x \\rangle =  \\big(\\mathbf{c}^H\\mathbf c\\big)\\big(\\mathbf b^H\\mathbf b\\big) =  trace\\Big(\\big(\\mathbf {\\mathbf {bc}}^H\\big)^H \\big( \\mathbf {\\mathbf {bc}}^H\\big)\\Big) \n",
    " \\geq \\big \\vert trace\\Big( \\big(\\mathbf {bc}^H\\big)^2 \\Big)\\big \\vert =  \\big \\vert trace\\Big( \\mathbf c^H \\mathbf {bc}^H\\mathbf {b}\\Big)\\big \\vert  = \\big \\vert \\mathbf c^H \\mathbf b \\big \\vert^2 = \\big \\vert \\mathbf b^H \\mathbf c \\big \\vert^2  = \\big \\vert \\langle \\mathbf x, \\mathbf y \\rangle \\big \\vert^2 $\n",
    "\n",
    "again by applying Schur's Inequality to the special case of a rank one matrix, we generate the above inequality, i.e. that $trace\\Big(\\big(\\mathbf {\\mathbf {bc}}^H\\big)^H \\big( \\mathbf {\\mathbf {bc}}^H\\big)\\Big) \n",
    " \\geq \\big \\vert trace\\Big( \\big(\\mathbf {bc}^H\\big)^2 \\Big)\\big \\vert $\n",
    "\n",
    "This extension confirms that Cauchy's Inequality still applies to (finite) inner products that have relaxed the positive definite criterion to positive semi-definite.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alternative Approach that does not use Schur Inequality:\n",
    "\n",
    "note that instead of using the Schur Inequality,  consider the maximum eigenvalue problem, where we have a rank one matrix, $\\mathbf B$.\n",
    "\n",
    "\n",
    "$\\mathbf B = \\mathbf{xy}^H$.  While we know that $\\mathbf B$ is a rank one matrix, the argument to be made is even more general: the magnitude of the largest eigenvalue of $\\big(\\mathbf {BB}\\big)$ is $\\leq$ the largest eigenvalue of $\\mathbf B^H \\mathbf B$, or equivalently, the magnitude of the largest eigenvalue of $\\mathbf B$ ($\\lambda_1$) is $\\leq$ the largest singular value of $\\mathbf B$ ($\\sigma_1$).\n",
    "\n",
    "The approach taken here uses quadratic forms.  So consider the case of maximizing $\\mathbf B^H \\mathbf B$ vs $\\mathbf {BB}$\n",
    "\n",
    "max $\\big \\vert \\mathbf v^H \\mathbf B^H \\mathbf B \\mathbf v \\big \\vert \\geq$ max $\\big \\vert \\mathbf v^H \\mathbf{BB}\\mathbf v \\big \\vert$\n",
    "\n",
    "where we constrain the length (2 norm) of $\\mathbf v$, which for simplicity will be one: $\\mathbf v^H \\mathbf v = 1 = \\big \\vert \\big \\vert \\mathbf v \\big \\vert \\big \\vert_2^{2} = \\big \\vert \\big \\vert \\mathbf v \\big \\vert \\big \\vert_2$ \n",
    "\n",
    "We know via diagonalization arguments (and Lagrange Multipliers), that some quadratic form $\\big \\vert \\mathbf v^H \\mathbf C \\mathbf v\\big \\vert$ subject to $\\mathbf v ^H \\mathbf v = 1$ is maximized when all of $\\mathbf v$ is allocated to the eigenvalue(s) with the largest magnitude of the Hermitian matrix $\\mathbf C$.  Unfortunately, we have no reason to believe $\\mathbf {BB}$ is Hermitian or even non-defective, which complicates things a bit.  But consider having well ordered eigenvalues for $\\mathbf B$ where $\\big \\vert \\lambda_1 \\big \\vert \\geq \\big \\vert\\lambda_2 \\big \\vert\\geq \\big \\vert \\lambda_3 \\big \\vert \\geq ... \\geq \\big \\vert\\lambda_n \\big \\vert$, with associated eigenvectors $\\mathbf v_1, \\mathbf v_2, \\mathbf v_3, ... , \\mathbf v_n$.  Note that there is a simple argument which tells us that allocating to eigenvector $\\mathbf v_k$ where $k \\geq 2$ is (weakly) dominated by $\\mathbf v_1$.  \n",
    "\n",
    "$\\big \\vert \\mathbf v_1^H \\mathbf{BB}\\mathbf v_1 \\big \\vert \\geq \\big \\vert \\mathbf v_k^H \\mathbf{BB}\\mathbf v_k \\big \\vert$\n",
    "\n",
    "$\\big \\vert \\mathbf v_1^H \\mathbf{B}\\big(\\mathbf B \\mathbf v_1\\big) \\big \\vert \\geq \\big \\vert \\mathbf v_k^H \\mathbf{B}\\big( \\mathbf B \\mathbf v_k \\big) \\big \\vert$\n",
    "\n",
    "$\\big \\vert \\mathbf v_1^H \\mathbf{B} \\lambda_1 \\mathbf v_1 \\big \\vert \\geq \\big \\vert\\mathbf v_k^H \\mathbf{B}\\lambda_k \\mathbf v_k \\big \\vert$\n",
    "\n",
    "$\\big \\vert \\lambda_1 \\mathbf v_1^H \\big(\\mathbf{B} \\mathbf v_1\\big) \\big \\vert \\geq \\big \\vert \\lambda_k \\mathbf v_k^H \\big(\\mathbf{B} \\mathbf v_k\\big) \\big \\vert$\n",
    "\n",
    "$\\big \\vert \\lambda_1 \\mathbf v_1^H \\lambda_1 \\mathbf v_1 \\big \\vert \\geq \\big \\vert \\lambda_k \\mathbf v_k^H \\lambda_k \\mathbf v_k \\big \\vert$\n",
    "\n",
    "$\\big \\vert \\lambda_1^2 (\\mathbf v_1^H \\mathbf v_1) \\big \\vert \\geq \\big \\vert \\lambda_k^2 (\\mathbf v_k^H \\mathbf v_k) \\big \\vert$\n",
    "\n",
    "$\\big \\vert \\lambda_1^2 *1\\big \\vert \\geq \\big \\vert \\lambda_k^2 *1 \\big \\vert$  \n",
    "$\\big \\vert \\lambda_1^2\\big \\vert \\geq \\big \\vert\\lambda_k^2 \\big \\vert$  \n",
    "$\\big \\vert \\lambda_1\\big \\vert^2 \\geq \\big \\vert\\lambda_k \\big \\vert^2$  \n",
    "$\\big \\vert \\lambda_1\\big \\vert \\geq \\big \\vert\\lambda_k \\big \\vert$   \n",
    "\n",
    "hence we have a simple exchange argument that tells us any time we allocate to $\\mathbf v_k$ we can get a result greater than or equal to it, by allocating that amount instead to $\\mathbf v_1$.  Thus, with respect to a maximization problem using the eigenvectors of $\\mathbf B$, we can do no better than choosing the eigenpair $\\lambda_1, \\mathbf v_1$.\n",
    "\n",
    "We return to our original equation, with respect to eigenvalues:\n",
    "\n",
    "max $\\big \\vert  \\mathbf v^H \\mathbf B^H \\mathbf B \\mathbf v \\big \\vert \\geq$ max $\\big \\vert \\mathbf v_1^H \\mathbf{BB}\\mathbf v_1 \\big \\vert$\n",
    "\n",
    "max $\\big \\vert \\mathbf v^H \\mathbf B^H \\mathbf B \\mathbf v \\big \\vert = $ max $ \\mathbf v^H \\mathbf B^H \\mathbf B \\mathbf v  \\geq \\big \\vert \\lambda_1^2\\big \\vert $\n",
    "\n",
    "note that we always have the option / backup plan, on the left hand side, of also allocating to $\\mathbf v_1$.  Put differently, if we are lazy, we know that by setting $\\mathbf v := \\mathbf v_1$ we'll always get a 'payoff' with magnitude equal to $\\big\\vert\\lambda_1\\big\\vert^2$ -- thus when maximizing the magnitude of $\\big \\vert \\mathbf v^H \\mathbf B^H \\mathbf B \\mathbf v \\big \\vert$ we'll get a result at least as good as $\\big\\vert\\lambda_1\\big\\vert^2$.  Symbolically, this is shown below.\n",
    "\n",
    "$\\mathbf v_1^H \\mathbf B^H \\mathbf B \\mathbf v_1 = \\big \\vert \\lambda_1\\big \\vert^2 $  \n",
    "\n",
    "$\\big(\\mathbf v_1^H \\mathbf B^H\\big) \\big(\\mathbf B \\mathbf v_1 \\big) = \\big \\vert \\lambda_1\\big \\vert^2 $  \n",
    "\n",
    "$\\big(\\mathbf {B}\\mathbf v_1\\big)^H \\big(\\mathbf B \\mathbf v_1 \\big) = \\big \\vert \\lambda_1 \\big \\vert^2 $  \n",
    "$\\big(\\lambda_1 \\mathbf v_1\\big)^H \\big(\\lambda_1 \\mathbf v_1 \\big) = \\big \\vert \\lambda_1 \\big \\vert^2 $  \n",
    "$\\lambda_1^H \\lambda_1 \\big(\\mathbf v_1^H \\mathbf v_1\\big) = \\big \\vert \\lambda_1 \\big \\vert^2 $  \n",
    "$\\lambda_1^H \\lambda_1 *1  = \\big \\vert \\lambda_1\\big \\vert^2 $    \n",
    "$\\lambda_1^H \\lambda_1  = \\big \\vert \\lambda_1 \\big \\vert^2 $  \n",
    "\n",
    "\n",
    "- - - - \n",
    "*begin detour: reminder about complex number maths*   \n",
    "  \n",
    "Consider that we can simply note that $\\big \\vert \\lambda_1^2\\big \\vert = \\big \\vert \\lambda_1 \\big \\vert^2 = \\lambda_1^H \\lambda_1$.\n",
    "\n",
    "Alternatively, for a more granular view, consider the case where:  \n",
    "$\\lambda_1 = \\alpha - \\beta i $, where $\\alpha$ and $\\beta$ are real valued scalars. Accordingly, the magnitude of $\\lambda_1$ is $\\big\\vert \\lambda_1\\big\\vert = \\big(\\alpha^2 + \\beta^2\\big)^\\frac{1}{2}$. Then $\\lambda_1^2 = \\alpha^2 + \\beta^2  i^2 - 2\\alpha\\beta i = \\alpha^2 - \\beta^2 - 2\\alpha\\beta i$, with magnitude of \n",
    "\n",
    "$\\big \\vert \\lambda_1^2\\big \\vert = \\Big(\\big(\\alpha^2 - \\beta^2\\big)^2 + \\big( - 2\\alpha\\beta\\big)^2\\Big)^{\\frac{1}{2}}= \\Big(\\alpha^4 + \\beta^4 - 2 \\alpha^2 \\beta^2 + 4 \\alpha^2\\beta^2)\\Big)^{\\frac{1}{2}} $ \n",
    "\n",
    "$\\big \\vert \\lambda_1^2\\big \\vert = \\Big(\\alpha^4 + \\beta^4 + 2 \\alpha^2 \\beta^2 \\Big)^{\\frac{1}{2}}$\n",
    "\n",
    "and note that $\\lambda_1 ^H \\lambda_1 = \\alpha^2 + \\beta^2$, with magnitude equal to   \n",
    "\n",
    "$\\big \\vert \\lambda_1^H \\lambda_1 \\big \\vert = \\Big(\\big(\\alpha^2 + \\beta^2\\big)^2 + \\big(0\\big)^2 \\Big)^{\\frac{1}{2}} = \\Big(\\alpha^4 + \\beta^4 + 2 \\alpha \\beta \\Big)^{\\frac{1}{2}} = \\big \\vert \\lambda_1^2\\big \\vert  $\n",
    "\n",
    "$ \\big \\vert \\lambda_1^H \\lambda_1 \\big \\vert = \\lambda_1 ^H \\lambda_1 = \\alpha^2 + \\beta^2  = \\Big(\\big(\\alpha^2 + \\beta^2\\big)^\\frac{1}{2}\\Big)^2 = \\big\\vert \\lambda_1\\big\\vert ^2$  \n",
    "  \n",
    "*end detour* \n",
    "- - - - \n",
    "\n",
    "Thus when trying to maximize the magnitude of $\\mathbf v^H \\mathbf B^H \\mathbf B \\mathbf v$, we have a lower bound equal to $ \\big \\vert \\lambda_1 \\big\\vert^2$.  Equivalently, we can say: $\\sigma_1^2 \\geq \\big \\vert \\lambda_1 \\big\\vert^2$ and $\\sigma_1 \\geq \\big \\vert \\lambda_1 \\big\\vert$, where $\\sigma_1$ is the largest singular value of $\\mathbf B$ and thus $\\sigma_1^2$ is the largest eigenvalue of $\\big(\\mathbf B^H \\mathbf B\\big)$\n",
    "\n",
    "For a simple example of this fact, consider:\n",
    "\n",
    "$\\mathbf B = \\left[\\begin{matrix}2 & 3 & 4\\\\4 & 10 & -1\\\\1 & 3 & 4\\end{matrix}\\right]$\n",
    "\n",
    "where $\\lambda_1 \\approx 11.57$, but $\\sigma_1 \\approx 11.87$, hence $\\sigma_1$ exceeds the lower bound set by $\\lambda_1$ \n",
    "- - - -\n",
    "Back to the original problem at hand, in the special case where $\\mathbf B$ is a square, rank one matrix  \n",
    "\n",
    "$trace \\big(\\mathbf B ^H \\mathbf B\\big) \\geq \\big \\vert trace\\big(\\mathbf{BB}\\big) \\big \\vert$, because  \n",
    "\n",
    "$trace \\big(\\mathbf B ^H \\mathbf B\\big) = \\sigma_1^2$ and $trace\\big(\\mathbf{BB}\\big) = \\lambda_1^2 $, and we know $\\sigma_1^2 \\geq \\big \\vert \\lambda_1 \\big\\vert^2 = \\big \\vert \\lambda_1^2 \\big\\vert $\n",
    "\n",
    "set: $\\mathbf B:= \\mathbf{xy}^H$ and Cauchy Schwartz simply follows\n",
    "\n",
    "$ \\big\\vert\\big\\vert \\mathbf{y}\\big\\vert\\big\\vert_{2}^2\\big\\vert\\big\\vert \\mathbf {x}\\big\\vert\\big\\vert_{2}^2 = \\big ( \\mathbf y^H \\mathbf y \\big ) \\big ( \\mathbf x^H \\mathbf x \\big ) = trace\\Big(\\big(\\mathbf {\\mathbf {xy}}^H\\big)^H \\big( \\mathbf {\\mathbf {xy}}^H\\big)\\Big) \n",
    "\\geq \\big \\vert trace\\Big( \\big(\\mathbf {xy}^H\\big) \\big(\\mathbf {xy}^H\\big)\\Big) \\big \\vert =  \\big \\vert trace\\Big( \\mathbf y^H \\mathbf {xy}^H\\mathbf {x}\\Big) \\big \\vert = \\big \\vert \\mathbf y^H \\mathbf x \\big \\vert^2 $\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Smallest Eigenvalues and Singular Values\n",
    "\n",
    "Note that the above analysis can be easily extended with respect to the magnitude of the smallest eigenvalue of $\\mathbf B$ and the smallest singular value of $\\mathbf B$, again where $\\mathbf B \\in \\mathbb C^{n x n}$.\n",
    "\n",
    "if we want to minimize $\\big(\\mathbf v_n^H \\mathbf B^H\\big) \\big(\\mathbf B \\mathbf v_n \\big) $ we always have the option of allocating to $\\lambda_n$\n",
    "\n",
    "recall our length constraint:  $\\big \\vert \\big \\vert \\mathbf v \\big \\vert \\big \\vert_2^{2} = 1$ \n",
    "\n",
    "\n",
    "$\\mathbf v_n^H \\mathbf B^H \\mathbf B \\mathbf v_n = \\big \\vert \\lambda_n^2\\big \\vert $  \n",
    "\n",
    "$\\big(\\mathbf v_n^H \\mathbf B^H\\big) \\big(\\mathbf B \\mathbf v_n \\big) = \\big \\vert \\lambda_n^2\\big \\vert $  \n",
    "\n",
    "$\\big(\\mathbf {B}\\mathbf v_n\\big)^H \\big(\\mathbf B \\mathbf v_n \\big) = \\big \\vert \\lambda_n^2 \\big \\vert $  \n",
    "$\\big(\\lambda_n \\mathbf v_n\\big)^H \\big(\\lambda_n \\mathbf v_1 \\big) = \\big \\vert \\lambda_n^2 \\big \\vert $  \n",
    "$\\lambda_n^H \\lambda_n \\big(\\mathbf v_1^H \\mathbf v_1\\big) = \\big \\vert \\lambda_n^2 \\big \\vert $  \n",
    "$\\lambda_n^H \\lambda_n *1  = \\big \\vert \\lambda_n^2\\big \\vert $    \n",
    "$\\lambda_n^H \\lambda_n  = \\big \\vert \\lambda_n \\big \\vert^2 $  \n",
    "\n",
    "Since allocating everything to $\\sigma_n^2$ is the (weakly) dominant solution for minimizing $\\mathbf v^H \\mathbf B^H \\mathbf B \\mathbf v$, we can upper bound $\\sigma_n^2 \\leq \\big\\vert \\lambda_n\\big \\vert^2 $ and equivalently say that $\\sigma_n \\leq \\big\\vert \\lambda_n\\big \\vert $\n",
    "\n",
    "\n",
    "# Quadratic Forms and recovering a Frobenius Norm\n",
    "\n",
    "as usual, assume we have well ordered singular values\n",
    "\n",
    "$\\sigma_1 \\geq \\sigma_2 \\geq .... \\geq \\sigma_n \\geq 0$\n",
    "\n",
    "It is worth remarking that if we were to do our optimization problem\n",
    "\n",
    "max $\\mathbf v_1^H \\mathbf B^H \\mathbf B \\mathbf v_1$\n",
    "\n",
    "we recover $\\sigma_1^2$.  Then if we continue doing this optimization problem for $k = \\{2, 3, 4, ... , n\\}$\n",
    "\n",
    "max $\\mathbf v_k^H \\mathbf B^H \\mathbf B \\mathbf v_k$\n",
    "\n",
    "where each $\\big \\Vert \\mathbf v_k\\big \\Vert_2^2 = 1$, **with the added constraint that** $\\mathbf v_k \\perp \\mathbf v_j$, for $j = \\{1, 2, ... , k-1\\}$\n",
    "\n",
    "i.e. each $\\mathbf v_k$ is mutually orthonormal to the $\\mathbf v$'s that come before it,\n",
    "\n",
    "Then we recover \n",
    "\n",
    "$\\mathbf V = \n",
    "\\bigg[\\begin{array}{c|c|c|c}\n",
    "\\mathbf v_1 & \\mathbf v_2 &\\cdots & \\mathbf v_{n}\n",
    "\\end{array}\\bigg]\n",
    "$   \n",
    "\n",
    "where $\\mathbf V$ is a unitary matrix (or in reals, orthogonal).  Put differently, we recover a coordinate system.  \n",
    "\n",
    "And more to the point, we also can collect the 'payoffs': $\\{ \\sigma_1^2, \\sigma_2^2, \\sigma_3^2, ..., \\sigma_n^2 \\}$. from this maximization process.  We could sum up all of these squared singular values, and get\n",
    "\n",
    "$\\sigma_1^2 + \\sigma_2^2+ \\sigma_3^2+ ...+\\sigma_n^2 = \\big \\Vert \\mathbf B \\big \\Vert_F^2$\n",
    "\n",
    "i.e. when we sum up all of these 'payoffs' from our complete quadratic form process, we get the squared Frobenius norm for our matrix $\\mathbf B$. \n",
    "\n",
    "\n",
    "# On Unitary Matrices\n",
    "Note that there is a special case of interest.  Suppose that we have a square unitary matrix $\\mathbf Q$. We know that all eigenvalues of $\\mathbf Q $ have magnitude of 1.  Why? There are multiple approaches, but an elegant one uses the above knowledge with the singular value decomposition:\n",
    "\n",
    "\n",
    "$\\mathbf Q = \\mathbf{U \\Sigma V}^H$\n",
    "\n",
    "$\\mathbf Q^H \\mathbf Q  = \\mathbf I = \\big(\\mathbf{U \\Sigma V}^H\\big)^H \\mathbf{U \\Sigma V}^H  =\\mathbf V \\mathbf\\Sigma^2  \\mathbf V^H$\n",
    "\n",
    "left multiply by $\\mathbf V^H$ and right multiply by $\\mathbf V$, recalling\n",
    "that $\\mathbf V$ is a square, full rank matrix\n",
    "\n",
    "$\\mathbf V^H \\mathbf I \\mathbf V = \\mathbf I = \\mathbf V^H \\big(\\mathbf V \\mathbf\\Sigma^2  \\mathbf V^H\\big) \\mathbf V =\\mathbf \\Sigma^2$\n",
    "\n",
    "$\\mathbf I = \\mathbf \\Sigma^2$\n",
    "\n",
    "recalling that each singular value, by construction, is real and non-negative, we can then determine:\n",
    "\n",
    "$\\mathbf I = \\mathbf \\Sigma$  \n",
    "\n",
    "Thus we know that $\\mathbf \\Sigma$ is itself the Identity matrix (i.e. $\\sigma_1 = \\sigma_2 = ... = \\sigma_n = 1$)\n",
    "\n",
    "When we consider the eigenvalues of $\\mathbf Q$, where $\\big \\vert \\lambda_1 \\big \\vert \\geq \\big \\vert\\lambda_2 \\big \\vert\\geq \\big \\vert \\lambda_3 \\big \\vert \\geq ... \\geq \\big \\vert\\lambda_n \\big \\vert$, we know that $1 = \\sigma_1 \\geq \\big \\vert \\lambda_1 \\big \\vert$ and we know that $\\big \\vert \\lambda_n \\big \\vert \\geq \\sigma_n = 1$.  Every item in our sequence of eigenvalue magnitudes is bounded above and below by one.  Thus all eigenvalues of a unitary (or in Reals, othogonal) matrix must have magnitue equal to one.\n",
    "\n",
    "$1 = \\sigma_1 \\geq \\big \\vert \\lambda_1 \\big \\vert \\geq \\big \\vert\\lambda_2 \\big \\vert\\geq \\big \\vert \\lambda_3 \\big \\vert \\geq ... \\geq \\big \\vert\\lambda_n \\big \\vert \\geq \\sigma_n = 1$\n",
    "\n",
    "can be re-written as\n",
    "\n",
    "$1 = \\sigma_1 = \\big \\vert \\lambda_1 \\big \\vert = \\big \\vert\\lambda_2 \\big \\vert = \\big \\vert \\lambda_3 \\big \\vert = ... = \\big \\vert\\lambda_n \\big \\vert =\\sigma_n = 1$\n",
    "\n",
    "\n",
    "- - - - \n",
    "\n",
    "# On Nilpotent Matrices\n",
    "\n",
    "The final sequences of inequalities for some arbitary square matrix:\n",
    "\n",
    "$\\sigma_1 \\geq \\big \\vert \\lambda_1 \\big \\vert \\geq \\big \\vert\\lambda_2 \\big \\vert\\geq \\big \\vert \\lambda_3 \\big \\vert \\geq ... \\geq \\big \\vert\\lambda_n \\big \\vert \\geq \\sigma_n $\n",
    "\n",
    "is quite useful.  \n",
    "\n",
    "A nilpotent matrix $\\mathbf A$ is some $n$ x $n$ matrix where are after finite number of iterations, it becomes the zero matrix.  Thus $\\mathbf A^r = \\mathbf 0$ for some finite, natural number $r$.  (We can tighten the bound and say $r \\leq n$, but this is not really needed here.) \n",
    "\n",
    "**Claim:** a nilpotent matrix has all eigenvalues equal to zero.\n",
    "\n",
    "There are numerous ways to prove this.  The most slick uses the analysis earlier in this post and does the following:\n",
    "\n",
    "**Proof: **  \n",
    "$\\mathbf A$ has eigenvalues of $\\big \\vert \\lambda_1 \\big \\vert \\geq \\big \\vert\\lambda_2 \\big \\vert\\geq \\big \\vert \\lambda_3 \\big \\vert \\geq ... \\geq \\big \\vert\\lambda_n \\big \\vert $\n",
    "\n",
    "and \n",
    "\n",
    "$\\big(\\mathbf A^r\\big)$ has eigenvalues of $\\big \\vert \\lambda_1^r \\big \\vert \\geq \\big \\vert\\lambda_2^r \\big \\vert\\geq \\big \\vert \\lambda_3^r \\big \\vert \\geq ... \\geq \\big \\vert\\lambda_n^r \\big \\vert$\n",
    "\n",
    "or equivalently \n",
    "\n",
    "$\\big(\\mathbf A^r\\big)$ has eigenvalues of $\\big \\vert \\lambda_1 \\big \\vert^r \\geq \\big \\vert\\lambda_2 \\big \\vert^r \\geq \\big \\vert \\lambda_3 \\big \\vert^r \\geq ... \\geq \\big \\vert\\lambda_n \\big \\vert^r $\n",
    "\n",
    "We can do SVD on $\\big(\\mathbf A^r\\big)$ and see  \n",
    "$\\big(\\mathbf A^r\\big) = \\mathbf U \\mathbf \\Sigma \\mathbf V^H = \\mathbf 0$, where $\\mathbf U$ and $\\mathbf V$ are full rank unitary matrices (or orthogonal if dealing with Reals)  \n",
    "\n",
    "Hence:\n",
    "\n",
    "$ \\mathbf \\Sigma = \\mathbf U^H\\big(\\mathbf A^r\\big)\\mathbf V = \\mathbf U^H \\big(\\mathbf 0\\big)\\mathbf V = \\mathbf 0 $\n",
    "\n",
    "That is, all singular values of $\\big(\\mathbf A^r\\big)$  are equal to zero \n",
    "\n",
    "Thus we know that for $ \\big(\\mathbf A^r\\big)$  \n",
    "\n",
    "$0 = \\sigma_1 \\geq \\big \\vert \\lambda_1^r \\big \\vert \\geq \\big \\vert\\lambda_2^r \\big \\vert\\geq \\big \\vert \\lambda_3^r \\big \\vert \\geq ... \\geq \\big \\vert\\lambda_n^r \\big \\vert \\geq \\sigma_n = 0$\n",
    "\n",
    "Since all eigenvalue magnitudes are bounded above and below by zero, we restate this as  \n",
    "$0 = \\lambda_1^r  = \\lambda_2^r = \\lambda_3^r = ... = \\lambda_n^r  = 0$\n",
    "\n",
    "take the $r$th root and we see that all eigenvalues of the nilpotent matrix $\\mathbf A$ must be zero\n",
    "\n",
    "$0 = \\lambda_1  = \\lambda_2 =  \\lambda_3 = ... = \\lambda_n  = 0$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix Norms \n",
    "\n",
    "consider two n x n matrices $\\mathbf A$ and $\\mathbf B$.  \n",
    "\n",
    "now consider $\\big \\Vert \\mathbf {AB} \\big \\Vert_2 = \\sigma_1$, i.e. the operator norm for $\\big(\\mathbf {AB}\\big)$.  \n",
    "\n",
    "As always we order singular values as $\\sigma_1 \\geq \\sigma_2 \\geq ... \\geq \\sigma_n \\geq 0$.  And again we consider any vectors $\\mathbf x$, $\\mathbf y$, $\\mathbf z$ with the constraint that $\\big \\Vert \\mathbf x\\big \\Vert_2^2 = 1$, $\\big \\Vert \\mathbf y\\big \\Vert_2^2 = 1$, and $\\big \\Vert \\mathbf z\\big \\Vert_2^2 = 1$.\n",
    "\n",
    "The operator norm of $\\big(\\mathbf {AB}\\big)$ can be considered as a quadratic form whereby we look for \n",
    "\n",
    "\n",
    "max $\\mathbf z^H \\big(\\mathbf B^H \\mathbf A^H \\mathbf{AB}\\big) \\mathbf z$\n",
    "\n",
    "we claim that this is upperbounded by maximizing $\\big(\\mathbf x^H \\mathbf A^H\\mathbf A \\mathbf x\\big)\\big(\\mathbf y^H \\mathbf B^H \\mathbf B \\mathbf y\\big)$ which is equivalent to $\\sigma_{1,A}^2 \\sigma_{1,B}^2$\n",
    "\n",
    "We can see this by assuming $\\mathbf {Bz} =\\mathbf x$.  I.e. if $\\mathbf {Bz}$ is equal to our optimal $\\mathbf x$, then we have  $\\big(\\mathbf z^H \\mathbf B^H\\big) \\mathbf A^H \\mathbf{A}\\big( \\mathbf B \\mathbf z \\big)= \\mathbf x^H \\mathbf A^H \\mathbf A \\mathbf x $ and hence we've optimized the internal part of that equation.  But in fact $\\mathbf {Bz} = \\alpha \\mathbf x$.  I.e. $\\mathbf {Bz}$ actually gives us a scaled version of $\\mathbf x$.   So we now consider the fact that we want to maximize $\\big \\Vert \\mathbf {Bz} \\big \\Vert_2$  i.e. maximize the $\\alpha$ in $\\alpha \\mathbf x$.  This is equivalent to maximizing $\\big(\\mathbf y^H \\mathbf B^H \\mathbf B \\mathbf y\\big)$, and we know that this is given by $\\sigma_{B,1}^2$.  Hence maximizing $\\mathbf z^H \\big(\\mathbf B^H \\mathbf A^H \\mathbf{AB}\\big) \\mathbf z$ is upper bounded by maximizing the inside, which returns $\\sigma_{A,1}^2$ and scaling that by a maximal $\\alpha$ which is given by $\\sigma_{B,1}^2$ \n",
    "\n",
    "Thus $\\big \\Vert \\mathbf{AB} \\big \\Vert_2^2 \\leq \\sigma_{A,1}^2\\sigma_{B,1}^2 = \\big \\Vert \\mathbf{A} \\big \\Vert_2^2 \\big \\Vert \\mathbf{B} \\big \\Vert_2^2 $\n",
    "\n",
    "\n",
    "Now if we wanted to maximize $\\big \\Vert \\mathbf{AB} \\big \\Vert_2^2 $ with the constraint that the solution is orthogonal to the (right) singular vectors associated with $\\sigma_{A,1}$, we could upper bound this with $\\sigma_{A,2}^2 \\sigma_{B,1}^2$, and if we wanted to do a maximization that was orthogonal to the (right) singular vectors associated with  $\\{\\big(\\sigma_{A,2}, \\sigma_{B,1}\\big), \\big(\\sigma_{A,1}, \\sigma_{B,1}\\big)\\}$ and we could upper bound that by $\\sigma_{A,3}^2 \\sigma_{B,1}^2$, and so on.  \n",
    "\n",
    "If we were to add all of these upper bounds up, what we'd get is \n",
    "\n",
    "$\\sigma_{B,1}^2 \\big(\\sigma_{A,1}^2 + \\sigma_{A,2}^2 + ... + \\sigma_{A,n}^2\\big) = \\sigma_{B,1}^2 trace\\big(\\mathbf A^H \\mathbf A\\big) = \\sigma_{B,1}^2 \\big \\Vert \\mathbf A \\big \\Vert_F^2 = \\big \\Vert \\mathbf B\\big \\Vert_2^2\\big \\Vert \\mathbf A \\big \\Vert_F^2$\n",
    "\n",
    "This process is merely an extension of a preceding section titled \"Quadratic Forms and recovering a Frobenius Norm\"... and we note that if we did this for all $\\mathbf z_k$, we'd recovered a coordinate system $\\mathbf Z$.  If we added up all of the associated 'payoff's we'd recover  $\\big \\Vert \\mathbf{AB} \\big \\Vert_F^2$.  By the above, we have upper bounds for all of those 'payoffs'. \n",
    "\n",
    "Thus we can say \n",
    "\n",
    "$\\big \\Vert \\mathbf{AB} \\big \\Vert_F^2 \\leq \\big \\Vert \\mathbf B\\big \\Vert_2^2 \\big \\Vert\\mathbf A \\big \\Vert_F^2$\n",
    "\n",
    "of course we can recall that \n",
    "\n",
    "$\\big \\Vert \\mathbf B\\big \\Vert_F^2 = \\big \\Vert \\mathbf B\\big \\Vert_2^2 + \\sigma_{B,2}^2 + \\sigma_{B,3}^2 + .... + \\sigma_{B,n}^2 = \\sigma_{B,1}^2+ \\sigma_{B,2}^2 + \\sigma_{B,3}^2 + .... + \\sigma_{B,n}^2 $\n",
    "\n",
    "recalling that each singular value $\\sigma_{B, k}$ is real and non-negative.  Thus if we wanted to loosen up the above bound, we could say\n",
    "\n",
    "$\\big \\Vert \\mathbf{AB} \\big \\Vert_F^2 \\leq \\big \\Vert \\mathbf B\\big \\Vert_F^2\\big \\Vert \\mathbf A \\big \\Vert_F^2$\n",
    "\n",
    "or taking the square root of both sides \n",
    "\n",
    "$\\big \\Vert \\mathbf{AB} \\big \\Vert_F \\leq \\big \\Vert \\mathbf B\\big \\Vert_F \\big \\Vert \\mathbf A \\big \\Vert_F$\n",
    "\n",
    "This should jump out at us as being a variant of Cauchy-Schwarz which tells us that \n",
    "\n",
    "$\\big \\vert\\mathbf a^H \\mathbf b\\big \\vert^2 \\leq \\big \\Vert \\mathbf a \\big \\Vert_2^2 \\big \\Vert \\mathbf b\\big \\Vert_2^2$\n",
    "\n",
    "\n",
    "\n",
    "*note: a lot of the above is proved in a different, much more satisfying way, below, under \"Hermitian Positive Semi Definite Trace Inequalities\"*  \n",
    "\n",
    "E.g. suppose $\\mathbf X^H \\mathbf X = \\mathbf A$ and $\\mathbf Y \\mathbf Y^H = \\mathbf B$,  i.e. both $\\mathbf B$ and $\\mathbf A$ are Hermitian positive semi definite.  Using the above proof with respect to squared Frobenius norms, we see\n",
    "\n",
    "$\\big \\Vert \\mathbf{XY} \\big \\Vert_F^2 \\leq \\big \\Vert \\mathbf X \\big \\Vert_F^2\\big \\Vert \\mathbf Y \\big \\Vert_F^2$\n",
    "\n",
    "but we could also say\n",
    "\n",
    "$\\big \\Vert \\mathbf{XY} \\big \\Vert_F^2 = trace\\big(\\mathbf Y^H \\mathbf X^H \\mathbf X \\mathbf Y \\big) =  trace\\big(\\mathbf Y \\mathbf Y^H \\mathbf X^H \\mathbf X \\big) = trace\\big(\\mathbf {BA}\\big)= trace\\big(\\mathbf A \\mathbf B\\big)$\n",
    "\n",
    "then apply the below inequality that if $\\mathbf A$ and $\\mathbf B$ are both Hermitian positive semidefinite, \n",
    "$trace\\big(\\mathbf A \\mathbf B\\big) \\leq trace\\big(\\mathbf A\\big) trace\\big( \\mathbf B\\big)$\n",
    "\n",
    "$\\big \\Vert \\mathbf{XY} \\big \\Vert_F^2 \\leq trace\\big(\\mathbf A\\big) trace\\big( \\mathbf B\\big) = \\big \\Vert \\mathbf{X} \\big \\Vert_F^2\\big \\Vert \\mathbf{Y} \\big \\Vert_F^2$\n",
    "\n",
    "something similar is shown with respect to operator norms as well.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**extension: Markov Chains must have linearly independent eigenvalues associated with eigenvalues of magnitude 1**\n",
    "\n",
    "The argument is a proof by contradiction using Jordan Forms.  In essence, assume the above statement is not true, and we find it violates the above matrix norms.  This is an extension modified answer for problem 4.15 In Stochastic Processes by Gallagher (problem 3.15 in \"Discrete Stochastic Processes\" in MIT OCW.\n",
    "\n",
    "start by showing a 3 x 3 Jordan Block of the form\n",
    "\n",
    "$\\mathbf J_i = \\begin{bmatrix}\n",
    "\\lambda_i & 1 & 0 \\\\ \n",
    "0 & \\lambda_i & 1\\\\ \n",
    "0 & 0 & \\lambda_i \n",
    "\\end{bmatrix}$\n",
    "\n",
    "then \n",
    "\n",
    "$\\mathbf J_i^n = \\begin{bmatrix}\n",
    "\\lambda_i^n & n \\lambda_i^{n-1} & \\binom{n}{2} \\lambda_i^{n-2} \\\\ \n",
    "0 & \\lambda_i^n & n \\lambda_i^{n-1}\\\\ \n",
    "0 & 0 & \\lambda_i^n \n",
    "\\end{bmatrix}$\n",
    "\n",
    "There are various ways to expand or shrink this, but most succinctly we see that diagonal elements (eigenvalues) multiply exponentially with n, as we'd expect.  The off diagonal element that are non-zero occur when we do not have enough linearly indenpendent eigenvectors for $\\lambda_i$, and as a lower bound, we can say that they too grow exponentially (albeit it at one or two iterations less per 'step' than the diagonals) and are scaled by n. \n",
    "\n",
    "From here, notice that for any $\\big \\vert \\lambda_i \\big\\vert \\lt 1$, that $\\lim_{n \\to \\infty}\\mathbf J_i^n = \\mathbf {0} $.  However if $\\big\\vert \\lambda_i\\big\\vert = 1$, then the off diaonal elements also tend to infinity.  (We don't consider the case of $\\big\\vert \\lambda_i \\big\\vert \\gt 1$, because as noted in the Gerschgorin discs writeup, Markov Chains cannot have eigenvalues with magnitude $\\gt 1$.) \n",
    "\n",
    "\n",
    "Now suppose we have a defective markov chain transition matrix that is $m$ x $m$.  I.e. it factorizes so $\\mathbf A = \\mathbf {PJP}^{-1}$, where $\\mathbf J$ is the jordan form that has non-zero off-diagonal elements because we $\\mathbf A$ is defective.  \n",
    "\n",
    "We also can say:\n",
    "\n",
    "$\\mathbf J^n = \\mathbf P^{-1} \\mathbf A^n \\mathbf P $, or using associativity $\\mathbf J^n = \\mathbf P^{-1} \\big(\\mathbf A^n \\mathbf P \\big)$\n",
    "\n",
    "applying the above Frobenius norm inequality, twice,\n",
    "\n",
    "$ \\big \\Vert \\mathbf J^n \\big \\Vert_F^2=\\big \\Vert \\mathbf P^{-1} \\big(\\mathbf A^n \\mathbf P \\big) \\big \\Vert_F^2 \\leq \\big \\Vert \\mathbf P^{-1} \\big \\Vert_F^2\\big \\Vert \\big(\\mathbf A^n \\mathbf P \\big) \\big \\Vert_F^2 \\leq \\big \\Vert \\mathbf P^{-1} \\big \\Vert_F^2\\big \\Vert \\mathbf A^n   \\big \\Vert_F^2\\big \\Vert  \\mathbf P  \\big \\Vert_F^2$\n",
    "\n",
    "\n",
    "Also recalling that valid transition matrix $\\mathbf A^n$ has all entries as real valued non-negative, and either all columns or all rows sum to one.  Put differently, every value in $\\mathbf A$ is in $[0,1]$, and  thus we can upper bound the squared Frobenius norm of $\\mathbf A^n$ by the ones matrix.\n",
    "\n",
    "$\\big \\Vert \\mathbf A^n  \\big \\Vert_F^2 \\leq \\big \\Vert \\mathbf {11}^T  \\big \\Vert_F^2 = m^2$\n",
    "\n",
    "now multiply both sides by $\\big \\Vert \\mathbf P^{-1} \\big \\Vert_F^2$ and $\\big \\Vert  \\mathbf P  \\big \\Vert_F^2$, which are real valued, positive scalars, and we get:  \n",
    "\n",
    "$ \\big \\Vert \\mathbf J^n \\big \\Vert_F^2 \\leq \\big \\Vert \\mathbf P^{-1} \\big \\Vert_F^2\\big \\Vert \\mathbf A^n  \\big \\Vert_F^2 \\big \\Vert  \\mathbf P  \\big \\Vert_F^2 \\leq m^2 \\big \\Vert \\mathbf P^{-1} \\big \\Vert_F^2  \\big \\Vert  \\mathbf P  \\big \\Vert_F^2 $\n",
    "\n",
    "notice that for any given transition matrix $\\mathbf A$, the right hand side of the equality is some fixed finite number, and it does not vary with respect to $n$.  The contradiction comes by assuming that we do not have enough linearly independent eigenvectors with eigenvalue magnitude of $1$.  Hence for any given $\\mathbf A$ we have a fixed upper bound on the right hand side that does not vary with $n$.  Yet if we have jordan blocks (i.e. super diagonal values of 1) associated with $\\big \\vert\\lambda_i \\big \\vert =1$, then we can find large enough $n$ such that the left hand side has a larger Frobenius norm than the right hand side, which is a contradiction.  Hence we know there cannot be aka a shortage of linearly independent eigenvectors, with respect to eigenvalues of magnitude one in a Markov chain transition matrix. \n",
    "\n",
    "\n",
    "**extension: Projection Matrices must be diagonalizable**  \n",
    "\n",
    "A projector, aka an idempotent matrix is an $n$ x $n$ matrix $\\mathbf A$, where \n",
    "\n",
    "$\\mathbf A = \\mathbf A^2$\n",
    "\n",
    "**claim**  \n",
    "\n",
    "$\\mathbf A$ must diagonalizable.\n",
    "\n",
    "**proof**\n",
    "\n",
    "$\\mathbf A$ has only eigenvalues equal to $0$ and $1$\n",
    "\n",
    "that is, for each eigenvector $\\mathbf x$, we have $\\mathbf A^2 \\mathbf x = \\lambda_k \\mathbf A \\mathbf x = \n",
    "\\lambda_k^2 \\mathbf x = \\lambda_k \\mathbf x = \\mathbf {Ax}$\n",
    "\n",
    "thus $\\lambda_k^2 = \\lambda_k$, which occurs **iff** $\\lambda_k = 0$ or $\\lambda_k = 1$.\n",
    "\n",
    "now we consider the possibility that $\\mathbf A$ is defective and write out its Jordan Form, similarity transform\n",
    "\n",
    "$\\mathbf P^{-1} \\mathbf A^k \\mathbf P =\\mathbf J^k$ \n",
    "\n",
    "hence we have \n",
    "\n",
    "$\\mathbf P^{-1} \\mathbf A \\mathbf P =\\mathbf J = \\mathbf J^2 = \\mathbf P^{-1} \\mathbf A^2 \\mathbf P $ \n",
    "\n",
    "also notice that since $\\mathbf A = \\mathbf A^2$, then we can left multiply both by $\\mathbf A$ and see that \n",
    "\n",
    "$\\mathbf A^2 = \\mathbf A^3$, hence $\\mathbf A = \\mathbf A^3$. We can further do this process such that $\\mathbf A = \\mathbf A^k$\n",
    "\n",
    "\n",
    "Thus our relationship is $\\mathbf J = \\mathbf J^k$ for any natural number $k$. Recalling that $\\mathbf J$ has eigenvalues equal to zero or one on the diagonal, and at most all 1s in the strictly upper triangular portion, we can upperbound the squared frobenius norm of $\\mathbf J$ with $\\big \\Vert \\mathbf {11}^T \\big \\Vert_F^2$.  Further, we collect all of the eigenvalues in a diagonal matrix $\\mathbf D$ and remark that $\\mathbf J$ has a Frobenius norm strictly greater than $\\mathbf D$ unless $\\mathbf A$ is diagonalizable. \n",
    "\n",
    "if defective $ \\big \\Vert \\mathbf D \\big \\Vert_F^2  \\lt \\big \\Vert \\mathbf J^k \\big \\Vert_F^2 = \\big \\Vert \\mathbf J \\big \\Vert_F^2 \\lt \\big \\Vert \\mathbf {11}^T \\big \\Vert_F^2$.  \n",
    "\n",
    "if diagonalizable $ \\big \\Vert \\mathbf D \\big \\Vert_F^2  = \\big \\Vert \\mathbf J^k \\big \\Vert_F^2 = \\big \\Vert \\mathbf J \\big \\Vert_F^2 \\lt \\big \\Vert \\mathbf {11}^T \\big \\Vert_F^2$.  \n",
    "\n",
    "We know that the matrix cannot have a shortage of linearly independent eigenvectors for $\\lambda_k = 1$.  Why? Repeat the exact same argument used above for eigenvalues with magnitude 1 in Markov Chains.  The right hand side is fixed in magnitude, but we can find large enough (finite) $k$ that creates a Frobenius norm that exceeds this bound if we have any Jordan blocks (i.e. shortage of linearly independent eigenvectors) with respect to eigenvalues equal to one.  Thus we conclude that the geometric multiplicity = algebraic multiplicity for $\\lambda = 1$\n",
    "\n",
    "Now, with respect to eigenvalues equal to zero, recall that if the matrix is defective, we have jordan blocks given by: \n",
    "\n",
    "$\\mathbf J_i = \\begin{bmatrix}\n",
    "\\lambda_i & 1 & 0 \\\\ \n",
    "0 & \\lambda_i & 1\\\\ \n",
    "0 & 0 & \\lambda_i \n",
    "\\end{bmatrix}$\n",
    "\n",
    "and, for $k\\gt 2$, we have \n",
    "\n",
    "$\\mathbf J_I^k = \\begin{bmatrix}\n",
    "\\lambda_i^k & k \\lambda_i^{k-1} & \\binom{k}{2} \\lambda_i^{k-2} \\\\ \n",
    "0 & \\lambda_i^k & k \\lambda_i^{k-1}\\\\ \n",
    "0 & 0 & \\lambda_i^k \n",
    "\\end{bmatrix}$\n",
    "\n",
    "Yet this block is nilpotent since $\\lambda = 0$ and thus we can select large enough $k$ (e.g. $k:=n$) and say that $\\mathbf J_I^n = \\mathbf 0$.  But the only way $\\big \\Vert\\mathbf J_I\\big \\Vert_F^2 = \\big \\Vert\\mathbf J_I^n\\big \\Vert_F^2 = \\big \\Vert\\mathbf 0\\big \\Vert_F^2$, is if $\\mathbf J_I$ is itself a zero matrix (i.e. we know the $\\lambda_i = 0$ for this block, and the off diagonal ones cannot exist).\n",
    "\n",
    "since we know that there are no off diagonal elements with respect to $\\lambda = 1$ or with respect to $\\lambda = 0$, we know \n",
    "\n",
    "$\\big \\Vert \\mathbf D \\big \\Vert_F^2 = \\big \\Vert \\mathbf J \\big \\Vert_F^2$\n",
    "\n",
    "which proves that $\\mathbf A$ is diagonalizable (i.e. not defective).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**The rest of this posting consists of inequalities from Zhang's *Linear Algebra: Challenging Problems for Students*, which contains a wealth of interesting exercises that proceed in a thoughtful manner, much like a guided proof.  (Unfortunately, the solutions in the back are frequently some mixture of terse and nearly incomprehensible -- that said I'd still recommend the book for the very high quality exercises.)  **\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Misc notes on Hermitian Matrices\n",
    "\n",
    "if we let $\\mathbf A$ and $\\mathbf B$ both be $n$ x $n$ Hermitian matrices, \n",
    "\n",
    "first, notice $\\mathbf {AB} $ has the same eigenvalues as $\\mathbf {BA}$.  (In general we know that they have the same non-zero eigenvalues with the same Algebraic multiplicities -- and since they have the same dimension $n$ there must be the same number of 'leftover' eigenvalues that are zeros.  A proof is contained in \"Vandermonde_Matrices_Permutations_and_Discrete_Fourier_Transform.ipybn\" )\n",
    "\n",
    "\n",
    "now notice $\\big(\\mathbf{AB}\\big)^H = \\mathbf B^H \\mathbf A^H = \\mathbf {BA}$.  Thus we conclude that $\\mathbf {BA}$ has the same eigenvalues (with same algebraic multipilicities) as $\\mathbf {AB}$ and the same as the transposed conjugate $\\big(\\mathbf {BA}\\big)^H$.   This means that all eigenvalues in $\\mathbf {AB}$ must be either real, or come in conjugate pairs.   \n",
    "\n",
    "This means that $trace\\Big(\\big(\\mathbf{AB}\\big)^k\\Big)$ is real valued for any natural number $k$.  \n",
    "\n",
    "- - - - -\n",
    "*begin interlude*  \n",
    "Here is a different take.  The below claims are mostly interested in $\\big(\\mathbf {AB}\\big)$ and $\\big(\\mathbf {AB}\\big)^2$.  For the first case, notice that $\\big( \\mathbf A - \\mathbf B\\big)$ is a Hermitian matrix, and if we square it, it is hermitian positive semi definite matrix-- and hence its trace must be real, non-negative, which we denote as $\\gamma$.  \n",
    "\n",
    "$trace\\Big( \\big(\\mathbf A - \\mathbf B\\big)^2 \\Big) = trace\\big(\\mathbf A^2\\big) + trace\\big(\\mathbf B^2\\big) - trace\\big(\\mathbf{AB}\\big) - trace\\big(\\mathbf{BA}\\big) = \\gamma$\n",
    "\n",
    "$trace\\big(\\mathbf A^2\\big) + trace\\big(\\mathbf B^2\\big) - 2*trace\\big(\\mathbf{AB}\\big) = \\gamma$\n",
    "\n",
    "$trace\\big(\\mathbf A^2\\big) + trace\\big(\\mathbf B^2\\big)  = \\gamma + 2*trace\\big(\\mathbf{AB}\\big)$\n",
    "\n",
    "the left hand side is the sum of traces of 2 Hermitian positive semi-definite matrices and hence is real, non-negative, thus the right hand side is as well. We know that $\\gamma$ is real, thus $trace\\big(\\mathbf{AB}\\big)$ must be as well. \n",
    "\n",
    "Also consider \n",
    "\n",
    "$\\Big(\\mathbf{AB} + \\mathbf{BA}\\Big)^2 = \\Big(\\mathbf{AB} + \\big(\\mathbf{AB}\\big)^H\\Big)^2$\n",
    "\n",
    "Where $\\mathbf{AB}$ plus its conjugate transpose creates a new matrix that is Hermitian.  Thus this new matrix given by $\\Big(\\mathbf{AB} + \\mathbf{BA}\\Big)$ has real eigenvalues, and a real trace.  We denote the trace of the square of this new matrix as $\\gamma$.\n",
    "\n",
    "$trace\\Big( \\big( \\mathbf{AB} + \\mathbf{BA}\\big)^2 \\Big) = \\gamma = trace\\big(\\mathbf{ABAB}\\big) + trace\\big(\\mathbf{BABA} \\big) + trace\\big(\\mathbf{ABBA}\\big) + trace\\big(\\mathbf{BAAB}\\big)$\n",
    "\n",
    "$trace\\Big( \\big( \\mathbf{AB} + \\mathbf{BA}\\big)^2 \\Big) = \\gamma  = 2*trace\\big(\\mathbf{ABAB}\\big) + 2*trace\\big(\\mathbf{ABBA}\\big)$\n",
    "\n",
    "$\\frac{1}{2}\\gamma = trace\\big(\\mathbf{ABAB}\\big) + trace\\big(\\mathbf{ABBA}\\big)$\n",
    "\n",
    "$\\frac{1}{2}\\gamma -  trace\\big(\\mathbf{ABBA}\\big) = trace\\Big(\\mathbf{\\big(AB\\big)^2}\\Big)$\n",
    "\n",
    "since gamma is real, and $(\\mathbf{ABBA}\\big) $ is Hermitian, and hence has a real trace, then the left hand side has a real trace.  This means that the right hand side must have a real trace as well.  \n",
    "\n",
    "*end interlude*\n",
    "- - - - -\n",
    "\n",
    "\n",
    "**claim:**    \n",
    "\n",
    "$trace\\Big(\\big(\\mathbf {AB}\\big)^2\\Big)  \\leq trace \\big(\\mathbf A^2 \\mathbf B^2\\big)$\n",
    "\n",
    "**proof:**  \n",
    "\n",
    "using the cyclic property of trace, and the fact that $\\mathbf A$ and $\\mathbf B$ are both Hermitian, we can rewrite the right hand side as:\n",
    "\n",
    "$trace \\big(\\mathbf A^2 \\mathbf B^2\\big) = trace \\big(\\mathbf {AA} \\mathbf {BB}\\big) = trace \\big(\\mathbf A \\mathbf {BBA}\\big) = trace \\big(\\mathbf A^H \\mathbf B^H \\mathbf {BA}\\big) = trace \\Big(\\big(\\mathbf{BA}\\big)^H \\big(\\mathbf {BA}\\big) \\Big)$\n",
    "\n",
    "Let $\\mathbf C: = \\mathbf {AB}$\n",
    "\n",
    "This means we can rewrite our claim as \n",
    "\n",
    "\n",
    "$trace\\big(\\mathbf {C}^2\\big) \\leq trace \\big(\\mathbf{C}^H \\mathbf {C}\\big)$\n",
    "\n",
    "and by applying the Schur inequality, we know \n",
    "\n",
    "$trace\\big(\\mathbf {C}^2\\big) \\leq \\big \\vert trace\\big(\\mathbf {C}^2\\big) \\big \\vert \\leq trace \\big(\\mathbf{C}^H \\mathbf {C}\\big)$\n",
    "\n",
    "again, recalling that $trace\\big( \\mathbf{C}^k\\big)$ is real valued for any natural number $k$\n",
    "\n",
    "\n",
    "**claim:**  \n",
    " \n",
    "$\\Big(trace\\big(\\mathbf {AB}\\big)\\Big)^2 \\leq trace \\big(\\mathbf A^2\\big) trace \\big(\\mathbf B^2\\big)$\n",
    "\n",
    "- - - - \n",
    "*begin interlude* \n",
    "\n",
    "\"smashing\" $\\mathbf B$ and $\\mathbf A$ down into  vectors\n",
    "\n",
    "Note we denote $\\mathbf B = \\bigg[\\begin{array}{c|c|c|c}\n",
    "\\mathbf b_1 & \\mathbf b_2 &\\cdots & \\mathbf b_n\\end{array}\\bigg] $\n",
    "\n",
    "and $vec\\big(\\mathbf B\\big)  =\\mathbf b= \\begin{bmatrix}\n",
    "\\mathbf b_1 \\\\ \n",
    "\\mathbf b_2\\\\ \n",
    "\\mathbf b_3\\\\ \n",
    "\\vdots \\\\ \n",
    "\\mathbf b_n\n",
    "\\end{bmatrix}$\n",
    "\n",
    "where $\\mathbf b$ has $\\mathbf b_1$ with $\\mathbf b_2$ \"glued\" to the bottom of it, with $\\mathbf b_3$ \"glued\" to the bottom of that ... with $\\mathbf b_n$ glued to the bottom of that.  \n",
    "\n",
    "Thus we see that $\\big \\Vert \\mathbf B \\big \\Vert_F^2 = \\mathbf b ^H \\mathbf b$\n",
    "\n",
    "The same for $\\mathbf A$\n",
    "\n",
    "Note we denote $\\mathbf A = \\bigg[\\begin{array}{c|c|c|c}\n",
    "\\mathbf a_1 & \\mathbf a_2 &\\cdots & \\mathbf a_n\\end{array}\\bigg] $\n",
    "\n",
    "and $vec\\big(\\mathbf A\\big)  = \\mathbf a = \\begin{bmatrix}\\mathbf a_1 \\\\ \n",
    "\\mathbf a_2\\\\ \n",
    "\\mathbf a_3\\\\ \n",
    "\\vdots \\\\ \n",
    "\\mathbf a_n\n",
    "\\end{bmatrix}$\n",
    "\n",
    "$\\big \\Vert \\mathbf A \\big \\Vert_F^2 = \\mathbf a^H \\mathbf a$\n",
    "\n",
    "*end interlude*  \n",
    "- - - - \n",
    "\n",
    "** proof:**    \n",
    "\n",
    "because $\\mathbf A = \\mathbf A^H$ we can rewrite the left hand side as  \n",
    "\n",
    "$\\Big(trace\\big(\\mathbf {AB}\\big)\\Big)^2  = \\Big(trace\\big(\\mathbf {A}^H \\mathbf B \\big)\\Big)^2 = \\Big(trace\\big(\\mathbf a^H \\mathbf b \\big)\\Big)^2 = \\big(\\mathbf a^H \\mathbf b\\big)^2$\n",
    "\n",
    "\n",
    "And for the right hand side we can rewrite it as:\n",
    "\n",
    "$trace \\big(\\mathbf A^2\\big) trace \\big(\\mathbf B^2\\big) = trace \\big(\\mathbf {A}^H\\mathbf A\\big) trace \\big(\\mathbf B^H \\mathbf B \\big) = trace \\big(\\mathbf a^H \\mathbf a\\big) trace \\big(\\mathbf b^H \\mathbf b\\big) = \\big(\\mathbf {a}^H \\mathbf a \\big)\\big(\\mathbf b^H \\mathbf b\\big)$\n",
    "\n",
    "hence our claim reduces to a simple application of Cauchy Schwartz:\n",
    "\n",
    "i.e. \n",
    "\n",
    "$\\Big(trace\\big(\\mathbf {AB}\\big)\\Big)^2 \\leq trace \\big(\\mathbf A^2\\big) trace \\big(\\mathbf B^2\\big)$\n",
    "\n",
    "is equivalent to saying \n",
    "\n",
    "$ (\\mathbf a^H \\mathbf b\\big)^2 \\leq \\big(\\mathbf {a}^H \\mathbf a \\big)\\big(\\mathbf b^H \\mathbf b\\big)$\n",
    "\n",
    "recalling the fact that $\\big(\\mathbf {AB}\\big)$ has a real valued trace, thus $\\Big(trace\\big(\\mathbf {AB}\\big)\\Big)^2 = \\big(\\mathbf b^H \\mathbf a\\big)^2 = \\big(\\mathbf a^H \\mathbf b\\big)^2$ is real valued as well.  \n",
    "\n",
    "\n",
    "\n",
    "**claim :  **\n",
    "\n",
    "$trace\\big(\\mathbf{AB}\\big) \\leq \\frac{1}{2}\\Big(trace\\big(\\mathbf A^2 \\big) + trace\\big(\\mathbf B^2\\big)\\Big)$\n",
    "\n",
    "**proof:  **\n",
    "\n",
    "$\\mathbf C := \\mathbf A - \\mathbf B$\n",
    "\n",
    "$\\mathbf C^H = \\mathbf A^H - \\mathbf B^H = \\mathbf A - \\mathbf B = \\mathbf C$\n",
    "\n",
    "hence $\\mathbf C$ is Hermitian\n",
    "\n",
    "$\\mathbf C^2 = \\mathbf C \\mathbf C = \\mathbf C^H \\mathbf C$\n",
    "\n",
    "hence $\\mathbf C^2$ is Hermitian positive semi-definite, thus its trace must be real valued and $\\geq 0$\n",
    "\n",
    "$trace\\big(\\mathbf C^2\\big) = trace\\Big( \\big(\\mathbf A - \\mathbf B\\big)^2 \\Big) \\geq 0$\n",
    "\n",
    "$trace\\Big( \\big(\\mathbf A - \\mathbf B\\big)^2 \\Big) = trace\\Big(\\mathbf A^2 + \\mathbf B^2 - \\mathbf {AB} - \\mathbf {BA}\\Big) \\geq 0$  \n",
    "\n",
    "$trace\\Big( \\big(\\mathbf A - \\mathbf B\\big)^2 \\Big) = trace\\big(\\mathbf A^2 \\big) + trace\\big(\\mathbf B^2\\big) - 2* trace\\big(\\mathbf {AB}\\big) \\geq 0$  \n",
    "\n",
    "$trace\\big(\\mathbf A^2 \\big) + trace\\big(\\mathbf B^2\\big) \\geq 2* trace\\big(\\mathbf {AB}\\big) $\n",
    "\n",
    "which we can re-arrange as \n",
    "\n",
    "$trace\\big(\\mathbf{AB}\\big) \\leq \\frac{1}{2}\\Big(trace\\big(\\mathbf A^2 \\big) + trace\\big(\\mathbf B^2\\big)\\Big)$\n",
    "- - - -\n",
    "(4.38 and 4.39)\n",
    "\n",
    "consider $\\mathbf A$ which is Hermitian positive definite and $\\mathbf B$ which is Hermitian (both are $n$ x $n$).  \n",
    "\n",
    "- The eigenvalues of $\\big(\\mathbf A \\mathbf B\\big)$ are the same as those in $\\big(\\mathbf A^{\\frac{1}{2}} \\mathbf B\\mathbf A^{\\frac{1}{2}}\\big)$, which is Hermitian, and hence both matrices have all real eigenvalues\n",
    "\n",
    "- The eigenvalues of $\\big(\\mathbf A^{-1} \\mathbf B\\big)$ are the same as those in $\\big(\\mathbf A^{\\frac{-1}{2}} \\mathbf B\\mathbf A^{\\frac{-1}{2}}\\big)$, which is Hermitian, and hence both matrices have all real eigenvalues\n",
    "\n",
    "- This is proven as an extension in \"Vandermonde_Matrices_Permutations_and_Discrete_Fourier_Transform.ipynb\" which states that the matrix given by $\\big(\\mathbf G \\mathbf H\\big)$ has the same non-zero eigenvalues as $\\big(\\mathbf H \\mathbf G\\big)$, and if both matrices are square, then they have the same dimension and hence have the same number of zero eigenvalues as well\n",
    "\n",
    "- $\\big(\\mathbf A + \\mathbf B\\big)$ is positive semi definite **iff** each eigenvalue of $\\big(\\mathbf A^{-1}\\mathbf B\\big)\\geq -1$.  Why?  We examine $\\big(\\mathbf A + \\mathbf B\\big)$ then  multiply left side and right side by $\\mathbf A^{\\frac{-1}{2}}$ which gives us another Hermitian matrix $\\mathbf A^{\\frac{-1}{2}}\\big(\\mathbf A + \\mathbf B\\big)   \\mathbf A^{\\frac{-1}{2}} = \\mathbf I + \\mathbf A^{\\frac{-1}{2}} \\mathbf B \\mathbf A^{\\frac{-1}{2}}  $ -- and Sylvester's Law of Inertia tells us that the number of positive eigenvalues, negative eigenvalues and zero eigenvalues of $\\big(\\mathbf A + \\mathbf B\\big)$ is intact when we look instead at $\\mathbf A^{\\frac{-1}{2}}\\big(\\mathbf A + \\mathbf B\\big)   \\mathbf A^{\\frac{-1}{2}}$.  Thus $\\mathbf A^{\\frac{-1}{2}} \\mathbf B \\mathbf A^{\\frac{-1}{2}}$ must have all eigenvalues being $\\geq -1$ for positive semidefiniteness to remain intact (i.e. they must all be $\\geq 0$ once we increase them by one, which comes from adding the identity matrix to this).  Recall that $\\mathbf A^{\\frac{-1}{2}} \\mathbf B \\mathbf A^{\\frac{-1}{2}}$ has the same eigenvalues as $\\mathbf A^{-1} \\mathbf B$, which completes the problem. \n",
    "\n",
    "- With $\\mathbf A$ positive definite, we know that $\\big(\\mathbf {AB}\\big)$ is diagonalizable because it is similar to $\\mathbf A^{\\frac{1}{2}} \\mathbf B \\mathbf A^{\\frac{1}{2}}$ which is Hermitian and thus must be diagonalizable.  Specifically: where $\\mathbf S = \\mathbf A^{\\frac{1}{2}}$, we can see $\\mathbf S^{-1} \\big(\\mathbf {AB}\\big)\\mathbf S= \\mathbf A^{\\frac{1}{2}} \\mathbf B \\mathbf A^{\\frac{1}{2}}$.  However if $\\mathbf A$ is singular, we lose this similarity transform, and hence guaranties about the diagonalizability of $\\big(\\mathbf {AB}\\big)$ go away.  While your author generally creates random matrices to show existence, in this case he consulted the book for a simple example of defectiveness when multiplying Hermitian postive semidefinite $\\mathbf A$ with Hermitian $\\mathbf B$.  The example is:  $\\begin{bmatrix}\n",
    "1 &1 \\\\ \n",
    " 1& 1\n",
    "\\end{bmatrix} \\begin{bmatrix}\n",
    "1 &0 \\\\ \n",
    " 0& -1\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "1 & -1 \\\\ \n",
    " 1& -1\n",
    "\\end{bmatrix}$, where the right hand side is a rank one matrix with zero trace and hence must be defective.  (See 'Diagonalization_of_rank_one_matrices.ipynb' for more information.) note in the above example we can still see that \n",
    "$\\begin{bmatrix}\n",
    "1 &1 \\\\ \n",
    " 1& 1\n",
    "\\end{bmatrix}^\\frac{1}{2} \\begin{bmatrix}\n",
    "1 &0 \\\\ \n",
    " 0& -1\n",
    "\\end{bmatrix}\\begin{bmatrix}\n",
    "1 &1 \\\\ \n",
    " 1& 1\n",
    "\\end{bmatrix}^\\frac{1}{2} = \\begin{bmatrix}\n",
    "0 & 0 \\\\ \n",
    " 0& 0\n",
    "\\end{bmatrix}$  \n",
    "exists, has the same eigenvalues (i.e. both zero), and hence is Hermitian and hence is diagonalizable, and hence is the zero matrix.  \n",
    "\n",
    "\n",
    "\n",
    "- - - - \n",
    "\n",
    "\n",
    "**SPECIAL NOTE: in the below problem** $\\mathbf A$** is a 'regular matrix' i.e. not Hermitian, etc.**\n",
    "\n",
    "We are, in effect, looking at the two different ways to 'force' $\\mathbf A$ to be Hermitian -- by averaging it with its conjugate transpose, or in effect looking at $\\mathbf A^H \\mathbf A$.  \n",
    "\n",
    "all matrices are $n$ x $n$ and the scalar field is $\\mathbb C$\n",
    "\n",
    "\n",
    "(4.40)\n",
    "**claim:**  \n",
    "\n",
    "$\\lambda_{max}\\Big(\\frac{1}{2}\\big(\\mathbf A + \\mathbf A^H \\big)\\Big) \\leq \\sigma_{max}\\Big(\\mathbf A\\Big)$\n",
    "\n",
    "**proof:**  \n",
    "\n",
    "let $\\mathbf B := \\mathbf A^H$\n",
    "\n",
    "$\\mathbf C := \\mathbf A + \\mathbf B  = \\mathbf A + \\mathbf A^H$\n",
    "\n",
    "note that the $\\mathbf C$ is Hermitian, so its eigenvalues are real.  Its maximal eigenvalue *in magnitude* is equal to its largest singular value.  That is, $\\Big \\vert \\big \\vert \\lambda\\big \\vert_{max} \\big(\\mathbf C\\big)\\Big \\vert = \\sigma_{max}\\big(\\mathbf C\\big)$\n",
    "- - - - \n",
    "*technical note:* $\\Big \\vert \\big \\vert \\lambda\\big \\vert_{max} \\big(\\mathbf C\\big)\\Big \\vert$ either refers to $\\lambda_{1,C}$ or $\\big \\vert \\lambda_{n,C}\\big \\vert$.  If the former has largest magnitude it must be positive and if $\\lambda_{n,C}$ has largest magntiude, it must be negative. \n",
    "\n",
    "with ordering, as always: \n",
    "$\\lambda_{1,C} \\geq \\lambda_{2,C} \\geq \\lambda_{3,C} \\geq ... \\geq \\lambda_{n-1,C} \\geq \\lambda_{n,C}$\n",
    "\n",
    "- - - -\n",
    "\n",
    "note that $\\sigma_{max}\\big(\\mathbf B\\big) = \\sigma_{max}\\big(\\mathbf A\\big)$\n",
    "\n",
    "we know from \"SingularValue_Inequalities.ipynb\" that \n",
    "\n",
    "$\\sigma_{A,1} + \\sigma_{B,1} \\geq \\sigma_{C,1}$\n",
    "\n",
    "or equivalently \n",
    "\n",
    "$\\sigma_{max}\\big(\\mathbf A\\big)+ \\sigma_{max}\\big(\\mathbf B\\big) \\geq \\sigma_{max}\\big(\\mathbf C\\big)$\n",
    "\n",
    "which can be re-written as: \n",
    "\n",
    "$2 \\sigma_{max}\\big(\\mathbf A\\big) \\geq \\sigma_{max}\\big(\\mathbf C\\big) = \\Big \\vert \\big \\vert \\lambda\\big \\vert_{max} \\big(\\mathbf C\\big)\\Big \\vert \\geq \\lambda_{1,C} = \\lambda_{max}\\big(\\mathbf C\\big) $ \n",
    "\n",
    "\n",
    "\n",
    "$\\sigma_{max}\\big(\\mathbf A\\big) \\geq \\frac{1}{2} \\lambda_{max}\\big(\\mathbf C\\big)$\n",
    "\n",
    "$\\sigma_{max}\\big(\\mathbf A\\big) \\geq \\frac{1}{2} \\lambda_{max}\\Big(\\big(\\mathbf A + \\mathbf A^H \\big)\\Big) =  \\lambda_{max}\\Big(\\frac{\\mathbf A + \\mathbf A^H }{2}\\Big)$\n",
    "\n",
    "*alternative proof* \n",
    "\n",
    "This is quite similar to the above (though the singular value inequalities proof page only contemplated real matrices so this a touch different)\n",
    "\n",
    "where $\\big \\Vert \\mathbf x \\big \\Vert_2^2 = 1$ and $\\big \\Vert \\mathbf y \\big \\Vert_2^2 = 1$, $\\big \\Vert \\mathbf z \\big \\Vert_2^2 = 1$ \n",
    "\n",
    "maximize $real\\Big(\\mathbf x^H \\mathbf A \\mathbf x \\Big)$ = maximize $real\\Big(\\mathbf x^H \\mathbf A^H \\mathbf x \\Big)$\n",
    "\n",
    "hence \n",
    "\n",
    "maximize $real\\Big(\\mathbf x^H \\mathbf A \\mathbf x \\Big)  = real\\Big(\\mathbf x^H \\big( \\frac{\\mathbf A  + \\mathbf A^H }{2}\\big) \\mathbf x \\Big) =   \\Big(\\mathbf x^H \\big( \\frac{\\mathbf A  + \\mathbf A^H}{2} \\big) \\mathbf x \\Big) = \\lambda_{max}\\Big( \\frac{\\mathbf A  + \\mathbf A^H}{2}\\Big) $\n",
    "\n",
    "recalling that the scalar result of a quadratic form over a Hermitian matrix -- specifically $\\big( \\frac{\\mathbf A  + \\mathbf A^H}{2} \\big)$ -- must be real.  \n",
    "\n",
    "but we know \n",
    "\n",
    "maximize $real\\Big(\\mathbf x^H \\mathbf A \\mathbf x \\Big) \\leq $ maximize $real\\Big(\\mathbf y^H \\mathbf A \\mathbf z \\Big) = \\sigma_{max}\\big(\\mathbf A\\big) $\n",
    "\n",
    "because we can always get the same 'payoff' on the right hand side by setting $\\mathbf y:=\\mathbf x$ and $ \\mathbf z:= \\mathbf x$, thus the right hand side must have a 'payoff' at least as high as the left hand side\n",
    "\n",
    "hence \n",
    "\n",
    "$\\lambda_{max}\\Big( \\frac{\\mathbf A  + \\mathbf A^H}{2}\\Big) = $ max $real\\Big(\\mathbf x^H \\mathbf A \\mathbf x \\Big) \\leq $max $real \\Big(\\mathbf y^H \\mathbf A \\mathbf z \\Big) = \\sigma_{max}\\big(\\mathbf A\\big)$\n",
    "\n",
    "or more succinctly:  \n",
    "$\\lambda_{max}\\Big( \\frac{\\mathbf A  + \\mathbf A^H}{2}\\Big) \\leq \\sigma_{max}\\big(\\mathbf A\\big)$\n",
    "\n",
    "\n",
    "**claim:**  \n",
    "\n",
    "$trace\\Big(\\big(\\frac{\\mathbf A + \\mathbf A^H }{2}\\big)^2\\Big) \\leq trace\\Big(\\mathbf A^H \\mathbf A\\Big)$\n",
    "\n",
    "**proof:**  \n",
    "\n",
    "$trace\\Big(\\big(\\frac{\\mathbf A + \\mathbf A^H }{2}\\big)^2\\Big) = \\frac{1}{4} trace\\Big(\\mathbf A^2 + \\big(\\mathbf A^H\\big)^2 +\\mathbf A^H \\mathbf A + \\mathbf A \\mathbf A^H\\Big) \\leq trace\\Big(\\mathbf A^H \\mathbf A\\Big)$\n",
    "\n",
    "re-write this as\n",
    "\n",
    "$trace\\Big(\\big(\\frac{\\mathbf A + \\mathbf A^H }{2}\\big)^2\\Big) = \\frac{1}{4}trace\\Big(\\mathbf A^2 + \\big(\\mathbf A^H\\big)^2\\Big) + \\frac{1}{4} trace\\Big( \\mathbf A^H \\mathbf A\\Big) +  \\frac{1}{4} trace\\Big(\\mathbf A \\mathbf A^H\\Big)  \\leq trace\\Big(\\mathbf A^H \\mathbf A\\Big)$\n",
    "\n",
    "$trace\\Big(\\big(\\frac{\\mathbf A + \\mathbf A^H }{2}\\big)^2\\Big)= \\frac{1}{4}trace\\Big(\\mathbf A^2 + \\big(\\mathbf A^H\\big)^2\\Big) + \\frac{1}{2}trace\\Big(\\mathbf A^H \\mathbf A\\Big) \\leq trace\\Big(\\mathbf A^H \\mathbf A\\Big)$\n",
    "\n",
    "Thus if we can prove:\n",
    "\n",
    "$\\frac{1}{4}trace\\Big(\\mathbf A^2 + \\big(\\mathbf A^H\\big)^2\\Big) + \\frac{1}{2}trace\\Big(\\mathbf A^H \\mathbf A\\Big) \\leq trace\\Big(\\mathbf A^H \\mathbf A\\Big)$\n",
    "\n",
    "then we are done.  Let's simplify this to\n",
    "\n",
    "$\\frac{1}{4}trace\\Big(\\mathbf A^2 + \\big(\\mathbf A^H\\big)^2\\Big) \\leq \\frac{1}{2} trace\\Big(\\mathbf A^H \\mathbf A\\Big)$\n",
    "\n",
    "multiply both sides by 2\n",
    "\n",
    "$\\frac{1}{2}\\Big(trace\\big(\\mathbf A^2 \\big) + trace \\Big(\\big(\\mathbf A^H\\big)^2\\Big)\\Big) \\leq trace\\Big(\\mathbf A^H \\mathbf A\\Big)$\n",
    "\n",
    "- - - -\n",
    "note that, except for complex conjugation (which offset each other),\n",
    "\n",
    "$trace\\big(\\mathbf A^2 \\big) = trace \\Big(\\big(\\mathbf A^H\\big)^2\\Big)$\n",
    "\n",
    "To be exact, we'd say:  \n",
    "\n",
    "$real\\Big(trace\\big(\\mathbf A^2 \\big)\\Big) = real\\Big(trace \\Big(\\big(\\mathbf A^H\\big)^2\\Big)\\Big)$\n",
    "\n",
    "Thus \n",
    "\n",
    "$real\\Big(trace\\big(\\mathbf A^2 \\big) \\Big) +real\\Big( trace \\Big(\\big(\\mathbf A^H\\big)^2\\Big)\\Big) = 2*real\\Big(trace\\big(\\mathbf A^2 \\big) \\Big) \\leq 2\\big \\vert trace\\big(\\mathbf A^2 \\big) \\big \\vert$\n",
    "\n",
    "- - - -\n",
    "we return to our to our main inequality, and apply the above fact (inclusive of the $\\frac{1}{2}$ scaling)\n",
    "\n",
    "$\\frac{1}{2}\\Big(trace\\big(\\mathbf A^2 \\big) + trace \\Big(\\big(\\mathbf A^H\\big)^2\\Big)\\Big) = \\frac{2}{2} real\\Big(trace\\big(\\mathbf A^2 \\big) \\Big) \\leq \\frac{2}{2}\\big \\vert trace\\big(\\mathbf A^2 \\big) \\big \\vert = \\big \\vert trace\\big(\\mathbf {AA} \\big)\\big \\vert \\leq  trace\\Big(\\mathbf A^H \\mathbf A\\Big)$\n",
    " \n",
    "and we confirm that \n",
    "\n",
    "$\\big \\vert trace\\big(\\mathbf {AA} \\big) \\big \\vert \\leq  trace\\Big(\\mathbf A^H \\mathbf A\\Big)$\n",
    "\n",
    "via the expanded Schur Inequality (see the writeup in \"Schurs_Inequality.ipynb\").  This completes the proof.\n",
    "\n",
    "**extension**\n",
    "\n",
    "The last claim was \n",
    "\n",
    "that \n",
    "\n",
    "$trace\\Big(\\big(\\frac{\\mathbf A + \\mathbf A^H }{2}\\big)^2\\Big) \\leq trace\\Big(\\mathbf A^H \\mathbf A\\Big)$\n",
    "\n",
    "but what if we wanted the square root of the above, before taking the trace?  \n",
    "\n",
    "where $\\mathbf X$ is the positive square root of the right hand side? i.e.  \n",
    "\n",
    "$\\mathbf X:= \\Big(\\mathbf A^H \\mathbf A\\Big)^{\\frac{1}{2}}$\n",
    "\n",
    "and for convenience\n",
    "\n",
    "$\\mathbf Y:= \\big(\\frac{\\mathbf A + \\mathbf A^H }{2}\\big)$\n",
    "\n",
    "is it true that \n",
    "\n",
    "$trace\\Big(\\mathbf Y \\Big) \\leq trace\\Big(\\mathbf X\\Big)$\n",
    "\n",
    "yes.  And noting that the eigenvalues and singular values of $\\big(\\mathbf Y\\big)$ are all real, and identical, except that the former may have a negative sign in front of them, we'll prove an even stronger claim:\n",
    "\n",
    "**claim**\n",
    "\n",
    "$\\sigma_{1,Y} + \\sigma_{2,Y} + ... + \\sigma_{n,Y} \\leq \\sigma_{1,X} + \\sigma_{2,X} + ... + \\sigma_{n,X} $\n",
    "\n",
    "or equivalently \n",
    "\n",
    "$\\sum_{k=1}^n \\sigma_k \\Big(\\big(\\frac{\\mathbf A + \\mathbf A^H }{2}\\big)\\Big) \\leq \\sum_{k=1}^n \\sigma_k \\Big(\\mathbf A\\Big) $\n",
    "\n",
    "where $\\sigma_k \\Big(\\mathbf Z\\Big)$ denotes the kth singular value of some matrix $\\mathbf Z$\n",
    "\n",
    "**proof**\n",
    "\n",
    "putting these interesting things together, we have:\n",
    "\n",
    "$trace\\Big(\\mathbf Y \\Big) = \\lambda_{1,Y} + \\lambda_{2,Y} + ... + \\lambda_{n,Y} \\leq \\sigma_{1,Y} + \\sigma_{2,Y} + ... + \\sigma_{n,Y} \\leq \\sigma_{1,X} + \\sigma_{2,X} + ... + \\sigma_{n,X} =  \\sum_{k=1}^n \\sigma_k \\Big(\\mathbf A\\Big)= trace\\Big(\\mathbf X\\Big)$\n",
    "\n",
    "where $\\lambda_{1,Y} + \\lambda_{2,Y} + ... + \\lambda_{n,Y} \\leq \\big\\vert\\lambda_{1,Y}\\big\\vert + \\big\\vert\\lambda_{2,Y}\\big\\vert + ... + \\big\\vert\\lambda_{n,Y}\\big\\vert = \\sigma_{1,Y} + \\sigma_{2,Y} + ... + \\sigma_{n,Y}$, by the triangle inequality.  \n",
    "\n",
    "The proof is simply that \n",
    "\n",
    "$\\sigma_{1,Y} + \\sigma_{2,Y} + ... + \\sigma_{n,Y} \\leq \\frac{1}{2}\\big(\\sigma_{1,A} + \\sigma_{2,A} + ... + \\sigma_{n,A}\\big) + \\frac{1}{2}\\big(\\sigma_{1,A} + \\sigma_{2,A} + ... + \\sigma_{n,A}\\big)= \\sigma_{1,A} + \\sigma_{2,A} + ... + \\sigma_{n,A} = \\sum_{k=1}^n \\sigma_k \\Big(\\mathbf A\\Big) $\n",
    "\n",
    "where we use the \"L1 norm for singular values\" contained in \"SingularValue_Inequalities.ipynb\"\n",
    "\n",
    "and for avoidance of doubt, our original equation is:\n",
    "\n",
    "$\\mathbf Y = \\frac{1}{2} \\mathbf A + \\frac{1}{2}\\mathbf A^H$\n",
    "\n",
    "but in the format of that proof we'd have $\\mathbf C:= \\mathbf Y$ and $\\mathbf A := \\frac{1}{2} \\mathbf A$ and $\\mathbf B : = \\frac{1}{2}\\mathbf A^H$\n",
    "\n",
    "noting that the singluar values of $\\mathbf B$ (and our 'new' $\\mathbf A$) are exactly half of the singular values of our 'original' $\\mathbf A$.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hermitian Positive Semi Definite Trace Inequalities\n",
    "\n",
    "where $\\mathbf A$ and $\\mathbf B$ are both Hermitian Positive (Semi) Definite Matrices\n",
    "\n",
    "**claim: ** \n",
    "\n",
    "$trace\\big(\\mathbf{AB}\\big) \\geq 0$ \n",
    "\n",
    "that is, the the above trace is always real and non-negative.  \n",
    "\n",
    "**proof:**\n",
    "\n",
    "$trace\\big(\\mathbf{AB}\\big) = trace\\big(\\mathbf A^{\\frac{1}{2}}\\mathbf A^{\\frac{1}{2}} \\mathbf B^{\\frac{1}{2}}\\mathbf B^{\\frac{1}{2}}\\big)= trace\\big(\\mathbf B^{\\frac{1}{2}}\\mathbf A^{\\frac{1}{2}} \\mathbf A^{\\frac{1}{2}}\\mathbf B^{\\frac{1}{2}}\\big) = trace\\Big(\\big(\\mathbf B^{\\frac{1}{2}}\\big)^H \\big(\\mathbf A^{\\frac{1}{2}}\\big)^H \\mathbf A^{\\frac{1}{2}}\\mathbf B^{\\frac{1}{2}}\\Big) = trace\\Big(\\big(\\mathbf A^{\\frac{1}{2}} \\mathbf B^{\\frac{1}{2}}\\big)^H \\big( \\mathbf A^{\\frac{1}{2}}\\mathbf B^{\\frac{1}{2}}\\big)\\Big)$ \n",
    "\n",
    "$trace\\big(\\mathbf{AB}\\big) = \\big \\Vert \\mathbf A^{\\frac{1}{2}} \\mathbf B^{\\frac{1}{2}}\\big \\Vert_F^2$\n",
    "\n",
    "Alternatively, we could also note that $\\big(\\mathbf{AB}\\big) = \\Big(\\mathbf A^{\\frac{1}{2}}\\big(\\mathbf A^{\\frac{1}{2}} \\mathbf B\\big)\\Big)$  and must have the same eigenvalues as $\\Big(\\big(\\mathbf A^{\\frac{1}{2}} \\mathbf B\\big)\\mathbf A^{\\frac{1}{2}}\\Big)$ which is Hermitian Positive semi-defintie, and hence has all real, non-negative eigenvalues.  Since the trace gives the sum of those eigenvalues, the trace must be real, non-negative.  \n",
    "\n",
    "\n",
    "**claim: **\n",
    "\n",
    "$trace\\big(\\mathbf{AB}\\big) \\leq trace\\big(\\mathbf A\\big) trace \\big(\\mathbf B \\big) \\leq \\frac{1}{2}\\Big(trace\\big(\\mathbf A\\big)^2 + trace\\big(\\mathbf B\\big)^2\\Big)$\n",
    "\n",
    "**commentary: **\n",
    "\n",
    "The left side of the inequality is of particular interest.  \n",
    "\n",
    "$trace\\big(\\mathbf{AB}\\big) \\leq trace\\big(\\mathbf A\\big) trace \\big(\\mathbf B \\big) $\n",
    "\n",
    "can be rewritten as:\n",
    "\n",
    "$trace\\big(\\mathbf{AB}\\big) \\leq \\big(\\sum_{i = 1}^{n} \\mathbf{A}_{i,i}\\big) \\big(\\sum_{i = 1}^{n} \\mathbf{B}_{i,i}\\big)$\n",
    "\n",
    "since $\\mathbf A$ and $\\mathbf B$ are both Hermitian positive semi-definite, we may recall the Hadamard Inequality (see \"HadamardInequality.ipynb\"), and draw an analogy with determinants.\n",
    "\n",
    "noting that $det\\big(\\mathbf{AB}\\big) = det\\big(\\mathbf{A}\\big) det\\big(\\mathbf{B}\\big)$,\n",
    "\n",
    "we use the Hadarmard inequality:  \n",
    "\n",
    "$det\\big(\\mathbf{A}\\big) \\leq \\big(\\prod_{i = 1}^{n} \\mathbf{A}_{i,i}\\big)$  \n",
    "$det\\big(\\mathbf{B}\\big) \\leq \\big(\\prod_{i = 1}^{n} \\mathbf{B}_{i,i}\\big)$  \n",
    "\n",
    "hence \n",
    "\n",
    "$det\\big(\\mathbf{AB}\\big) \\leq \\big(\\prod_{i = 1}^{n} \\mathbf{A}_{i,i}\\big) \\big(\\prod_{i = 1}^{n} \\mathbf{B}_{i,i}\\big)$\n",
    "\n",
    "which seems analogous  to  \n",
    "\n",
    "$trace\\big(\\mathbf{AB}\\big) \\leq \\big(\\sum_{i = 1}^{n} \\mathbf{A}_{i,i}\\big) \\big(\\sum_{i = 1}^{n} \\mathbf{B}_{i,i}\\big)$\n",
    "\n",
    "**proof:  **  \n",
    "\n",
    "*For the left side of the the inequality:*  \n",
    "\n",
    "$ trace\\big(\\mathbf{AB}\\big) = \\big \\Vert \\mathbf A^{\\frac{1}{2}} \\mathbf B^{\\frac{1}{2}}\\big \\Vert_F^2 \\leq \\big \\Vert \\mathbf A^{\\frac{1}{2}}\\big \\Vert_F^2 \\big \\Vert \\mathbf B^{\\frac{1}{2}}\\big \\Vert_F^2 =  trace\\Big(\\big(\\mathbf A^{\\frac{1}{2}}\\big)^H \\mathbf A^{\\frac{1}{2}}\\Big) trace\\Big(\\big(\\mathbf B^{\\frac{1}{2}}\\big)^H \\mathbf B^{\\frac{1}{2}}\\Big) = trace\\big(\\mathbf A\\big) trace \\big(\\mathbf B \\big) $\n",
    "\n",
    "where we use the derivation under \"Matrix Norms\" for the proof that \n",
    "\n",
    " $\\big \\Vert \\mathbf A^{\\frac{1}{2}} \\mathbf B^{\\frac{1}{2}}\\big \\Vert_F^2 \\leq \\big \\Vert \\mathbf A^{\\frac{1}{2}}\\big \\Vert_F^2 \\big \\Vert \\mathbf B^{\\frac{1}{2}}\\big \\Vert_F^2 $\n",
    "\n",
    "- - - -\n",
    "*begin alternative proof*\n",
    "\n",
    "Consider that for Hermitian Positive (Semi) Definite matrices, we have strictly real, non-negative values along the diagonal of said matrices and amongst their eigenvalues.  \n",
    "\n",
    "Thus we could interpret this inequality as mutliplying two finite series, and noting that \n",
    "\n",
    "\n",
    "$(\\lambda_1 + \\lambda_2 +... + \\lambda_n)(\\sigma_1 + \\sigma_2 + ... + \\sigma_n) \\geq \\lambda_1 \\sigma_1 + \\lambda_2 \\sigma_2 +... + \\lambda_n \\sigma_n$\n",
    "\n",
    "because every term in the series is real and non-negative\n",
    "\n",
    "To map this to our problem simply let: \n",
    "\n",
    "$\\mathbf A = \\mathbf {Q \\Lambda Q}^H$\n",
    "\n",
    "$trace\\big(\\mathbf A\\big) = \\lambda_1 + \\lambda_2 +... + \\lambda_n$\n",
    "\n",
    "$trace\\big(\\mathbf B\\big) = \\sigma_1 + \\sigma_2 + ... + \\sigma_n$\n",
    "\n",
    "\n",
    "$trace\\big(\\mathbf {AB}\\big) = trace\\big(\\mathbf {Q\\Lambda Q}^H \\mathbf B\\big) = trace\\big(\\mathbf {\\Lambda Q}^H \\mathbf B\\mathbf Q\\big)$\n",
    "\n",
    "Now define a new matrix $\\mathbf C := \\mathbf Q^H \\mathbf{BQ}$, which is still Hermitian, and similar to $\\mathbf B$, and hence Positive Semi Definite.  We see that \n",
    "\n",
    "$trace\\big(\\mathbf {AB}\\big) = trace\\big(\\mathbf {\\Lambda C}\\big) $\n",
    "\n",
    "\n",
    "$trace\\big(\\mathbf {\\Lambda C}\\big)= \\lambda_1 c_{1,1} + \\lambda_2 c_{2,2} + ... + \\lambda_n c_{n,n} \\leq (\\lambda_1 + \\lambda_2 +... + \\lambda_n)(c_{1,1} + c_{2,2} + ... + c_{n,n}) = trace\\big(\\mathbf A\\big) trace\\big(\\mathbf C\\big) $\n",
    "\n",
    "Noting that $\\mathbf B$ and $\\mathbf C$ are similar, and hence have the same eigenvalues:\n",
    "\n",
    "$trace\\big(\\mathbf {AB}\\big) \\leq trace\\big(\\mathbf A\\big) trace\\big(\\mathbf C\\big)= trace\\big(\\mathbf A\\big)trace\\big(\\mathbf B\\big)$ \n",
    "\n",
    "\n",
    "*end alternative proof*\n",
    "- - - -\n",
    "\n",
    "*For the right hand side of the inequality:*  \n",
    "\n",
    "$\\big(\\mathbf A - \\mathbf B\\big)$ is a Hermitian matrix, and hence its trace given by  \n",
    "\n",
    "$trace \\big(\\mathbf A - \\mathbf B \\big) = trace \\big(\\mathbf A\\big) - trace \\big(\\mathbf B \\big)$\n",
    "\n",
    "is a real number.  Whenever we square a real number, the result must be $\\geq 0$\n",
    "\n",
    "\n",
    "$\\Big(trace \\big(\\mathbf A\\big) - trace \\big(\\mathbf B \\big)\\Big)^2 \\geq 0 $  \n",
    "$trace \\big(\\mathbf A\\big)^2 + trace \\big(\\mathbf B \\big)^2 - 2 *trace\\big(\\mathbf A\\big)trace\\big(\\mathbf B\\big) \\geq 0$\n",
    "\n",
    "\n",
    "hence we see that \n",
    "\n",
    "$\\frac{1}{2}trace \\big(\\mathbf A\\big)^2 + \\frac{1}{2} trace \\big(\\mathbf B \\big)^2 \\geq trace\\big(\\mathbf A\\big)trace \\big(\\mathbf B\\big) $\n",
    "\n",
    "This proves that \n",
    "\n",
    "$trace\\big(\\mathbf{AB}\\big) \\leq trace\\big(\\mathbf A\\big) trace \\big(\\mathbf B \\big) \\leq \\frac{1}{2}\\Big(trace\\big(\\mathbf A\\big)^2 + trace\\big(\\mathbf B\\big)^2\\Big)$\n",
    "\n",
    "**claim:**  \n",
    "\n",
    "$trace\\big(\\mathbf{AB}\\big) \\leq \\lambda_{max}\\big(\\mathbf A\\big) trace\\big(\\mathbf B\\big)$\n",
    "\n",
    "**proof:  **  \n",
    "\n",
    "for convenience we start by noting $trace\\big(\\mathbf{AB}\\big) = trace\\big(\\mathbf{BA}\\big)$\n",
    "\n",
    "$trace\\big(\\mathbf{BA}\\big) = \\big \\Vert \\mathbf B^{\\frac{1}{2}} \\mathbf A^{\\frac{1}{2}}\\big \\Vert_F^2 \\leq  \\big \\Vert \\mathbf A^{\\frac{1}{2}} \\big \\Vert_2^2 \\big \\Vert\\mathbf B^{\\frac{1}{2}} \\big \\Vert_F^2 = \\sigma_{A,1} *trace\\Big(\\big(\\mathbf B^{\\frac{1}{2}}\\big)^H \\mathbf B^{\\frac{1}{2}} \\Big) = \\lambda_{A,1}* trace\\big(\\mathbf B^{\\frac{1}{2}} \\mathbf B^{\\frac{1}{2}} \\big) = \\lambda_{max}\\big(\\mathbf A\\big) trace\\big(\\mathbf B\\big)$  \n",
    "\n",
    "using results from the \"Matrix Norms\" section to justify the inequality, and noting that the largest squared singular value of $\\mathbf A^{\\frac{1}{2}}$ is the largest singular value of $\\mathbf A$ which equals  $\\sigma_{A,1}$.  Also noticing that because $\\mathbf A$ is Hermitian positive semi-definite, its singular values are equal to its eigenvalues.  \n",
    "\n",
    "*begin alternative proof*\n",
    "\n",
    "leveraging the preceding alternative proof, note that\n",
    "\n",
    "$trace\\big(\\mathbf {A B}\\big) = trace\\big(\\mathbf {\\Lambda C}\\big)= \\lambda_1 c_{1,1} + \\lambda_2 c_{2,2} + ... + \\lambda_n c_{n,n} \\leq \\lambda_1 (c_{1,1} + c_{2,2} + ... + c_{n,n}) = \\lambda_1 trace\\big(\\mathbf B\\big) $\n",
    "\n",
    "recalling that the eigenvalues of $\\mathbf A$ are ordered such that $\\lambda_1 \\geq \\lambda_2 \\geq .... \\geq \\lambda_n \\geq 0$, each diagonal element of $\\mathbf C$ is real valued, non-negative, and $trace\\big(\\mathbf B\\big) = trace\\big(\\mathbf C\\big)$.\n",
    "\n",
    "\n",
    "*end alternative proof*  \n",
    "**claim:**  \n",
    "\n",
    "\n",
    "$trace\\big(\\mathbf{AB}\\big) \\leq \\frac{1}{4} \\Big( trace\\big(\\mathbf A\\big)+ trace\\big(\\mathbf B\\big)\\Big)^2 $\n",
    "\n",
    "**commentary:  **  \n",
    "\n",
    "This is a simple extension of an earlier inequality: \n",
    "\n",
    "$trace\\big(\\mathbf{AB}\\big) \\leq trace\\big(\\mathbf A\\big) trace \\big(\\mathbf B \\big) \\leq \\frac{1}{2}\\Big(trace\\big(\\mathbf A\\big)^2 + trace\\big(\\mathbf B\\big)^2\\Big)$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**proof:**  \n",
    "\n",
    "we start off with \n",
    "\n",
    "\n",
    "$2 * trace\\big(\\mathbf{AB}\\big) \\leq trace\\big(\\mathbf A\\big)^2 + trace\\big(\\mathbf B\\big)^2$\n",
    "\n",
    "add $2*trace\\big(\\mathbf A\\big)trace\\big(\\mathbf B\\big)$ to both sides, noticing that  $trace\\big(\\mathbf{AB}\\big) \\leq trace\\big(\\mathbf A\\big)trace\\big(\\mathbf B\\big)$,  we have   \n",
    "\n",
    "$4 * trace\\big(\\mathbf{AB}\\big) \\leq 2 * trace\\big(\\mathbf{AB}\\big) + 2*trace\\big(\\mathbf A\\big)trace\\big(\\mathbf B\\big) \\leq trace\\big(\\mathbf A\\big)^2 + trace\\big(\\mathbf B\\big)^2 + 2*trace\\big(\\mathbf A\\big)trace\\big(\\mathbf B\\big)$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Thus we have \n",
    "\n",
    "\n",
    "$4 * trace\\big(\\mathbf{AB}\\big) \\leq  trace\\big(\\mathbf A\\big)^2 + trace\\big(\\mathbf B\\big)^2 + 2*trace\\big(\\mathbf A\\big)trace\\big(\\mathbf B\\big) = \\Big(trace\\big(\\mathbf A\\big) + trace\\big(\\mathbf B\\big)\\Big)^2$\n",
    "\n",
    "\n",
    "giving us \n",
    "\n",
    "$trace\\big(\\mathbf{AB}\\big) \\leq  \\frac{1}{4}\\Big(trace\\big(\\mathbf A\\big) + trace\\big(\\mathbf B\\big)\\Big)^2$\n",
    "\n",
    "**claim: **  \n",
    "\n",
    "$\\big \\vert trace\\big(\\mathbf{UA}\\big)\\big \\vert \\leq trace\\big(\\mathbf A\\big)$ \n",
    "\n",
    "Where both matrices are $n$ x $n$, and $\\mathbf A$ is Hermitian positive semi-definite, and $\\mathbf U$ is unitary.\n",
    "\n",
    "**proof:  **  \n",
    "\n",
    "since $\\mathbf U$ is unitary, it is also normal and is unitarily diagonalizable as $\\mathbf U = \\mathbf{VDV}^H$\n",
    "\n",
    "$\\big \\vert trace\\big(\\mathbf{UA}\\big)\\big \\vert = \\big \\vert trace\\big(\\mathbf{VDV}^H \\mathbf  A\\big)\\big \\vert = \\big \\vert trace\\big(\\mathbf{DV}^H \\mathbf  A\\mathbf V \\big)\\big \\vert \\leq trace\\big(\\mathbf A\\big)$ \n",
    "\n",
    "let $\\mathbf B := \\mathbf V^H \\mathbf {AV}$\n",
    "\n",
    "$\\big \\vert trace\\big(\\mathbf{UA}\\big)\\big \\vert = \\big \\vert trace\\big(\\mathbf{DB}\\big)\\big \\vert \\leq  trace\\big(\\mathbf B\\big) = trace\\big(\\mathbf A\\big)$ \n",
    "\n",
    "here we simply observe, by direct application of triangle inequality:  \n",
    "\n",
    "$trace\\big(\\mathbf A\\big) = \\sum_{k=1}^n a_{k,k} = \\sum_{k=1}^n b_{k,k} = \\sum_{k=1}^n \\big \\vert d_{k,k}\\big \\vert \\big \\vert b_{k,k}\\big\\vert = \\sum_{k=1}^n \\big \\vert d_{k,k} b_{k,k}\\big\\vert \\geq \\big \\vert \\big(\\sum_{k=1}^n d_{k,k} b_{k,k}\\big) \\big\\vert$\n",
    "\n",
    "\n",
    "*begin alternative proof*  \n",
    "  \n",
    "since $\\mathbf A$ is Hermitian positive semi-definite, we may unitarily diagonalize it in the form of\n",
    "\n",
    "$\\mathbf A = \\mathbf{QDQ}^H$\n",
    "\n",
    "$\\big \\vert trace\\big(\\mathbf{UA}\\big)\\big \\vert = \\big \\vert trace\\big(\\mathbf U \\mathbf{QDQ}^H\\big)\\big \\vert = \\big \\vert trace\\big(\\mathbf Q^H\\mathbf {UQ} \\mathbf{D}\\big)\\big \\vert  \\leq trace\\big(\\mathbf A\\big)$ \n",
    "\n",
    "where $\\mathbf V := \\mathbf Q^H\\mathbf {UQ}$\n",
    "\n",
    "because $\\mathbf V$ is unitary, each column has a length (2 norm) of 1, and hence each diagonal entry has a magnitude in $[0,1]$  \n",
    "\n",
    "$\\big \\vert trace\\big(\\mathbf{UA}\\big)\\big \\vert = \\big \\vert trace\\big(\\mathbf V \\mathbf{D}\\big)\\big \\vert  \\leq trace\\big(\\mathbf A\\big)$ \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "$\\big \\vert \\big(\\sum_{k=1}^n v_{k,k} \\lambda_k \\big)\\big \\vert  \\leq  \\sum_{k=1}^n \\big \\vert v_{k,k} \\lambda_k\\big \\vert = \\sum_{k=1}^n \\big \\vert v_{k,k}\\big \\vert \\lambda_k  \\leq \\sum_{k=1}^n \\lambda_k = trace\\big(\\mathbf D\\big) = trace\\big(\\mathbf A\\big) $ \n",
    "\n",
    "\n",
    "\n",
    "by fact that each $\\lambda_k\\geq 0$ and hence scaling any entry by some real non-negative number $\\leq 1$ results in a series that is at most the same size, and finally by application of the triangle inequality.  \n",
    "\n",
    "*end alternative proof*  \n",
    "\n",
    "*begin inner product oriented alternative proof:* \n",
    "\n",
    "take the Hermitian postive square root and get \n",
    "\n",
    "$\\mathbf {BB} = \\mathbf B^H \\mathbf B = \\mathbf B \\mathbf B^H = \\mathbf A$ \n",
    "\n",
    "$\\big \\vert trace\\big(\\mathbf{UA}\\big)\\big \\vert = \\big \\vert trace\\big(\\mathbf{UBB^H}\\big)\\big \\vert =\\big \\vert trace\\big(\\mathbf B^H \\mathbf{UB}\\big)\\big \\vert = \\big \\vert \\langle \\mathbf B, \\mathbf {UB} \\rangle \\big \\vert  \\leq \\big \\Vert \\mathbf B \\big \\Vert_F \\big \\Vert \\mathbf {UB} \\big \\Vert_F = \\big \\Vert \\mathbf B \\big \\Vert_F \\big \\Vert \\mathbf {B} \\big \\Vert_F = \\big \\Vert \\mathbf B \\big \\Vert_F^2 =  trace\\big(\\mathbf B^H \\mathbf B \\big) = trace\\big(\\mathbf A\\big)$ \n",
    "\n",
    "by direct application of Cauchy Schwarz to inner product given by the trace.  (Vec operator can be of help in further interpretting this for some.)  \n",
    "\n",
    "\n",
    "\n",
    "*end second alternative proof*\n",
    "\n",
    "where we recognize that each diagonal element of $\\mathbf A$ and $\\mathbf B$ is real, non-negative (as required by Hermitian positive semi-definite matrices), each diagonal element of $\\mathbf D$ has a magnitude of one (as required for unitary matrices) and the inequality stated is a direct application of triangle inequality. \n",
    "\n",
    "** claim: **  \n",
    "\n",
    "where $\\mathbf Y$ is $n$ x $n$  Hermitian, it is also positive semi-definite, **iff** \n",
    "\n",
    "$trace\\big(\\mathbf {YX}\\big) \\geq 0 $ for all Hermitian positive semi definite $\\mathbf X$\n",
    "\n",
    "**remark:  This is something I saw elsewhere (not in Zhang) but it is amenable to the same techniques used there**  \n",
    "\n",
    "**proof: **\n",
    "\n",
    "Let $\\mathbf X = \\sigma_1 \\mathbf x_1 \\mathbf x_1^H + \\sigma_2 \\mathbf x_2 \\mathbf x_2^H + ... + \\sigma_n \\mathbf x_n \\mathbf x_n^H$ \n",
    "\n",
    "where each $\\sigma_k \\geq 0$\n",
    "\n",
    "\n",
    "$trace\\Big(\\mathbf {YX}\\Big) = trace\\Big(\\mathbf Y \\big(\\sigma_1 \\mathbf x_1 \\mathbf x_1^H + \\sigma_2 \\mathbf x_2 \\mathbf x_2^H + ... + \\sigma_n \\mathbf x_n \\mathbf x_n^H \\big) \\Big) =  \\sigma_1 trace\\Big(\\mathbf Y  \\mathbf x_1 \\mathbf x_1^H\\Big)  + \\sigma_2 trace\\Big(\\mathbf Y  \\mathbf x_2 \\mathbf x_2^H \\Big) ... + \\sigma_n trace\\Big(\\mathbf Y  \\mathbf x_n \\mathbf x_n^H\\Big) \\geq 0 $ \n",
    "\n",
    "$trace\\Big(\\mathbf {YX}\\Big) = \\sigma_1 \\Big( \\mathbf x_1^H \\mathbf Y  \\mathbf x_1 \\Big)  + \\sigma_2 \\Big(\\mathbf x_2^H  \\mathbf Y  \\mathbf x_2 \\Big) ... + \\sigma_n \\Big( \\mathbf x_n^H \\mathbf Y  \\mathbf x_n\\Big) \\geq 0$\n",
    "\n",
    "the trace will obviously be $\\geq 0$ if each term in the finite series is $\\geq 0$.  Now we hone in on an important special subset: since this is for *any* $\\mathbf X$, this also includes the special case where $\\sigma_r = 0$ for $r = \\{2, 3, ...,n\\}$ and we select $\\mathbf x_1$ to be any vector we like. Thus the claim reduces to \n",
    "\n",
    "$trace\\Big(\\mathbf {YX}\\Big) = \\sigma_1 \\Big( \\mathbf x_1^H \\mathbf Y  \\mathbf x_1 \\Big) \\geq 0$\n",
    "\n",
    "for all $\\mathbf x_1$ and any $\\sigma_1 \\geq 0$, which is the familiar test for positive semi-definiteness of $\\mathbf Y$.  \n",
    "- - - -\n",
    "\n",
    "*alternative proof*  \n",
    "\n",
    "this approach may be more slick, but perhaps less intuitive.\n",
    "\n",
    "$\\mathbf Y = \\mathbf Q \\mathbf \\Lambda \\mathbf Q^H$  \n",
    "$\\mathbf X = \\mathbf{V\\Sigma V^H}$\n",
    "\n",
    "\n",
    "we know that $\\mathbf X$ is any arbitrary Hermitian positive semi-definite matrix, and need to verify that the trace inequality means $\\mathbf Y$ must be Hermitian positive semi-definite as well.  \n",
    "\n",
    "$trace\\Big(\\mathbf {YX}\\Big) = trace\\Big(\\mathbf Y \\big(\\mathbf{V\\Sigma V^H}\\big) \\Big) = trace\\Big(\\big(\\mathbf V^H \\mathbf{Y V}\\big)\\mathbf \\Sigma \\Big) = trace\\Big(\\mathbf B \\mathbf \\Sigma \\Big) = \\sum_{i=1}^n b_{i,i}\\sigma_i \\geq 0 $  \n",
    "\n",
    "where $\\mathbf B = \\big(\\mathbf V^H \\mathbf{Y V}\\big)$.  If $\\mathbf Y$ is Hermitian positive semi-definite, then its diagonal elements are real non-negative and thus $\\mathbf B$ (which is also Hermitian positive semi-definite) has real non-negative diagonal elements.  \n",
    "\n",
    "The above should seem intuitve and proves the *if* but not the **iff**.  We now consider the other leg:  \n",
    "\n",
    "\n",
    "$trace\\Big(\\mathbf {YX}\\Big) = trace\\Big(\\big(\\mathbf Q \\mathbf \\Lambda \\mathbf Q^H\\big) \\mathbf X \\Big) = trace\\Big(\\mathbf \\Lambda \\big( \\mathbf Q^H \\mathbf X  \\mathbf Q\\big) \\Big)=  trace\\Big(\\mathbf \\Lambda \\big(\\mathbf C \\big) \\Big) = \\sum_{i=1}^n c_{i,i}\\lambda_i \\geq 0$ \n",
    "\n",
    "where $\\big(\\mathbf C \\big) = \\big( \\mathbf Q^H \\mathbf X  \\mathbf Q\\big)$ and $\\mathbf C$ is Hermitian positive semi-definite like $\\mathbf X$, thus each $ c_{i,i} \\geq 0$.  \n",
    "\n",
    "Consider the case where $\\mathbf Y$ has an eigenvalue less than zero.  E.g. suppose $\\lambda_n \\lt 0$ but $\\lambda_r \\geq 0$ for $r = \\{1, 2, 3, ..., n-1\\}$.  If this is true, then \n",
    "\n",
    "$c_{n,n} \\lambda_n + \\sum_{r=1}^{n-1} c_{r,r}\\lambda_r \\ngeq 0$\n",
    "\n",
    "for some large enough $c_{n,n}$.  There are some slick applications of the Kronecker product to solve for any given $\\mathbf C$, but the easiest approach is to select some $\\mathbf C$ where $c_{n,n} = 1$ and *all* other cells in $\\mathbf C$ are zero.  Such a matrix is still Hermitian positive semi-definite, and from here, for any given $\\mathbf Y$ we can multiply out its eigenvectors and get the appropriate $\\mathbf X = \\big( \\mathbf Q \\mathbf C  \\mathbf Q^H\\big)$, which is Hermitian positive semi-definite, yet violates the above inequality. Hence for the inequality to hold *all* eigenvalues of $\\mathbf X$ must be real non-negative.  And since we have determined that $\\mathbf X$ is Hermitian with strictly non-negative eigenvalues in order for the inequality to always hold, then $\\mathbf X$ must be Hermitian positive semi-definite.  This completes the proof.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**claim:**\n",
    "\n",
    "where $\\mathbf B$ is Hermitian positive semi-definite, and $\\mathbf Q$ has orthonormal columns but generally is **not** square,\n",
    "\n",
    "$trace\\big(\\mathbf B\\big) \\geq trace\\big(\\mathbf Q^H \\mathbf B \\mathbf Q\\big)$\n",
    "\n",
    "**remark:**\n",
    "I saw this as part of assignment 3, here: http://www4.ncsu.edu/~asaibab/classes/ma723/\n",
    "\n",
    "The suggested approach involves using Cauchy eigenvalue interlacing, and also blocked matrices, in order to prove a special Cauchy interlacing setup, the above trace relation, and also a related claim involving determinants.  Setting aside the determinant, this suggested approach seemed like overkill, except as part of an interlacing or blocked matrix drill.  The below proof makes use of SVD and cyclic property of the trace to prove this claim.  \n",
    "\n",
    "**proof: ** \n",
    "\n",
    "let $\\mathbf Q = \\mathbf U \\mathbf \\Sigma \\mathbf V^H$ \n",
    "\n",
    "where $\\mathbf U$ and $\\mathbf V$ are square, but $\\mathbf \\Sigma$ in general is tall and skinny. Note that $\\mathbf Q^H \\mathbf Q = \\mathbf I_k$ where $k$ is the number of columns in $\\mathbf Q$.  Hence we confirm that each singular value $\\{\\sigma_1, \\sigma_2, ..., \\sigma_k\\}$ has a magnitude of one, and hence a value of one (since singular values are, by construction, real, non-negative.) \n",
    "\n",
    "\n",
    "$trace\\big(\\mathbf B\\big) \\geq trace\\big(\\mathbf Q^H \\mathbf B \\mathbf Q\\big) = trace\\big(\\mathbf V \\mathbf \\Sigma^H \\mathbf U^H \\mathbf B \\mathbf U \\mathbf \\Sigma \\mathbf V^H \\big) = trace\\big(\\mathbf U^H \\mathbf B \\mathbf U \\mathbf \\Sigma \\mathbf \\Sigma^H \\big)$\n",
    "\n",
    "assign $\\mathbf C := \\mathbf U^H \\mathbf B \\mathbf U$   \n",
    "\n",
    "$\\mathbf C$ is Hermitian positive semi-definite as well  \n",
    "$\\big(\\mathbf \\Sigma \\mathbf \\Sigma^H \\big)$ is a diagonal matrix with $k$ entries equal to one, and all else equal to 0.\n",
    "\n",
    "\n",
    "$trace\\big(\\mathbf B\\big) = trace\\big(\\mathbf C\\big) = c_{1,1} + c_{2,2} + ... + c_{k,k} + c_{k+1,k+1} + ... + c_{n,n} \\geq c_{1,1} + c_{2,2} + ... + c_{k,k}  = trace\\Big(\\big(\\mathbf C\\big( \\mathbf \\Sigma \\mathbf \\Sigma^H \\big)\\Big)$\n",
    "\n",
    "because each diagonal entry of $\\mathbf C$ is real non-negative, by virtue of it being Hermitian positive semi-definite.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Hermitian Positive Semi Definite Inequalities with Hadamard Product \n",
    "\n",
    "*note 1: This apparently is covered under the Schur Product Theorem, which is an interesting albeit esoteric part of matrix theory.* \n",
    "\n",
    "*note 2: The inequalities in here are covered via a diagonalization argument.  For a different proof of these relations see my writeup \"Kronecker_Product.ipynd\" -- specifically the subsection called \"Remark on Kronecker Product vs Hadamard Product\".     *\n",
    "\n",
    "\n",
    "consider Hermitian Positive Semi Definite Matrices $\\mathbf A$ and $\\mathbf B$.  Then consider the matrix $\\mathbf C = \\mathbf A \\circ \\mathbf B$, where $\\circ$ denotes the Hadamard product.    \n",
    "\n",
    "**claim:  **\n",
    "$\\mathbf C $ is Hermitian positive semi-definite\n",
    "\n",
    "\n",
    "*Reminder:*\n",
    "Hadamard products distribute accross matrix addition because scalar multiplication distributes across scalar addition. \n",
    "\n",
    "example:\n",
    "\n",
    "$\\mathbf Z = \\big(\\mathbf X + \\mathbf Y\\big) \\circ \\mathbf W = \\mathbf X \\circ \\mathbf W + \\mathbf Y \\circ \\mathbf W$\n",
    "\n",
    "that is\n",
    "\n",
    "$z_{i,j} = (x_{i,j} + y_{i,j})*w_{i,j} = x_{i,j}w_{i,j} + y_{i,j}w_{i,j}$\n",
    "\n",
    "\n",
    "**proof:  **\n",
    "\n",
    "$c_{i,j} = a_{i,j} * b_{i,j} = \\bar{a_{j,i}}*\\bar{b_{j,i}} = \\bar{c_{j,i}}$\n",
    "\n",
    "By inspection we see that $\\mathbf C$ is Hermitian. \n",
    "\n",
    "Now the claim of positive semi definitiness means $\\mathbf x^H \\mathbf C \\mathbf x \\geq 0 $ for all $\\mathbf x$. \n",
    "\n",
    "next we use the decomposition employed in \"julia_hmm_viterbi_as_qp_and_lp_upload\" contained in the Optimization aka \"markov_optimization\" folder.  \n",
    "\n",
    "$\\mathbf x^H \\mathbf C \\mathbf x  = \\mathbf 1^H \\big(\\mathbf C \\circ \\mathbf x\\mathbf x^H \\big)\\mathbf 1 = sum\\big(\\mathbf C \\circ \\mathbf x\\mathbf x^H \\big)$\n",
    "\n",
    "where \"sum\" denotes adding up each scalar entry of the matrix $\\big( \\mathbf C \\circ \\mathbf x\\mathbf x^H \\big)$.\n",
    "\n",
    "Hence we are evaluating:\n",
    "\n",
    "$sum\\big(\\mathbf C \\circ \\mathbf x\\mathbf x^H \\big) = sum\\big(\\mathbf A \\circ \\mathbf B \\circ \\mathbf x\\mathbf x^H \\big)$\n",
    "\n",
    "\n",
    "using associativity and commutativity of the Hadamard product, we have\n",
    "\n",
    "$sum\\big(\\mathbf A \\circ \\mathbf B \\circ \\mathbf x\\mathbf x^H \\big) = sum\\Big( \\mathbf A \\circ \\mathbf x\\mathbf x^H \\circ \\mathbf B   \\Big) = sum\\Big(\\big(\\mathbf A \\circ \\mathbf x\\mathbf x^H\\big) \\circ \\mathbf B   \\Big)$\n",
    "\n",
    "since $\\mathbf A$ is Hermitiain Positive Semi Definite, we can re-write it as \n",
    "\n",
    "$\\mathbf A = (\\lambda_1 \\mathbf p_1 \\mathbf p_1^H + \\lambda_2 \\mathbf p_2 \\mathbf p_2^H + ... + \\lambda_n \\mathbf p_n \\mathbf p_n^H)$ \n",
    "\n",
    "where each $\\lambda_k$ is real valued and non-negative\n",
    "\n",
    "$sum\\Big(\\big(\\mathbf A \\circ \\mathbf x\\mathbf x^H\\big) \\circ \\mathbf B   \\Big) = sum\\Big(\\big((\\lambda_1 \\mathbf p_1 \\mathbf p_1^H + \\lambda_2 \\mathbf p_2 \\mathbf p_2^H + ... + \\lambda_n \\mathbf p_n \\mathbf p_n^H) \\circ \\mathbf x\\mathbf x^H\\big) \\circ \\mathbf B   \\Big) $\n",
    "\n",
    "$sum\\Big(\\big(\\mathbf A \\circ \\mathbf x\\mathbf x^H\\big) \\circ \\mathbf B   \\Big) = sum\\Big(\\big(\\lambda_1 \\mathbf p_1 \\mathbf p_1^H \\circ \\mathbf x\\mathbf x^H + \\lambda_2 \\mathbf p_2 \\mathbf p_2^H \\circ \\mathbf x\\mathbf x^H + ... + \\lambda_n \\mathbf p_n \\mathbf p_n^H\\circ \\mathbf x\\mathbf x^H \\big) \\circ \\mathbf B   \\Big)$\n",
    "\n",
    "re-writing this with sigma notation, we have\n",
    "\n",
    "$sum\\Big(\\big( \\sum_{k=1}^n \\lambda_k \\mathbf p_k \\mathbf p_k^H \\circ \\mathbf x\\mathbf x^H \\big) \\circ \\mathbf B   \\Big)$\n",
    "\n",
    "now we definte $\\mathbf y_k$, where\n",
    "\n",
    "$\\mathbf y_k := \\mathbf p_k \\circ \\mathbf x$\n",
    "\n",
    "hence $\\mathbf y_k \\mathbf y_k^H = \\mathbf p_k \\mathbf p_k^H \\circ \\mathbf x\\mathbf x^H $\n",
    "\n",
    "where for avoidance of doubt, we double check the indices:\n",
    "\n",
    "$\\big(\\mathbf y_k \\mathbf y_k^H\\big)_{i,j} = (p_{k,i}  * x_{i}) * (\\bar{p_{k,j}}* \\bar{x_{j}}) = (p_{k,i} * \\bar{p_{k,j}})  * (x_{i}  * \\bar{x_{j}}) = \\big(\\mathbf p_k \\mathbf p_k^H\\big)_{i,j} \\circ \\big(\\mathbf{xx}^H\\big)_{i,j}=   \\big(\\mathbf p_k \\mathbf p_k^H \\circ \\mathbf{xx}^H\\big)_{i,j}$ \n",
    "\n",
    "our expression becomes\n",
    "\n",
    "$sum\\Big(\\big( \\sum_{k=1}^n \\lambda_k \\mathbf y_k \\mathbf y_k^H \\big) \\circ \\mathbf B   \\Big) = sum\\Big( \\sum_{k=1}^n \\lambda_k \\mathbf y_k \\mathbf y_k^H \\circ \\mathbf B   \\Big)$\n",
    "\n",
    "now, recognizing that when we have a finite number of terms, we can always interchange linear operators like $sum()$ and $\\sum$\n",
    "\n",
    "$sum\\Big( \\sum_{k=1}^n \\lambda_k \\mathbf y_k \\mathbf y_k^H \\circ \\mathbf B   \\Big) = \\sum_{k=1}^n  sum\\Big( \\lambda_k \\mathbf y_k \\mathbf y_k^H \\circ \\mathbf B   \\Big)$\n",
    "\n",
    "- - - -\n",
    "side note: the above is interchange is equivalent to recognizing that $\\mathbf 1^H \\big( \\mathbf W + \\mathbf X\\big)\\mathbf 1 = \\mathbf 1^H\\big(\\mathbf W\\big)\\mathbf 1 + \\mathbf 1^H\\big(\\mathbf X\\big)\\mathbf 1$\n",
    "- - - -\n",
    "\n",
    "$\\sum_{k=1}^n  sum\\big( \\lambda_k \\mathbf y_k \\mathbf y_k^H \\circ \\mathbf B   \\big) = \\sum_{k=1}^n  \\lambda_k \\Big(sum\\big(  \\mathbf y_k \\mathbf y_k^H \\circ \\mathbf B   \\big)\\Big) = \\sum_{k=1}^n  \\lambda_k \\Big(sum\\big( \\mathbf B \\circ \\mathbf y_k \\mathbf y_k^H \\big)\\Big) = \\sum_{k=1}^n  \\lambda_k \\Big(\\mathbf y_k^H \\mathbf B \\mathbf y_k\\Big)$\n",
    "\n",
    "recognizing our original quadratic form decomposition $\\mathbf y_k^H \\mathbf B \\mathbf y_k = sum\\big( \\mathbf B \\circ \\mathbf y_k \\mathbf y_k^H \\big)$ \n",
    "\n",
    "since $\\mathbf B$ is Hermitian positive semi-definite, we know that $\\mathbf y_k^H \\mathbf B \\mathbf y_k \\geq 0$ for any $\\mathbf y_k$, and we recall that $\\lambda_k \\geq 0$ \n",
    "\n",
    "Hence we say \n",
    "\n",
    "$\\sum_{k=1}^n  \\lambda_k \\big(\\mathbf y_k^H \\mathbf B \\mathbf y_k\\big) =  \\lambda_1 \\big(\\mathbf y_1^H \\mathbf B \\mathbf y_1\\big) + \\lambda_2 \\big(\\mathbf y_2^H \\mathbf B \\mathbf y_2\\big) + ... + \\lambda_n \\big(\\mathbf y_n^H \\mathbf B \\mathbf y_n\\big) \\geq 0$\n",
    "\n",
    "because each term in that series is itself $\\geq 0$.\n",
    "\n",
    "This proves that $\\mathbf C = \\mathbf A \\circ \\mathbf B$ is a Hermitian positive semi-definite matrix.\n",
    "\n",
    "**claim:  **  \n",
    "\n",
    "$\\lambda_{1, A}* \\lambda_{1,B} \\geq \\lambda_{1, C}$\n",
    "\n",
    "where $\\mathbf C = \\mathbf A \\circ \\mathbf B$\n",
    "\n",
    "alternatively stated as:  \n",
    "$\\lambda \\big(\\mathbf A\\big)_{max} \\lambda \\big(\\mathbf B\\big)_{max} \\geq \\lambda \\big(\\mathbf A \\circ \\mathbf B\\big)_{max}$\n",
    "\n",
    "and as always eigenvalues are well ordered so that $\\lambda_1 \\geq \\lambda_2 \\geq ... \\geq \\lambda_n \\geq 0$ for any of these Hermitian positive semi-definite matrices\n",
    "\n",
    "The claim is trivially an equality if $\\mathbf A = \\mathbf 0$ or $\\mathbf B = \\mathbf 0$, hence we assume that neither matrix is the zero matrix, i.e. that $\\big \\Vert \\mathbf A \\big \\Vert_F \\gt 0$ and $\\big \\Vert \\mathbf B \\big \\Vert_F \\gt 0$\n",
    "\n",
    "**proof:**  \n",
    "\n",
    "\n",
    "$ \\lambda_{1,A} \\mathbf I - \\mathbf A = \\lambda_{1,A} \\mathbf I - \\mathbf {PDP}^H = \\lambda_{1,A} \\mathbf {PIP}^H - \\mathbf {PDP}^H = \\mathbf P \\big(\\lambda_{1,A} \\mathbf I - \\mathbf D\\big)\\mathbf P^H$\n",
    "\n",
    "when we inspect the diagonal entries of the diagonal matrix given by, we see:\n",
    "\n",
    "$\\big(\\lambda_{1,A} \\mathbf I - \\mathbf D\\big)_{k,k} = \\lambda_{1, A} - \\lambda_{k, A} \\geq 0$\n",
    "\n",
    "hence the matrix $\\big(\\lambda_{1,A} \\mathbf I - \\mathbf A \\big)$ is Hermitian positive semi-definite.  \n",
    "\n",
    "now consider:\n",
    "\n",
    "$\\big(\\lambda_{1,A} \\mathbf I - \\mathbf A \\big)\\circ \\mathbf B$\n",
    "\n",
    "Based on the preceding proof, the matrix that results from this, too, must be Hermitian positive semi-definite.  \n",
    "\n",
    "Hence we say \n",
    "\n",
    "$\\mathbf x^H \\Big(\\big(\\lambda_{1,A} \\mathbf I - \\mathbf A \\big)\\circ \\mathbf B\\Big)\\mathbf x \\geq 0$\n",
    "\n",
    "for any $\\mathbf x$.\n",
    "\n",
    "$\\mathbf x^H \\Big(\\big(\\lambda_{1,A} \\mathbf I - \\mathbf A \\big)\\circ \\mathbf  B\\Big)\\mathbf x = \\mathbf x^H \\Big(\\big(\\lambda_{1,A} \\mathbf I\\big) \\circ \\mathbf B \\Big)\\mathbf x - \\mathbf x^H \\Big(\\mathbf A \\circ \\mathbf B\\Big)\\mathbf x \\geq 0$\n",
    "\n",
    "for any $\\mathbf x$\n",
    "\n",
    "$\\mathbf x^H \\Big(\\big(\\lambda_{1,A} \\mathbf I\\big) \\circ \\mathbf B \\Big)\\mathbf x \\geq \\mathbf x^H \\Big(\\mathbf A \\circ \\mathbf B\\Big)\\mathbf x = \\mathbf x^H \\mathbf C \\mathbf x$\n",
    "\n",
    "for any $\\mathbf x$\n",
    "\n",
    "now set the constraint that $\\Vert \\mathbf x \\Vert_2^2 = 1$.  The right hand side is maximized with $\\mathbf x : = \\mathbf x_1$ which is the eigenvector associated with $\\lambda_{1,C}$\n",
    "\n",
    "$\\mathbf x_1^H \\Big(\\big(\\lambda_{1,A} \\mathbf I\\big) \\circ \\mathbf B \\Big)\\mathbf x_1 = \\lambda_{1,A} * \\mathbf x_1^H \\Big( \\mathbf I \\circ \\mathbf B \\Big)\\mathbf x_1 \\geq \\lambda_{1,C} = \\mathbf x_1^H \\mathbf C \\mathbf x_1$\n",
    "\n",
    "Hence we know that the Hermitian positive semi-definite matrix given by $\\big(\\mathbf I \\circ \\mathbf B \\big)$ must have at least one eigenvalue that can be scaled by $\\lambda \\big(\\mathbf A\\big)_{max}$ and the resulting product $\\geq \\lambda \\big(\\mathbf C\\big)_{max}$.\n",
    "\n",
    "If we are able to prove that the maximal eigenvalue of $\\mathbf B$ is at least as big as the maximal eigenvalue of $\\big(\\mathbf I \\circ \\mathbf B \\big)$, then we are done.  \n",
    "\n",
    "where $\\mathbf H := \\mathbf I \\circ \\mathbf B$.  \n",
    "\n",
    "Notice that $\\mathbf H$ in effect takes all of the diagonal entries of $\\mathbf B$ (and keeps their ordering intact), and then zeros out all other entries of $\\mathbf B$.\n",
    "\n",
    "What we have proven so far can be re-written as \n",
    "\n",
    "$\\lambda_{1,A} * \\lambda_{1, H} \\geq \\lambda_{1,C} $\n",
    "\n",
    "Further we now claim:\n",
    "\n",
    "$\\lambda_{1,A} * \\lambda_{1, B} \\geq \\lambda_{1,A} * \\lambda_{1, H} \\geq \\lambda_{1,C} $\n",
    "\n",
    "To justify this, consider the following: \n",
    "\n",
    "max $\\mathbf y^H \\mathbf B \\mathbf y = \\lambda_{1, B}$\n",
    "\n",
    "subject to the constraint that $\\Vert \\mathbf y \\Vert_2^2 = 1$\n",
    "\n",
    "we can always choose to restrict ourself to just one standard basis vector $\\mathbf e_k$, for $k = \\{1, 2, ..., n\\}$.  For avoidance of doubt, the standard basis vectors are shown below:\n",
    "\n",
    "$\\bigg[\\begin{array}{c|c|c|c}\n",
    "\\mathbf e_1 & \\mathbf  e_2 &\\cdots &\\mathbf  e_n\n",
    "\\end{array}\\bigg] = \\mathbf I$\n",
    "\n",
    "\n",
    "which makes the optimization\n",
    "\n",
    "max $\\mathbf e_k^H \\mathbf B \\mathbf e_k = \\lambda_{1, H}$\n",
    "\n",
    "Thus\n",
    "\n",
    "max $\\mathbf y^H \\mathbf B \\mathbf y = \\lambda_{1, B}  \\geq$ max $ \\mathbf e_k^H \\mathbf B \\mathbf e_k = \\lambda_{1, H}$, again with the constrain that $\\Vert \\mathbf y \\Vert_2^2 = 1$. \n",
    "\n",
    "Hence $\\lambda_{1, B} \\geq \\lambda_{1, H}$\n",
    "\n",
    "To conclude we have: \n",
    "\n",
    "$\\lambda_{1,A} * \\lambda_{1, B} \\geq \\lambda_{1,A} * \\lambda_{1, H} \\geq \\lambda_{1,C} $\n",
    "\n",
    "or more succinctly,\n",
    "\n",
    "$\\lambda \\big(\\mathbf A\\big)_{max} \\lambda \\big(\\mathbf B\\big)_{max} \\geq \\lambda \\big(\\mathbf A\\circ \\mathbf B\\big)_{max}$\n",
    "\n",
    "and the claim is proven\n",
    "\n",
    "- - - -\n",
    "*begin alternative proof*  \n",
    "\n",
    "Another way to prove this, which seems a bit more intuitive, is to leverage the work done in the first part to prove that $\\big(\\mathbf A \\circ \\mathbf B\\big)$ is a  Hermitian positive semi definite matrix.  The final expression we had was\n",
    "\n",
    "$ \\sum_{k=1}^n  \\lambda_k \\big(\\mathbf y_k^H \\mathbf B \\mathbf y_k\\big) = \\lambda_1 \\big(\\mathbf y_1^H \\mathbf B \\mathbf y_1\\big) + \\lambda_2 \\big(\\mathbf y_2^H \\mathbf B \\mathbf y_2\\big) + ... + \\lambda_n \\big(\\mathbf y_n^H \\mathbf B \\mathbf y_n\\big) \\geq 0$\n",
    "\n",
    "Recalling that the $\\lambda_k$'s were the eigenvalues of $\\mathbf A$, and that $\\lambda_1 \\geq \\lambda_2 \\geq ... \\geq \\lambda_n \\geq 0$, we can upper bound this series as follows:\n",
    "\n",
    "$ \\sum_{k=1}^n  \\lambda_1 \\big(\\mathbf y_k^H \\mathbf B \\mathbf y_k\\big) \\geq \\sum_{k=1}^n  \\lambda_k \\big(\\mathbf y_k^H \\mathbf B \\mathbf y_k\\big) \\geq 0$\n",
    "\n",
    "From here we work backward and examine the impact of homegenizing the eigenvalues of $\\mathbf A$, specificially recalling \n",
    "\n",
    "$sum\\Big(\\big(\\mathbf A \\circ \\mathbf x\\mathbf x^H\\big) \\circ \\mathbf B   \\Big) = sum\\Big(\\big((\\lambda_1 \\mathbf p_1 \\mathbf p_1^H + \\lambda_2 \\mathbf p_2 \\mathbf p_2^H + ... + \\lambda_n \\mathbf p_n \\mathbf p_n^H) \\circ \\mathbf x\\mathbf x^H\\big) \\circ \\mathbf B   \\Big) $\n",
    "\n",
    "now becomes\n",
    "\n",
    "$sum\\Big(\\big( \\lambda_1 ( \\mathbf p_1 \\mathbf p_1^H + \\mathbf p_2 \\mathbf p_2^H + ... + \\mathbf p_n \\mathbf p_n^H) \\circ \\mathbf x\\mathbf x^H\\big) \\circ \\mathbf B   \\Big) = sum\\Big( \\lambda_1 \\big( \\mathbf P \\mathbf P^H\\big) \\circ \\mathbf x\\mathbf x^H \\circ \\mathbf B   \\Big) = sum\\Big(\\lambda \\big(\\mathbf A\\big)_{max} \\big( \\mathbf I\\big) \\circ \\mathbf x\\mathbf x^H \\circ \\mathbf B   \\Big) $\n",
    "\n",
    "which we can re-arrange to\n",
    "\n",
    "$\\lambda \\big(\\mathbf A\\big)_{max} * sum\\Big( \\big(\\mathbf I \\circ \\mathbf B\\big) \\circ \\mathbf x \\mathbf x^H     \\Big) = \\lambda \\big(\\mathbf A\\big)_{max} *\\mathbf x^H \\big(\\mathbf I \\circ \\mathbf B\\big)\\mathbf x $\n",
    "\n",
    "\n",
    "which tells us that at a minimum $\\lambda \\big(\\mathbf A\\big)_{max} *\\lambda \\big(\\mathbf I\\circ \\mathbf B\\big)_{max} \\geq \\lambda \\big(\\mathbf A\\circ \\mathbf B\\big)_{max}$\n",
    "\n",
    "because \n",
    "\n",
    "$\\lambda_1 \\big(\\mathbf y_1^H \\mathbf B \\mathbf y_1\\big) + \\lambda_1 \\big(\\mathbf y_2^H \\mathbf B \\mathbf y_2\\big) + ... + \\lambda_1 \\big(\\mathbf y_n^H \\mathbf B \\mathbf y_n\\big) \\geq  \\lambda_1 \\big(\\mathbf y_1^H \\mathbf B \\mathbf y_1\\big) + \\lambda_2 \\big(\\mathbf y_2^H \\mathbf B \\mathbf y_2\\big) + ... + \\lambda_n \\big(\\mathbf y_n^H \\mathbf B \\mathbf y_n\\big)$\n",
    "\n",
    "for any $\\mathbf x$, (subject to $\\Vert \\mathbf x \\Vert_2 = 1$) including $\\mathbf x$ that maximizes the right hand side, which gives an answer $= \\lambda \\big(\\mathbf A\\circ \\mathbf B\\big)_{max}$.  Hence we know that the left hand side has at least one solution (read: eigenvalue associated with $\\lambda \\big(\\mathbf I\\circ \\mathbf B\\big)_{max}$ times $\\lambda \\big(\\mathbf A\\big)_{max}$ ) that is greater than or equal to the right hand side.  \n",
    "\n",
    "from here we again notice that, subject to the constraint $\\Vert \\mathbf z \\Vert_2 = 1$\n",
    "\n",
    "max $\\mathbf z^H \\mathbf B \\mathbf z = \\lambda \\big(\\mathbf B\\big)_{max}  \\geq $ max $\\mathbf e_k^H \\mathbf B \\mathbf e_k = \\lambda \\big(\\mathbf I \\circ \\mathbf B\\big)_{max}$\n",
    "\n",
    "recalling that $\\bigg[\\begin{array}{c|c|c|c}\n",
    "\\mathbf e_1 & \\mathbf  e_2 &\\cdots &\\mathbf  e_n\n",
    "\\end{array}\\bigg] = \\mathbf I$\n",
    "\n",
    "and we conclude with\n",
    "\n",
    "$\\lambda \\big(\\mathbf A\\big)_{max} *\\lambda \\big(\\mathbf B\\big)_{max}  \\geq \\lambda \\big(\\mathbf A\\big)_{max} *\\lambda \\big(\\mathbf I\\circ \\mathbf B\\big)_{max} \\geq \\lambda \\big(\\mathbf A\\circ \\mathbf B\\big)_{max}$\n",
    "\n",
    "*end alternative proof*  \n",
    "\n",
    "*commentary*\n",
    "\n",
    "It is interesting to think about the eigenvalues of $\\mathbf B$ and $\\big(\\mathbf I \\circ \\mathbf B\\big)$.  We can use Gerschgorin discs (which we can flatten to line segments if we want because we know the eigenvalues are real) to bound the eigenvalues of $\\mathbf B$, noting that the center point of the discs are given exactly by $\\big(\\mathbf I \\circ \\mathbf B\\big)$.  \n",
    "\n",
    "Let's suppose that $\\mathbf B \\neq \\big(\\mathbf I \\circ \\mathbf B\\big)$ and that there are no zeros along the diagonal.  \n",
    "\n",
    "Then we know that these matrices eigenvalues sum to be the same amount:\n",
    "\n",
    "$trace\\big(\\mathbf B\\big) = trace\\big(\\mathbf I \\circ \\mathbf B\\big)$\n",
    "\n",
    "since $\\big(\\mathbf I \\circ \\mathbf B\\big)$ is diagonal, we know its determinant is the product of those entries (which is $\\gt 0$ because we assume no zeros on diagonal). From applying the Hadamard Inequality, we know that \n",
    "\n",
    "$det\\big(\\mathbf B\\big) \\leq det\\big(\\mathbf I \\circ \\mathbf B\\big)$\n",
    "\n",
    "However, we also know that if we square both sides, the left hand side will have a higher trace:\n",
    "\n",
    "$trace\\big(\\mathbf B^2\\big) = trace\\big(\\mathbf B^H \\mathbf B \\big) = \\Vert \\mathbf B\\Vert_F^2 \\geq \\Vert \\big( \\mathbf I \\circ  \\mathbf B\\big) \\Vert_F^2 = trace\\Big(\\big(\\mathbf I \\circ \\mathbf B\\big)^2\\Big)$\n",
    "\n",
    "This would seem to suggest that that $\\mathbf B$ has more extreme eigenvalues than $\\big(\\mathbf I \\circ \\mathbf B\\big)$, that when you add them all up, they are the same, but when you multiply them, you get a smaller product.  However, when you square them, the small ones decrease, but in the spirit of Jensen's Inequality, we'd observe that that $\\frac{1}{2}(\\lambda_1^2 + \\lambda_n^2) \\geq (\\frac{1}{2}(\\lambda_1 + \\lambda_n))^2$\n",
    "\n",
    "And this is what our quadratic form tells us, again, where $\\Vert \\mathbf z \\Vert_2 = 1$  \n",
    "\n",
    "max $\\mathbf z^H \\mathbf B \\mathbf z = \\lambda \\big(\\mathbf B\\big)_{max}  \\geq $ max $\\mathbf e_k^H \\mathbf B \\mathbf e_k = \\lambda \\big(\\mathbf I \\circ \\mathbf B\\big)_{max}$\n",
    "\n",
    "and if we want to minimize the quadratic form, we see,\n",
    "\n",
    "min $\\mathbf z^H \\mathbf B \\mathbf z = \\lambda \\big(\\mathbf B\\big)_{min}  \\leq $ min $\\mathbf e_k^H \\mathbf B \\mathbf e_k = \\lambda \\big(\\mathbf I \\circ \\mathbf B\\big)_{min}$\n",
    "\n",
    "Because $\\mathbf e_k$'s are a proper subset of what we can choose our $\\mathbf z$ from, and hence our optimal $\\mathbf z$ must give a result at least as good as using $\\mathbf e_k$.  \n",
    "\n",
    "\n",
    "- - - - \n",
    "\n",
    "**claim:**  \n",
    "\n",
    "$trace\\big(\\mathbf A \\circ \\mathbf B\\big) \\leq trace\\big(\\mathbf A\\big) trace\\big(\\mathbf B\\big)$\n",
    "\n",
    "**proof:**  \n",
    "\n",
    "$trace\\big(\\mathbf A \\circ \\mathbf B\\big) = a_{1,1}*b_{1,1} + a_{2,2}*b_{2,2} +... + a_{n,n}*b_{n,n} \\leq (a_{1,1} + a_{2,2} +... + a_{n,n}) (b_{1,1} + b_{2,2} +... + b_{n,n}) = trace\\big(\\mathbf A\\big) trace\\big(\\mathbf B\\big)$\n",
    "\n",
    "because each diagonal entry $a_{k,k}$ and $b_{k,k}$ is real valued and non-negative, and hence all cross terms are real valued and non-negative.  \n",
    "\n",
    "\n",
    "**claim:**  \n",
    "\n",
    "$trace\\big(\\mathbf A \\circ \\mathbf B\\big) \\leq \\frac{1}{2} trace\\big(\\mathbf A \\circ \\mathbf A + \\mathbf B \\circ \\mathbf B\\big)$\n",
    "\n",
    "**proof:**\n",
    "\n",
    "a very simple way to prove this claim is to notice that $\\mathbf C:= \\mathbf A - \\mathbf B$, is a Hermitian matrix.  And $\\mathbf C$, like any Hermitian matrix, must have real valued entries on its diagonal (or else they could not be equal to their conjugate, and we would not have $\\mathbf C = \\mathbf C^H$).  Thus when we square the real valued entries along the diagonal, we see $\\mathbf c_{k,k} * \\mathbf c_{k,k} \\geq 0$.  \n",
    "\n",
    "Hence we can say:\n",
    "\n",
    "$trace\\big(\\mathbf C \\circ \\mathbf C\\big) \\geq 0$\n",
    "\n",
    "$trace\\Big(\\big(\\mathbf A - \\mathbf B\\big) \\circ \\big(\\mathbf A - \\mathbf B\\big)\\Big) \\geq 0$\n",
    "\n",
    "$trace\\Big( \\mathbf A \\circ \\mathbf A + \\mathbf B \\circ \\mathbf B - 2 *\\mathbf A \\circ \\mathbf B\\Big) \\geq 0$\n",
    "\n",
    "$trace\\Big( \\mathbf A \\circ \\mathbf A + \\mathbf B \\circ \\mathbf B\\Big) - 2* trace\\Big(\\mathbf A \\circ \\mathbf B\\Big) \\geq 0$\n",
    "\n",
    "$trace\\Big( \\mathbf A \\circ \\mathbf A + \\mathbf B \\circ \\mathbf B\\Big) \\geq 2* trace\\Big(\\mathbf A \\circ \\mathbf B\\Big) $\n",
    "\n",
    "\n",
    "$\\frac{1}{2} trace\\Big( \\mathbf A \\circ \\mathbf A + \\mathbf B \\circ \\mathbf B\\Big) \\geq trace\\Big(\\mathbf A \\circ \\mathbf B\\Big) $\n",
    "\n",
    "which completes the proof\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [py35]",
   "language": "python",
   "name": "Python [py35]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
