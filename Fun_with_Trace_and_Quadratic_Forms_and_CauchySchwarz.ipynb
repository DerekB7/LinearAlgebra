{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Proof of Cauchy Schwarz\n",
    "\n",
    "For simplicity the field will be reals, though the results can be easily extended to complex numbers.  \n",
    "\n",
    "consider some vector $\\mathbf x =  \\begin{bmatrix}\n",
    "x_1\\\\ \n",
    "x_2\\\\ \n",
    "\\vdots \\\\ \n",
    "x_n\n",
    "\\end{bmatrix}$\n",
    "\n",
    "and some other vector,  $\\mathbf y =  \\begin{bmatrix}\n",
    "y_1\\\\ \n",
    "y_2\\\\ \n",
    "\\vdots \\\\ \n",
    "y_n\n",
    "\\end{bmatrix}$\n",
    "\n",
    "now consider the outer product given by \n",
    "\n",
    "$\\mathbf {xy}^T =  \\begin{bmatrix}\n",
    "x_1 y_1 & x_1 y_2 &...& x_1 y_n\\\\ \n",
    "x_2 y_1 & x_2 y_2 & ... & x_2 y_n \\\\ \n",
    "\\vdots & \\vdots & \\ddots &\\vdots \\\\ \n",
    "x_n y_1 & x_n y_2 &.... & x_n y_n\n",
    "\\end{bmatrix}$\n",
    "\n",
    "\n",
    "From here, consider where we want the squared Frobenius norm of $\\mathbf {xy}^T$ \n",
    "\n",
    "$\\big\\vert\\big\\vert \\mathbf {xy}^T\\big\\vert\\big\\vert_{F}^2 = trace\\Big(\\big(\\mathbf {xy}^T\\big)^T \\big( \\mathbf {xy}^T\\big)\\Big) = trace\\Big( \\mathbf {yx}^T \\mathbf {xy}^T\\Big) = trace\\Big( \\mathbf{y}^T \\mathbf{yx}^T \\mathbf {x}\\Big) = \\mathbf {y}^T\\mathbf y \\mathbf x^T \\mathbf {x}  = \\big\\vert\\big\\vert \\mathbf{y}\\big\\vert\\big\\vert_{2}^2\\big\\vert\\big\\vert \\mathbf {x}\\big\\vert\\big\\vert_{2}^2$\n",
    "\n",
    "where the middle equalities made use of the cyclic property of the trace\n",
    "\n",
    "We conclude the proof with the following:\n",
    "\n",
    "$ \\big\\vert\\big\\vert \\mathbf{y}\\big\\vert\\big\\vert_{2}^2\\big\\vert\\big\\vert \\mathbf {x}\\big\\vert\\big\\vert_{2}^2 = trace\\Big(\\big(\\mathbf {\\mathbf {xy}}^T\\big)^T \\big( \\mathbf {\\mathbf {xy}}^T\\big)\\Big) \n",
    "\\geq trace\\Big( \\big(\\mathbf {xy}^T\\big) \\big(\\mathbf {xy}^T\\big)\\Big) =  trace\\Big( \\mathbf y^T \\mathbf {xy}^T\\mathbf {x}\\Big) = trace\\Big( \\big(\\mathbf y^T \\mathbf {x}\\big) \\big(\\mathbf y^T \\mathbf {x}\\big)\\Big) = \\big(\\mathbf y^T \\mathbf x\\big)^2$\n",
    "\n",
    "hence \n",
    "\n",
    "$\\big\\vert\\big\\vert \\mathbf{y}\\big\\vert\\big\\vert_{2}^2\\big\\vert\\big\\vert \\mathbf {x}\\big\\vert\\big\\vert_{2}^2 = \\Big(\\Sigma_{i=1}^{n}y_i^2\\Big) \\Big(\\Sigma_{i=1}^{n}x_i^2\\Big)  \\geq \\Big(\\Sigma_{i=1}^{n}x_iy_i\\Big)^2= \\big(\\mathbf y^T \\mathbf x\\big)^2 $\n",
    "\n",
    "\n",
    "noting that \n",
    "\n",
    "$trace\\Big(\\big(\\mathbf {\\mathbf {xy}}^T\\big)^T \\big( \\mathbf {\\mathbf {xy}}^T\\big)\\Big)\n",
    "\\geq trace\\Big( \\big(\\mathbf {xy}^T\\big) \\big(\\mathbf {xy}^T\\big)\\Big) $\n",
    "\n",
    "via the Schur Inequality.  (Also see the cell below -- we can ignore Schur Inequality and directly interpret this in terms of maximal eigenvalues.) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alternative Approach that does not use Schur Inequality:\n",
    "\n",
    "note that instead of using the Schur Inequality (which strictly speaking should have had an absolute value / magnitude included in there),  consider the maximum eigenvalue problem, where we have a rank one matrix, $\\mathbf B$, this time over a complex numbers field\n",
    "\n",
    "\n",
    "$\\mathbf B = \\mathbf{xy}^H$.  While we know that $\\mathbf B$ is a rank one matrix, the argument to be made is even more general: the magnitude of the largest eigenvalue of $\\big(\\mathbf {BB}\\big)$ is $\\leq$ the largest eigenvalue of $\\mathbf B^H \\mathbf B$, or equivalently, the magnitude of the largest eigenvalue of $\\mathbf B$ ($\\lambda_1$) is $\\leq$ the largest singular value of $\\mathbf B$ ($\\sigma_1$).\n",
    "\n",
    "The approach taken here uses quadratic forms.  So consider the case of maximizing $\\mathbf B^H \\mathbf B$ vs $\\mathbf {BB}$\n",
    "\n",
    "max $\\big \\vert \\mathbf v^H \\mathbf B^H \\mathbf B \\mathbf v \\big \\vert \\geq$ max $\\big \\vert \\mathbf v^H \\mathbf{BB}\\mathbf v \\big \\vert$\n",
    "\n",
    "where we constrain the length (2 norm) of $\\mathbf v$, which for simplicity will be one: $\\mathbf v^H \\mathbf v = 1 = \\big \\vert \\big \\vert \\mathbf v \\big \\vert \\big \\vert_2^{2} = \\big \\vert \\big \\vert \\mathbf v \\big \\vert \\big \\vert_2$ \n",
    "\n",
    "We know via diagonalization arguments (and Lagrange Multipliers), that some quadratic form $\\big \\vert \\mathbf v^H \\mathbf C \\mathbf v\\big \\vert$ subject to $\\mathbf v ^H \\mathbf v = 1$ is maximized when all of $\\mathbf v$ is allocated to the eigenvalue(s) with the largest magnitude of the Hermitian matrix $\\mathbf C$.  Unfortunately, we have no reason to believe $\\mathbf {BB}$ is Hermitian or even non-defective, which complicates things a bit.  But consider having well ordered eigenvalues for $\\mathbf B$ where $\\big \\vert \\lambda_1 \\big \\vert \\geq \\big \\vert\\lambda_2 \\big \\vert\\geq \\big \\vert \\lambda_3 \\big \\vert \\geq ... \\geq \\big \\vert\\lambda_n \\big \\vert$, with associated eigenvectors $\\mathbf v_1, \\mathbf v_2, \\mathbf v_3, ... , \\mathbf v_n$.  Note that there is a simple argument which tells us that allocating to eigenvector $\\mathbf v_k$ where $k \\geq 2$ is (weakly) dominated by $\\mathbf v_1$.  \n",
    "\n",
    "$\\big \\vert \\mathbf v_1^H \\mathbf{BB}\\mathbf v_1 \\big \\vert \\geq \\big \\vert \\mathbf v_k^H \\mathbf{BB}\\mathbf v_k \\big \\vert$\n",
    "\n",
    "$\\big \\vert \\mathbf v_1^H \\mathbf{B}\\big(\\mathbf B \\mathbf v_1\\big) \\big \\vert \\geq \\big \\vert \\mathbf v_k^H \\mathbf{B}\\big( \\mathbf B \\mathbf v_k \\big) \\big \\vert$\n",
    "\n",
    "$\\big \\vert \\mathbf v_1^H \\mathbf{B} \\lambda_1 \\mathbf v_1 \\big \\vert \\geq \\big \\vert\\mathbf v_k^H \\mathbf{B}\\lambda_k \\mathbf v_k \\big \\vert$\n",
    "\n",
    "$\\big \\vert \\lambda_1 \\mathbf v_1^H \\big(\\mathbf{B} \\mathbf v_1\\big) \\big \\vert \\geq \\big \\vert \\lambda_k \\mathbf v_k^H \\big(\\mathbf{B} \\mathbf v_k\\big) \\big \\vert$\n",
    "\n",
    "$\\big \\vert \\lambda_1 \\mathbf v_1^H \\lambda_1 \\mathbf v_1 \\big \\vert \\geq \\big \\vert \\lambda_k \\mathbf v_k^H \\lambda_k \\mathbf v_k \\big \\vert$\n",
    "\n",
    "$\\big \\vert \\lambda_1^2 (\\mathbf v_1^H \\mathbf v_1) \\big \\vert \\geq \\big \\vert \\lambda_k^2 (\\mathbf v_k^H \\mathbf v_k) \\big \\vert$\n",
    "\n",
    "$\\big \\vert \\lambda_1^2 *1\\big \\vert \\geq \\big \\vert \\lambda_k^2 *1 \\big \\vert$  \n",
    "$\\big \\vert \\lambda_1^2\\big \\vert \\geq \\big \\vert\\lambda_k^2 \\big \\vert$  \n",
    "$\\big \\vert \\lambda_1\\big \\vert^2 \\geq \\big \\vert\\lambda_k \\big \\vert^2$  \n",
    "$\\big \\vert \\lambda_1\\big \\vert \\geq \\big \\vert\\lambda_k \\big \\vert$   \n",
    "\n",
    "hence we have a simple exchange argument that tells us any time we allocate to $\\mathbf v_k$ we can get a result greater than or equal to it, by allocating that amount instead to $\\mathbf v_1$.  Thus we can do no better than allocating everything in $\\mathbf v$ to the eigenpair $\\lambda_1, \\mathbf v_1$\n",
    "\n",
    "We return to our original equation, with respect to eigenvalues:\n",
    "\n",
    "max $\\big \\vert  \\mathbf v^H \\mathbf B^H \\mathbf B \\mathbf v \\big \\vert \\geq$ max $\\big \\vert \\mathbf v_1^H \\mathbf{BB}\\mathbf v_1 \\big \\vert$\n",
    "\n",
    "max $\\big \\vert \\mathbf v^H \\mathbf B^H \\mathbf B \\mathbf v \\big \\vert = $ max $ \\mathbf v^H \\mathbf B^H \\mathbf B \\mathbf v  \\geq \\big \\vert \\lambda_1^2\\big \\vert $\n",
    "\n",
    "note that we always have the option / backup plan, on the left hand side, of also allocating to $\\mathbf v_1$.  Put differently, if we are lazy, we know that by setting $\\mathbf v := \\mathbf v_1$ we'll always get a 'payoff' with magnitude equal to $\\big\\vert\\lambda_1^2\\big\\vert$ -- thus when maximizing the magnitude of $\\big \\vert \\mathbf v^H \\mathbf B^H \\mathbf B \\mathbf v \\big \\vert$ we'll get a result at least as good as $\\big\\vert\\lambda_1^2\\big\\vert$.  Symbolically, this is shown below.\n",
    "\n",
    "$\\mathbf v_1^H \\mathbf B^H \\mathbf B \\mathbf v_1 = \\big \\vert \\lambda_1^2\\big \\vert $  \n",
    "\n",
    "$\\big(\\mathbf v_1^H \\mathbf B^H\\big) \\big(\\mathbf B \\mathbf v_1 \\big) = \\big \\vert \\lambda_1^2\\big \\vert $  \n",
    "\n",
    "$\\big(\\mathbf {B}\\mathbf v_1\\big)^H \\big(\\mathbf B \\mathbf v_1 \\big) = \\big \\vert \\lambda_1^2 \\big \\vert $  \n",
    "$\\big(\\lambda_1 \\mathbf v_1\\big)^H \\big(\\lambda_1 \\mathbf v_1 \\big) = \\big \\vert \\lambda_1^2 \\big \\vert $  \n",
    "$\\lambda_1^H \\lambda_1 \\big(\\mathbf v_1^H \\mathbf v_1\\big) = \\big \\vert \\lambda_1^2 \\big \\vert $  \n",
    "$\\lambda_1^H \\lambda_1 *1  = \\big \\vert \\lambda_1^2\\big \\vert $    \n",
    "$\\lambda_1^H \\lambda_1  = \\big \\vert \\lambda_1^2 \\big \\vert $  \n",
    "\n",
    "Where the final statement is true by the math of complex numbers. A quick detour justifying this is shown below.\n",
    "\n",
    "- - - - \n",
    "To justify this, consider that we can simply note that $\\big \\vert \\lambda_1^2\\big \\vert = \\big \\vert \\lambda_1 \\big \\vert^2 = \\lambda_1^H \\lambda_1$.\n",
    "\n",
    "Alternatively, for a more granular view, consider the case where:  \n",
    "$\\lambda_1 = \\alpha - \\beta i $, where $\\alpha$ and $\\beta$ are real valued scalars. Accordingly, the magnitude of $\\lambda_1$ is $\\big\\vert \\lambda_1\\big\\vert = \\big(\\alpha^2 + \\beta^2\\big)^\\frac{1}{2}$. Then $\\lambda_1^2 = \\alpha^2 + \\beta^2  i^2 - 2\\alpha\\beta i = \\alpha^2 - \\beta^2 - 2\\alpha\\beta i$, with magnitude of \n",
    "\n",
    "$\\big \\vert \\lambda_1^2\\big \\vert = \\Big(\\big(\\alpha^2 - \\beta^2\\big)^2 + \\big( - 2\\alpha\\beta\\big)^2\\Big)^{\\frac{1}{2}}= \\Big(\\alpha^4 + \\beta^4 - 2 \\alpha^2 \\beta^2 + 4 \\alpha^2\\beta^2)\\Big)^{\\frac{1}{2}} $ \n",
    "\n",
    "$\\big \\vert \\lambda_1^2\\big \\vert = \\Big(\\alpha^4 + \\beta^4 + 2 \\alpha^2 \\beta^2 \\Big)^{\\frac{1}{2}}$\n",
    "\n",
    "and note that $\\lambda_1 ^H \\lambda_1 = \\alpha^2 + \\beta^2$, with magnitude equal to   \n",
    "\n",
    "$\\big \\vert \\lambda_1^H \\lambda_1 \\big \\vert = \\Big(\\big(\\alpha^2 + \\beta^2\\big)^2 + \\big(0\\big)^2 \\Big)^{\\frac{1}{2}} = \\Big(\\alpha^4 + \\beta^4 + 2 \\alpha \\beta \\Big)^{\\frac{1}{2}} = \\big \\vert \\lambda_1^2\\big \\vert  $\n",
    "\n",
    "$ \\big \\vert \\lambda_1^H \\lambda_1 \\big \\vert = \\lambda_1 ^H \\lambda_1 = \\alpha^2 + \\beta^2  = \\Big(\\big(\\alpha^2 + \\beta^2\\big)^\\frac{1}{2}\\Big)^2 = \\big\\vert \\lambda_1\\big\\vert ^2$\n",
    "- - - - \n",
    "\n",
    "Thus when trying to maximize the magnitude of $\\mathbf v^H \\mathbf B^H \\mathbf B \\mathbf v$, we have a lower bound equal to $ \\big \\vert \\lambda_1^2 \\big\\vert$.  Equivalently, we can say: $\\sigma_1^2 \\geq \\big \\vert \\lambda_1^2 \\big\\vert$ and $\\sigma_1 \\geq \\big \\vert \\lambda_1 \\big\\vert$, where $\\sigma_1$ is the largest singular value of $\\mathbf B$ and thus $\\sigma_1^2$ is the largest eigenvalue of $\\mathbf B^H \\mathbf B$\n",
    "\n",
    "For a simple example of this fact, consider:\n",
    "\n",
    "$\\mathbf B = \\left[\\begin{matrix}2 & 3 & 4\\\\4 & 10 & -1\\\\1 & 3 & 4\\end{matrix}\\right]$\n",
    "\n",
    "where $\\lambda_1 \\approx 11.57$, but $\\sigma_1 \\approx 11.87$, hence $\\sigma_1$ exceeds the lower bound set by $\\lambda_1$ \n",
    "- - - -\n",
    "Back to the original problem at hand, in the special case where $\\mathbf B$ is a square, rank one matrix  \n",
    "\n",
    "$trace \\big(\\mathbf B ^H \\mathbf B\\big) \\geq \\big \\vert trace\\big(\\mathbf{BB}\\big) \\big \\vert$, because  \n",
    "\n",
    "$trace \\big(\\mathbf B ^H \\mathbf B\\big) = \\sigma_1^2$ and $trace\\big(\\mathbf{BB}\\big) = \\lambda_1^2 $, and we know $\\sigma_1^2 \\geq \\big \\vert \\lambda_1^2 \\big\\vert$\n",
    "\n",
    "set: $\\mathbf B:= \\mathbf{xy}^H$ and Cauchy Schwartz simply follows\n",
    "\n",
    "$ \\big\\vert\\big\\vert \\mathbf{y}\\big\\vert\\big\\vert_{2}^2\\big\\vert\\big\\vert \\mathbf {x}\\big\\vert\\big\\vert_{2}^2 = \\big ( \\mathbf y^H \\mathbf y \\big ) \\big ( \\mathbf x^H \\mathbf x \\big ) = trace\\Big(\\big(\\mathbf {\\mathbf {xy}}^H\\big)^H \\big( \\mathbf {\\mathbf {xy}}^H\\big)\\Big) \n",
    "\\geq \\big \\vert trace\\Big( \\big(\\mathbf {xy}^H\\big) \\big(\\mathbf {xy}^H\\big)\\Big) \\big \\vert =  \\big \\vert trace\\Big( \\mathbf y^H \\mathbf {xy}^H\\mathbf {x}\\Big) \\big \\vert = \\big \\vert \\mathbf y^H \\mathbf x \\big \\vert^2 $\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Smallest Eigenvalues and Singular Values\n",
    "\n",
    "Note that the above analysis can be easily extended with respect to the magnitude of the smallest eigenvalue of $\\mathbf B$ and the smallest singular value of $\\mathbf B$, again where $\\mathbf B \\in \\mathbb C^{n x n}$.\n",
    "\n",
    "for $k \\lt n$\n",
    "\n",
    "(i.e. $\\big \\vert \\lambda_k \\big \\vert \\geq \\big \\vert \\lambda_n \\big \\vert$)\n",
    "\n",
    "thus if we want to minimize $\\big(\\mathbf v_1^H \\mathbf B^H\\big) \\big(\\mathbf B \\mathbf v_1 \\big) $ we always have the option of allocating to $\\lambda_n$\n",
    "\n",
    "$\\mathbf v_n^H \\mathbf B^H \\mathbf B \\mathbf v_n = \\big \\vert \\lambda_n^2\\big \\vert $  \n",
    "\n",
    "$\\big(\\mathbf v_n^H \\mathbf B^H\\big) \\big(\\mathbf B \\mathbf v_n \\big) = \\big \\vert \\lambda_n^2\\big \\vert $  \n",
    "\n",
    "$\\big(\\mathbf {B}\\mathbf v_n\\big)^H \\big(\\mathbf B \\mathbf v_n \\big) = \\big \\vert \\lambda_n^2 \\big \\vert $  \n",
    "$\\big(\\lambda_n \\mathbf v_n\\big)^H \\big(\\lambda_n \\mathbf v_1 \\big) = \\big \\vert \\lambda_n^2 \\big \\vert $  \n",
    "$\\lambda_n^H \\lambda_n \\big(\\mathbf v_1^H \\mathbf v_1\\big) = \\big \\vert \\lambda_n^2 \\big \\vert $  \n",
    "$\\lambda_n^H \\lambda_n *1  = \\big \\vert \\lambda_n^2\\big \\vert $    \n",
    "$\\lambda_n^H \\lambda_n  = \\big \\vert \\lambda_n \\big \\vert^2 $  \n",
    "\n",
    "Since allocating everything to $\\sigma_n^2$ is the (weakly) dominant solution for minimizing $\\mathbf v^H \\mathbf B^H \\mathbf B \\mathbf v$, we can upper bound $\\sigma_n^2 \\leq \\big\\vert \\lambda_n\\big \\vert^2 $ and equivalently say that $\\sigma_n \\leq \\big\\vert \\lambda_n\\big \\vert $\n",
    "\n",
    "\n",
    "# Quadratic Forms and recovering a Frobenius Norm\n",
    "\n",
    "as usual, assume we have well ordered singular values\n",
    "\n",
    "$\\sigma_1 \\geq \\sigma_2 \\geq .... \\geq \\sigma_n \\geq 0$\n",
    "\n",
    "It is worth remarking that if we were to do our optimization problem\n",
    "\n",
    "max $\\mathbf v_1^H \\mathbf B^H \\mathbf B \\mathbf v_1$\n",
    "\n",
    "we recover $\\sigma_1^2$.  Then if we continue doing this optimization problem for $k = \\{2, 3, 4, ... , n\\}$\n",
    "\n",
    "max $\\mathbf v_k^H \\mathbf B^H \\mathbf B \\mathbf v_k$\n",
    "\n",
    "where each $\\big \\Vert \\mathbf v_k\\big \\Vert_2^2 = 1$, **with the added constraint that** $\\mathbf v_k \\perp \\mathbf v_j$, for $j = \\{1, 2, ... , k-1\\}$\n",
    "\n",
    "i.e. each $\\mathbf v_k$ is mutually orthonormal to the $\\mathbf v$'s that come before it,\n",
    "\n",
    "Then we recover \n",
    "\n",
    "$\\mathbf V = \n",
    "\\bigg[\\begin{array}{c|c|c|c}\n",
    "\\mathbf v_1 & \\mathbf v_2 &\\cdots & \\mathbf v_{n}\n",
    "\\end{array}\\bigg]\n",
    "$   \n",
    "\n",
    "where $\\mathbf V$ is a unitary matrix (or in reals, orthogonal).  Put differently, we recover a coordinate system.  \n",
    "\n",
    "And more to the point, we also can collect the 'payoffs': $\\{ \\sigma_1^2, \\sigma_2^2, \\sigma_3^2, ..., \\sigma_n^2 \\}$. from this maximization process.  We could sum up all of these squared singular values, and get\n",
    "\n",
    "$\\sigma_1^2 + \\sigma_2^2+ \\sigma_3^2+ ...+\\sigma_n^2 = \\big \\Vert \\mathbf B \\big \\Vert_F^2$\n",
    "\n",
    "i.e. when we sum up all of these 'payoffs' from our complete quadratic form process, we get the squared Frobenius norm for our matrix $\\mathbf B$. \n",
    "\n",
    "\n",
    "# On Unitary Matrices\n",
    "Note that there is a special case of interest.  Suppose that we have a square unitary matrix $\\mathbf Q$. We know that all eigenvalues of $\\mathbf Q $ have magnitude of 1.  Why? There are multiple approaches, but an elegant one uses the above knowledge with the singular value decomposition:\n",
    "\n",
    "\n",
    "$\\mathbf Q = \\mathbf{U \\Sigma V}^H$\n",
    "\n",
    "$\\mathbf Q^H \\mathbf Q  = \\mathbf I = \\big(\\mathbf{U \\Sigma V}^H\\big)^H \\mathbf{U \\Sigma V}^H  =\\mathbf V \\mathbf\\Sigma^2  \\mathbf V^H$\n",
    "\n",
    "left multiply by $\\mathbf V^H$ and right multiply by $\\mathbf V$, recalling\n",
    "that $\\mathbf V$ is a square, full rank matrix\n",
    "\n",
    "$\\mathbf V^H \\mathbf I \\mathbf V = \\mathbf I = \\mathbf V^H \\big(\\mathbf V \\mathbf\\Sigma^2  \\mathbf V^H\\big) \\mathbf V =\\mathbf \\Sigma^2$\n",
    "\n",
    "$\\mathbf I = \\mathbf \\Sigma^2$\n",
    "\n",
    "recalling that each singular value, by construction, is real and non-negative, we can then determine:\n",
    "\n",
    "$\\mathbf I = \\mathbf \\Sigma$  \n",
    "\n",
    "Thus we know that $\\mathbf \\Sigma$ is itself the Identity matrix (i.e. $\\sigma_1 = \\sigma_2 = ... = \\sigma_n = 1$)\n",
    "\n",
    "When we consider the eigenvalues of $\\mathbf Q$, where $\\big \\vert \\lambda_1 \\big \\vert \\geq \\big \\vert\\lambda_2 \\big \\vert\\geq \\big \\vert \\lambda_3 \\big \\vert \\geq ... \\geq \\big \\vert\\lambda_n \\big \\vert$, we know that $1 = \\sigma_1 \\geq \\big \\vert \\lambda_1 \\big \\vert$ and we know that $\\big \\vert \\lambda_n \\big \\vert \\geq \\sigma_n = 1$.  Every item in our sequence of eigenvalue magnitudes is bounded above and below by one.  Thus all eigenvalues of a unitary (or in Reals, othogonal) matrix must have magnitue equal to one.\n",
    "\n",
    "$1 = \\sigma_1 \\geq \\big \\vert \\lambda_1 \\big \\vert \\geq \\big \\vert\\lambda_2 \\big \\vert\\geq \\big \\vert \\lambda_3 \\big \\vert \\geq ... \\geq \\big \\vert\\lambda_n \\big \\vert \\geq \\sigma_n = 1$\n",
    "\n",
    "can be re-written as\n",
    "\n",
    "$1 = \\sigma_1 = \\big \\vert \\lambda_1 \\big \\vert = \\big \\vert\\lambda_2 \\big \\vert = \\big \\vert \\lambda_3 \\big \\vert = ... = \\big \\vert\\lambda_n \\big \\vert =\\sigma_n = 1$\n",
    "\n",
    "\n",
    "- - - - \n",
    "\n",
    "# On Nilpotent Matrices\n",
    "\n",
    "The final sequences of inequalities for some arbitary square matrix:\n",
    "\n",
    "$\\sigma_1 \\geq \\big \\vert \\lambda_1 \\big \\vert \\geq \\big \\vert\\lambda_2 \\big \\vert\\geq \\big \\vert \\lambda_3 \\big \\vert \\geq ... \\geq \\big \\vert\\lambda_n \\big \\vert \\geq \\sigma_n $\n",
    "\n",
    "is quite useful.  \n",
    "\n",
    "A nilpotent matrix $\\mathbf A$ is some $n$ x $n$ matrix where are after finite number of iterations, it becomes the zero matrix.  Thus $\\mathbf A^r = \\mathbf 0$ for some finite, natural number $r$.  (We can tighten the bound and say $r \\leq n$, but this is not really needed here.) \n",
    "\n",
    "**Claim:** a nilpotent matrix has all eigenvalues equal to zero.\n",
    "\n",
    "There are numerous ways to prove this.  The most slick uses the analysis earlier in this post and does the following:\n",
    "\n",
    "**Proof: **  \n",
    "$\\mathbf A$ has eigenvalues of $\\big \\vert \\lambda_1 \\big \\vert \\geq \\big \\vert\\lambda_2 \\big \\vert\\geq \\big \\vert \\lambda_3 \\big \\vert \\geq ... \\geq \\big \\vert\\lambda_n \\big \\vert $\n",
    "\n",
    "and \n",
    "\n",
    "$\\big(\\mathbf A^r\\big)$ has eigenvalues of $\\big \\vert \\lambda_1^r \\big \\vert \\geq \\big \\vert\\lambda_2^r \\big \\vert\\geq \\big \\vert \\lambda_3^r \\big \\vert \\geq ... \\geq \\big \\vert\\lambda_n^r \\big \\vert$\n",
    "\n",
    "or equivalently \n",
    "\n",
    "$\\big(\\mathbf A^r\\big)$ has eigenvalues of $\\big \\vert \\lambda_1 \\big \\vert^r \\geq \\big \\vert\\lambda_2 \\big \\vert^r \\geq \\big \\vert \\lambda_3 \\big \\vert^r \\geq ... \\geq \\big \\vert\\lambda_n \\big \\vert^r $\n",
    "\n",
    "We can do SVD on $\\big(\\mathbf A^r\\big)$ and see  \n",
    "$\\big(\\mathbf A^r\\big) = \\mathbf U \\mathbf \\Sigma \\mathbf V^H = \\mathbf 0$, where $\\mathbf U$ and $\\mathbf V$ are full rank unitary matrices (or orthogonal if dealing with Reals)  \n",
    "\n",
    "Hence:\n",
    "\n",
    "$ \\mathbf \\Sigma = \\mathbf U^H\\big(\\mathbf A^r\\big)\\mathbf V = \\mathbf U^H \\big(\\mathbf 0\\big)\\mathbf V = \\mathbf 0 $\n",
    "\n",
    "That is, all singular values of $\\big(\\mathbf A^r\\big)$  are equal to zero \n",
    "\n",
    "Thus we know that for $ \\big(\\mathbf A^r\\big)$  \n",
    "\n",
    "$0 = \\sigma_1 \\geq \\big \\vert \\lambda_1^r \\big \\vert \\geq \\big \\vert\\lambda_2^r \\big \\vert\\geq \\big \\vert \\lambda_3^r \\big \\vert \\geq ... \\geq \\big \\vert\\lambda_n^r \\big \\vert \\geq \\sigma_n = 0$\n",
    "\n",
    "Since all eigenvalue magnitudes are bounded above and below by zero, we restate this as  \n",
    "$0 = \\lambda_1^r  = \\lambda_2^r = \\lambda_3^r = ... = \\lambda_n^r  = 0$\n",
    "\n",
    "take the $r$th root and we see that all eigenvalues of the nilpotent matrix $\\mathbf A$ must be zero\n",
    "\n",
    "$0 = \\lambda_1  = \\lambda_2 =  \\lambda_3 = ... = \\lambda_n  = 0$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix Norms \n",
    "\n",
    "consider two n x n matrices $\\mathbf A$ and $\\mathbf B$.  \n",
    "\n",
    "now consider $\\big \\Vert \\mathbf {AB} \\big \\Vert_2 = \\sigma_1$, i.e. the operator norm for $\\big(\\mathbf {AB}\\big)$.  \n",
    "\n",
    "As always we order singular values as $\\sigma_1 \\geq \\sigma_2 \\geq ... \\geq \\sigma_n \\geq 0$.  And again we consider any vectors $\\mathbf x$, $\\mathbf y$, $\\mathbf z$ with the constraint that $\\big \\Vert \\mathbf x\\big \\Vert_2^2 = 1$, $\\big \\Vert \\mathbf y\\big \\Vert_2^2 = 1$, and $\\big \\Vert \\mathbf z\\big \\Vert_2^2 = 1$.\n",
    "\n",
    "The operator norm of $\\big(\\mathbf {AB}\\big)$ can be considered as a quadratic form whereby we look for \n",
    "\n",
    "\n",
    "max $\\mathbf z^H \\big(\\mathbf B^H \\mathbf A^H \\mathbf{AB}\\big) \\mathbf z$\n",
    "\n",
    "we claim that this is upperbounded by maximizing $\\big(\\mathbf x^H \\mathbf A^H\\mathbf A \\mathbf x\\big)\\big(\\mathbf y^H \\mathbf B^H \\mathbf B \\mathbf y\\big)$ which is equivalent to $\\sigma_{1,A}^2 \\sigma_{1,B}^2$\n",
    "\n",
    "We can see this by assuming $\\mathbf {Bz} =\\mathbf x$.  I.e. if $\\mathbf {Bz}$ is equal to our optimal $\\mathbf x$, then we have  $\\big(\\mathbf z^H \\mathbf B^H\\big) \\mathbf A^H \\mathbf{A}\\big( \\mathbf B \\mathbf z \\big)= \\mathbf x^H \\mathbf A^H \\mathbf A \\mathbf x $ and hence we've optimized the internal part of that equation.  But in fact $\\mathbf {Bz} = \\alpha \\mathbf x$.  I.e. $\\mathbf {Bz}$ actually gives us a scaled version of $\\mathbf x$.   So we now consider the fact that we want to maximize $\\big \\Vert \\mathbf {Bz} \\big \\Vert_2$  i.e. maximize the $\\alpha$ in $\\alpha \\mathbf x$.  This is equivalent to maximizing $\\big(\\mathbf y^H \\mathbf B^H \\mathbf B \\mathbf y\\big)$, and we know that this is given by $\\sigma_{B,1}^2$.  Hence maximizing $\\mathbf z^H \\big(\\mathbf B^H \\mathbf A^H \\mathbf{AB}\\big) \\mathbf z$ is upper bounded by maximizing the inside, which returns $\\sigma_{A,1}^2$ and scaling that by a maximal $\\alpha$ which is given by $\\sigma_{B,1}^2$ \n",
    "\n",
    "Thus $\\big \\Vert \\mathbf{AB} \\big \\Vert_2^2 \\leq \\sigma_{A,1}^2\\sigma_{B,1}^2 = \\big \\Vert \\mathbf{A} \\big \\Vert_2^2 \\big \\Vert \\mathbf{B} \\big \\Vert_2^2 $\n",
    "\n",
    "\n",
    "Now if we wanted to maximize $\\big \\Vert \\mathbf{AB} \\big \\Vert_2^2 $ with the constraint that the solution is orthogonal to the (right) singular vectors associated with $\\sigma_{A,1}$, we could upper bound this with $\\sigma_{A,2}^2 \\sigma_{B,1}^2$, and if we wanted to do a maximization that was orthogonal to the (right) singular vectors associated with  $\\{\\big(\\sigma_{A,2}, \\sigma_{B,1}\\big), \\big(\\sigma_{A,1}, \\sigma_{B,1}\\big)\\}$ and we could upper bound that by $\\sigma_{A,3}^2 \\sigma_{B,1}^2$, and so on.  \n",
    "\n",
    "If we were to add all of these upper bounds up, what we'd get is \n",
    "\n",
    "$\\sigma_{B,1}^2 \\big(\\sigma_{A,1}^2 + \\sigma_{A,2}^2 + ... + \\sigma_{A,n}^2\\big) = \\sigma_{B,1}^2 trace\\big(\\mathbf A^H \\mathbf A\\big) = \\sigma_{B,1}^2 \\big \\Vert \\mathbf A \\big \\Vert_F^2 = \\big \\Vert \\mathbf B\\big \\Vert_2^2\\big \\Vert \\mathbf A \\big \\Vert_F^2$\n",
    "\n",
    "This process is merely an extension of a preceding section titled \"Quadratic Forms and recovering a Frobenius Norm\"... and we note that if we did this for all $\\mathbf z_k$, we'd recovered a coordinate system $\\mathbf Z$.  If we added up all of the associated 'payoff's we'd recover  $\\big \\Vert \\mathbf{AB} \\big \\Vert_F^2$.  By the above, we have upper bounds for all of those 'payoffs'. \n",
    "\n",
    "Thus we can say \n",
    "\n",
    "$\\big \\Vert \\mathbf{AB} \\big \\Vert_F^2 \\leq \\big \\Vert \\mathbf B\\big \\Vert_2^2 \\big \\Vert\\mathbf A \\big \\Vert_F^2$\n",
    "\n",
    "of course we can recall that \n",
    "\n",
    "$\\big \\Vert \\mathbf B\\big \\Vert_F^2 = \\big \\Vert \\mathbf B\\big \\Vert_2^2 + \\sigma_{B,2}^2 + \\sigma_{B,3}^2 + .... + \\sigma_{B,n}^2 = \\sigma_{B,1}^2+ \\sigma_{B,2}^2 + \\sigma_{B,3}^2 + .... + \\sigma_{B,n}^2 $\n",
    "\n",
    "recalling that each singular value $\\sigma_{B, k}$ is real and non-negative.  Thus if we wanted to loosen up the above bound, we could say\n",
    "\n",
    "$\\big \\Vert \\mathbf{AB} \\big \\Vert_F^2 \\leq \\big \\Vert \\mathbf B\\big \\Vert_F^2\\big \\Vert \\mathbf A \\big \\Vert_F^2$\n",
    "\n",
    "or taking the square root of both sides \n",
    "\n",
    "$\\big \\Vert \\mathbf{AB} \\big \\Vert_F \\leq \\big \\Vert \\mathbf B\\big \\Vert_F \\big \\Vert \\mathbf A \\big \\Vert_F$\n",
    "\n",
    "This should jump out at us as being a variant of Cauchy-Schwarz which tells us that \n",
    "\n",
    "$\\big \\vert\\mathbf a^H \\mathbf b\\big \\vert^2 \\leq \\big \\Vert \\mathbf a \\big \\Vert_2^2 \\big \\Vert \\mathbf b\\big \\Vert_2^2$\n",
    "\n",
    "\n",
    "\n",
    "*note: some of the above is proved in a different, more satisfying way, below, under \"Hermitian Positive Semi Definite Trace Inequalities\"*  \n",
    "\n",
    "E.g. suppose $\\mathbf X^H \\mathbf X = \\mathbf A$ and $\\mathbf Y \\mathbf Y^H = \\mathbf B$,  i.e. both $\\mathbf B$ and $\\mathbf A$ are Hermitian positive semi definite.  Using the above proof with respect to squared Frobenius norms, we see\n",
    "\n",
    "$\\big \\Vert \\mathbf{XY} \\big \\Vert_F^2 \\leq \\big \\Vert \\mathbf X \\big \\Vert_F^2\\big \\Vert \\mathbf Y \\big \\Vert_F^2$\n",
    "\n",
    "but we could also say\n",
    "\n",
    "$\\big \\Vert \\mathbf{XY} \\big \\Vert_F^2 = trace\\big(\\mathbf Y^H \\mathbf X^H \\mathbf X \\mathbf Y \\big) =  trace\\big(\\mathbf Y \\mathbf Y^H \\mathbf X^H \\mathbf X \\big) = trace\\big(\\mathbf {BA}\\big)= trace\\big(\\mathbf A \\mathbf B\\big)$\n",
    "\n",
    "then apply the below inequality that if $\\mathbf A$ and $\\mathbf B$ are both Hermitian positive semidefinite, \n",
    "$trace\\big(\\mathbf A \\mathbf B\\big) \\leq trace\\big(\\mathbf A\\big) trace\\big( \\mathbf B\\big)$\n",
    "\n",
    "$\\big \\Vert \\mathbf{XY} \\big \\Vert_F^2 \\leq trace\\big(\\mathbf A\\big) trace\\big( \\mathbf B\\big) = \\big \\Vert \\mathbf{X} \\big \\Vert_F^2\\big \\Vert \\mathbf{Y} \\big \\Vert_F^2$\n",
    "\n",
    "something similar is shown with respect to operator norms as well.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**extension: Markov Chains must have linearly independent eigenvalues associated with eigenvalues of magnitude 1**\n",
    "\n",
    "The argument is a proof by contradiction using Jordan Forms.  In essence, assume the above statement is not true, and we find it violates the above matrix norms.  This is an extension modified answer for problem 4.15 In Stochastic Processes by Gallagher (problem 3.15 in \"Discrete Stochastic Processes\" in MIT OCW.\n",
    "\n",
    "start by showin a 3 x 3 Jordan Block of the form\n",
    "\n",
    "$\\mathbf J_i = \\begin{bmatrix}\n",
    "\\lambda_i & 1 & 0 \\\\ \n",
    "0 & \\lambda_i & 1\\\\ \n",
    "0 & 0 & \\lambda_i \n",
    "\\end{bmatrix}$\n",
    "\n",
    "then \n",
    "\n",
    "$\\mathbf J_i^n = \\begin{bmatrix}\n",
    "\\lambda_i^n & n \\lambda_i^{n-1} & \\binom{n}{2} \\lambda_i^{n-2} \\\\ \n",
    "0 & \\lambda_i & n \\lambda_i^{n-1}\\\\ \n",
    "0 & 0 & \\lambda_i \n",
    "\\end{bmatrix}$\n",
    "\n",
    "There are various ways to expand or shrink this, but most succinctly we see that diagonal elements (eigenvalues) multiply exponentially with n, as we'd expect.  The off diagonal element that are non-zero occur when we do not have enough linearly indenpendent eigenvectors for $\\lambda_i$, and as a lower bound, we can say that they too grow exponentially (albeit it at one or two iterations less per 'step' than the diagonals) and are scaled by n. \n",
    "\n",
    "From here, notice that for any $\\vert \\lambda_i\\vert \\lt 1$, that $\\lim_{n \\to \\infty}\\mathbf J_i^n = \\mathbf O $.  However if $\\vert \\lambda_i\\vert = 1$, then the off diaonal elements also tend to infinity.  (We don't consider the case of $\\vert \\lambda_i\\vert \\gt 1$, because as noted in the Gerschgorin discs writeup, Markov Chains cannot have eigenvalues with magnitude $\\gt 1$.) \n",
    "\n",
    "\n",
    "Now suppose we have a defective markov chain transition matrix that is $m$ x $m$.  I.e. it factorizes so $\\mathbf A = \\mathbf {PJP}^{-1}$, where $\\mathbf J$ is the jordan form that has non-zero off-diagonal elements because we $\\mathbf A$ is defective.  \n",
    "\n",
    "We also can say:\n",
    "\n",
    "$\\mathbf J^n = \\mathbf P^{-1} \\mathbf A^n \\mathbf P $, or using associativity $\\mathbf J^n = \\mathbf P^{-1} \\big(\\mathbf A^n \\mathbf P \\big)$\n",
    "\n",
    "applying the above Frobenius norm inequality, twice,\n",
    "\n",
    "$ \\big \\Vert \\mathbf J^n \\big \\Vert_F^2=\\big \\Vert \\mathbf P^{-1} \\big(\\mathbf A^n \\mathbf P \\big) \\big \\Vert_F^2 \\leq \\big \\Vert \\mathbf P^{-1} \\big \\Vert_F^2\\big \\Vert \\big(\\mathbf A^n \\mathbf P \\big) \\big \\Vert_F^2 \\leq \\big \\Vert \\mathbf P^{-1} \\big \\Vert_F^2\\big \\Vert \\mathbf A^n   \\big \\Vert_F^2\\big \\Vert  \\mathbf P  \\big \\Vert_F^2$\n",
    "\n",
    "\n",
    "Also recalling that valid transition matrix $\\mathbf A^n$ has all entries as real valued non-negative, and either all columns or all rows sum to one.  Put differently, every value in $\\mathbf A$ is in $[0,1]$, and  thus we can upper bound the squared Frobenius norm of $\\mathbf A^n$ by the ones matrix.\n",
    "\n",
    "$\\big \\Vert \\mathbf A^n  \\big \\Vert_F^2 \\leq \\big \\Vert \\mathbf {11}^T  \\big \\Vert_F^2 = m^2$\n",
    "\n",
    "now multiply both sides by $\\big \\Vert \\mathbf P^{-1} \\big \\Vert_F^2$ and $\\big \\Vert  \\mathbf P  \\big \\Vert_F^2$, which are real valued, positive scalars, and we get:  \n",
    "\n",
    "$ \\big \\Vert \\mathbf J^n \\big \\Vert_F^2 \\leq \\big \\Vert \\mathbf P^{-1} \\big \\Vert_F^2\\big \\Vert \\mathbf A^n  \\big \\Vert_F^2 \\big \\Vert  \\mathbf P  \\big \\Vert_F^2 \\leq \\big \\Vert \\mathbf P^{-1} \\big \\Vert_F^2\\big \\Vert m^2 \\big \\Vert_F^2\\big \\Vert  \\mathbf P  \\big \\Vert_F^2 $\n",
    "\n",
    "notice that for any given transition matrix $\\mathbf A$, the right hand side of the equality is some fixed finite number, and it does not vary with respect to $n$.  The contradiction comes by assuming that we do not have enough linearly independent eigenvectors with eigenvalue magnitude of $1$.  Hence for any given $\\mathbf A$ we have a fixed upper bound on the right hand side that does not vary with $n$.  Yet if we have jordan blocks associated with $\\big \\vert\\lambda_i \\big \\vert =1$, then we can find large enough $n$ such that the left hand side has a larger Frobenius norm than the right hand side, which is a contradiction.  Hence we know there cannot be jordan blocks, aka a shortage of linearly independent eigenvectors, with respect to eigenvalues of magnitude one in a Markov chain transition matrix. \n",
    "\n",
    "\n",
    "**extension: Projection Matrices must be diagonalizable**  \n",
    "\n",
    "A projector, aka an idempotent matrix is an $n$ x $n$ matrix $\\mathbf A$, where \n",
    "\n",
    "$\\mathbf A = \\mathbf A^2$\n",
    "\n",
    "$\\mathbf A$ must diagonalizable.\n",
    "\n",
    "**proof**\n",
    "\n",
    "$\\mathbf A$ has only eigenvalues equal to $0$ and $1$\n",
    "\n",
    "that is, for each eigenvector $\\mathbf x$, we have $\\mathbf A^2 \\mathbf x = \\lambda_k \\mathbf A \\mathbf x = \n",
    "\\lambda_k^2 \\mathbf x = \\lambda_k \\mathbf x = \\mathbf {Ax}$\n",
    "\n",
    "thus $\\lambda_k^2 = \\lambda_k$, which occurs **iff** $\\lambda_k = 0$ or $\\lambda_k = 1$.\n",
    "\n",
    "now we consider the possibility that $\\mathbf A$ is defective and write out its Jordan Form, similarity transform\n",
    "\n",
    "$\\mathbf P^{-1} \\mathbf A^k \\mathbf P =\\mathbf J^k$ \n",
    "\n",
    "hence we have \n",
    "\n",
    "$\\mathbf P^{-1} \\mathbf A \\mathbf P =\\mathbf J = \\mathbf J^2 = \\mathbf P^{-1} \\mathbf A^2 \\mathbf P $ \n",
    "\n",
    "also notice that since $\\mathbf A = \\mathbf A^2$, then we can left multiply both by $\\mathbf A$ and see that \n",
    "\n",
    "$\\mathbf A^2 = \\mathbf A^3$, hence $\\mathbf A = \\mathbf A^3$. We can further do this process such that $\\mathbf A = \\mathbf A^k$\n",
    "\n",
    "\n",
    "Thus our relationship is $\\mathbf J = \\mathbf J^k$ for any natural number k. Recalling that $\\mathbf J$ has eigenvalues equal to zero or one on the diagonal, and at most all 1s in the strictly upper triangular portion, we can upperbound the squared frobenius norm of $\\mathbf J$ with $\\big \\Vert \\mathbf {11}^T \\big \\Vert_F^2$.  Further, we collect all of the eigenvalues in a diagonal matrix $\\mathbf D$ and remark that $\\mathbf J$ has a Frobenius norm strictly greater than $\\mathbf D$ unless $\\mathbf A$ is diagonalizable. \n",
    "\n",
    "if defective $ \\big \\Vert \\mathbf D \\big \\Vert_F^2  \\lt \\big \\Vert \\mathbf J^k \\big \\Vert_F^2 = \\big \\Vert \\mathbf J \\big \\Vert_F^2 \\lt \\big \\Vert \\mathbf {11}^T \\big \\Vert_F^2$.  \n",
    "\n",
    "if diagonalizable $ \\big \\Vert \\mathbf D \\big \\Vert_F^2  = \\big \\Vert \\mathbf J^k \\big \\Vert_F^2 = \\big \\Vert \\mathbf J \\big \\Vert_F^2 \\lt \\big \\Vert \\mathbf {11}^T \\big \\Vert_F^2$.  \n",
    "\n",
    "We know that the matrix cannot have a shortage of linearly independent eigenvectors for $\\lambda_k = 1$.  Why? Repeat the exact same argument used above for eigenvalues with magnitude 1 in Markov Chains.  The right hand side is fixed in magnitude, but we can find large enough (finite) $k$ that creates a Frobenius norm that exceeds this bound if we have any Jordan blocks (i.e. linearly dependent eigenvectors) with respect to eigenvalues equal to one.  Thus we conclude that the matrix geometric multiplicity = algebraic multiplicity for $\\lambda = 1$\n",
    "\n",
    "Now, with respect to eigenvalues equal to zero, recall that if the matrix is defective, we have jordan blocks given by: \n",
    "\n",
    "$\\mathbf J_i = \\begin{bmatrix}\n",
    "\\lambda_i & 1 & 0 \\\\ \n",
    "0 & \\lambda_i & 1\\\\ \n",
    "0 & 0 & \\lambda_i \n",
    "\\end{bmatrix}$\n",
    "\n",
    "and, for $k\\gt 2$, we have \n",
    "\n",
    "$\\mathbf J_I^k = \\begin{bmatrix}\n",
    "\\lambda_i^k & k \\lambda_i^{k-1} & \\binom{k}{2} \\lambda_i^{k-2} \\\\ \n",
    "0 & \\lambda_i & k \\lambda_i^{k-1}\\\\ \n",
    "0 & 0 & \\lambda_i \n",
    "\\end{bmatrix}$\n",
    "\n",
    "Yet this block is nilpotent since $\\lambda = 0$ and thus we can select large enough $k$ (e.g. $k:=n$) and say that $\\mathbf J_I^n = \\mathbf 0$ \n",
    "\n",
    "since we know that there are no off diagonal elements with respect to $\\lambda = 1$ or with respect to $\\lambda = 0$ for $\\mathbf J_I^n $, we know that $\\mathbf J^n $ is diagonal.   \n",
    "\n",
    "I.e.\n",
    "$\\big \\Vert \\mathbf D \\big \\Vert_F^2 =\\big \\Vert \\mathbf J^n \\big \\Vert_F^2$\n",
    "\n",
    "but since $\\mathbf J^n = \\mathbf J^2 = \\mathbf J$, we have \n",
    "\n",
    "$\\big \\Vert \\mathbf D \\big \\Vert_F^2 = \\big \\Vert \\mathbf J^2 \\big \\Vert_F^2 = \\big \\Vert \\mathbf J \\big \\Vert_F^2$\n",
    "\n",
    "which proves that $\\mathbf A$ is diagonalizable (i.e. not defective).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**The rest of this posting consists of inequalities from Zhang's *Linear Algebra: Challenging Problems for Students*, which contains a wealth of interesting exercises that proceed in a thoughtful manner, much like a guided proof.  (Unfortunately, the solutions in the back are frequently some mixture of terse and nearly incomprehensible -- that said I'd still recommend the book for the exercises alone)  **\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Misc notes on Hermitian Matrices\n",
    "\n",
    "if we let $\\mathbf A$ and $\\mathbf B$ both be $n$ x $n$ Hermitian matrices, \n",
    "\n",
    "first, notice $\\mathbf {AB} $ has the same eigenvalues as $\\mathbf {BA}$.  (In general we know that they have the same non-zero eigenvalues with the same Algebraic multiplicities -- and since they have the same dimension $n$ there must be the same number of 'leftover' eigenvalues that are zeros.  A proof is contained in \"Vandermonde_Matrices_Permutations_and_Discrete_Fourier_Transform.ipybn\" )\n",
    "\n",
    "\n",
    "now notice $\\big(\\mathbf{AB}\\big)^H = \\mathbf B^H \\mathbf A^H = \\mathbf {BA}$.  Thus we conclude that $\\mathbf {BA}$ has the same eigenvalues (with same algebraic multipilicities) as $\\mathbf {AB}$ and the same as the transposed conjugate $\\big(\\mathbf {BA}\\big)^H$.   This means that all eigenvalues in $\\mathbf {AB}$ must be either real, or come in conjugate pairs.   \n",
    "\n",
    "This means that $trace\\Big(\\big(\\mathbf{AB}\\big)^k\\Big)$ is real valued for any natural number $k$.  \n",
    "\n",
    "- - - - -\n",
    "*begin interlude*  \n",
    "Here is a different take.  The below claims are mostly interested in $\\big(\\mathbf {AB}\\big)$ and $\\big(\\mathbf {AB}\\big)^2$.  For the first case, notice that $\\big( \\mathbf A - \\mathbf B\\big)$ is a Hermitian matrix, and if we square it, it is hermitian positive semi definite matrix-- and hence its trace must be real, non-negative, which we denote as $\\gamma$.  \n",
    "\n",
    "$trace\\Big( \\big(\\mathbf A - \\mathbf B\\big)^2 \\Big) = trace\\big(\\mathbf A^2\\big) + trace\\big(\\mathbf B^2\\big) - trace\\big(\\mathbf{AB}\\big) - trace\\big(\\mathbf{BA}\\big) = \\gamma$\n",
    "\n",
    "$trace\\big(\\mathbf A^2\\big) + trace\\big(\\mathbf B^2\\big) - 2*trace\\big(\\mathbf{AB}\\big) = \\gamma$\n",
    "\n",
    "$trace\\big(\\mathbf A^2\\big) + trace\\big(\\mathbf B^2\\big)  = \\gamma + 2*trace\\big(\\mathbf{AB}\\big)$\n",
    "\n",
    "the left hand side is the sum of traces of 2 Hermitian positive semi-definite matrices and hence is real, non-negative, thus the right hand side is as well. We know that $\\gamma$ is real, thus $trace\\big(\\mathbf{AB}\\big)$ must be as well. \n",
    "\n",
    "Also consider \n",
    "\n",
    "$\\Big(\\mathbf{AB} + \\mathbf{BA}\\Big)^2 = \\Big(\\mathbf{AB} + \\big(\\mathbf{AB}\\big)^H\\Big)^2$\n",
    "\n",
    "Where $\\mathbf{AB}$ plus its conjugate transpose creates a new matrix that is Hermitian.  Thus this new matrix (and its square) have real eigenvalues, and a real trace.  We denote the trace of the square of this new matrix as $\\gamma$\n",
    "\n",
    "$trace\\Big( \\big( \\mathbf{AB} + \\mathbf{BA}\\big)^2 \\Big) = \\gamma = trace\\big(\\mathbf{ABAB}\\big) + trace\\big(\\mathbf{BABA} \\big) + trace\\big(\\mathbf{ABBA}\\big) + trace\\big(\\mathbf{BAAB}\\big)$\n",
    "\n",
    "$trace\\Big( \\big( \\mathbf{AB} + \\mathbf{BA}\\big)^2 \\Big) = \\gamma  = 2*trace\\big(\\mathbf{ABAB}\\big) + 2*trace\\big(\\mathbf{ABBA}\\big)$\n",
    "\n",
    "$\\frac{1}{2}\\gamma = trace\\big(\\mathbf{ABAB}\\big) + trace\\big(\\mathbf{ABBA}\\big)$\n",
    "\n",
    "$\\frac{1}{2}\\gamma -  trace\\big(\\mathbf{ABBA}\\big) = trace\\Big(\\mathbf{\\big(AB\\big)^2}\\Big)$\n",
    "\n",
    "since gamma is real, and $(\\mathbf{ABBA}\\big) $ is Hermitian, and hence has a real trace, then the left hand side has a real trace.  This means that the right hand side must have a real trace as well.  \n",
    "\n",
    "*end interlude*\n",
    "- - - - -\n",
    "\n",
    "\n",
    "**claim:**    \n",
    "\n",
    "$trace\\Big(\\big(\\mathbf {AB}\\big)^2\\Big)  \\leq trace \\big(\\mathbf A^2 \\mathbf B^2\\big)$\n",
    "\n",
    "**proof:**  \n",
    "\n",
    "using the cyclic property of trace, and the fact that $\\mathbf A$ and $\\mathbf B$ are both Hermitian, we can rewrite the right hand side as:\n",
    "\n",
    "$trace \\big(\\mathbf A^2 \\mathbf B^2\\big) = trace \\big(\\mathbf {AA} \\mathbf {BB}\\big) = trace \\big(\\mathbf A \\mathbf {BBA}\\big) = trace \\big(\\mathbf A^H \\mathbf B^H \\mathbf {BA}\\big) = trace \\Big(\\big(\\mathbf{BA}\\big)^H \\big(\\mathbf {BA}\\big) \\Big)$\n",
    "\n",
    "Let $\\mathbf C: = \\mathbf {AB}$\n",
    "\n",
    "This means we can rewrite our claim as \n",
    "\n",
    "\n",
    "$trace\\big(\\mathbf {C}^2\\big) \\leq trace \\big(\\mathbf{C}^H \\mathbf {C}\\big)$\n",
    "\n",
    "and by applying the Schur inequality, we know \n",
    "\n",
    "$trace\\big(\\mathbf {C}^2\\big) \\leq \\big \\vert trace\\big(\\mathbf {C}^2\\big) \\big \\vert \\leq trace \\big(\\mathbf{C}^H \\mathbf {C}\\big)$\n",
    "\n",
    "again, recalling that $trace\\big( \\mathbf{C}^k\\big)$ is real valued for any natural number $k$\n",
    "\n",
    "\n",
    "**claim:**  \n",
    " \n",
    "$\\Big(trace\\big(\\mathbf {AB}\\big)\\Big)^2 \\leq trace \\big(\\mathbf A^2\\big) trace \\big(\\mathbf B^2\\big)$\n",
    "\n",
    "- - - - \n",
    "*begin interlude* \n",
    "\n",
    "\"smashing\" $\\mathbf B$ and $\\mathbf A$ down into  vectors\n",
    "\n",
    "Note we denote $\\mathbf B = \\bigg[\\begin{array}{c|c|c|c}\n",
    "\\mathbf b_1 & \\mathbf b_2 &\\cdots & \\mathbf b_n\\end{array}\\bigg] $\n",
    "\n",
    "and $vec\\big(\\mathbf B\\big)  =\\mathbf b= \\begin{bmatrix}\n",
    "\\mathbf b_1 \\\\ \n",
    "\\mathbf b_2\\\\ \n",
    "\\mathbf b_3\\\\ \n",
    "\\vdots \\\\ \n",
    "\\mathbf b_n\n",
    "\\end{bmatrix}$\n",
    "\n",
    "where $\\mathbf b$ has $\\mathbf b_1$ with $\\mathbf b_2$ \"glued\" to the bottom of it, with $\\mathbf b_3$ \"glued\" to the bottom of that ... with $\\mathbf b_n$ glued to the bottom of that.  \n",
    "\n",
    "Thus we see that $\\big \\Vert \\mathbf B \\big \\Vert_F^2 = \\mathbf b ^H \\mathbf b$\n",
    "\n",
    "The same for $\\mathbf A$\n",
    "\n",
    "Note we denote $\\mathbf A = \\bigg[\\begin{array}{c|c|c|c}\n",
    "\\mathbf a_1 & \\mathbf a_2 &\\cdots & \\mathbf a_n\\end{array}\\bigg] $\n",
    "\n",
    "and $vec\\big(\\mathbf A\\big)  = \\mathbf a = \\begin{bmatrix}\\mathbf a_1 \\\\ \n",
    "\\mathbf a_2\\\\ \n",
    "\\mathbf a_3\\\\ \n",
    "\\vdots \\\\ \n",
    "\\mathbf a_n\n",
    "\\end{bmatrix}$\n",
    "\n",
    "$\\big \\Vert \\mathbf A \\big \\Vert_F^2 = \\mathbf a^H \\mathbf a$\n",
    "\n",
    "*end interlude*  \n",
    "- - - - \n",
    "\n",
    "** proof:**    \n",
    "\n",
    "because $\\mathbf A = \\mathbf A^H$ we can rewrite the left hand side as  \n",
    "\n",
    "$\\Big(trace\\big(\\mathbf {AB}\\big)\\Big)^2  = \\Big(trace\\big(\\mathbf {A}^H \\mathbf B \\big)\\Big)^2 = \\Big(trace\\big(\\mathbf a^H \\mathbf b \\big)\\Big)^2 = \\big(\\mathbf a^H \\mathbf b\\big)^2$\n",
    "\n",
    "\n",
    "And for the right hand side we can rewrite it as:\n",
    "\n",
    "$trace \\big(\\mathbf A^2\\big) trace \\big(\\mathbf B^2\\big) = trace \\big(\\mathbf {A}^H\\mathbf A\\big) trace \\big(\\mathbf B^H \\mathbf B \\big) = trace \\big(\\mathbf a^H \\mathbf a\\big) trace \\big(\\mathbf b^H \\mathbf b\\big) = \\big(\\mathbf {a}^H \\mathbf a \\big)\\big(\\mathbf b^H \\mathbf b\\big)$\n",
    "\n",
    "hence our claim reduces to a simple application of Cauchy Schwartz:\n",
    "\n",
    "i.e. \n",
    "\n",
    "$\\Big(trace\\big(\\mathbf {AB}\\big)\\Big)^2 \\leq trace \\big(\\mathbf A^2\\big) trace \\big(\\mathbf B^2\\big)$\n",
    "\n",
    "is equivalent to saying \n",
    "\n",
    "$ (\\mathbf a^H \\mathbf b\\big)^2 \\leq \\big(\\mathbf {a}^H \\mathbf a \\big)\\big(\\mathbf b^H \\mathbf b\\big)$\n",
    "\n",
    "recalling the fact that $\\big(\\mathbf {AB}\\big)$ has a real valued trace, thus $\\Big(trace\\big(\\mathbf {AB}\\big)\\Big)^2 = \\big(\\mathbf b^H \\mathbf a\\big)^2 = \\big(\\mathbf a^H \\mathbf b\\big)^2$ is real valued as well.  \n",
    "\n",
    "\n",
    "\n",
    "**claim :  **\n",
    "\n",
    "$trace\\big(\\mathbf{AB}\\big) \\leq \\frac{1}{2}\\Big(trace\\big(\\mathbf A^2 \\big) + trace\\big(\\mathbf B^2\\big)\\Big)$\n",
    "\n",
    "**proof:  **\n",
    "\n",
    "$\\mathbf C := \\mathbf A - \\mathbf B$\n",
    "\n",
    "$\\mathbf C^H = \\mathbf A^H - \\mathbf B^H = \\mathbf A - \\mathbf B = \\mathbf C$\n",
    "\n",
    "hence $\\mathbf C$ is Hermitian\n",
    "\n",
    "$\\mathbf C^2 = \\mathbf C \\mathbf C = \\mathbf C^H \\mathbf C$\n",
    "\n",
    "hence $\\mathbf C^2$ is Hermitian positive semi-definite, thus its trace must be real valued and $\\geq 0$\n",
    "\n",
    "$trace\\big(\\mathbf C^2\\big) = trace\\Big( \\big(\\mathbf A - \\mathbf B\\big)^2 \\Big) \\geq 0$\n",
    "\n",
    "$trace\\Big( \\big(\\mathbf A - \\mathbf B\\big)^2 \\Big) = trace\\Big(\\mathbf A^2 + \\mathbf B^2 - \\mathbf {AB} - \\mathbf {BA}\\Big) \\geq 0$  \n",
    "\n",
    "$trace\\Big( \\big(\\mathbf A - \\mathbf B\\big)^2 \\Big) = trace\\big(\\mathbf A^2 \\big) + trace\\big(\\mathbf B^2\\big) - 2* trace\\big(\\mathbf {AB}\\big) \\geq 0$  \n",
    "\n",
    "$trace\\big(\\mathbf A^2 \\big) + trace\\big(\\mathbf B^2\\big) \\geq 2* trace\\big(\\mathbf {AB}\\big) $\n",
    "\n",
    "which we can re-arrange as \n",
    "\n",
    "$trace\\big(\\mathbf{AB}\\big) \\leq \\frac{1}{2}\\Big(trace\\big(\\mathbf A^2 \\big) + trace\\big(\\mathbf B^2\\big)\\Big)$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hermitian Positive Semi Definite Trace Inequalities\n",
    "\n",
    "where $\\mathbf A$ and $\\mathbf B$ are both Hermitian Positive (Semi) Definite Matrices\n",
    "\n",
    "**claim: ** \n",
    "\n",
    "$trace\\big(\\mathbf{AB}\\big) \\geq 0$ \n",
    "\n",
    "that is, the the above trace is always real and non-negative.  \n",
    "\n",
    "**proof:**\n",
    "\n",
    "$trace\\big(\\mathbf{AB}\\big) = trace\\big(\\mathbf A^{\\frac{1}{2}}\\mathbf A^{\\frac{1}{2}} \\mathbf B^{\\frac{1}{2}}\\mathbf B^{\\frac{1}{2}}\\big)= trace\\big(\\mathbf B^{\\frac{1}{2}}\\mathbf A^{\\frac{1}{2}} \\mathbf A^{\\frac{1}{2}}\\mathbf B^{\\frac{1}{2}}\\big) = trace\\Big(\\big(\\mathbf B^{\\frac{1}{2}}\\big)^H \\big(\\mathbf A^{\\frac{1}{2}}\\big)^H \\mathbf A^{\\frac{1}{2}}\\mathbf B^{\\frac{1}{2}}\\Big) = trace\\Big(\\big(\\mathbf A^{\\frac{1}{2}} \\mathbf B^{\\frac{1}{2}}\\big)^H \\big( \\mathbf A^{\\frac{1}{2}}\\mathbf B^{\\frac{1}{2}}\\big)\\Big)$ \n",
    "\n",
    "$trace\\big(\\mathbf{AB}\\big) = \\big \\Vert \\mathbf A^{\\frac{1}{2}} \\mathbf B^{\\frac{1}{2}}\\big \\Vert_F^2$\n",
    "\n",
    "Alternatively, we could also note that $\\big(\\mathbf{AB}\\big) = \\Big(\\mathbf A^{\\frac{1}{2}}\\big(\\mathbf A^{\\frac{1}{2}} \\mathbf B\\big)\\Big)$  and must have the same eigenvalues as $\\Big(\\big(\\mathbf A^{\\frac{1}{2}} \\mathbf B\\big)\\mathbf A^{\\frac{1}{2}}\\Big)$ which is Hermitian Positive semi-defintie, and hence has all real, non-negative eigenvalues.  Since the trace gives the sum of those eigenvalues, the trace must be real, non-negative.  \n",
    "\n",
    "\n",
    "**claim: **\n",
    "\n",
    "$trace\\big(\\mathbf{AB}\\big) \\leq trace\\big(\\mathbf A\\big) trace \\big(\\mathbf B \\big) \\leq \\frac{1}{2}\\Big(trace\\big(\\mathbf A\\big)^2 + trace\\big(\\mathbf B\\big)^2\\Big)$\n",
    "\n",
    "**commentary: **\n",
    "\n",
    "The left side of the inequality is of particular interest.  \n",
    "\n",
    "$trace\\big(\\mathbf{AB}\\big) \\leq trace\\big(\\mathbf A\\big) trace \\big(\\mathbf B \\big) $\n",
    "\n",
    "can be rewritten as:\n",
    "\n",
    "$trace\\big(\\mathbf{AB}\\big) \\leq \\big(\\sum_{i = 1}^{n} \\mathbf{A}_{i,i}\\big) \\big(\\sum_{i = 1}^{n} \\mathbf{B}_{i,i}\\big)$\n",
    "\n",
    "since $\\mathbf A$ and $\\mathbf B$ are both Hermitian positive semi-definite, we may recall the Hadamard Inequality (see \"HadamardInequality.ipynb\"), and draw an analogy with determinants.\n",
    "\n",
    "noting that $det\\big(\\mathbf{AB}\\big) = det\\big(\\mathbf{A}\\big) det\\big(\\mathbf{B}\\big)$,\n",
    "\n",
    "we use the Hadarmard inequality:  \n",
    "\n",
    "$det\\big(\\mathbf{A}\\big) \\leq \\big(\\prod_{i = 1}^{n} \\mathbf{A}_{i,i}\\big)$  \n",
    "$det\\big(\\mathbf{B}\\big) \\leq \\big(\\prod_{i = 1}^{n} \\mathbf{B}_{i,i}\\big)$  \n",
    "\n",
    "hence \n",
    "\n",
    "$det\\big(\\mathbf{AB}\\big) \\leq \\big(\\prod_{i = 1}^{n} \\mathbf{A}_{i,i}\\big) \\big(\\prod_{i = 1}^{n} \\mathbf{B}_{i,i}\\big)$\n",
    "\n",
    "which seems analogous  to  \n",
    "\n",
    "$trace\\big(\\mathbf{AB}\\big) \\leq \\big(\\sum_{i = 1}^{n} \\mathbf{A}_{i,i}\\big) \\big(\\sum_{i = 1}^{n} \\mathbf{B}_{i,i}\\big)$\n",
    "\n",
    "**proof:  **  \n",
    "\n",
    "*For the left side of the the inequality:*  \n",
    "\n",
    "$ trace\\big(\\mathbf{AB}\\big) = \\big \\Vert \\mathbf A^{\\frac{1}{2}} \\mathbf B^{\\frac{1}{2}}\\big \\Vert_F^2 \\leq \\big \\Vert \\mathbf A^{\\frac{1}{2}}\\big \\Vert_F^2 \\big \\Vert \\mathbf B^{\\frac{1}{2}}\\big \\Vert_F^2 =  trace\\Big(\\big(\\mathbf A^{\\frac{1}{2}}\\big)^H \\mathbf A^{\\frac{1}{2}}\\Big) trace\\Big(\\big(\\mathbf B^{\\frac{1}{2}}\\big)^H \\mathbf B^{\\frac{1}{2}}\\Big) = trace\\big(\\mathbf A\\big) trace \\big(\\mathbf B \\big) $\n",
    "\n",
    "where we use the derivation under \"Matrix Norms\" for the proof that \n",
    "\n",
    " $\\big \\Vert \\mathbf A^{\\frac{1}{2}} \\mathbf B^{\\frac{1}{2}}\\big \\Vert_F^2 \\leq \\big \\Vert \\mathbf A^{\\frac{1}{2}}\\big \\Vert_F^2 \\big \\Vert \\mathbf B^{\\frac{1}{2}}\\big \\Vert_F^2 $\n",
    "\n",
    "- - - -\n",
    "*begin alternative proof*\n",
    "\n",
    "Consider that for Hermitian Positive (Semi) Definite matrices, we have strictly real, non-negative values along the diagonal of said matrices and amongst their eigenvalues.  \n",
    "\n",
    "Thus we could interpret this inequality as mutliplying two finite series, and noting that \n",
    "\n",
    "\n",
    "$(\\lambda_1 + \\lambda_2 +... + \\lambda_n)(\\sigma_1 + \\sigma_2 + ... + \\sigma_n) \\geq \\lambda_1 \\sigma_1 + \\lambda_2 \\sigma_2 +... + \\lambda_n \\sigma_n$\n",
    "\n",
    "because every term in the series is real and non-negative\n",
    "\n",
    "To map this to our problem simply let: \n",
    "\n",
    "$\\mathbf A = \\mathbf {Q \\Lambda Q}^H$\n",
    "\n",
    "$trace\\big(\\mathbf A\\big) = \\lambda_1 + \\lambda_2 +... + \\lambda_n$\n",
    "\n",
    "$trace\\big(\\mathbf B\\big) = \\sigma_1 + \\sigma_2 + ... + \\sigma_n$\n",
    "\n",
    "\n",
    "$trace\\big(\\mathbf {AB}\\big) = trace\\big(\\mathbf {Q\\Lambda Q}^H \\mathbf B\\big) = trace\\big(\\mathbf {\\Lambda Q}^H \\mathbf B\\mathbf Q\\big)$\n",
    "\n",
    "Now define a new matrix $\\mathbf C := \\mathbf Q^H \\mathbf{BQ}$, which is still Hermitian, and similar to $\\mathbf B$, and hence Positive Semi Definite.  We see that \n",
    "\n",
    "$trace\\big(\\mathbf {AB}\\big) = trace\\big(\\mathbf {\\Lambda C}\\big) $\n",
    "\n",
    "\n",
    "$trace\\big(\\mathbf {\\Lambda C}\\big)= \\lambda_1 c_{1,1} + \\lambda_2 c_{2,2} + ... + \\lambda_n c_{n,n} \\leq (\\lambda_1 + \\lambda_2 +... + \\lambda_n)(c_{1,1} + c_{2,2} + ... + c_{n,n}) = trace\\big(\\mathbf A\\big) trace\\big(\\mathbf C\\big) $\n",
    "\n",
    "Noting that $\\mathbf B$ and $\\mathbf C$ are similar, and hence have the same eigenvalues:\n",
    "\n",
    "$trace\\big(\\mathbf {AB}\\big) \\leq trace\\big(\\mathbf A\\big) trace\\big(\\mathbf C\\big)= trace\\big(\\mathbf A\\big)trace\\big(\\mathbf B\\big)$ \n",
    "\n",
    "\n",
    "*end alternative proof*\n",
    "- - - -\n",
    "\n",
    "*For the right hand side of the inequality:*  \n",
    "\n",
    "$\\big(\\mathbf A - \\mathbf B\\big)$ is a Hermitian matrix, and hence its trace given by  \n",
    "\n",
    "$trace \\big(\\mathbf A - \\mathbf B \\big) = trace \\big(\\mathbf A\\big) - trace \\big(\\mathbf B \\big)$\n",
    "\n",
    "is a real number.  Whenever we square a real number, the result must be $\\geq 0$\n",
    "\n",
    "\n",
    "$\\Big(trace \\big(\\mathbf A\\big) - trace \\big(\\mathbf B \\big)\\Big)^2 \\geq 0 $  \n",
    "$trace \\big(\\mathbf A\\big)^2 + trace \\big(\\mathbf B \\big)^2 - 2 *trace\\big(\\mathbf A\\big)trace\\big(\\mathbf B\\big) \\geq 0$\n",
    "\n",
    "\n",
    "hence we see that \n",
    "\n",
    "$\\frac{1}{2}trace \\big(\\mathbf A\\big)^2 + \\frac{1}{2} trace \\big(\\mathbf B \\big)^2 \\geq trace\\big(\\mathbf A\\big)trace \\big(\\mathbf B\\big) $\n",
    "\n",
    "This proves that \n",
    "\n",
    "$trace\\big(\\mathbf{AB}\\big) \\leq trace\\big(\\mathbf A\\big) trace \\big(\\mathbf B \\big) \\leq \\frac{1}{2}\\Big(trace\\big(\\mathbf A\\big)^2 + trace\\big(\\mathbf B\\big)^2\\Big)$\n",
    "\n",
    "**claim:**  \n",
    "\n",
    "$trace\\big(\\mathbf{AB}\\big) \\leq \\lambda_{max}\\big(\\mathbf A\\big) trace\\big(\\mathbf B\\big)$\n",
    "\n",
    "**proof:  **  \n",
    "\n",
    "for convenience we start by noting $trace\\big(\\mathbf{AB}\\big) = trace\\big(\\mathbf{BA}\\big)$\n",
    "\n",
    "$trace\\big(\\mathbf{BA}\\big) = \\big \\Vert \\mathbf B^{\\frac{1}{2}} \\mathbf A^{\\frac{1}{2}}\\big \\Vert_F^2 \\leq  \\big \\Vert \\mathbf A^{\\frac{1}{2}} \\big \\Vert_2^2 \\big \\Vert\\mathbf B^{\\frac{1}{2}} \\big \\Vert_F^2 = \\sigma_{A,1} *trace\\Big(\\big(\\mathbf B^{\\frac{1}{2}}\\big)^H \\mathbf B^{\\frac{1}{2}} \\Big) = \\lambda_{A,1}* trace\\big(\\mathbf B^{\\frac{1}{2}} \\mathbf B^{\\frac{1}{2}} \\big) = \\lambda_{max}\\big(\\mathbf A\\big) trace\\big(\\mathbf B\\big)$  \n",
    "\n",
    "using results from the \"Matrix Norms\" section to justify the inequality, and noting that the largest squared singular value of $\\mathbf A^{\\frac{1}{2}}$ is the largest singular value of $\\mathbf A$ which equals  $\\sigma_{A,1}$.  Also noticing that because $\\mathbf A$ is Hermitian positive semi-definite, its singular values are equal to its eigenvalues.  \n",
    "\n",
    "*begin alternative proof*\n",
    "\n",
    "leveraging the preceding alternative proof, note that\n",
    "\n",
    "$trace\\big(\\mathbf {A B}\\big) = trace\\big(\\mathbf {\\Lambda C}\\big)= \\lambda_1 c_{1,1} + \\lambda_2 c_{2,2} + ... + \\lambda_n c_{n,n} \\leq \\lambda_1 (c_{1,1} + c_{2,2} + ... + c_{n,n}) = \\lambda_1 trace\\big(\\mathbf B\\big) $\n",
    "\n",
    "recalling that the eigenvalues of $\\mathbf A$ are ordered such that $\\lambda_1 \\geq \\lambda_2 \\geq .... \\geq \\lambda_n \\geq 0$, each diagonal element of $\\mathbf C$ is real valued, non-negative, and $trace\\big(\\mathbf B\\big) = trace\\big(\\mathbf C\\big)$.\n",
    "\n",
    "\n",
    "*end alternative proof*  \n",
    "**claim:**  \n",
    "\n",
    "\n",
    "$trace\\big(\\mathbf{AB}\\big) \\leq \\frac{1}{4} \\Big( trace\\big(\\mathbf A\\big)+ trace\\big(\\mathbf B\\big)\\Big)^2 $\n",
    "\n",
    "**commentary:  **  \n",
    "\n",
    "This is a simple extension of an earlier inequality: \n",
    "\n",
    "$trace\\big(\\mathbf{AB}\\big) \\leq trace\\big(\\mathbf A\\big) trace \\big(\\mathbf B \\big) \\leq \\frac{1}{2}\\Big(trace\\big(\\mathbf A\\big)^2 + trace\\big(\\mathbf B\\big)^2\\Big)$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**proof:**  \n",
    "\n",
    "we start off with \n",
    "\n",
    "\n",
    "$2 * trace\\big(\\mathbf{AB}\\big) \\leq trace\\big(\\mathbf A\\big)^2 + trace\\big(\\mathbf B\\big)^2$\n",
    "\n",
    "add $2*trace\\big(\\mathbf A\\big)trace\\big(\\mathbf B\\big)$ to both sides, noticing that  $trace\\big(\\mathbf{AB}\\big) \\leq trace\\big(\\mathbf A\\big)trace\\big(\\mathbf B\\big)$,  we have   \n",
    "\n",
    "$4 * trace\\big(\\mathbf{AB}\\big) \\leq 2 * trace\\big(\\mathbf{AB}\\big) + 2*trace\\big(\\mathbf A\\big)trace\\big(\\mathbf B\\big) \\leq trace\\big(\\mathbf A\\big)^2 + trace\\big(\\mathbf B\\big)^2 + 2*trace\\big(\\mathbf A\\big)trace\\big(\\mathbf B\\big)$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Thus we have \n",
    "\n",
    "\n",
    "$4 * trace\\big(\\mathbf{AB}\\big) \\leq  trace\\big(\\mathbf A\\big)^2 + trace\\big(\\mathbf B\\big)^2 + 2*trace\\big(\\mathbf A\\big)trace\\big(\\mathbf B\\big) = \\Big(trace\\big(\\mathbf A\\big) + trace\\big(\\mathbf B\\big)\\Big)^2$\n",
    "\n",
    "\n",
    "giving us \n",
    "\n",
    "$trace\\big(\\mathbf{AB}\\big) \\leq  \\frac{1}{4}\\Big(trace\\big(\\mathbf A\\big) + trace\\big(\\mathbf B\\big)\\Big)^2$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Hermitian Positive Semi Definite Inequalities with Hadamard Product \n",
    "\n",
    "\n",
    "consider Hermitian Positive Semi Definite Matrices $\\mathbf A$ and $\\mathbf B$.  Then consider the matrix $\\mathbf C = \\mathbf A \\circ \\mathbf B$, where $\\circ$ denotes the Hadamard product.    \n",
    "\n",
    "\n",
    "**claim:  **\n",
    "$\\mathbf C $ is Hermitian positive semi-definite\n",
    "\n",
    "\n",
    "*Reminder:*\n",
    "Hadamard products distribute accross matrix addition because scalar multiplication distributes across scalar addition. \n",
    "\n",
    "example:\n",
    "\n",
    "$\\mathbf Z = \\big(\\mathbf X + \\mathbf Y\\big) \\circ \\mathbf W = \\mathbf X \\circ \\mathbf W + \\mathbf Y \\circ \\mathbf W$\n",
    "\n",
    "that is\n",
    "\n",
    "$z_{i,j} = (x_{i,j} + y_{i,j})*w_{i,j} = x_{i,j}w_{i,j} + y_{i,j}w_{i,j}$\n",
    "\n",
    "\n",
    "**proof:  **\n",
    "\n",
    "$c_{i,j} = a_{i,j} * b_{i,j} = \\bar{a_{j,i}}*\\bar{b_{j,i}} = \\bar{c_{j,i}}$\n",
    "\n",
    "By inspection we see that $\\mathbf C$ is Hermitian. \n",
    "\n",
    "Now the claim of positive semi definitiness means $\\mathbf x^H \\mathbf C \\mathbf x \\geq 0 $ for all $\\mathbf x$. \n",
    "\n",
    "next we use the decomposition employed in \"julia_hmm_viterbi_as_qp_and_lp_upload\" contained in the Optimization aka \"markov_optimization\" folder.  \n",
    "\n",
    "$\\mathbf x^H \\mathbf C \\mathbf x  = \\mathbf 1^H \\big(\\mathbf C \\circ \\mathbf x\\mathbf x^H \\big)\\mathbf 1 = sum\\big(\\mathbf C \\circ \\mathbf x\\mathbf x^H \\big)$\n",
    "\n",
    "where \"sum\" denotes adding up each scalar entry of the matrix $\\big( \\mathbf C \\circ \\mathbf x\\mathbf x^H \\big)$.\n",
    "\n",
    "Hence we are evaluating:\n",
    "\n",
    "$sum\\big(\\mathbf C \\circ \\mathbf x\\mathbf x^H \\big) = sum\\big(\\mathbf A \\circ \\mathbf B \\circ \\mathbf x\\mathbf x^H \\big)$\n",
    "\n",
    "\n",
    "using associativity and commutativity of the Hadamard product, we have\n",
    "\n",
    "$sum\\big(\\mathbf A \\circ \\mathbf B \\circ \\mathbf x\\mathbf x^H \\big) = sum\\Big( \\mathbf A \\circ \\mathbf x\\mathbf x^H \\circ \\mathbf B   \\Big) = sum\\Big(\\big(\\mathbf A \\circ \\mathbf x\\mathbf x^H\\big) \\circ \\mathbf B   \\Big)$\n",
    "\n",
    "since $\\mathbf A$ is Hermitiain Positive Semi Definite, we can re-write it as \n",
    "\n",
    "$\\mathbf A = (\\lambda_1 \\mathbf p_1 \\mathbf p_1^H + \\lambda_2 \\mathbf p_2 \\mathbf p_2^H + ... + \\lambda_n \\mathbf p_n \\mathbf p_n^H)$ \n",
    "\n",
    "where each $\\lambda_k$ is real valued and non-negative\n",
    "\n",
    "$sum\\Big(\\big(\\mathbf A \\circ \\mathbf x\\mathbf x^H\\big) \\circ \\mathbf B   \\Big) = sum\\Big(\\big((\\lambda_1 \\mathbf p_1 \\mathbf p_1^H + \\lambda_2 \\mathbf p_2 \\mathbf p_2^H + ... + \\lambda_n \\mathbf p_n \\mathbf p_n^H) \\circ \\mathbf x\\mathbf x^H\\big) \\circ \\mathbf B   \\Big) $\n",
    "\n",
    "$sum\\Big(\\big(\\mathbf A \\circ \\mathbf x\\mathbf x^H\\big) \\circ \\mathbf B   \\Big) = sum\\Big(\\big(\\lambda_1 \\mathbf p_1 \\mathbf p_1^H \\circ \\mathbf x\\mathbf x^H + \\lambda_2 \\mathbf p_2 \\mathbf p_2^H \\circ \\mathbf x\\mathbf x^H + ... + \\lambda_n \\mathbf p_n \\mathbf p_n^H\\circ \\mathbf x\\mathbf x^H \\big) \\circ \\mathbf B   \\Big)$\n",
    "\n",
    "re-writing this with sigma notation, we have\n",
    "\n",
    "$sum\\Big(\\big( \\sum_{k=1}^n \\lambda_k \\mathbf p_k \\mathbf p_k^H \\circ \\mathbf x\\mathbf x^H \\big) \\circ \\mathbf B   \\Big)$\n",
    "\n",
    "now we definte $\\mathbf y_k$, where\n",
    "\n",
    "$\\mathbf y_k := \\mathbf p_k \\circ \\mathbf x$\n",
    "\n",
    "hence $\\mathbf y_k \\mathbf y_k^H = \\mathbf p_k \\mathbf p_k^H \\circ \\mathbf x\\mathbf x^H $\n",
    "\n",
    "where for avoidance of doubt, we double check the indices:\n",
    "\n",
    "$\\big(\\mathbf y_k \\mathbf y_k^H\\big)_{i,j} = (p_{k,i}  * x_{i}) * (\\bar{p_{k,j}}* \\bar{x_{j}}) = (p_{k,i} * \\bar{p_{k,j}})  * (x_{i}  * \\bar{x_{j}}) = \\big(\\mathbf p_k \\mathbf p_k^H\\big)_{i,j} \\circ \\big(\\mathbf{xx}^H\\big)_{i,j}=   \\big(\\mathbf p_k \\mathbf p_k^H \\circ \\mathbf{xx}^H\\big)_{i,j}$ \n",
    "\n",
    "our expression becomes\n",
    "\n",
    "$sum\\Big(\\big( \\sum_{k=1}^n \\lambda_k \\mathbf y_k \\mathbf y_k^H \\big) \\circ \\mathbf B   \\Big) = sum\\Big( \\sum_{k=1}^n \\lambda_k \\mathbf y_k \\mathbf y_k^H \\circ \\mathbf B   \\Big)$\n",
    "\n",
    "now, recognizing that when we have a finite number of terms, we can always interchange linear operators like $sum()$ and $\\sum$\n",
    "\n",
    "$sum\\Big( \\sum_{k=1}^n \\lambda_k \\mathbf y_k \\mathbf y_k^H \\circ \\mathbf B   \\Big) = \\sum_{k=1}^n  sum\\Big( \\lambda_k \\mathbf y_k \\mathbf y_k^H \\circ \\mathbf B   \\Big)$\n",
    "\n",
    "- - - -\n",
    "side note: the above is interchange is equivalent to recognizing that $\\mathbf 1^H \\big( \\mathbf W + \\mathbf X\\big)\\mathbf 1 = \\mathbf 1^H\\big(\\mathbf W\\big)\\mathbf 1 + \\mathbf 1^H\\big(\\mathbf X\\big)\\mathbf 1$\n",
    "- - - -\n",
    "\n",
    "$\\sum_{k=1}^n  sum\\big( \\lambda_k \\mathbf y_k \\mathbf y_k^H \\circ \\mathbf B   \\big) = \\sum_{k=1}^n  \\lambda_k \\Big(sum\\big(  \\mathbf y_k \\mathbf y_k^H \\circ \\mathbf B   \\big)\\Big) = \\sum_{k=1}^n  \\lambda_k \\Big(sum\\big( \\mathbf B \\circ \\mathbf y_k \\mathbf y_k^H \\big)\\Big) = \\sum_{k=1}^n  \\lambda_k \\Big(\\mathbf y_k^H \\mathbf B \\mathbf y_k\\Big)$\n",
    "\n",
    "recognizing our original quadratic form decomposition $\\mathbf y_k^H \\mathbf B \\mathbf y_k = sum\\big( \\mathbf B \\circ \\mathbf y_k \\mathbf y_k^H \\big)$ \n",
    "\n",
    "since $\\mathbf B$ is Hermitian positive semi-definite, we know that $\\mathbf y_k^H \\mathbf B \\mathbf y_k \\geq 0$ for any $\\mathbf y_k$, and we recall that $\\lambda_k \\geq 0$ \n",
    "\n",
    "Hence we say \n",
    "\n",
    "$\\sum_{k=1}^n  \\lambda_k \\big(\\mathbf y_k^H \\mathbf B \\mathbf y_k\\big) =  \\lambda_1 \\big(\\mathbf y_1^H \\mathbf B \\mathbf y_1\\big) + \\lambda_2 \\big(\\mathbf y_2^H \\mathbf B \\mathbf y_2\\big) + ... + \\lambda_n \\big(\\mathbf y_n^H \\mathbf B \\mathbf y_n\\big) \\geq 0$\n",
    "\n",
    "because each term in that series is itself $\\geq 0$.\n",
    "\n",
    "This proves that $\\mathbf C = \\mathbf A \\circ \\mathbf B$ is a Hermitian positive semi-definite matrix.\n",
    "\n",
    "**claim:  **  \n",
    "\n",
    "$\\lambda_{1, A}* \\lambda_{1,B} \\geq \\lambda_{1, C}$\n",
    "\n",
    "where $\\mathbf C = \\mathbf A \\circ \\mathbf B$\n",
    "\n",
    "alternatively stated as:  \n",
    "$\\lambda \\big(\\mathbf A\\big)_{max} \\lambda \\big(\\mathbf B\\big)_{max} \\geq \\lambda \\big(\\mathbf A \\circ \\mathbf B\\big)_{max}$\n",
    "\n",
    "and as always eigenvalues are well ordered so that $\\lambda_1 \\geq \\lambda_2 \\geq ... \\geq \\lambda_n \\geq 0$ for any of these Hermitian positive semi-definite matrices\n",
    "\n",
    "The claim is trivially an equality if $\\mathbf A = \\mathbf 0$ or $\\mathbf B = \\mathbf 0$, hence we assume that neither matrix is the zero matrix, i.e. that $\\big \\Vert \\mathbf A \\big \\Vert_F \\gt 0$ and $\\big \\Vert \\mathbf B \\big \\Vert_F \\gt 0$\n",
    "\n",
    "**proof:**  \n",
    "\n",
    "\n",
    "$ \\lambda_{1,A} \\mathbf I - \\mathbf A = \\lambda_{1,A} \\mathbf I - \\mathbf {PDP}^H = \\lambda_{1,A} \\mathbf {PIP}^H - \\mathbf {PDP}^H = \\mathbf P \\big(\\lambda_{1,A} \\mathbf I - \\mathbf D\\big)\\mathbf P^H$\n",
    "\n",
    "when we inspect the diagonal entries of the diagonal matrix given by, we see:\n",
    "\n",
    "$\\big(\\lambda_{1,A} \\mathbf I - \\mathbf D\\big)_{k,k} = \\lambda_{1, A} - \\lambda_{k, A} \\geq 0$\n",
    "\n",
    "hence the matrix $\\big(\\lambda_{1,A} \\mathbf I - \\mathbf A \\big)$ is Hermitian positive semi-definite.  \n",
    "\n",
    "now consider:\n",
    "\n",
    "$\\big(\\lambda_{1,A} \\mathbf I - \\mathbf A \\big)\\circ \\mathbf B$\n",
    "\n",
    "Based on the preceding proof, the matrix that results from this, too, must be Hermitian positive semi-definite.  \n",
    "\n",
    "Hence we say \n",
    "\n",
    "$\\mathbf x^H \\Big(\\big(\\lambda_{1,A} \\mathbf I - \\mathbf A \\big)\\circ \\mathbf B\\Big)\\mathbf x \\geq 0$\n",
    "\n",
    "for any $\\mathbf x$.\n",
    "\n",
    "$\\mathbf x^H \\Big(\\big(\\lambda_{1,A} \\mathbf I - \\mathbf A \\big)\\circ \\mathbf  B\\Big)\\mathbf x = \\mathbf x^H \\Big(\\big(\\lambda_{1,A} \\mathbf I\\big) \\circ \\mathbf B \\Big)\\mathbf x - \\mathbf x^H \\Big(\\mathbf A \\circ \\mathbf B\\Big)\\mathbf x \\geq 0$\n",
    "\n",
    "for any $\\mathbf x$\n",
    "\n",
    "$\\mathbf x^H \\Big(\\big(\\lambda_{1,A} \\mathbf I\\big) \\circ \\mathbf B \\Big)\\mathbf x \\geq \\mathbf x^H \\Big(\\mathbf A \\circ \\mathbf B\\Big)\\mathbf x = \\mathbf x^H \\mathbf C \\mathbf x$\n",
    "\n",
    "for any $\\mathbf x$\n",
    "\n",
    "now set the constraint that $\\Vert \\mathbf x \\Vert_2^2 = 1$.  The right hand side is maximized with $\\mathbf x : = \\mathbf x_1$ which is the eigenvector associated with $\\lambda_{1,C}$\n",
    "\n",
    "$\\mathbf x_1^H \\Big(\\big(\\lambda_{1,A} \\mathbf I\\big) \\circ \\mathbf B \\Big)\\mathbf x_1 = \\lambda_{1,A} * \\mathbf x_1^H \\Big( \\mathbf I \\circ \\mathbf B \\Big)\\mathbf x_1 \\geq \\lambda_{1,C} = \\mathbf x_1^H \\mathbf C \\mathbf x_1$\n",
    "\n",
    "Hence we know that the Hermitian positive semi-definite matrix given by $\\big(\\mathbf I \\circ \\mathbf B \\big)$ must have at least one eigenvalue that can be scaled by $\\lambda \\big(\\mathbf A\\big)_{max}$ and the resulting product $\\geq \\lambda \\big(\\mathbf C\\big)_{max}$.\n",
    "\n",
    "If we are able to prove that the maximal eigenvalue of $\\mathbf B$ is at least as big as the maximal eigenvalue of $\\big(\\mathbf I \\circ \\mathbf B \\big)$, then we are done.  \n",
    "\n",
    "where $\\mathbf H := \\mathbf I \\circ \\mathbf B$.  \n",
    "\n",
    "Notice that $\\mathbf H$ in effect takes all of the diagonal entries of $\\mathbf B$ (and keeps their ordering intact), and then zeros out all other entries of $\\mathbf B$.\n",
    "\n",
    "What we have proven so far can be re-written as \n",
    "\n",
    "$\\lambda_{1,A} * \\lambda_{1, H} \\geq \\lambda_{1,C} $\n",
    "\n",
    "Further we now claim:\n",
    "\n",
    "$\\lambda_{1,A} * \\lambda_{1, B} \\geq \\lambda_{1,A} * \\lambda_{1, H} \\geq \\lambda_{1,C} $\n",
    "\n",
    "To justify this, consider the following: \n",
    "\n",
    "max $\\mathbf y^H \\mathbf B \\mathbf y = \\lambda_{1, B}$\n",
    "\n",
    "subject to the constraint that $\\Vert \\mathbf y \\Vert_2^2 = 1$\n",
    "\n",
    "we can always choose to restrict ourself to just one standard basis vector $\\mathbf e_k$, for $k = \\{1, 2, ..., n\\}$.  For avoidance of doubt, the standard basis vectors are shown below:\n",
    "\n",
    "$\\bigg[\\begin{array}{c|c|c|c}\n",
    "\\mathbf e_1 & \\mathbf  e_2 &\\cdots &\\mathbf  e_n\n",
    "\\end{array}\\bigg] = \\mathbf I$\n",
    "\n",
    "\n",
    "which makes the optimization\n",
    "\n",
    "max $\\mathbf e_k^H \\mathbf B \\mathbf e_k = \\lambda_{1, H}$\n",
    "\n",
    "Thus\n",
    "\n",
    "max $\\mathbf y^H \\mathbf B \\mathbf y = \\lambda_{1, B}  \\geq$ max $ \\mathbf e_k^H \\mathbf B \\mathbf e_k = \\lambda_{1, H}$, again with the constrain that $\\Vert \\mathbf y \\Vert_2^2 = 1$. \n",
    "\n",
    "Hence $\\lambda_{1, B} \\geq \\lambda_{1, H}$\n",
    "\n",
    "To conclude we have: \n",
    "\n",
    "$\\lambda_{1,A} * \\lambda_{1, B} \\geq \\lambda_{1,A} * \\lambda_{1, H} \\geq \\lambda_{1,C} $\n",
    "\n",
    "or more succinctly,\n",
    "\n",
    "$\\lambda \\big(\\mathbf A\\big)_{max} \\lambda \\big(\\mathbf B\\big)_{max} \\geq \\lambda \\big(\\mathbf A\\circ \\mathbf B\\big)_{max}$\n",
    "\n",
    "and the claim is proven\n",
    "\n",
    "- - - -\n",
    "*begin alternative proof*  \n",
    "\n",
    "Another way to prove this, which seems a bit more intuitive, is to leverage the work done in the first part to prove that $\\big(\\mathbf A \\circ \\mathbf B\\big)$ is a  Hermitian positive semi definite matrix.  The final expression we had was\n",
    "\n",
    "$ \\sum_{k=1}^n  \\lambda_k \\big(\\mathbf y_k^H \\mathbf B \\mathbf y_k\\big) = \\lambda_1 \\big(\\mathbf y_1^H \\mathbf B \\mathbf y_1\\big) + \\lambda_2 \\big(\\mathbf y_2^H \\mathbf B \\mathbf y_2\\big) + ... + \\lambda_n \\big(\\mathbf y_n^H \\mathbf B \\mathbf y_n\\big) \\geq 0$\n",
    "\n",
    "Recalling that the $\\lambda_k$'s were the eigenvalues of $\\mathbf A$, and that $\\lambda_1 \\geq \\lambda_2 \\geq ... \\geq \\lambda_n \\geq 0$, we can upper bound this series as follows:\n",
    "\n",
    "$ \\sum_{k=1}^n  \\lambda_1 \\big(\\mathbf y_k^H \\mathbf B \\mathbf y_k\\big) \\geq \\sum_{k=1}^n  \\lambda_k \\big(\\mathbf y_k^H \\mathbf B \\mathbf y_k\\big) \\geq 0$\n",
    "\n",
    "From here we work backward and examine the impact of homegenizing the eigenvalues of $\\mathbf A$, specificially recalling \n",
    "\n",
    "$sum\\Big(\\big(\\mathbf A \\circ \\mathbf x\\mathbf x^H\\big) \\circ \\mathbf B   \\Big) = sum\\Big(\\big((\\lambda_1 \\mathbf p_1 \\mathbf p_1^H + \\lambda_2 \\mathbf p_2 \\mathbf p_2^H + ... + \\lambda_n \\mathbf p_n \\mathbf p_n^H) \\circ \\mathbf x\\mathbf x^H\\big) \\circ \\mathbf B   \\Big) $\n",
    "\n",
    "now becomes\n",
    "\n",
    "$sum\\Big(\\big( \\lambda_1 ( \\mathbf p_1 \\mathbf p_1^H + \\mathbf p_2 \\mathbf p_2^H + ... + \\mathbf p_n \\mathbf p_n^H) \\circ \\mathbf x\\mathbf x^H\\big) \\circ \\mathbf B   \\Big) = sum\\Big( \\lambda_1 \\big( \\mathbf P \\mathbf P^H\\big) \\circ \\mathbf x\\mathbf x^H \\circ \\mathbf B   \\Big) = sum\\Big(\\lambda \\big(\\mathbf A\\big)_{max} \\big( \\mathbf I\\big) \\circ \\mathbf x\\mathbf x^H \\circ \\mathbf B   \\Big) $\n",
    "\n",
    "which we can re-arrange to\n",
    "\n",
    "$\\lambda \\big(\\mathbf A\\big)_{max} * sum\\Big( \\big(\\mathbf I \\circ \\mathbf B\\big) \\circ \\mathbf x \\mathbf x^H     \\Big) = \\lambda \\big(\\mathbf A\\big)_{max} *\\mathbf x^H \\big(\\mathbf I \\circ \\mathbf B\\big)\\mathbf x $\n",
    "\n",
    "\n",
    "which tells us that at a minimum $\\lambda \\big(\\mathbf A\\big)_{max} *\\lambda \\big(\\mathbf I\\circ \\mathbf B\\big)_{max} \\geq \\lambda \\big(\\mathbf A\\circ \\mathbf B\\big)_{max}$\n",
    "\n",
    "because \n",
    "\n",
    "$\\lambda_1 \\big(\\mathbf y_1^H \\mathbf B \\mathbf y_1\\big) + \\lambda_1 \\big(\\mathbf y_2^H \\mathbf B \\mathbf y_2\\big) + ... + \\lambda_1 \\big(\\mathbf y_n^H \\mathbf B \\mathbf y_n\\big) \\geq  \\lambda_1 \\big(\\mathbf y_1^H \\mathbf B \\mathbf y_1\\big) + \\lambda_2 \\big(\\mathbf y_2^H \\mathbf B \\mathbf y_2\\big) + ... + \\lambda_n \\big(\\mathbf y_n^H \\mathbf B \\mathbf y_n\\big)$\n",
    "\n",
    "for any $\\mathbf x$, (subject to $\\Vert \\mathbf x \\Vert_2 = 1$) including $\\mathbf x$ that maximizes the right hand side, which gives an answer $= \\lambda \\big(\\mathbf A\\circ \\mathbf B\\big)_{max}$.  Hence we know that the left hand side has at least one solution (read: eigenvalue associated with $\\lambda \\big(\\mathbf I\\circ \\mathbf B\\big)_{max}$ times $\\lambda \\big(\\mathbf A\\big)_{max}$ ) that is greater than or equal to the right hand side.  \n",
    "\n",
    "from here we again notice that, subject to the constraint $\\Vert \\mathbf z \\Vert_2 = 1$\n",
    "\n",
    "max $\\mathbf z^H \\mathbf B \\mathbf z = \\lambda \\big(\\mathbf B\\big)_{max}  \\geq $ max $\\mathbf e_k^H \\mathbf B \\mathbf e_k = \\lambda \\big(\\mathbf I \\circ \\mathbf B\\big)_{max}$\n",
    "\n",
    "recalling that $\\bigg[\\begin{array}{c|c|c|c}\n",
    "\\mathbf e_1 & \\mathbf  e_2 &\\cdots &\\mathbf  e_n\n",
    "\\end{array}\\bigg] = \\mathbf I$\n",
    "\n",
    "and we conclude with\n",
    "\n",
    "$\\lambda \\big(\\mathbf A\\big)_{max} *\\lambda \\big(\\mathbf B\\big)_{max}  \\geq \\lambda \\big(\\mathbf A\\big)_{max} *\\lambda \\big(\\mathbf I\\circ \\mathbf B\\big)_{max} \\geq \\lambda \\big(\\mathbf A\\circ \\mathbf B\\big)_{max}$\n",
    "\n",
    "*end alternative proof*  \n",
    "\n",
    "*commentary*\n",
    "\n",
    "It is interesting to think about the eigenvalues of $\\mathbf B$ and $\\big(\\mathbf I \\circ \\mathbf B\\big)$.  We can use Gerschgorin discs (which we can flatten to line segments if we want because we know the eigenvalues are real) to bound the eigenvalues of $\\mathbf B$, noting that the center point of the discs are given exactly by $\\big(\\mathbf I \\circ \\mathbf B\\big)$.  \n",
    "\n",
    "Let's suppose that $\\mathbf B \\neq \\big(\\mathbf I \\circ \\mathbf B\\big)$ and that there are no zeros along the diagonal.  \n",
    "\n",
    "Then we know that these matrices eigenvalues sum to be the same amount:\n",
    "\n",
    "$trace\\big(\\mathbf B\\big) = trace\\big(\\mathbf I \\circ \\mathbf B\\big)$\n",
    "\n",
    "since $\\big(\\mathbf I \\circ \\mathbf B\\big)$ is diagonal, we know its determinant is the product of those entries (which is $\\gt 0$ because we assume no zeros on diagonal). From applying the Hadamard Inequality, we know that \n",
    "\n",
    "$det\\big(\\mathbf B\\big) \\leq det\\big(\\mathbf I \\circ \\mathbf B\\big)$\n",
    "\n",
    "However, we also know that if we square both sides, the left hand side will have a higher trace:\n",
    "\n",
    "$trace\\big(\\mathbf B^2\\big) = trace\\big(\\mathbf B^H \\mathbf B \\big) = \\Vert \\mathbf B\\Vert_F^2 \\geq \\Vert \\big( \\mathbf I \\circ  \\mathbf B\\big) \\Vert_F^2 = trace\\Big(\\big(\\mathbf I \\circ \\mathbf B\\big)^2\\Big)$\n",
    "\n",
    "This would seem to suggest that that $\\mathbf B$ has more extreme eigenvalues than $\\big(\\mathbf I \\circ \\mathbf B\\big)$, that when you add them all up, they are the same, but when you multiply them, you get a smaller product.  However, when you square them, the small ones decrease, but in the spirit of Jensen's Inequality, we'd observe that that $\\frac{1}{2}(\\lambda_1^2 + \\lambda_n^2) \\geq (\\frac{1}{2}(\\lambda_1 + \\lambda_n))^2$\n",
    "\n",
    "And this is what our quadratic form tells us, again, where $\\Vert \\mathbf z \\Vert_2 = 1$  \n",
    "\n",
    "max $\\mathbf z^H \\mathbf B \\mathbf z = \\lambda \\big(\\mathbf B\\big)_{max}  \\geq $ max $\\mathbf e_k^H \\mathbf B \\mathbf e_k = \\lambda \\big(\\mathbf I \\circ \\mathbf B\\big)_{max}$\n",
    "\n",
    "and if we want to minimize the quadratic form, we see,\n",
    "\n",
    "min $\\mathbf z^H \\mathbf B \\mathbf z = \\lambda \\big(\\mathbf B\\big)_{min}  \\leq $ min $\\mathbf e_k^H \\mathbf B \\mathbf e_k = \\lambda \\big(\\mathbf I \\circ \\mathbf B\\big)_{min}$\n",
    "\n",
    "Because $\\mathbf e_k$'s are a proper subset of what we can choose our $\\mathbf z$ from, and hence our optimal $\\mathbf z$ must give a result at least as good as using $\\mathbf e_k$.  \n",
    "\n",
    "\n",
    "- - - - \n",
    "\n",
    "**claim:**  \n",
    "\n",
    "$trace\\big(\\mathbf A \\circ \\mathbf B\\big) \\leq trace\\big(\\mathbf A\\big) trace\\big(\\mathbf B\\big)$\n",
    "\n",
    "**proof:**  \n",
    "\n",
    "$trace\\big(\\mathbf A \\circ \\mathbf B\\big) = a_{1,1}*b_{1,1} + a_{2,2}*b_{2,2} +... + a_{n,n}*b_{n,n} \\leq (a_{1,1} + a_{2,2} +... + a_{n,n}) (b_{1,1} + b_{2,2} +... + b_{n,n}) = trace\\big(\\mathbf A\\big) trace\\big(\\mathbf B\\big)$\n",
    "\n",
    "because each diagonal entry $a_{k,k}$ and $b_{k,k}$ is real valued and non-negative, and hence all cross terms are real valued and non-negative.  \n",
    "\n",
    "\n",
    "**claim:**  \n",
    "\n",
    "$trace\\big(\\mathbf A \\circ \\mathbf B\\big) \\leq \\frac{1}{2} trace\\big(\\mathbf A \\circ \\mathbf A + \\mathbf B \\circ \\mathbf B\\big)$\n",
    "\n",
    "**proof:**\n",
    "\n",
    "a very simple way to prove this claim is to notice that $\\mathbf C:= \\mathbf A - \\mathbf B$, is a Hermitian matrix.  And $\\mathbf C$, like any Hermitian matrix, must have real valued entries on its diagonal (or else they could not be equal to their conjugate, and we would not have $\\mathbf C = \\mathbf C^H$).  Thus when we square the real valued entries along the diagonal, we see $\\mathbf c_{k,k} * \\mathbf c_{k,k} \\geq 0$.  \n",
    "\n",
    "Hence we can say:\n",
    "\n",
    "$trace\\big(\\mathbf C \\circ \\mathbf C\\big) \\geq 0$\n",
    "\n",
    "$trace\\Big(\\big(\\mathbf A - \\mathbf B\\big) \\circ \\big(\\mathbf A - \\mathbf B\\big)\\Big) \\geq 0$\n",
    "\n",
    "$trace\\Big( \\mathbf A \\circ \\mathbf A + \\mathbf B \\circ \\mathbf B - 2 *\\mathbf A \\circ \\mathbf B\\Big) \\geq 0$\n",
    "\n",
    "$trace\\Big( \\mathbf A \\circ \\mathbf A + \\mathbf B \\circ \\mathbf B\\Big) - 2* trace\\Big(\\mathbf A \\circ \\mathbf B\\Big) \\geq 0$\n",
    "\n",
    "$trace\\Big( \\mathbf A \\circ \\mathbf A + \\mathbf B \\circ \\mathbf B\\Big) \\geq 2* trace\\Big(\\mathbf A \\circ \\mathbf B\\Big) $\n",
    "\n",
    "\n",
    "$\\frac{1}{2} trace\\Big( \\mathbf A \\circ \\mathbf A + \\mathbf B \\circ \\mathbf B\\Big) \\geq trace\\Big(\\mathbf A \\circ \\mathbf B\\Big) $\n",
    "\n",
    "which completes the proof\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [py35]",
   "language": "python",
   "name": "Python [py35]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
