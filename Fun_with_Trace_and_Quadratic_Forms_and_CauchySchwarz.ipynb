{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Proof of Cauchy Schwarz\n",
    "\n",
    "the underlying scalars are in $\\mathbb C$ and we have $n $ x $1$ vectors. \n",
    "\n",
    "consider some vector $\\mathbf x =  \\begin{bmatrix}\n",
    "x_1\\\\ \n",
    "x_2\\\\ \n",
    "\\vdots \\\\ \n",
    "x_n\n",
    "\\end{bmatrix}$\n",
    "\n",
    "and some other vector,  $\\mathbf y =  \\begin{bmatrix}\n",
    "y_1\\\\ \n",
    "y_2\\\\ \n",
    "\\vdots \\\\ \n",
    "y_n\n",
    "\\end{bmatrix}$\n",
    "\n",
    "now consider the outer product given by \n",
    "\n",
    "$\\mathbf {xy}^H =  \\begin{bmatrix}\n",
    "x_1 \\bar{y_1} & x_1 \\bar{y_2} &...& x_1 \\bar{y_n}\\\\ \n",
    "x_2 \\bar{y_1} & x_2 \\bar{y_2} & ... & x_2 \\bar{y_n} \\\\ \n",
    "\\vdots & \\vdots & \\ddots &\\vdots \\\\ \n",
    "x_n \\bar{y_1} & x_n \\bar{y_2} &.... & x_n \\bar{y_n}\n",
    "\\end{bmatrix}$\n",
    "\n",
    "\n",
    "From here, consider where we want the squared Frobenius norm of $\\mathbf {xy}^H$ \n",
    "\n",
    "$\\big\\vert\\big\\vert \\mathbf {xy}^H\\big\\vert\\big\\vert_{F}^2 = \\text{trace}\\Big(\\big(\\mathbf {xy}^H\\big)^H \\big( \\mathbf {xy}^H\\big)\\Big) = \\text{trace}\\Big( \\mathbf {yx}^H \\mathbf {xy}^H\\Big) = \\text{trace}\\Big( \\mathbf{y}^H \\mathbf{yx}^H \\mathbf {x}\\Big) = \\mathbf {y}^H\\mathbf y \\mathbf x^H \\mathbf {x}  = \\big\\vert\\big\\vert \\mathbf{y}\\big\\vert\\big\\vert_{2}^2\\big\\vert\\big\\vert \\mathbf {x}\\big\\vert\\big\\vert_{2}^2$\n",
    "\n",
    "where the middle equalities made use of the cyclic property of the trace\n",
    "\n",
    "We conclude the proof with the following:\n",
    "\n",
    "$ \\big\\vert\\big\\vert \\mathbf{y}\\big\\vert\\big\\vert_{2}^2\\big\\vert\\big\\vert \\mathbf {x}\\big\\vert\\big\\vert_{2}^2 =  \\text{trace}\\Big(\\big(\\mathbf {\\mathbf {xy}}^H\\big)^H \\big( \\mathbf {\\mathbf {xy}}^H\\big)\\Big) \n",
    " \\geq \\big \\vert \\text{trace}\\Big( \\big(\\mathbf {xy}^H\\big) \\big(\\mathbf {xy}^H\\big)\\Big)\\big \\vert =  \\big \\vert \\text{trace}\\Big( \\mathbf y^H \\mathbf {xy}^H\\mathbf {x}\\Big)\\big \\vert  = \\big \\vert \\text{trace}\\Big( \\big(\\mathbf y^H \\mathbf {x}\\big) \\big(\\mathbf y^H \\mathbf {x}\\big)\\Big)\\big \\vert = \\big \\vert \\mathbf y^H \\mathbf x \\big \\vert^2 $\n",
    "\n",
    "hence \n",
    "\n",
    "$\\big\\vert\\big\\vert \\mathbf{y}\\big\\vert\\big\\vert_{2}^2\\big\\vert\\big\\vert \\mathbf {x}\\big\\vert\\big\\vert_{2}^2 = \\Big(\\Sigma_{i=1}^{n}\\big\\vert y_i\\big\\vert^2\\Big) \\Big(\\Sigma_{i=1}^{n}\\big\\vert x_i\\big\\vert^2\\Big)  \\geq \\big \\vert\\Big(\\Sigma_{i=1}^{n}x_i \\bar{y_i}\\Big)^2\\big \\vert= \\big \\vert \\mathbf y^H \\mathbf x \\big \\vert^2 $\n",
    "\n",
    "\n",
    "noting that \n",
    "\n",
    "$\\text{trace}\\Big(\\big(\\mathbf {\\mathbf {xy}}^H\\big)^H \\big( \\mathbf {\\mathbf {xy}}^H\\big)\\Big)\n",
    "\\geq \\big \\vert \\text{trace}\\Big( \\big(\\mathbf {xy}^H\\big) \\big(\\mathbf {xy}^H\\big)\\Big)\\big \\vert $\n",
    "\n",
    "via the Schur Inequality in the special case of a rank one matrix (and hence triangle inequality is *not* required here).  \n",
    "\n",
    "- - - - \n",
    "\n",
    "**extension:**  \n",
    "Suppose we we relax the requirement that an inner product is positive definite, and instead allow it to be positive semi-definite.  (Sometimes this may be referred to as a semi inner product.)\n",
    "\n",
    "\n",
    "for some Hermitian positive semi-definite $\\mathbf A$:   \n",
    "$\\langle \\mathbf x, \\mathbf y \\rangle = \\mathbf x^H \\mathbf A \\mathbf y =  \\big(\\mathbf A^{\\frac{1}{2}} \\mathbf x\\big)^H \\big(\\mathbf A^{\\frac{1}{2}}\\mathbf y\\big)$\n",
    "\n",
    "\n",
    "$\\mathbf b:= \\mathbf A^{\\frac{1}{2}} \\mathbf x$  \n",
    "$\\mathbf c:= \\mathbf A^{\\frac{1}{2}} \\mathbf y$  \n",
    "\n",
    "$\\mathbf \\langle \\mathbf x, \\mathbf y \\rangle = \\mathbf b^H \\mathbf c$\n",
    "\n",
    "and we verify that Cauchy's Inequality still holds by repeating the above argument: \n",
    "\n",
    "$\\langle \\mathbf y, \\mathbf y \\rangle \\langle \\mathbf x, \\mathbf x \\rangle =  \\big(\\mathbf{c}^H\\mathbf c\\big)\\big(\\mathbf b^H\\mathbf b\\big) =  \\text{trace}\\Big(\\big(\\mathbf {\\mathbf {bc}}^H\\big)^H \\big( \\mathbf {\\mathbf {bc}}^H\\big)\\Big) \n",
    " \\geq \\big \\vert \\text{trace}\\Big( \\big(\\mathbf {bc}^H\\big)^2 \\Big)\\big \\vert =  \\big \\vert \\text{trace}\\Big( \\mathbf c^H \\mathbf {bc}^H\\mathbf {b}\\Big)\\big \\vert  = \\big \\vert \\mathbf c^H \\mathbf b \\big \\vert^2 = \\big \\vert \\mathbf b^H \\mathbf c \\big \\vert^2  = \\big \\vert \\langle \\mathbf x, \\mathbf y \\rangle \\big \\vert^2 $\n",
    "\n",
    "again by applying Schur's Inequality to the special case of a rank one matrix, we generate the above inequality, i.e. that $\\text{trace}\\Big(\\big(\\mathbf {\\mathbf {bc}}^H\\big)^H \\big( \\mathbf {\\mathbf {bc}}^H\\big)\\Big) \n",
    " \\geq \\big \\vert \\text{trace}\\Big( \\big(\\mathbf {bc}^H\\big)^2 \\Big)\\big \\vert $\n",
    "\n",
    "This extension confirms that Cauchy's Inequality still applies to (finite) inner products that have relaxed the positive definite criterion to positive semi-definite.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Traditional Cauchy Schwarz, over Reals:**  \n",
    "\n",
    "$\\big \\Vert \\mathbf x - \\mathbf y\\big \\Vert_2 \\geq 0$ \n",
    "\n",
    "by construction, the 2 norm add up real non-negative values and as a result is real non-negative\n",
    "\n",
    "$\\big \\Vert \\mathbf x - \\mathbf y\\big \\Vert_2^2 = \\big \\Vert \\mathbf x \\big \\Vert_2^2 - \\mathbf x^T \\mathbf y - \\mathbf y^T \\mathbf x + \\big \\Vert \\mathbf y\\big \\Vert_2^2 \\geq 0$ \n",
    "\n",
    "$\\big \\Vert \\mathbf x \\big \\Vert_2^2 + \\big \\Vert \\mathbf y\\big \\Vert_2^2 \\geq 2 \\mathbf y^T \\mathbf x $ \n",
    "\n",
    "$\\mathbf y^T \\mathbf x \\leq \\frac{1}{2}\\big(\\big \\Vert \\mathbf x \\big \\Vert_2^2 + \\big \\Vert \\mathbf y\\big \\Vert_2^2\\big) $ \n",
    "\n",
    "repeat argument and rescale by $\\alpha, \\gamma$ to get lengths of one for each vector on RHS (assuming each vector is non-zero -- if either vector is zero, the inequality follows by inspection)\n",
    "\n",
    "$\\big \\Vert \\alpha \\mathbf x - \\gamma \\mathbf y\\big \\Vert_2^2 = \\big \\Vert \\alpha^2  \\mathbf x \\big \\Vert_2^2 - \\mathbf \\alpha \\gamma  \\mathbf x^T \\mathbf y - \\gamma \\alpha \\mathbf y^T  \\mathbf x + \\big \\Vert \\mathbf \\gamma^2 \\mathbf y\\big \\Vert_2^2 \\geq 0$ \n",
    "\n",
    "$\\gamma \\alpha \\mathbf y^T \\mathbf x \\leq \\big \\vert \\gamma \\alpha \\mathbf y^T \\mathbf x\\big \\vert = \\gamma \\alpha \\big \\vert  \\mathbf y^T \\mathbf x\\big \\vert  \\leq \\big \\vert\\frac{1}{2}\\big(1 +1 \\big)\\big \\vert = \\frac{1}{2}\\big(1 +1 \\big) =1$ \n",
    "\n",
    "discover that $\\big \\Vert \\alpha^2  \\mathbf x \\big \\Vert_2^2 = \\alpha^2 \\big \\Vert   \\mathbf x \\big \\Vert_2^2 = 1 \\to \\alpha := \\frac{1}{\\big \\Vert   \\mathbf x \\big \\Vert_2}$\n",
    "\n",
    "and by nearly identical argument: \n",
    "\n",
    "$\\gamma := \\frac{1}{\\big \\Vert y\\big \\Vert_2}$\n",
    "\n",
    "$\\gamma \\alpha \\mathbf y^T \\mathbf x  \\leq \\frac{1}{2}\\big(1 +1 \\big) = 1\\to \\mathbf y^T \\mathbf x = \\frac{1}{\\gamma}\\frac{1}{\\alpha}  = \\big \\Vert \\mathbf x \\big \\Vert_2 \\big \\Vert \\mathbf y \\big \\Vert_2 $ \n",
    "\n",
    "some additional clever maneuvering is needed to accomodate complex numbers using this approach, however.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Another Cauchy Schwarz proof, using determinants, over complex scalar field** \n",
    "\n",
    "$\\mathbf Z := \\begin{bmatrix}\n",
    "\\mathbf x & \\mathbf y\n",
    "\\end{bmatrix}$\n",
    "\n",
    "where $\\mathbf Z$ is $n$ x $2$, for some natural number $n \\geq 2$\n",
    "\n",
    "consider the matrix \n",
    "\n",
    "$\\mathbf A := \\mathbf Z^H \\mathbf Z = \\begin{bmatrix}\n",
    "\\mathbf x^H \\mathbf x & \\mathbf x^H\\mathbf y \\\\ \n",
    "\\mathbf y^H\\mathbf x & \\mathbf y^H \\mathbf y\n",
    "\\end{bmatrix}$ \n",
    "\n",
    "*First: Examine the equality conditions:*  \n",
    "\n",
    "note that $rank(\\mathbf A) = 2$ unless $\\mathbf x = \\eta \\mathbf y$ -- this immediately gives the required equality conditions -- i.e. if the two vectors aren't scalar multiples of each other then \n",
    "\n",
    "$\\text{det}\\big(\\mathbf A\\big) \\neq 0$, i.e. \n",
    "\n",
    "$\\big(\\mathbf x^H \\mathbf x\\big)\\big(\\mathbf y^H \\mathbf y\\big) -\\big( \\mathbf x^H\\mathbf y \\big) \\big(\\mathbf y^H\\mathbf x\\big) = \\big(\\mathbf x^H \\mathbf x\\big)\\big(\\mathbf y^H \\mathbf y\\big) -\\big( \\mathbf y^H\\mathbf x \\big)^H \\big(\\mathbf y^H\\mathbf x\\big) = \\big \\Vert \\mathbf x \\big \\Vert_2^2 \\big \\Vert \\mathbf y\\big \\Vert_2^2 - \\big \\vert \\mathbf y^H\\mathbf x \\big \\vert^2  \\neq 0 $\n",
    "\n",
    "which can be rewritten as  \n",
    "\n",
    "$\\big \\Vert \\mathbf x \\big \\Vert_2^2 \\big \\Vert \\mathbf y\\big \\Vert_2^2  \\neq \\big \\vert \\mathbf y^H\\mathbf x \\big \\vert^2 = \\big \\vert \\mathbf x^H\\mathbf y \\big \\vert^2 $\n",
    "\n",
    "hence we have an equality if and only if $\\mathbf Z$ has linearly dependent columns aka $\\mathbf A$ is singular, aka when $\\text{det}\\big(\\mathbf A\\big)=0$.  \n",
    "\n",
    "\n",
    "*Second: The strict inequality*  \n",
    "for convenience, we focus on the strict inequality (i.e. non-singular $\\mathbf A$ aka 2 linearly independent columns in $\\mathbf Z$) in what follows:   \n",
    "\n",
    "We are in an inner product space, but can't yet take for granted the triangle inequality (as it implies Cauchy Schwarz).  We can verify by inspection / take for granted the positive definiteness of the 2 norm, i.e. for any $\\mathbf b$ \n",
    "\n",
    "$\\big \\Vert \\mathbf b\\big \\Vert_2^2 = \\mathbf b^H \\mathbf b \\geq 0$ with equality **iff** $\\mathbf b = \\mathbf 0$.  \n",
    "\n",
    "we equivalently confirm that that \n",
    "\n",
    "$\\mathbf c^H \\mathbf A \\mathbf c \\gt 0$ for any $\\mathbf c \\neq \\mathbf 0$\n",
    "\n",
    "- - - -\n",
    "because \n",
    "\n",
    "first for any $\\mathbf c \\neq \\mathbf 0$:  \n",
    "\n",
    "$\\mathbf b:= \\mathbf Z\\mathbf c \\neq \\mathbf 0$\n",
    "\n",
    "Note: given that $\\mathbf Z$ has 2 linearly independent columns, $\\mathbf b = \\mathbf 0$ **iff** $\\mathbf c = \\mathbf 0$\n",
    "\n",
    "second: repeating the argument, using positive definiteness:  \n",
    "\n",
    "$\\mathbf c^H \\mathbf A \\mathbf c = \\mathbf c^H \\mathbf Z^H \\mathbf Z \\mathbf c =  \\big(\\mathbf c^H \\mathbf Z^H\\big) \\big(\\mathbf Z \\mathbf c\\big) =  \\big(\\mathbf {Zc}\\big)^H \\big(\\mathbf Z \\mathbf c\\big)= \\mathbf b^H \\mathbf b = \\big \\Vert \\mathbf b\\big\\Vert_2^2 \\gt 0$ for any $\\mathbf c \\neq \\mathbf 0$\n",
    "\n",
    "note that the above tells us that the quadratic form given by \n",
    "\n",
    "$\\mathbf c^H \\mathbf A \\mathbf c $ is positive for *all* $\\mathbf c \\neq \\mathbf 0$, which means several things, including that the quadratic form is always real valued.  \n",
    "- - - - \n",
    "*why eigenvalues must be positive*  \n",
    "\n",
    "We know that $\\mathbf A$ has eigenvalues given by solving $\\text{det}\\big(\\mathbf A - \\lambda \\mathbf I\\big) = 0$.  Note: we don't need fundamental theorem of algebra or anything particularly sophisticated to know this -- we merely need to know how to use the quadratic formula.  \n",
    "\n",
    "\n",
    "Specifically we know that for each eigenvalue $\\lambda_k$, we have \n",
    "\n",
    "$\\lambda_k \\big \\Vert \\mathbf v_k \\big \\Vert_2^2= \\mathbf v_k^H \\mathbf{A v}_k \\gt 0$ \n",
    "\n",
    "dividing both sides by $\\big \\Vert \\mathbf v_k \\big \\Vert_2^2$ which is postive, by since we know that $\\mathbf v_k \\neq \\mathbf 0$ and the positive definteness of the 2 norm, giving us \n",
    "\n",
    "$\\lambda_k \\gt 0$, i.e. each eigenvalue must be real valued and positive.  \n",
    "- - - -\n",
    "Thus when $rank\\big(\\mathbf A\\big) =2$   \n",
    "\n",
    "$\\text{det}\\big(\\mathbf A\\big) = \\lambda_1 \\lambda_2 \\gt 0$\n",
    "\n",
    "$\\text{det}\\big(\\mathbf A\\big) =\\big(\\mathbf x^H \\mathbf x\\big)\\big(\\mathbf y^H \\mathbf y\\big) -\\big( \\mathbf x^H\\mathbf y \\big) \\big(\\mathbf y^H\\mathbf x\\big) = \\big \\Vert \\mathbf x \\big \\Vert_2^2 \\big \\Vert \\mathbf y\\big \\Vert_2^2 - \\big \\vert \\mathbf x^H\\mathbf y \\big \\vert^2   \\gt 0$\n",
    "\n",
    "which gives us the strict inequality\n",
    "\n",
    "$\\big \\Vert \\mathbf x \\big \\Vert_2^2 \\big \\Vert \\mathbf y\\big \\Vert_2^2 \\gt \\big \\vert \\mathbf x^H\\mathbf y \\big \\vert^2$ \n",
    "\n",
    "when there is no $\\eta$ such that $\\mathbf x = \\eta \\mathbf y$ \n",
    "\n",
    "and putting everything together:  \n",
    "\n",
    "$\\big \\Vert \\mathbf x \\big \\Vert_2^2 \\big \\Vert \\mathbf y\\big \\Vert_2^2 \\geq \\big \\vert \\mathbf x^H\\mathbf y \\big \\vert^2$ \n",
    "\n",
    "or taking advantage of positivity we can square root both sides:  \n",
    "\n",
    "$\\big \\Vert \\mathbf x \\big \\Vert_2 \\big \\Vert \\mathbf y\\big \\Vert_2 \\geq \\big \\vert \\mathbf x^H\\mathbf y \\big \\vert$  \n",
    "\n",
    "with equality **iff**  there is some $\\eta$ such that $\\mathbf x = \\eta \\mathbf y$  \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A better approach to Triangle Inequality:**   \n",
    "a technique discussed in *Cauchy-Schwarz Masterclass* (which easily generalizes to Minkowski Triangle Inequality if we replace the below with Hoelder's Inequality instead of Cauchy-Schwarz)  \n",
    "\n",
    "*define* \n",
    "the 2 norm of some vector $\\mathbf z \\in \\mathbb R^n$ as   \n",
    "$\\big \\Vert \\mathbf z \\big \\Vert_2 =  \\langle \\mathbf w^*, \\mathbf z\\rangle$  \n",
    "where $\\mathbf w^*$ is the vector that maximize the above inner product (given the below constraint)  \n",
    "subject to $\\big(\\sum_{i=1}^n \\big \\vert w_i^*\\big \\vert^2\\big)^\\frac{1}{2} = 1$   \n",
    "(Cauchy-Schwarz tells us that $\\mathbf w^* \\propto \\mathbf z$ for this maximization)\n",
    "\n",
    "now consider the case of \n",
    "$\\mathbf z := \\mathbf x + \\mathbf y$  \n",
    "\n",
    "$\\big \\Vert \\mathbf z \\big \\Vert_2 $  \n",
    "$=\\langle \\mathbf w_0^*, \\mathbf z\\rangle$  \n",
    "$=\\langle \\mathbf w_0^*, \\mathbf x + \\mathbf y\\rangle$  \n",
    "$= \\langle \\mathbf w_0^*, \\mathbf x \\rangle + \\langle \\mathbf w_0^*, \\mathbf y \\rangle$  \n",
    "$\\leq \\langle \\mathbf w_1^*, \\mathbf x \\rangle + \\langle \\mathbf w_2^*, \\mathbf y \\rangle $  \n",
    "$=\\big \\Vert \\mathbf x \\big \\Vert_2 + \\big \\Vert \\mathbf y \\big \\Vert_2$   \n",
    "\n",
    "because 2 choices of maximization are at least as good as one  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This self generalizes to the case of the 2 norm of some vector $\\mathbf z \\in \\mathbb C^n$ as   \n",
    "$\\big \\Vert \\mathbf z \\big \\Vert_2 :=  \\big \\vert \\langle \\mathbf w^*, \\mathbf z\\rangle \\big \\vert$  \n",
    "where $\\mathbf w^*$ is the vector that maximize the above inner product (given the below constraint)  \n",
    "subject to $\\big(\\sum_{i=1}^n \\big \\vert w_i^*\\big \\vert^2\\big)^\\frac{1}{2} = 1$   \n",
    "(Cauchy-Schwarz tells us that $\\mathbf w^* \\propto \\mathbf z$ for this maximization)\n",
    "\n",
    "now consider the case of \n",
    "$\\mathbf z := \\mathbf x + \\mathbf y$  \n",
    "\n",
    "$\\big \\Vert \\mathbf z \\big \\Vert_2 $  \n",
    "$=\\big \\vert \\langle \\mathbf w_0^*, \\mathbf z\\rangle \\big \\vert$  \n",
    "$=\\big \\vert \\langle \\mathbf w_0^*, \\mathbf x + \\mathbf y\\rangle \\big \\vert$  \n",
    "$= \\big \\vert \\langle \\mathbf w_0^*, \\mathbf x \\rangle + \\langle \\mathbf w_0^*, \\mathbf y \\rangle \\big \\vert$  \n",
    "$\\leq \\big \\vert \\langle \\mathbf w_1^*, \\mathbf x \\rangle+ \\langle\\mathbf w_2^*, \\mathbf y \\rangle \\big \\vert $  \n",
    "$\\leq \\big \\vert \\langle \\mathbf w_1^*, \\mathbf x \\rangle\\big \\vert  + \\big \\vert \\langle \\mathbf w_2^*, \\mathbf y \\rangle \\big \\vert $  \n",
    "$=\\big \\Vert \\mathbf x \\big \\Vert_2 + \\big \\Vert \\mathbf y \\big \\Vert_2$   \n",
    "\n",
    "by (i) because 2 choices of maximization are at least as good as one and (ii) the triangle inequality applied to the addition of two complex scalars    \n",
    "\n",
    "For avoidance of doubt, (ii) is implied by the previous result on real triangle inequality.  That is, for purposes of addition, we can treat any complex number as a vector space in $\\mathbb R^2$  and using the standard basis vectors observe that \n",
    "\n",
    "$\\big \\vert (a^{(0)} + b^{(0)}i) + (a^{(1)} + b^{(1)}i)\\big \\vert $  \n",
    "$=  \\big \\Vert \\big(a^{(0)}\\mathbf e_1 + b^{(0)} \\mathbf e_2\\big) + \\big(a^{(1)}\\mathbf e_1 + b^{(1)}\\mathbf e_2\\big)\\big \\Vert_2$  \n",
    "$=  \\big \\Vert \\mathbf a^{(0)} + \\mathbf a^{(1)}\\big \\Vert_2$  \n",
    "$\\leq  \\big \\Vert \\mathbf a^{(0)}\\big \\Vert_2 + \\big \\Vert \\mathbf a^{(1)}\\big \\Vert_2$  \n",
    "$= \\big \\vert a^{(0)} + b^{(0)}i \\big \\vert + \\big \\vert a^{(1)} + b^{(1)}i\\big \\vert $   \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alternative Approach :  \n",
    "\n",
    "note that instead of using the above methods,  consider the maximum eigenvalue problem, where we have a rank one matrix, $\\mathbf B$.\n",
    "\n",
    "\n",
    "$\\mathbf B = \\mathbf{xy}^H$.  While we know that $\\mathbf B$ is a rank one matrix, the argument to be made is even more general: the magnitude of the largest eigenvalue of $\\big(\\mathbf {BB}\\big)$ is $\\leq$ the largest eigenvalue of $\\mathbf B^H \\mathbf B$, or equivalently, the magnitude of the largest eigenvalue of $\\mathbf B$ ($\\lambda_1$) is $\\leq$ the largest singular value of $\\mathbf B$ ($\\sigma_1$).\n",
    "\n",
    "The approach taken here uses quadratic forms.  So consider the case of maximizing $\\mathbf B^H \\mathbf B$ vs $\\mathbf {BB}$\n",
    "\n",
    "max $\\big \\vert \\mathbf v^H \\mathbf B^H \\mathbf B \\mathbf v \\big \\vert \\geq$ max $\\big \\vert \\mathbf v^H \\mathbf{BB}\\mathbf v \\big \\vert$\n",
    "\n",
    "where we constrain the length (2 norm) of $\\mathbf v$, which for simplicity will be one: $\\mathbf v^H \\mathbf v = 1 = \\big \\vert \\big \\vert \\mathbf v \\big \\vert \\big \\vert_2^{2} = \\big \\vert \\big \\vert \\mathbf v \\big \\vert \\big \\vert_2$ \n",
    "\n",
    "We know via diagonalization arguments (and Lagrange Multipliers), that some quadratic form $\\big \\vert \\mathbf v^H \\mathbf C \\mathbf v\\big \\vert$ subject to $\\mathbf v ^H \\mathbf v = 1$ is maximized when all of $\\mathbf v$ is allocated to the eigenvalue(s) with the largest magnitude of the Hermitian matrix $\\mathbf C$.  Unfortunately, we have no reason to believe $\\mathbf {BB}$ is Hermitian or even non-defective, which complicates things a bit.  But consider having well ordered eigenvalues for $\\mathbf B$ where $\\big \\vert \\lambda_1 \\big \\vert \\geq \\big \\vert\\lambda_2 \\big \\vert\\geq \\big \\vert \\lambda_3 \\big \\vert \\geq ... \\geq \\big \\vert\\lambda_n \\big \\vert$, with associated eigenvectors $\\mathbf v_1, \\mathbf v_2, \\mathbf v_3, ... , \\mathbf v_n$.  Note that there is a simple argument which tells us that allocating to eigenvector $\\mathbf v_k$ where $k \\geq 2$ is (weakly) dominated by $\\mathbf v_1$.  \n",
    "\n",
    "$\\big \\vert \\mathbf v_1^H \\mathbf{BB}\\mathbf v_1 \\big \\vert \\geq \\big \\vert \\mathbf v_k^H \\mathbf{BB}\\mathbf v_k \\big \\vert$\n",
    "\n",
    "$\\big \\vert \\mathbf v_1^H \\mathbf{B}\\big(\\mathbf B \\mathbf v_1\\big) \\big \\vert \\geq \\big \\vert \\mathbf v_k^H \\mathbf{B}\\big( \\mathbf B \\mathbf v_k \\big) \\big \\vert$\n",
    "\n",
    "$\\big \\vert \\mathbf v_1^H \\mathbf{B} \\lambda_1 \\mathbf v_1 \\big \\vert \\geq \\big \\vert\\mathbf v_k^H \\mathbf{B}\\lambda_k \\mathbf v_k \\big \\vert$\n",
    "\n",
    "$\\big \\vert \\lambda_1 \\mathbf v_1^H \\big(\\mathbf{B} \\mathbf v_1\\big) \\big \\vert \\geq \\big \\vert \\lambda_k \\mathbf v_k^H \\big(\\mathbf{B} \\mathbf v_k\\big) \\big \\vert$\n",
    "\n",
    "$\\big \\vert \\lambda_1 \\mathbf v_1^H \\lambda_1 \\mathbf v_1 \\big \\vert \\geq \\big \\vert \\lambda_k \\mathbf v_k^H \\lambda_k \\mathbf v_k \\big \\vert$\n",
    "\n",
    "$\\big \\vert \\lambda_1^2 (\\mathbf v_1^H \\mathbf v_1) \\big \\vert \\geq \\big \\vert \\lambda_k^2 (\\mathbf v_k^H \\mathbf v_k) \\big \\vert$\n",
    "\n",
    "$\\big \\vert \\lambda_1^2 \\cdot1\\big \\vert \\geq \\big \\vert \\lambda_k^2 \\cdot 1 \\big \\vert$  \n",
    "$\\big \\vert \\lambda_1^2\\big \\vert \\geq \\big \\vert\\lambda_k^2 \\big \\vert$  \n",
    "$\\big \\vert \\lambda_1\\big \\vert^2 \\geq \\big \\vert\\lambda_k \\big \\vert^2$  \n",
    "$\\big \\vert \\lambda_1\\big \\vert \\geq \\big \\vert\\lambda_k \\big \\vert$   \n",
    "\n",
    "hence we have a simple exchange argument that tells us any time we allocate to $\\mathbf v_k$ we can get a result greater than or equal to it, by allocating that amount instead to $\\mathbf v_1$.  Thus, with respect to a maximization problem using the eigenvectors of $\\mathbf B$, we can do no better than choosing the eigenpair $\\lambda_1, \\mathbf v_1$.\n",
    "\n",
    "We return to our original equation, with respect to eigenvalues:\n",
    "\n",
    "max $\\big \\vert  \\mathbf v^H \\mathbf B^H \\mathbf B \\mathbf v \\big \\vert \\geq$ max $\\big \\vert \\mathbf v_1^H \\mathbf{BB}\\mathbf v_1 \\big \\vert$\n",
    "\n",
    "max $\\big \\vert \\mathbf v^H \\mathbf B^H \\mathbf B \\mathbf v \\big \\vert = $ max $ \\mathbf v^H \\mathbf B^H \\mathbf B \\mathbf v  \\geq \\big \\vert \\lambda_1^2\\big \\vert $\n",
    "\n",
    "note that we always have the option / backup plan, on the left hand side, of also allocating to $\\mathbf v_1$.  Put differently, if we are lazy, we know that by setting $\\mathbf v := \\mathbf v_1$ we'll always get a 'payoff' with magnitude equal to $\\big\\vert\\lambda_1\\big\\vert^2$ -- thus when maximizing the magnitude of $\\big \\vert \\mathbf v^H \\mathbf B^H \\mathbf B \\mathbf v \\big \\vert$ we'll get a result at least as good as $\\big\\vert\\lambda_1\\big\\vert^2$.  Symbolically, this is shown below.\n",
    "\n",
    "$\\mathbf v_1^H \\mathbf B^H \\mathbf B \\mathbf v_1 = \\big \\vert \\lambda_1\\big \\vert^2 $  \n",
    "\n",
    "$\\big(\\mathbf v_1^H \\mathbf B^H\\big) \\big(\\mathbf B \\mathbf v_1 \\big) = \\big \\vert \\lambda_1\\big \\vert^2 $  \n",
    "\n",
    "$\\big(\\mathbf {B}\\mathbf v_1\\big)^H \\big(\\mathbf B \\mathbf v_1 \\big) = \\big \\vert \\lambda_1 \\big \\vert^2 $  \n",
    "$\\big(\\lambda_1 \\mathbf v_1\\big)^H \\big(\\lambda_1 \\mathbf v_1 \\big) = \\big \\vert \\lambda_1 \\big \\vert^2 $  \n",
    "$\\lambda_1^H \\lambda_1 \\big(\\mathbf v_1^H \\mathbf v_1\\big) = \\big \\vert \\lambda_1 \\big \\vert^2 $  \n",
    "$\\lambda_1^H \\lambda_1 \\cdot 1  = \\big \\vert \\lambda_1\\big \\vert^2 $    \n",
    "$\\lambda_1^H \\lambda_1  = \\big \\vert \\lambda_1 \\big \\vert^2 $  \n",
    "\n",
    "\n",
    "- - - - \n",
    "*begin detour: reminder about complex number maths*   \n",
    "  \n",
    "Consider that we can simply note that $\\big \\vert \\lambda_1^2\\big \\vert = \\big \\vert \\lambda_1 \\big \\vert^2 = \\lambda_1^H \\lambda_1$.\n",
    "\n",
    "Alternatively, for a more granular view, consider the case where:  \n",
    "$\\lambda_1 = \\alpha - \\beta i $, where $\\alpha$ and $\\beta$ are real valued scalars. Accordingly, the magnitude of $\\lambda_1$ is $\\big\\vert \\lambda_1\\big\\vert = \\big(\\alpha^2 + \\beta^2\\big)^\\frac{1}{2}$. Then $\\lambda_1^2 = \\alpha^2 + \\beta^2  i^2 - 2\\alpha\\beta i = \\alpha^2 - \\beta^2 - 2\\alpha\\beta i$, with magnitude of \n",
    "\n",
    "$\\big \\vert \\lambda_1^2\\big \\vert = \\Big(\\big(\\alpha^2 - \\beta^2\\big)^2 + \\big( - 2\\alpha\\beta\\big)^2\\Big)^{\\frac{1}{2}}= \\Big(\\alpha^4 + \\beta^4 - 2 \\alpha^2 \\beta^2 + 4 \\alpha^2\\beta^2)\\Big)^{\\frac{1}{2}} $ \n",
    "\n",
    "$\\big \\vert \\lambda_1^2\\big \\vert = \\Big(\\alpha^4 + \\beta^4 + 2 \\alpha^2 \\beta^2 \\Big)^{\\frac{1}{2}}$\n",
    "\n",
    "and note that $\\lambda_1 ^H \\lambda_1 = \\alpha^2 + \\beta^2$, with magnitude equal to   \n",
    "\n",
    "$\\big \\vert \\lambda_1^H \\lambda_1 \\big \\vert = \\Big(\\big(\\alpha^2 + \\beta^2\\big)^2 + \\big(0\\big)^2 \\Big)^{\\frac{1}{2}} = \\Big(\\alpha^4 + \\beta^4 + 2 \\alpha \\beta \\Big)^{\\frac{1}{2}} = \\big \\vert \\lambda_1^2\\big \\vert  $\n",
    "\n",
    "$ \\big \\vert \\lambda_1^H \\lambda_1 \\big \\vert = \\lambda_1 ^H \\lambda_1 = \\alpha^2 + \\beta^2  = \\Big(\\big(\\alpha^2 + \\beta^2\\big)^\\frac{1}{2}\\Big)^2 = \\big\\vert \\lambda_1\\big\\vert ^2$  \n",
    "  \n",
    "*end detour* \n",
    "- - - - \n",
    "\n",
    "Thus when trying to maximize the magnitude of $\\mathbf v^H \\mathbf B^H \\mathbf B \\mathbf v$, we have a lower bound equal to $ \\big \\vert \\lambda_1 \\big\\vert^2$.  Equivalently, we can say: $\\sigma_1^2 \\geq \\big \\vert \\lambda_1 \\big\\vert^2$ and $\\sigma_1 \\geq \\big \\vert \\lambda_1 \\big\\vert$, where $\\sigma_1$ is the largest singular value of $\\mathbf B$ and thus $\\sigma_1^2$ is the largest eigenvalue of $\\big(\\mathbf B^H \\mathbf B\\big)$\n",
    "\n",
    "For a simple example of this fact, consider:\n",
    "\n",
    "$\\mathbf B = \\left[\\begin{matrix}2 & 3 & 4\\\\4 & 10 & -1\\\\1 & 3 & 4\\end{matrix}\\right]$\n",
    "\n",
    "where $\\lambda_1 \\approx 11.57$, but $\\sigma_1 \\approx 11.87$, hence $\\sigma_1$ exceeds the lower bound set by $\\lambda_1$ \n",
    "- - - -\n",
    "Back to the original problem at hand, in the special case where $\\mathbf B$ is a square, rank one matrix  \n",
    "\n",
    "$\\text{trace}\\big(\\mathbf B ^H \\mathbf B\\big) \\geq \\big \\vert \\text{trace}\\big(\\mathbf{BB}\\big) \\big \\vert$, because  \n",
    "\n",
    "$\\text{trace}\\big(\\mathbf B ^H \\mathbf B\\big) = \\sigma_1^2$ and $\\text{trace}\\big(\\mathbf{BB}\\big) = \\lambda_1^2 $, and we know $\\sigma_1^2 \\geq \\big \\vert \\lambda_1 \\big\\vert^2 = \\big \\vert \\lambda_1^2 \\big\\vert $\n",
    "\n",
    "set: $\\mathbf B:= \\mathbf{xy}^H$ and Cauchy Schwartz simply follows\n",
    "\n",
    "$ \\big\\vert\\big\\vert \\mathbf{y}\\big\\vert\\big\\vert_{2}^2\\big\\vert\\big\\vert \\mathbf {x}\\big\\vert\\big\\vert_{2}^2 = \\big ( \\mathbf y^H \\mathbf y \\big ) \\big ( \\mathbf x^H \\mathbf x \\big ) = \\text{trace}\\Big(\\big(\\mathbf {\\mathbf {xy}}^H\\big)^H \\big( \\mathbf {\\mathbf {xy}}^H\\big)\\Big) \n",
    "\\geq \\big \\vert \\text{trace}\\Big( \\big(\\mathbf {xy}^H\\big) \\big(\\mathbf {xy}^H\\big)\\Big) \\big \\vert =  \\big \\vert \\text{trace}\\Big( \\mathbf y^H \\mathbf {xy}^H\\mathbf {x}\\Big) \\big \\vert = \\big \\vert \\mathbf y^H \\mathbf x \\big \\vert^2 $\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Smallest Eigenvalues and Singular Values\n",
    "\n",
    "Note that the above analysis can be easily extended with respect to the magnitude of the smallest eigenvalue of $\\mathbf B$ and the smallest singular value of $\\mathbf B$, again where $\\mathbf B \\in \\mathbb C^{n x n}$.\n",
    "\n",
    "if we want to minimize $\\big(\\mathbf v_n^H \\mathbf B^H\\big) \\big(\\mathbf B \\mathbf v_n \\big) $ we always have the option of allocating to $\\lambda_n$\n",
    "\n",
    "recall our length constraint:  $\\big \\vert \\big \\vert \\mathbf v \\big \\vert \\big \\vert_2^{2} = 1$ \n",
    "\n",
    "\n",
    "$\\mathbf v_n^H \\mathbf B^H \\mathbf B \\mathbf v_n = \\big \\vert \\lambda_n^2\\big \\vert $  \n",
    "\n",
    "$\\big(\\mathbf v_n^H \\mathbf B^H\\big) \\big(\\mathbf B \\mathbf v_n \\big) = \\big \\vert \\lambda_n^2\\big \\vert $  \n",
    "\n",
    "$\\big(\\mathbf {B}\\mathbf v_n\\big)^H \\big(\\mathbf B \\mathbf v_n \\big) = \\big \\vert \\lambda_n^2 \\big \\vert $  \n",
    "$\\big(\\lambda_n \\mathbf v_n\\big)^H \\big(\\lambda_n \\mathbf v_1 \\big) = \\big \\vert \\lambda_n^2 \\big \\vert $  \n",
    "$\\lambda_n^H \\lambda_n \\big(\\mathbf v_1^H \\mathbf v_1\\big) = \\big \\vert \\lambda_n^2 \\big \\vert $  \n",
    "$\\lambda_n^H \\lambda_n \\cdot 1  = \\big \\vert \\lambda_n^2\\big \\vert $    \n",
    "$\\lambda_n^H \\lambda_n  = \\big \\vert \\lambda_n \\big \\vert^2 $  \n",
    "\n",
    "Since allocating everything to $\\sigma_n^2$ is the (weakly) dominant solution for minimizing $\\mathbf v^H \\mathbf B^H \\mathbf B \\mathbf v$, we can upper bound $\\sigma_n^2 \\leq \\big\\vert \\lambda_n\\big \\vert^2 $ and equivalently say that $\\sigma_n \\leq \\big\\vert \\lambda_n\\big \\vert $\n",
    "\n",
    "\n",
    "# Quadratic Forms and recovering a Frobenius Norm\n",
    "\n",
    "as usual, assume we have well ordered singular values\n",
    "\n",
    "$\\sigma_1 \\geq \\sigma_2 \\geq .... \\geq \\sigma_n \\geq 0$\n",
    "\n",
    "It is worth remarking that if we were to do our optimization problem\n",
    "\n",
    "max $\\mathbf v_1^H \\mathbf B^H \\mathbf B \\mathbf v_1$\n",
    "\n",
    "we recover $\\sigma_1^2$.  Then if we continue doing this optimization problem for $k = \\{2, 3, 4, ... , n\\}$\n",
    "\n",
    "max $\\mathbf v_k^H \\mathbf B^H \\mathbf B \\mathbf v_k$\n",
    "\n",
    "where each $\\big \\Vert \\mathbf v_k\\big \\Vert_2^2 = 1$, **with the added constraint that** $\\mathbf v_k \\perp \\mathbf v_j$, for $j = \\{1, 2, ... , k-1\\}$\n",
    "\n",
    "i.e. each $\\mathbf v_k$ is mutually orthonormal to the $\\mathbf v$'s that come before it,\n",
    "\n",
    "Then we recover \n",
    "\n",
    "$\\mathbf V = \n",
    "\\bigg[\\begin{array}{c|c|c|c}\n",
    "\\mathbf v_1 & \\mathbf v_2 &\\cdots & \\mathbf v_{n}\n",
    "\\end{array}\\bigg]\n",
    "$   \n",
    "\n",
    "where $\\mathbf V$ is a unitary matrix (or in reals, orthogonal).  Put differently, we recover a coordinate system.  \n",
    "\n",
    "And more to the point, we also can collect the 'payoffs': $\\{ \\sigma_1^2, \\sigma_2^2, \\sigma_3^2, ..., \\sigma_n^2 \\}$. from this maximization process.  We could sum up all of these squared singular values, and get\n",
    "\n",
    "$\\sigma_1^2 + \\sigma_2^2+ \\sigma_3^2+ ...+\\sigma_n^2 = \\big \\Vert \\mathbf B \\big \\Vert_F^2$\n",
    "\n",
    "i.e. when we sum up all of these 'payoffs' from our complete quadratic form process, we get the squared Frobenius norm for our matrix $\\mathbf B$. \n",
    "\n",
    "\n",
    "# On Unitary Matrices\n",
    "Note that there is a special case of interest.  Suppose that we have a square unitary matrix $\\mathbf Q$. We know that all eigenvalues of $\\mathbf Q $ have magnitude of 1.  Why? There are multiple approaches, but an elegant one uses the above knowledge with the singular value decomposition:\n",
    "\n",
    "\n",
    "$\\mathbf Q = \\mathbf{U \\Sigma V}^H$\n",
    "\n",
    "$\\mathbf Q^H \\mathbf Q  = \\mathbf I = \\big(\\mathbf{U \\Sigma V}^H\\big)^H \\mathbf{U \\Sigma V}^H  =\\mathbf V \\mathbf\\Sigma^2  \\mathbf V^H$\n",
    "\n",
    "left multiply by $\\mathbf V^H$ and right multiply by $\\mathbf V$, recalling\n",
    "that $\\mathbf V$ is a square, full rank matrix\n",
    "\n",
    "$\\mathbf V^H \\mathbf I \\mathbf V = \\mathbf I = \\mathbf V^H \\big(\\mathbf V \\mathbf\\Sigma^2  \\mathbf V^H\\big) \\mathbf V =\\mathbf \\Sigma^2$\n",
    "\n",
    "$\\mathbf I = \\mathbf \\Sigma^2$\n",
    "\n",
    "recalling that each singular value, by construction, is real and non-negative, we can then determine:\n",
    "\n",
    "$\\mathbf I = \\mathbf \\Sigma$  \n",
    "\n",
    "Thus we know that $\\mathbf \\Sigma$ is itself the Identity matrix (i.e. $\\sigma_1 = \\sigma_2 = ... = \\sigma_n = 1$)\n",
    "\n",
    "When we consider the eigenvalues of $\\mathbf Q$, where $\\big \\vert \\lambda_1 \\big \\vert \\geq \\big \\vert\\lambda_2 \\big \\vert\\geq \\big \\vert \\lambda_3 \\big \\vert \\geq ... \\geq \\big \\vert\\lambda_n \\big \\vert$, we know that $1 = \\sigma_1 \\geq \\big \\vert \\lambda_1 \\big \\vert$ and we know that $\\big \\vert \\lambda_n \\big \\vert \\geq \\sigma_n = 1$.  Every item in our sequence of eigenvalue magnitudes is bounded above and below by one.  Thus all eigenvalues of a unitary (or in Reals, othogonal) matrix must have magnitue equal to one.\n",
    "\n",
    "$1 = \\sigma_1 \\geq \\big \\vert \\lambda_1 \\big \\vert \\geq \\big \\vert\\lambda_2 \\big \\vert\\geq \\big \\vert \\lambda_3 \\big \\vert \\geq ... \\geq \\big \\vert\\lambda_n \\big \\vert \\geq \\sigma_n = 1$\n",
    "\n",
    "can be re-written as\n",
    "\n",
    "$1 = \\sigma_1 = \\big \\vert \\lambda_1 \\big \\vert = \\big \\vert\\lambda_2 \\big \\vert = \\big \\vert \\lambda_3 \\big \\vert = ... = \\big \\vert\\lambda_n \\big \\vert =\\sigma_n = 1$\n",
    "\n",
    "\n",
    "- - - - \n",
    "\n",
    "# On Nilpotent Matrices\n",
    "\n",
    "The final sequences of inequalities for some arbitary square matrix:\n",
    "\n",
    "$\\sigma_1 \\geq \\big \\vert \\lambda_1 \\big \\vert \\geq \\big \\vert\\lambda_2 \\big \\vert\\geq \\big \\vert \\lambda_3 \\big \\vert \\geq ... \\geq \\big \\vert\\lambda_n \\big \\vert \\geq \\sigma_n $\n",
    "\n",
    "is quite useful.  \n",
    "\n",
    "A nilpotent matrix $\\mathbf A$ is some $n$ x $n$ matrix where are after finite number of iterations, it becomes the zero matrix.  Thus $\\mathbf A^r = \\mathbf 0$ for some finite, natural number $r$.  (We can tighten the bound and say $r \\leq n$, but this is not really needed here.) \n",
    "\n",
    "**Claim:** a nilpotent matrix has all eigenvalues equal to zero.\n",
    "\n",
    "There are numerous ways to prove this.  The most slick uses the analysis earlier in this post and does the following:\n",
    "\n",
    "**Proof:**  \n",
    "$\\mathbf A$ has eigenvalues of $\\big \\vert \\lambda_1 \\big \\vert \\geq \\big \\vert\\lambda_2 \\big \\vert\\geq \\big \\vert \\lambda_3 \\big \\vert \\geq ... \\geq \\big \\vert\\lambda_n \\big \\vert $\n",
    "\n",
    "and \n",
    "\n",
    "$\\big(\\mathbf A^r\\big)$ has eigenvalues of $\\big \\vert \\lambda_1^r \\big \\vert \\geq \\big \\vert\\lambda_2^r \\big \\vert\\geq \\big \\vert \\lambda_3^r \\big \\vert \\geq ... \\geq \\big \\vert\\lambda_n^r \\big \\vert$\n",
    "\n",
    "or equivalently \n",
    "\n",
    "$\\big(\\mathbf A^r\\big)$ has eigenvalues of $\\big \\vert \\lambda_1 \\big \\vert^r \\geq \\big \\vert\\lambda_2 \\big \\vert^r \\geq \\big \\vert \\lambda_3 \\big \\vert^r \\geq ... \\geq \\big \\vert\\lambda_n \\big \\vert^r $\n",
    "\n",
    "We can do SVD on $\\big(\\mathbf A^r\\big)$ and see  \n",
    "$\\big(\\mathbf A^r\\big) = \\mathbf U \\mathbf \\Sigma \\mathbf V^H = \\mathbf 0$, where $\\mathbf U$ and $\\mathbf V$ are full rank unitary matrices (or orthogonal if dealing with Reals)  \n",
    "\n",
    "Hence:\n",
    "\n",
    "$ \\mathbf \\Sigma = \\mathbf U^H\\big(\\mathbf A^r\\big)\\mathbf V = \\mathbf U^H \\big(\\mathbf 0\\big)\\mathbf V = \\mathbf 0 $\n",
    "\n",
    "That is, all singular values of $\\big(\\mathbf A^r\\big)$  are equal to zero \n",
    "\n",
    "Thus we know that for $ \\big(\\mathbf A^r\\big)$  \n",
    "\n",
    "$0 = \\sigma_1 \\geq \\big \\vert \\lambda_1^r \\big \\vert \\geq \\big \\vert\\lambda_2^r \\big \\vert\\geq \\big \\vert \\lambda_3^r \\big \\vert \\geq ... \\geq \\big \\vert\\lambda_n^r \\big \\vert \\geq \\sigma_n = 0$\n",
    "\n",
    "Since all eigenvalue magnitudes are bounded above and below by zero, we restate this as  \n",
    "$0 = \\lambda_1^r  = \\lambda_2^r = \\lambda_3^r = ... = \\lambda_n^r  = 0$\n",
    "\n",
    "take the $r$th root and we see that all eigenvalues of the nilpotent matrix $\\mathbf A$ must be zero\n",
    "\n",
    "$0 = \\lambda_1  = \\lambda_2 =  \\lambda_3 = ... = \\lambda_n  = 0$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix Norms \n",
    "\n",
    "(this is just a warmup...) \n",
    "\n",
    "consider two n x n matrices $\\mathbf A$ and $\\mathbf B$.  \n",
    "\n",
    "now consider $\\big \\Vert \\mathbf {AB} \\big \\Vert_2 = \\sigma_1$, i.e. the operator norm for $\\big(\\mathbf {AB}\\big)$.  \n",
    "\n",
    "As always we order singular values as $\\sigma_1 \\geq \\sigma_2 \\geq ... \\geq \\sigma_n \\geq 0$.  And again we consider any vectors $\\mathbf x$, $\\mathbf y$, $\\mathbf z$ with the constraint that $\\big \\Vert \\mathbf x\\big \\Vert_2^2 = 1$, $\\big \\Vert \\mathbf y\\big \\Vert_2^2 = 1$, and $\\big \\Vert \\mathbf z\\big \\Vert_2^2 = 1$.\n",
    "\n",
    "The operator norm of $\\big(\\mathbf {AB}\\big)$ can be considered as a quadratic form whereby we look for \n",
    "\n",
    "\n",
    "max $\\mathbf z^H \\big(\\mathbf B^H \\mathbf A^H \\mathbf{AB}\\big) \\mathbf z$\n",
    "\n",
    "we claim that this is upperbounded by maximizing $\\big(\\mathbf x^H \\mathbf A^H\\mathbf A \\mathbf x\\big)\\big(\\mathbf y^H \\mathbf B^H \\mathbf B \\mathbf y\\big)$ which is equivalent to $\\sigma_{1,A}^2 \\sigma_{1,B}^2$\n",
    "\n",
    "We can see this by assuming $\\mathbf {Bz} =\\mathbf x$.  I.e. if $\\mathbf {Bz}$ is equal to our optimal $\\mathbf x$, then we have  $\\big(\\mathbf z^H \\mathbf B^H\\big) \\mathbf A^H \\mathbf{A}\\big( \\mathbf B \\mathbf z \\big)= \\mathbf x^H \\mathbf A^H \\mathbf A \\mathbf x $ and hence we've optimized the internal part of that equation.  But in fact $\\mathbf {Bz} = \\alpha \\mathbf x$.  I.e. $\\mathbf {Bz}$ actually gives us a scaled version of $\\mathbf x$.   So we now consider the fact that we want to maximize $\\big \\Vert \\mathbf {Bz} \\big \\Vert_2$  i.e. maximize the $\\alpha$ in $\\alpha \\mathbf x$.  This is equivalent to maximizing $\\big(\\mathbf y^H \\mathbf B^H \\mathbf B \\mathbf y\\big)$, and we know that this is given by $\\sigma_{B,1}^2$.  Hence maximizing $\\mathbf z^H \\big(\\mathbf B^H \\mathbf A^H \\mathbf{AB}\\big) \\mathbf z$ is upper bounded by maximizing the inside, which returns $\\sigma_{A,1}^2$ and scaling that by a maximal $\\alpha$ which is given by $\\sigma_{B,1}^2$ \n",
    "\n",
    "Thus $\\big \\Vert \\mathbf{AB} \\big \\Vert_2^2 \\leq \\sigma_{A,1}^2\\sigma_{B,1}^2 = \\big \\Vert \\mathbf{A} \\big \\Vert_2^2 \\big \\Vert \\mathbf{B} \\big \\Vert_2^2 $\n",
    "\n",
    "\n",
    "Now if we wanted to maximize $\\big \\Vert \\mathbf{AB} \\big \\Vert_2^2 $ with the constraint that the solution is orthogonal to the (right) singular vectors associated with $\\sigma_{A,1}$, we could upper bound this with $\\sigma_{A,2}^2 \\sigma_{B,1}^2$, and if we wanted to do a maximization that was orthogonal to the (right) singular vectors associated with  $\\{\\big(\\sigma_{A,2}, \\sigma_{B,1}\\big), \\big(\\sigma_{A,1}, \\sigma_{B,1}\\big)\\}$ and we could upper bound that by $\\sigma_{A,3}^2 \\sigma_{B,1}^2$, and so on.  \n",
    "\n",
    "If we were to add all of these upper bounds up, what we'd get is \n",
    "\n",
    "$\\sigma_{B,1}^2 \\big(\\sigma_{A,1}^2 + \\sigma_{A,2}^2 + ... + \\sigma_{A,n}^2\\big) = \\sigma_{B,1}^2 \\text{trace}\\big(\\mathbf A^H \\mathbf A\\big) = \\sigma_{B,1}^2 \\big \\Vert \\mathbf A \\big \\Vert_F^2 = \\big \\Vert \\mathbf B\\big \\Vert_2^2\\big \\Vert \\mathbf A \\big \\Vert_F^2$\n",
    "\n",
    "This process is merely an extension of a preceding section titled \"Quadratic Forms and recovering a Frobenius Norm\"... and we note that if we did this for all $\\mathbf z_k$, we'd recovered a coordinate system $\\mathbf Z$.  If we added up all of the associated 'payoff's we'd recover  $\\big \\Vert \\mathbf{AB} \\big \\Vert_F^2$.  By the above, we have upper bounds for all of those 'payoffs'. \n",
    "\n",
    "Thus we can say \n",
    "\n",
    "$\\big \\Vert \\mathbf{AB} \\big \\Vert_F^2 \\leq \\big \\Vert \\mathbf B\\big \\Vert_2^2 \\big \\Vert\\mathbf A \\big \\Vert_F^2$\n",
    "- - - - \n",
    "**begin subsequent note:**  \n",
    "\n",
    "a *much* better way to prove the above is to consider \n",
    "\n",
    "$\\mathbf B = \n",
    "\\bigg[\\begin{array}{c|c|c|c}\n",
    "\\mathbf b_1 & \\mathbf b_2 &\\cdots & \\mathbf b_{n}\n",
    "\\end{array}\\bigg]\n",
    "$\n",
    "\n",
    "so \n",
    "$\\mathbf {AB} = \n",
    "\\bigg[\\begin{array}{c|c|c|c}\n",
    "\\mathbf A\\mathbf b_1 & \\mathbf A\\mathbf b_2 &\\cdots & \\mathbf A\\mathbf b_{n}\n",
    "\\end{array}\\bigg]\n",
    "$\n",
    "\n",
    "\n",
    "where we recognize the quadratic form argument that   \n",
    "$\\big \\Vert \\mathbf A\\mathbf b_k \\big \\Vert_2^2 \\leq \\sigma_1^2 \\big  \\Vert \\mathbf b_k\\big \\Vert_2^2$ \n",
    "\n",
    "i.e. where $\\sigma_1^2$ is the dominant eigenvalue of $\\mathbf A^* \\mathbf A$  \n",
    "\n",
    "and in effect sum over this bound.  \n",
    "\n",
    "\n",
    "hence  \n",
    "$0 \\leq \\Big \\Vert \\mathbf {AB}\\Big \\Vert_F^2 = \\sum_{k=1}^n \\big \\Vert \\mathbf A\\mathbf b_k\\big \\Vert_2^2 \\leq \\sum_{k=1}^n \\sigma_1^2 \\big \\Vert \\mathbf b_k\\big \\Vert_2^2 = \\sigma_1^2 \\sum_{k=1}^n  \\big \\Vert \\mathbf b_k\\big \\Vert_2^2$  \n",
    "\n",
    "taking square roots gives   \n",
    "$0 \\leq \\Big \\Vert \\mathbf {AB}\\Big \\Vert_F \\leq \\sigma_1 \\big( \\sum_{k=1}^n  \\big \\Vert \\mathbf b_k\\big \\Vert_2^2\\big)^\\frac{1}{2} = \\sigma_1\\Big \\Vert \\mathbf {B}\\Big \\Vert_F = \\Big \\Vert \\mathbf {A}\\Big \\Vert_2\\Big \\Vert \\mathbf {B}\\Big \\Vert_F $  \n",
    "\n",
    "\n",
    "a closely related point is that \n",
    "\n",
    "$\\big \\vert \\mathbf y^* \\mathbf {AB} \\mathbf x \\big \\vert \\leq \\big \\Vert \\mathbf {Ax} \\big \\Vert_2 \\big \\Vert \\mathbf {By} \\big \\Vert_2 \\leq \\sigma_{1,A}\\sigma_{1,B}\\big \\Vert \\mathbf x \\big \\Vert_2 \\big \\Vert \\mathbf y \\big \\Vert_2 $  \n",
    "  \n",
    "\n",
    "by Cauchy Schwarz  \n",
    "\n",
    "**end subsequent note**  \n",
    "- - - - \n",
    "\n",
    "of course we can recall that \n",
    "\n",
    "$\\big \\Vert \\mathbf B\\big \\Vert_F^2 = \\big \\Vert \\mathbf B\\big \\Vert_2^2 + \\sigma_{B,2}^2 + \\sigma_{B,3}^2 + .... + \\sigma_{B,n}^2 = \\sigma_{B,1}^2+ \\sigma_{B,2}^2 + \\sigma_{B,3}^2 + .... + \\sigma_{B,n}^2 $\n",
    "\n",
    "recalling that each singular value $\\sigma_{B, k}$ is real and non-negative.  Thus if we wanted to loosen up the above bound, we could say\n",
    "\n",
    "$\\big \\Vert \\mathbf{AB} \\big \\Vert_F^2 \\leq \\big \\Vert \\mathbf B\\big \\Vert_F^2\\big \\Vert \\mathbf A \\big \\Vert_F^2$\n",
    "\n",
    "or taking the square root of both sides \n",
    "\n",
    "$\\big \\Vert \\mathbf{AB} \\big \\Vert_F \\leq \\big \\Vert \\mathbf B\\big \\Vert_F \\big \\Vert \\mathbf A \\big \\Vert_F$\n",
    "\n",
    "This should jump out at us as being a variant of Cauchy-Schwarz which tells us that \n",
    "\n",
    "$\\big \\vert\\mathbf a^H \\mathbf b\\big \\vert^2 \\leq \\big \\Vert \\mathbf a \\big \\Vert_2^2 \\big \\Vert \\mathbf b\\big \\Vert_2^2$\n",
    "\n",
    "\n",
    "\n",
    "**note: a lot of the above is proved in a different more complete, and much more satisfying way, below, under \"Hermitian Positive Semi Definite Trace Inequalities\"**  \n",
    "\n",
    "E.g. suppose $\\mathbf X^H \\mathbf X = \\mathbf A$ and $\\mathbf Y \\mathbf Y^H = \\mathbf B$,  i.e. both $\\mathbf B$ and $\\mathbf A$ are Hermitian positive semi definite.  Using the above proof with respect to squared Frobenius norms, we see\n",
    "\n",
    "$\\big \\Vert \\mathbf{XY} \\big \\Vert_F^2 \\leq \\big \\Vert \\mathbf X \\big \\Vert_F^2\\big \\Vert \\mathbf Y \\big \\Vert_F^2$\n",
    "\n",
    "but we could also say\n",
    "\n",
    "$\\big \\Vert \\mathbf{XY} \\big \\Vert_F^2 = \\text{trace}\\big(\\mathbf Y^H \\mathbf X^H \\mathbf X \\mathbf Y \\big) =  \\text{trace}\\big(\\mathbf Y \\mathbf Y^H \\mathbf X^H \\mathbf X \\big) = \\text{trace}\\big(\\mathbf {BA}\\big)= \\text{trace}\\big(\\mathbf A \\mathbf B\\big)$\n",
    "\n",
    "then apply the below inequality that if $\\mathbf A$ and $\\mathbf B$ are both Hermitian positive semidefinite, \n",
    "$\\text{trace}\\big(\\mathbf A \\mathbf B\\big) \\leq \\text{trace}\\big(\\mathbf A\\big) \\text{trace}\\big( \\mathbf B\\big)$\n",
    "\n",
    "$\\big \\Vert \\mathbf{XY} \\big \\Vert_F^2 \\leq \\text{trace}\\big(\\mathbf A\\big) \\text{trace}\\big( \\mathbf B\\big) = \\big \\Vert \\mathbf{X} \\big \\Vert_F^2\\big \\Vert \\mathbf{Y} \\big \\Vert_F^2$\n",
    "\n",
    "something similar is shown with respect to operator norms as well.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**extension: Markov Chains must have linearly independent eigenvectors associated with eigenvalues of magnitude 1**\n",
    "\n",
    "The argument is a proof by contradiction using Jordan Forms.  In essence, assume the above statement is not true, and we find it violates the above matrix norms.  This is an extension modified answer for problem 4.15 In Stochastic Processes by Gallagher (problem 3.15 in \"Discrete Stochastic Processes\" in MIT OCW.\n",
    "\n",
    "start by showing a 3 x 3 Jordan Block of the form\n",
    "\n",
    "$\\mathbf J_i = \\begin{bmatrix}\n",
    "\\lambda_i & 1 & 0 \\\\ \n",
    "0 & \\lambda_i & 1\\\\ \n",
    "0 & 0 & \\lambda_i \n",
    "\\end{bmatrix}$\n",
    "\n",
    "then \n",
    "\n",
    "$\\mathbf J_i^n = \\begin{bmatrix}\n",
    "\\lambda_i^n & n \\lambda_i^{n-1} & \\binom{n}{2} \\lambda_i^{n-2} \\\\ \n",
    "0 & \\lambda_i^n & n \\lambda_i^{n-1}\\\\ \n",
    "0 & 0 & \\lambda_i^n \n",
    "\\end{bmatrix}$\n",
    "\n",
    "There are various ways to expand or shrink this, but most succinctly we see that diagonal elements (eigenvalues) multiply exponentially with n, as we'd expect.  The off diagonal element that are non-zero occur when we do not have enough linearly indenpendent eigenvectors for $\\lambda_i$, and as a lower bound, we can say that they too grow exponentially (albeit it at one or two iterations less per 'step' than the diagonals) and are scaled by n. \n",
    "\n",
    "From here, notice that for any $\\big \\vert \\lambda_i \\big\\vert \\lt 1$, that $\\lim_{n \\to \\infty}\\mathbf J_i^n = \\mathbf {0} $.  However if $\\big\\vert \\lambda_i\\big\\vert = 1$, then the off diaonal elements also tend to infinity.  (We don't consider the case of $\\big\\vert \\lambda_i \\big\\vert \\gt 1$, because as noted in the Gerschgorin discs writeup, Markov Chains cannot have eigenvalues with magnitude $\\gt 1$.) \n",
    "\n",
    "\n",
    "Now suppose we have a defective markov chain transition matrix that is $m$ x $m$.  I.e. it factorizes so $\\mathbf A = \\mathbf {PJP}^{-1}$, where $\\mathbf J$ is the jordan form that has non-zero off-diagonal elements because we $\\mathbf A$ is defective.  \n",
    "\n",
    "We also can say:\n",
    "\n",
    "$\\mathbf J^n = \\mathbf P^{-1} \\mathbf A^n \\mathbf P $, or using associativity $\\mathbf J^n = \\mathbf P^{-1} \\big(\\mathbf A^n \\mathbf P \\big)$\n",
    "\n",
    "applying the above Frobenius norm inequality, twice,\n",
    "\n",
    "$ \\big \\Vert \\mathbf J^n \\big \\Vert_F^2=\\big \\Vert \\mathbf P^{-1} \\big(\\mathbf A^n \\mathbf P \\big) \\big \\Vert_F^2 \\leq \\big \\Vert \\mathbf P^{-1} \\big \\Vert_F^2\\big \\Vert \\big(\\mathbf A^n \\mathbf P \\big) \\big \\Vert_F^2 \\leq \\big \\Vert \\mathbf P^{-1} \\big \\Vert_F^2\\big \\Vert \\mathbf A^n   \\big \\Vert_F^2\\big \\Vert  \\mathbf P  \\big \\Vert_F^2$\n",
    "\n",
    "\n",
    "Also recalling that valid transition matrix $\\mathbf A^n$ has all entries as real valued non-negative, and either all columns or all rows sum to one.  Put differently, every value in $\\mathbf A$ is in $[0,1]$, and  thus we can upper bound the squared Frobenius norm of $\\mathbf A^n$ by the ones matrix.\n",
    "\n",
    "$\\big \\Vert \\mathbf A^n  \\big \\Vert_F^2 \\leq \\big \\Vert \\mathbf {11}^T  \\big \\Vert_F^2 = m^2$\n",
    "\n",
    "now multiply both sides by $\\big \\Vert \\mathbf P^{-1} \\big \\Vert_F^2$ and $\\big \\Vert  \\mathbf P  \\big \\Vert_F^2$, which are real valued, positive scalars, and we get:  \n",
    "\n",
    "$ \\big \\Vert \\mathbf J^n \\big \\Vert_F^2 \\leq \\big \\Vert \\mathbf P^{-1} \\big \\Vert_F^2\\big \\Vert \\mathbf A^n  \\big \\Vert_F^2 \\big \\Vert  \\mathbf P  \\big \\Vert_F^2 \\leq m^2 \\big \\Vert \\mathbf P^{-1} \\big \\Vert_F^2  \\big \\Vert  \\mathbf P  \\big \\Vert_F^2 $\n",
    "\n",
    "notice that for any given transition matrix $\\mathbf A$, the right hand side of the equality is some fixed finite number, and it does not vary with respect to $n$.  The contradiction comes by assuming that we do not have enough linearly independent eigenvectors with eigenvalue magnitude of $1$.  Hence for any given $\\mathbf A$ we have a fixed upper bound on the right hand side that does not vary with $n$.  Yet if we have jordan blocks (i.e. super diagonal values of 1) associated with $\\big \\vert\\lambda_i \\big \\vert =1$, then we can find large enough $n$ such that the left hand side has a larger Frobenius norm than the right hand side, which is a contradiction.  Hence we know there cannot be aka a shortage of linearly independent eigenvectors, with respect to eigenvalues of magnitude one in a Markov chain transition matrix. \n",
    "\n",
    "\n",
    "**extension: Projection Matrices must be diagonalizable**  \n",
    "\n",
    "A projector, aka an idempotent matrix is an $n$ x $n$ matrix $\\mathbf A$, where \n",
    "\n",
    "$\\mathbf A = \\mathbf A^2$\n",
    "\n",
    "**claim**  \n",
    "\n",
    "$\\mathbf A$ must diagonalizable.\n",
    "\n",
    "**proof**\n",
    "\n",
    "$\\mathbf A$ has only eigenvalues equal to $0$ and $1$\n",
    "\n",
    "that is, for each eigenvector $\\mathbf x$, we have $\\mathbf A^2 \\mathbf x = \\lambda_k \\mathbf A \\mathbf x = \n",
    "\\lambda_k^2 \\mathbf x = \\lambda_k \\mathbf x = \\mathbf {Ax}$\n",
    "\n",
    "thus $\\lambda_k^2 = \\lambda_k$, which occurs **iff** $\\lambda_k = 0$ or $\\lambda_k = 1$.\n",
    "\n",
    "note: we could rewrite the above as \n",
    "\n",
    "$\\lambda_k^2 = \\lambda_k \\to \\lambda_k^2 - \\lambda_k = 0 \\to \\lambda_k(\\lambda_k - 1) - 0$, hence the roots / $\\lambda_k$ that satisfy this are $1$ and $0$.  \n",
    "\n",
    "\n",
    "now we consider the possibility that $\\mathbf A$ is defective and write out its Jordan Form, similarity transform\n",
    "\n",
    "$\\mathbf P^{-1} \\mathbf A^k \\mathbf P =\\mathbf J^k$ \n",
    "\n",
    "hence we have \n",
    "\n",
    "$\\mathbf P^{-1} \\mathbf A \\mathbf P =\\mathbf J = \\mathbf J^2 = \\mathbf P^{-1} \\mathbf A^2 \\mathbf P $ \n",
    "\n",
    "also notice that since $\\mathbf A = \\mathbf A^2$, then we can left multiply both by $\\mathbf A$ and see that \n",
    "\n",
    "$\\mathbf A^2 = \\mathbf A^3$, hence $\\mathbf A = \\mathbf A^3$. We can further do this process such that $\\mathbf A = \\mathbf A^k$\n",
    "\n",
    "\n",
    "Thus our relationship is $\\mathbf J = \\mathbf J^k$ for any natural number $k$. Recalling that $\\mathbf J$ has eigenvalues equal to zero or one on the diagonal, and at most all 1s in the strictly upper triangular portion, we can upperbound the squared frobenius norm of $\\mathbf J$ with $\\big \\Vert \\mathbf {11}^T \\big \\Vert_F^2$.  Further, we collect all of the eigenvalues in a diagonal matrix $\\mathbf D$ and remark that $\\mathbf J$ has a Frobenius norm strictly greater than $\\mathbf D$ unless $\\mathbf A$ is diagonalizable. \n",
    "\n",
    "if defective $ \\big \\Vert \\mathbf D \\big \\Vert_F^2  \\lt \\big \\Vert \\mathbf J^k \\big \\Vert_F^2 = \\big \\Vert \\mathbf J \\big \\Vert_F^2 \\lt \\big \\Vert \\mathbf {11}^T \\big \\Vert_F^2$.  \n",
    "\n",
    "if diagonalizable $ \\big \\Vert \\mathbf D \\big \\Vert_F^2  = \\big \\Vert \\mathbf J^k \\big \\Vert_F^2 = \\big \\Vert \\mathbf J \\big \\Vert_F^2 \\lt \\big \\Vert \\mathbf {11}^T \\big \\Vert_F^2$.  \n",
    "\n",
    "We know that the matrix cannot have a shortage of linearly independent eigenvectors for $\\lambda_k = 1$.  Why? Repeat the exact same argument used above for eigenvalues with magnitude 1 in Markov Chains.  The right hand side is fixed in magnitude, but we can find large enough (finite) $k$ that creates a Frobenius norm that exceeds this bound if we have any Jordan blocks (i.e. shortage of linearly independent eigenvectors) with respect to eigenvalues equal to one.  Thus we conclude that the geometric multiplicity = algebraic multiplicity for $\\lambda = 1$\n",
    "\n",
    "Now, with respect to eigenvalues equal to zero, recall that if the matrix is defective, we have jordan blocks given by: \n",
    "\n",
    "$\\mathbf J_i = \\begin{bmatrix}\n",
    "\\lambda_i & 1 & 0 \\\\ \n",
    "0 & \\lambda_i & 1\\\\ \n",
    "0 & 0 & \\lambda_i \n",
    "\\end{bmatrix}$\n",
    "\n",
    "and, for $k\\gt 2$, we have \n",
    "\n",
    "$\\mathbf J_I^k = \\begin{bmatrix}\n",
    "\\lambda_i^k & k \\lambda_i^{k-1} & \\binom{k}{2} \\lambda_i^{k-2} \\\\ \n",
    "0 & \\lambda_i^k & k \\lambda_i^{k-1}\\\\ \n",
    "0 & 0 & \\lambda_i^k \n",
    "\\end{bmatrix}$\n",
    "\n",
    "Yet this block is nilpotent since $\\lambda = 0$ and thus we can select large enough $k$ (e.g. $k:=n$) and say that $\\mathbf J_I^n = \\mathbf 0$.  But the only way $\\big \\Vert\\mathbf J_I\\big \\Vert_F^2 = \\big \\Vert\\mathbf J_I^n\\big \\Vert_F^2 = \\big \\Vert\\mathbf 0\\big \\Vert_F^2$, is if $\\mathbf J_I$ is itself a zero matrix (i.e. we know the $\\lambda_i = 0$ for this block, and the off diagonal ones cannot exist).\n",
    "\n",
    "since we know that there are no off diagonal elements with respect to $\\lambda = 1$ or with respect to $\\lambda = 0$, we know \n",
    "\n",
    "$\\big \\Vert \\mathbf D \\big \\Vert_F^2 = \\big \\Vert \\mathbf J \\big \\Vert_F^2$\n",
    "\n",
    "which proves that $\\mathbf A$ is diagonalizable (i.e. not defective).  \n",
    "\n",
    "**alternative proof:** \n",
    "\n",
    "A projector, aka an idempotent matrix is an $n$ x $n$ matrix $\\mathbf A$, where \n",
    "(note: we use projector and idempotent interchangeably as is commonly done e.g. in Meyer's *Matrix Analysis*.  Unfortunately it is a common convention to insist on projectors having some additional structure, e.g. being symmetric.  We do not use this other convention.)  \n",
    "\n",
    "\n",
    "our projector obeys:\n",
    "\n",
    "$\\mathbf A = \\mathbf A^2$\n",
    "\n",
    "i.e. \n",
    "\n",
    "$\\mathbf 0 = \\mathbf A^2 - \\mathbf A$  \n",
    "\n",
    "that is, the polynomial \n",
    "\n",
    "$p(x) = x^2 - x$ \n",
    "\n",
    "annihilates our matrix $\\mathbf A$.  \n",
    "\n",
    "In the special case that $\\mathbf A = \\mathbf 0$ or $\\mathbf A = \\mathbf I$ then there is nothing to prove as the zero matrix is already diagonal (alternatively: it it is the only nilpotent matrix that is diagonalizable), and the identity matrix is already diagonal (further it is the only matrix that has every non zero vector as an eigenvector).  \n",
    "\n",
    "So from here, assume $\\mathbf I \\neq \\mathbf A \\neq \\mathbf 0$ \n",
    "\n",
    "revisiting our annihilating polynomial \n",
    "\n",
    "$p(x) = x^2 - x$ \n",
    "\n",
    "$p(\\mathbf A) = \\mathbf A^2 - \\mathbf A = \\mathbf A\\big(\\mathbf A - \\mathbf I\\big)$ \n",
    "\n",
    "we find that is in fact the minimal polynomial of $\\mathbf A$ with no repeated factors and hence $\\mathbf A$ cannot be defective if it is a Projector.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# a better approach \n",
    "\n",
    "consider the involutary matrix \n",
    "\n",
    "$\\mathbf A^2 = \\mathbf I$ \n",
    "\n",
    "such a matrix must be diagonalizable  \n",
    "\n",
    "if $\\mathbf A = \\mathbf I$ or $\\mathbf A = -\\mathbf I$ \n",
    "there is nothing to prove -- it is already diagonalized.  \n",
    "\n",
    "supposing it is not \n",
    "\n",
    "then we have a minimal polynomial given by \n",
    "\n",
    "$\\mathbf A^2 - \\mathbf I = \\big(\\mathbf A - \\mathbf I\\big)\\big(\\mathbf A + \\mathbf I\\big)   = \\mathbf 0$   \n",
    "\n",
    "First this tells us that the above polynomial annihilates $\\mathbf A$ and hence the any eigenvalue of $\\mathbf A$ must be $+1$ or $-1$.  \n",
    "\n",
    "Furthermore, this tells us that for *any* $\\mathbf x \\neq 0$ \n",
    "\n",
    "$\\big(\\mathbf A - \\mathbf I\\big)\\big(\\mathbf A + \\mathbf I\\big)\\mathbf x = \\mathbf 0 $ \n",
    "\n",
    "hence $\\mathbf x$ is equal to a linear combination of vectors in the nullspace of \n",
    "\n",
    "$\\big(\\mathbf A - \\mathbf I\\big)$ and $\\big(\\mathbf A + \\mathbf I\\big)$.  Slightly more carefully, we know we have dimension $n$, and that the matrix $\\big(\\mathbf A + \\mathbf I\\big)$ has $k$ linearly indepedent vectors in it nullspace / kernel, and $n-k$ linearly independent vectors its column space / image. But each one of those $n-k$ linearly independent vector is then annhilated by $\\big(\\mathbf A - \\mathbf I\\big)$, and hence that means they are all in the nullspace/kernel of said matrix.  We've thus created a basis for dimension $n$ by $k$ + $n-k$ linearly independent vectors in the nullspaces of these respective matrix.  \n",
    "\n",
    "But all non-zero vectors in the nullspaces of these matrices are precisely the eigenvectors corresponding to eigenvalues of $\\mathbf A$.  Hence the eigenvectors of $\\mathbf A$ form a basis and we know that $\\mathbf A$ is diagonalizable.  \n",
    "\n",
    "\n",
    "**now, for any projector/ idempotent matrix**\n",
    "\n",
    "we can use scaling and shifting to write it as a involutory matrix, and hence be diagonalizable.  However a more  direct proof is to simply note that \n",
    "\n",
    "$\\mathbf P^2 = \\mathbf P \\longrightarrow \\mathbf P^2 - \\mathbf P = \\big(\\mathbf P - \\mathbf 0\\big)\\big(\\mathbf P - \\mathbf I\\big) = \\mathbf 0$  \n",
    "\n",
    "hence using the above result, we have a basis formed by the linearly independent vectors in  \n",
    "$\\text{null}\\big(\\mathbf P - \\mathbf 0\\big) \\bigcup \\text{null}\\big(\\mathbf P - \\mathbf I\\big)$   \n",
    "thus $\\mathbf P$ is diagonalizable.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**The bulk of the rest of this posting consists of inequalities from Zhang's *Linear Algebra: Challenging Problems for Students*, which contains numerous interesting exercises that proceed much like a guided proof.  (Unfortunately, the solutions in the back are frequently some mixture of terse and nearly incomprehensible -- that said your author quite likes the book because of the very high quality exercises.)  **\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Misc notes on Hermitian Matrices\n",
    "\n",
    "if we let $\\mathbf A$ and $\\mathbf B$ both be $n$ x $n$ Hermitian matrices, \n",
    "\n",
    "first, notice $\\mathbf {AB} $ has the same eigenvalues as $\\mathbf {BA}$.  (In general we know that they have the same non-zero eigenvalues with the same Algebraic multiplicities -- and since they have the same dimension $n$ there must be the same number of 'leftover' eigenvalues that are zeros.  A proof is contained in \"Vandermonde_Matrices_Permutations_and_Discrete_Fourier_Transform.ipybn\" )\n",
    "\n",
    "\n",
    "now notice $\\big(\\mathbf{AB}\\big)^H = \\mathbf B^H \\mathbf A^H = \\mathbf {BA}$.  Thus we conclude that $\\mathbf {BA}$ has the same eigenvalues (with same algebraic multipilicities) as $\\mathbf {AB}$ and the same as the transposed conjugate $\\big(\\mathbf {BA}\\big)^H$.   This means that all eigenvalues in $\\mathbf {AB}$ must be either real, or come in conjugate pairs.   \n",
    "\n",
    "This means that $\\text{trace}\\Big(\\big(\\mathbf{AB}\\big)^k\\Big)$ is real valued for any natural number $k$.  \n",
    "\n",
    "- - - - -\n",
    "*begin interlude*  \n",
    "Here is a different take.  The below claims are mostly interested in $\\big(\\mathbf {AB}\\big)$ and $\\big(\\mathbf {AB}\\big)^2$.  For the first case, notice that $\\big( \\mathbf A - \\mathbf B\\big)$ is a Hermitian matrix, and if we square it, it is hermitian positive semi definite matrix-- and hence its trace must be real, non-negative, which we denote as $\\gamma$.  \n",
    "\n",
    "$\\text{trace}\\Big( \\big(\\mathbf A - \\mathbf B\\big)^2 \\Big) = \\text{trace}\\big(\\mathbf A^2\\big) + \\text{trace}\\big(\\mathbf B^2\\big) - \\text{trace}\\big(\\mathbf{AB}\\big) - \\text{trace}\\big(\\mathbf{BA}\\big) = \\gamma$\n",
    "\n",
    "$\\text{trace}\\big(\\mathbf A^2\\big) + \\text{trace}\\big(\\mathbf B^2\\big) - 2\\cdot \\text{trace}\\big(\\mathbf{AB}\\big) = \\gamma$\n",
    "\n",
    "$\\text{trace}\\big(\\mathbf A^2\\big) + \\text{trace}\\big(\\mathbf B^2\\big)  = \\gamma + 2\\cdot \\text{trace}\\big(\\mathbf{AB}\\big)$\n",
    "\n",
    "the left hand side is the sum of traces of 2 Hermitian positive semi-definite matrices and hence is real, non-negative, thus the right hand side is as well. We know that $\\gamma$ is real, thus $\\text{trace}\\big(\\mathbf{AB}\\big)$ must be as well. \n",
    "\n",
    "Also consider \n",
    "\n",
    "$\\Big(\\mathbf{AB} + \\mathbf{BA}\\Big)^2 = \\Big(\\mathbf{AB} + \\big(\\mathbf{AB}\\big)^H\\Big)^2$\n",
    "\n",
    "Where $\\mathbf{AB}$ plus its conjugate transpose creates a new matrix that is Hermitian.  Thus this new matrix given by $\\Big(\\mathbf{AB} + \\mathbf{BA}\\Big)$ has real eigenvalues, and a real trace.  We denote the trace of the square of this new matrix as $\\gamma$.\n",
    "\n",
    "$\\text{trace}\\Big( \\big( \\mathbf{AB} + \\mathbf{BA}\\big)^2 \\Big) = \\gamma = \\text{trace}\\big(\\mathbf{ABAB}\\big) + \\text{trace}\\big(\\mathbf{BABA} \\big) + \\text{trace}\\big(\\mathbf{ABBA}\\big) + \\text{trace}\\big(\\mathbf{BAAB}\\big)$\n",
    "\n",
    "$\\text{trace}\\Big( \\big( \\mathbf{AB} + \\mathbf{BA}\\big)^2 \\Big) = \\gamma  = 2\\cdot \\text{trace}\\big(\\mathbf{ABAB}\\big) + 2\\cdot \\text{trace}\\big(\\mathbf{ABBA}\\big)$\n",
    "\n",
    "$\\frac{1}{2}\\gamma = \\text{trace}\\big(\\mathbf{ABAB}\\big) + \\text{trace}\\big(\\mathbf{ABBA}\\big)$\n",
    "\n",
    "$\\frac{1}{2}\\gamma -  \\text{trace}\\big(\\mathbf{ABBA}\\big) = \\text{trace}\\Big(\\mathbf{\\big(AB\\big)^2}\\Big)$\n",
    "\n",
    "since gamma is real, and $(\\mathbf{ABBA}\\big) $ is Hermitian, and hence has a real trace, then the left hand side has a real trace.  This means that the right hand side must have a real trace as well.  \n",
    "\n",
    "*end interlude*\n",
    "- - - - -\n",
    "\n",
    "\n",
    "**claim:**    \n",
    "\n",
    "$\\text{trace}\\Big(\\big(\\mathbf {AB}\\big)^2\\Big)  \\leq \\text{trace}\\big(\\mathbf A^2 \\mathbf B^2\\big)$\n",
    "\n",
    "**proof:**  \n",
    "\n",
    "using the cyclic property of trace, and the fact that $\\mathbf A$ and $\\mathbf B$ are both Hermitian, we can rewrite the right hand side as:\n",
    "\n",
    "$\\text{trace}\\big(\\mathbf A^2 \\mathbf B^2\\big) = \\text{trace}\\big(\\mathbf {AA} \\mathbf {BB}\\big) = \\text{trace}\\big(\\mathbf A \\mathbf {BBA}\\big) = \\text{trace}\\big(\\mathbf A^H \\mathbf B^H \\mathbf {BA}\\big) = \\text{trace}\\Big(\\big(\\mathbf{BA}\\big)^H \\big(\\mathbf {BA}\\big) \\Big)$\n",
    "\n",
    "Let $\\mathbf C: = \\mathbf {AB}$\n",
    "\n",
    "This means we can rewrite our claim as \n",
    "\n",
    "\n",
    "$\\text{trace}\\big(\\mathbf {C}^2\\big) \\leq \\text{trace}\\big(\\mathbf{C}^H \\mathbf {C}\\big)$\n",
    "\n",
    "and by applying the Schur inequality, we know \n",
    "\n",
    "$\\text{trace}\\big(\\mathbf {C}^2\\big) \\leq \\big \\vert \\text{trace}\\big(\\mathbf {C}^2\\big) \\big \\vert \\leq \\text{trace}\\big(\\mathbf{C}^H \\mathbf {C}\\big)$\n",
    "\n",
    "again, recalling that $\\text{trace}\\big( \\mathbf{C}^k\\big)$ is real valued for any natural number $k$\n",
    "\n",
    "\n",
    "**claim:**  \n",
    " \n",
    "$\\Big(\\text{trace}\\big(\\mathbf {AB}\\big)\\Big)^2 \\leq \\text{trace}\\big(\\mathbf A^2\\big) \\text{trace}\\big(\\mathbf B^2\\big)$\n",
    "\n",
    "- - - - \n",
    "*begin interlude* \n",
    "\n",
    "here we introduce the vec operator (also used in the Kronecker Products writeup) \n",
    "\n",
    "where we have $\\mathbf B = \\bigg[\\begin{array}{c|c|c|c}\n",
    "\\mathbf b_1 & \\mathbf b_2 &\\cdots & \\mathbf b_n\\end{array}\\bigg] $\n",
    "\n",
    "$\\text{vec}\\big(\\mathbf B\\big)  =\\mathbf b= \\begin{bmatrix}\n",
    "\\mathbf b_1 \\\\ \n",
    "\\mathbf b_2\\\\ \n",
    "\\mathbf b_3\\\\ \n",
    "\\vdots \\\\ \n",
    "\\mathbf b_n\n",
    "\\end{bmatrix}$\n",
    "\n",
    "where $\\mathbf b$ has $\\mathbf b_1$ with $\\mathbf b_2$ \"glued\" to the bottom of it, with $\\mathbf b_3$ \"glued\" to the bottom of that ... with $\\mathbf b_n$ glued to the bottom of that.  \n",
    "\n",
    "Thus we see that $\\big \\Vert \\mathbf B \\big \\Vert_F^2 = \\mathbf b ^H \\mathbf b$\n",
    "\n",
    "The same for $\\mathbf A$\n",
    "\n",
    "Note we denote $\\mathbf A = \\bigg[\\begin{array}{c|c|c|c}\n",
    "\\mathbf a_1 & \\mathbf a_2 &\\cdots & \\mathbf a_n\\end{array}\\bigg] $\n",
    "\n",
    "and $\\text{vec}\\big(\\mathbf A\\big)  = \\mathbf a = \\begin{bmatrix}\\mathbf a_1 \\\\ \n",
    "\\mathbf a_2\\\\ \n",
    "\\mathbf a_3\\\\ \n",
    "\\vdots \\\\ \n",
    "\\mathbf a_n\n",
    "\\end{bmatrix}$\n",
    "\n",
    "$\\big \\Vert \\mathbf A \\big \\Vert_F^2 = \\mathbf a^H \\mathbf a$\n",
    "\n",
    "*end interlude*  \n",
    "- - - - \n",
    "\n",
    "**proof:**    \n",
    "\n",
    "because $\\mathbf A = \\mathbf A^H$ we can rewrite the left hand side as  \n",
    "\n",
    "$\\Big(\\text{trace}\\big(\\mathbf {AB}\\big)\\Big)^2  = \\Big(\\text{trace}\\big(\\mathbf {A}^H \\mathbf B \\big)\\Big)^2 = \\Big(\\text{trace}\\big(\\mathbf a^H \\mathbf b \\big)\\Big)^2 = \\big(\\mathbf a^H \\mathbf b\\big)^2$\n",
    "\n",
    "\n",
    "And for the right hand side we can rewrite it as:\n",
    "\n",
    "$\\text{trace}\\big(\\mathbf A^2\\big) \\text{trace}\\big(\\mathbf B^2\\big) = \\text{trace}\\big(\\mathbf {A}^H\\mathbf A\\big) \\text{trace}\\big(\\mathbf B^H \\mathbf B \\big) = \\text{trace}\\big(\\mathbf a^H \\mathbf a\\big) \\text{trace}\\big(\\mathbf b^H \\mathbf b\\big) = \\big(\\mathbf {a}^H \\mathbf a \\big)\\big(\\mathbf b^H \\mathbf b\\big)$\n",
    "\n",
    "hence our claim reduces to a simple application of Cauchy Schwartz:\n",
    "\n",
    "i.e. \n",
    "\n",
    "$\\Big(\\text{trace}\\big(\\mathbf {AB}\\big)\\Big)^2 \\leq \\text{trace}\\big(\\mathbf A^2\\big) \\text{trace}\\big(\\mathbf B^2\\big)$\n",
    "\n",
    "is equivalent to saying \n",
    "\n",
    "$ (\\mathbf a^H \\mathbf b\\big)^2 \\leq \\big(\\mathbf {a}^H \\mathbf a \\big)\\big(\\mathbf b^H \\mathbf b\\big)$\n",
    "\n",
    "recalling the fact that $\\big(\\mathbf {AB}\\big)$ has a real valued trace, thus $\\Big(\\text{trace}\\big(\\mathbf {AB}\\big)\\Big)^2 = \\big(\\mathbf b^H \\mathbf a\\big)^2 = \\big(\\mathbf a^H \\mathbf b\\big)^2$ is real valued as well.  \n",
    "\n",
    "\n",
    "\n",
    "**claim :**\n",
    "\n",
    "$\\text{trace}\\big(\\mathbf{AB}\\big) \\leq \\frac{1}{2}\\Big(\\text{trace}\\big(\\mathbf A^2 \\big) + \\text{trace}\\big(\\mathbf B^2\\big)\\Big)$\n",
    "\n",
    "**proof:**\n",
    "\n",
    "$\\mathbf C := \\mathbf A - \\mathbf B$\n",
    "\n",
    "$\\mathbf C^H = \\mathbf A^H - \\mathbf B^H = \\mathbf A - \\mathbf B = \\mathbf C$\n",
    "\n",
    "hence $\\mathbf C$ is Hermitian\n",
    "\n",
    "$\\mathbf C^2 = \\mathbf C \\mathbf C = \\mathbf C^H \\mathbf C$\n",
    "\n",
    "hence $\\mathbf C^2$ is Hermitian positive semi-definite, thus its trace must be real valued and $\\geq 0$\n",
    "\n",
    "$\\text{trace}\\big(\\mathbf C^2\\big) = \\text{trace}\\Big( \\big(\\mathbf A - \\mathbf B\\big)^2 \\Big) \\geq 0$\n",
    "\n",
    "$\\text{trace}\\Big( \\big(\\mathbf A - \\mathbf B\\big)^2 \\Big) = \\text{trace}\\Big(\\mathbf A^2 + \\mathbf B^2 - \\mathbf {AB} - \\mathbf {BA}\\Big) \\geq 0$  \n",
    "\n",
    "$\\text{trace}\\Big( \\big(\\mathbf A - \\mathbf B\\big)^2 \\Big) = \\text{trace}\\big(\\mathbf A^2 \\big) + \\text{trace}\\big(\\mathbf B^2\\big) - 2\\cdot \\text{trace}\\big(\\mathbf {AB}\\big) \\geq 0$  \n",
    "\n",
    "$\\text{trace}\\big(\\mathbf A^2 \\big) + \\text{trace}\\big(\\mathbf B^2\\big) \\geq 2\\cdot \\text{trace}\\big(\\mathbf {AB}\\big) $\n",
    "\n",
    "which we can re-arrange as \n",
    "\n",
    "$\\text{trace}\\big(\\mathbf{AB}\\big) \\leq \\frac{1}{2}\\Big(\\text{trace}\\big(\\mathbf A^2 \\big) + \\text{trace}\\big(\\mathbf B^2\\big)\\Big)$\n",
    "- - - -\n",
    "(4.38 and 4.39)\n",
    "\n",
    "consider $\\mathbf A$ which is Hermitian positive definite and $\\mathbf B$ which is Hermitian (both are $n$ x $n$).  \n",
    "\n",
    "- The eigenvalues of $\\big(\\mathbf A \\mathbf B\\big)$ are the same as those in $\\big(\\mathbf A^{\\frac{1}{2}} \\mathbf B\\mathbf A^{\\frac{1}{2}}\\big)$, which is Hermitian, and hence both matrices have all real eigenvalues\n",
    "\n",
    "- The eigenvalues of $\\big(\\mathbf A^{-1} \\mathbf B\\big)$ are the same as those in $\\big(\\mathbf A^{\\frac{-1}{2}} \\mathbf B\\mathbf A^{\\frac{-1}{2}}\\big)$, which is Hermitian, and hence both matrices have all real eigenvalues\n",
    "\n",
    "- This is proven as an extension in \"Vandermonde_Matrices_Permutations_and_Discrete_Fourier_Transform.ipynb\" which states that the matrix given by $\\big(\\mathbf G \\mathbf H\\big)$ has the same non-zero eigenvalues as $\\big(\\mathbf H \\mathbf G\\big)$, and if both matrices are square, then they have the same dimension and hence have the same number of zero eigenvalues as well\n",
    "\n",
    "- $\\big(\\mathbf A + \\mathbf B\\big)$ is positive semi definite **iff** each eigenvalue of $\\big(\\mathbf A^{-1}\\mathbf B\\big)\\geq -1$.  Why?  We examine $\\big(\\mathbf A + \\mathbf B\\big)$ then  multiply left side and right side by $\\mathbf A^{\\frac{-1}{2}}$ which gives us another Hermitian matrix $\\mathbf A^{\\frac{-1}{2}}\\big(\\mathbf A + \\mathbf B\\big)   \\mathbf A^{\\frac{-1}{2}} = \\mathbf I + \\mathbf A^{\\frac{-1}{2}} \\mathbf B \\mathbf A^{\\frac{-1}{2}}  $ -- and Sylvester's Law of Inertia tells us that the number of positive eigenvalues, negative eigenvalues and zero eigenvalues of $\\big(\\mathbf A + \\mathbf B\\big)$ is intact when we look instead at $\\mathbf A^{\\frac{-1}{2}}\\big(\\mathbf A + \\mathbf B\\big)   \\mathbf A^{\\frac{-1}{2}}$.  Thus $\\mathbf A^{\\frac{-1}{2}} \\mathbf B \\mathbf A^{\\frac{-1}{2}}$ must have all eigenvalues being $\\geq -1$ for positive semidefiniteness to remain intact (i.e. they must all be $\\geq 0$ once we increase them by one, which comes from adding the identity matrix to this).  Recall that $\\mathbf A^{\\frac{-1}{2}} \\mathbf B \\mathbf A^{\\frac{-1}{2}}$ has the same eigenvalues as $\\mathbf A^{-1} \\mathbf B$, which completes the problem. \n",
    "\n",
    "- With $\\mathbf A$ positive definite, we know that $\\big(\\mathbf {AB}\\big)$ is diagonalizable because it is similar to $\\mathbf A^{\\frac{1}{2}} \\mathbf B \\mathbf A^{\\frac{1}{2}}$ which is Hermitian and thus must be diagonalizable.  Specifically: where $\\mathbf S = \\mathbf A^{\\frac{1}{2}}$, we can see $\\mathbf S^{-1} \\big(\\mathbf {AB}\\big)\\mathbf S= \\mathbf A^{\\frac{1}{2}} \\mathbf B \\mathbf A^{\\frac{1}{2}}$.  However if $\\mathbf A$ is singular, we lose this similarity transform, and hence guaranties about the diagonalizability of $\\big(\\mathbf {AB}\\big)$ go away.  While your author generally creates random matrices to show existence, in this case he consulted the book for a simple example of defectiveness when multiplying Hermitian postive semidefinite $\\mathbf A$ with Hermitian $\\mathbf B$.  The example is:  $\\begin{bmatrix}\n",
    "1 &1 \\\\ \n",
    " 1& 1\n",
    "\\end{bmatrix} \\begin{bmatrix}\n",
    "1 &0 \\\\ \n",
    " 0& -1\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "1 & -1 \\\\ \n",
    " 1& -1\n",
    "\\end{bmatrix}$, where the right hand side is a rank one matrix with zero trace and hence must be defective.  (See 'Diagonalization_of_rank_one_matrices.ipynb' for more information.) note in the above example we can still see that \n",
    "$\\begin{bmatrix}\n",
    "1 &1 \\\\ \n",
    " 1& 1\n",
    "\\end{bmatrix}^\\frac{1}{2} \\begin{bmatrix}\n",
    "1 &0 \\\\ \n",
    " 0& -1\n",
    "\\end{bmatrix}\\begin{bmatrix}\n",
    "1 &1 \\\\ \n",
    " 1& 1\n",
    "\\end{bmatrix}^\\frac{1}{2} = \\begin{bmatrix}\n",
    "0 & 0 \\\\ \n",
    " 0& 0\n",
    "\\end{bmatrix}$  \n",
    "exists, has the same eigenvalues (i.e. both zero), and hence is Hermitian and hence is diagonalizable, and hence is the zero matrix.  \n",
    "\n",
    "\n",
    "\n",
    "- - - - \n",
    "\n",
    "\n",
    "**SPECIAL NOTE: in the below problem** $\\mathbf A$** is a 'regular matrix' i.e. not Hermitian, etc.**\n",
    "\n",
    "We are, in effect, looking at the two different ways to 'force' $\\mathbf A$ to be Hermitian -- by averaging it with its conjugate transpose, or in effect looking at $\\mathbf A^H \\mathbf A$.  \n",
    "\n",
    "all matrices are $n$ x $n$ and the scalar field is $\\mathbb C$\n",
    "\n",
    "\n",
    "(4.40)\n",
    "**claim:**  \n",
    "\n",
    "$\\lambda_{max}\\Big(\\frac{1}{2}\\big(\\mathbf A + \\mathbf A^H \\big)\\Big) \\leq \\sigma_{max}\\Big(\\mathbf A\\Big)$\n",
    "\n",
    "**proof:**  \n",
    "\n",
    "let $\\mathbf B := \\mathbf A^H$\n",
    "\n",
    "$\\mathbf C := \\mathbf A + \\mathbf B  = \\mathbf A + \\mathbf A^H$\n",
    "\n",
    "note that the $\\mathbf C$ is Hermitian, so its eigenvalues are real.  Its maximal eigenvalue *in magnitude* is equal to its largest singular value.  That is, $\\Big \\vert \\big \\vert \\lambda\\big \\vert_{max} \\big(\\mathbf C\\big)\\Big \\vert = \\sigma_{max}\\big(\\mathbf C\\big)$\n",
    "- - - - \n",
    "*technical note:* $\\Big \\vert \\big \\vert \\lambda\\big \\vert_{max} \\big(\\mathbf C\\big)\\Big \\vert$ either refers to $\\lambda_{1,C}$ or $\\big \\vert \\lambda_{n,C}\\big \\vert$.  If the former has largest magnitude it must be positive and if $\\lambda_{n,C}$ has largest magntiude, it must be negative. \n",
    "\n",
    "with ordering, as always: \n",
    "$\\lambda_{1,C} \\geq \\lambda_{2,C} \\geq \\lambda_{3,C} \\geq ... \\geq \\lambda_{n-1,C} \\geq \\lambda_{n,C}$\n",
    "\n",
    "- - - -\n",
    "\n",
    "note that $\\sigma_{max}\\big(\\mathbf B\\big) = \\sigma_{max}\\big(\\mathbf A\\big)$\n",
    "\n",
    "we know from \"SingularValue_Inequalities.ipynb\" that \n",
    "\n",
    "$\\sigma_{A,1} + \\sigma_{B,1} \\geq \\sigma_{C,1}$\n",
    "\n",
    "or equivalently \n",
    "\n",
    "$\\sigma_{max}\\big(\\mathbf A\\big)+ \\sigma_{max}\\big(\\mathbf B\\big) \\geq \\sigma_{max}\\big(\\mathbf C\\big)$\n",
    "\n",
    "which can be re-written as: \n",
    "\n",
    "$2 \\sigma_{max}\\big(\\mathbf A\\big) \\geq \\sigma_{max}\\big(\\mathbf C\\big) = \\Big \\vert \\big \\vert \\lambda\\big \\vert_{max} \\big(\\mathbf C\\big)\\Big \\vert \\geq \\lambda_{1,C} = \\lambda_{max}\\big(\\mathbf C\\big) $ \n",
    "\n",
    "\n",
    "\n",
    "$\\sigma_{max}\\big(\\mathbf A\\big) \\geq \\frac{1}{2} \\lambda_{max}\\big(\\mathbf C\\big)$\n",
    "\n",
    "$\\sigma_{max}\\big(\\mathbf A\\big) \\geq \\frac{1}{2} \\lambda_{max}\\Big(\\big(\\mathbf A + \\mathbf A^H \\big)\\Big) =  \\lambda_{max}\\Big(\\frac{\\mathbf A + \\mathbf A^H }{2}\\Big)$\n",
    "\n",
    "*alternative proof* \n",
    "\n",
    "This is quite similar to the above (though the singular value inequalities proof page only contemplated real matrices so this a touch different)\n",
    "\n",
    "where $\\big \\Vert \\mathbf x \\big \\Vert_2^2 = 1$ and $\\big \\Vert \\mathbf y \\big \\Vert_2^2 = 1$, $\\big \\Vert \\mathbf z \\big \\Vert_2^2 = 1$ \n",
    "\n",
    "maximize $real\\Big(\\mathbf x^H \\mathbf A \\mathbf x \\Big)$ = maximize $real\\Big(\\mathbf x^H \\mathbf A^H \\mathbf x \\Big)$\n",
    "\n",
    "hence \n",
    "\n",
    "maximize:  \n",
    "$real\\Big(\\mathbf x^H \\mathbf A \\mathbf x \\Big)  = \\frac{1}{2}real\\Big(\\mathbf x^H \\big(\\mathbf A\\big) \\mathbf x \\Big) +\\frac{1}{2} real\\Big(\\mathbf x^H \\big( \\mathbf A^H\\big) \\mathbf x \\Big)  =   \\Big(\\mathbf x^H \\big( \\frac{\\mathbf A  + \\mathbf A^H}{2} \\big) \\mathbf x \\Big) = \\lambda_{max}\\Big( \\frac{\\mathbf A  + \\mathbf A^H}{2}\\Big) $\n",
    "\n",
    "recalling that the scalar result of a quadratic form over a Hermitian matrix -- specifically $\\big( \\frac{\\mathbf A  + \\mathbf A^H}{2} \\big)$ -- must be real.  \n",
    "\n",
    "but we know \n",
    "\n",
    "maximize $real\\Big(\\mathbf x^H \\mathbf A \\mathbf x \\Big) \\leq $ maximize $real\\Big(\\mathbf y^H \\mathbf A \\mathbf z \\Big) = \\sigma_{max}\\big(\\mathbf A\\big) $\n",
    "\n",
    "because we can always get the same 'payoff' on the right hand side by setting $\\mathbf y:=\\mathbf x$ and $ \\mathbf z:= \\mathbf x$, thus the right hand side must have a 'payoff' at least as high as the left hand side\n",
    "\n",
    "hence \n",
    "\n",
    "$\\lambda_{max}\\Big( \\frac{\\mathbf A  + \\mathbf A^H}{2}\\Big) = $ max $real\\Big(\\mathbf x^H \\mathbf A \\mathbf x \\Big) \\leq $max $real \\Big(\\mathbf y^H \\mathbf A \\mathbf z \\Big) = \\sigma_{max}\\big(\\mathbf A\\big)$\n",
    "\n",
    "or more succinctly:  \n",
    "$\\lambda_{max}\\Big( \\frac{\\mathbf A  + \\mathbf A^H}{2}\\Big) \\leq \\sigma_{max}\\big(\\mathbf A\\big)$\n",
    "\n",
    "**subsequent note:**   \n",
    "a better approach uses quasi-linearization and Cauchy-Schwarz (ref CS Masterclass notes).  In effect this is a form of triangle inequality.  \n",
    "\n",
    "where for each $k$, we have the constraint $\\big \\Vert \\mathbf x_k \\big \\Vert_2 = 1$   \n",
    "\n",
    "$\\lambda_{max}\\Big(\\frac{1}{2}\\big(\\mathbf A + \\mathbf A^H \\big)\\Big)$  \n",
    "$= \\frac{1}{2} \\text{max}\\Big\\{ \\mathbf x_0^T \\big(\\mathbf A + \\mathbf A^H \\big)\\mathbf x_0\\Big\\}$  \n",
    "$= \\frac{1}{2} \\text{max}\\Big\\{ \\mathbf x_0^T \\mathbf A \\mathbf x_0 + \\mathbf x_0^T \\mathbf A^H \\mathbf x_0\\Big\\}$  \n",
    "$\\leq \\frac{1}{2}\\Big(\\text{max}\\Big\\{ \\mathbf x_1^T \\mathbf A \\mathbf x_1\\Big\\} + \\text{max}\\Big\\{\\mathbf x_1^T \\mathbf A^H \\mathbf x_2\\Big\\}\\Big)$  \n",
    "$ \\leq \\frac{1}{2}\\Big( \\big \\Vert \\mathbf x_1 \\big \\Vert_2  \\big \\Vert \\mathbf A \\mathbf x_1 \\big \\Vert_2 + \\big \\Vert \\mathbf x_1\\big \\Vert_2 \\big \\Vert \\mathbf A^H \\mathbf x_2\\big \\Vert_2\\Big)$  \n",
    "$ = \\frac{1}{2}\\Big( \\big \\Vert \\mathbf A \\mathbf x_1 \\big \\Vert_2 + \\big \\Vert \\mathbf A^H \\mathbf x_2\\big \\Vert_2\\Big)$  \n",
    "$ = \\frac{1}{2}\\Big(\\sigma_{max}\\big(\\mathbf A\\big) + \\sigma_{max}\\big(\\mathbf A^H\\big)\\Big)$  \n",
    "$ = \\frac{1}{2}\\Big(2\\sigma_{max}\\big(\\mathbf A\\big)\\Big)$  \n",
    "$ = \\sigma_{max}\\big(\\mathbf A\\big)$  \n",
    "\n",
    "where the first inequality follows because 2 choices are better than one, and the second inequality is Cauchy Schwarz.      \n",
    "\n",
    "\n",
    "**end subsequent note:**  \n",
    "\n",
    "\n",
    "**claim:**  \n",
    "\n",
    "$\\text{trace}\\Big(\\big(\\frac{\\mathbf A + \\mathbf A^H }{2}\\big)^2\\Big) \\leq \\text{trace}\\Big(\\mathbf A^H \\mathbf A\\Big)$\n",
    "\n",
    "**proof:**  \n",
    "\n",
    "$\\text{trace}\\Big(\\big(\\frac{\\mathbf A + \\mathbf A^H }{2}\\big)^2\\Big) = \\frac{1}{4} \\text{trace}\\Big(\\mathbf A^2 + \\big(\\mathbf A^H\\big)^2 +\\mathbf A^H \\mathbf A + \\mathbf A \\mathbf A^H\\Big) \\leq \\text{trace}\\Big(\\mathbf A^H \\mathbf A\\Big)$\n",
    "\n",
    "re-write this as\n",
    "\n",
    "$\\text{trace}\\Big(\\big(\\frac{\\mathbf A + \\mathbf A^H }{2}\\big)^2\\Big) = \\frac{1}{4}\\text{trace}\\Big(\\mathbf A^2 + \\big(\\mathbf A^H\\big)^2\\Big) + \\frac{1}{4} \\text{trace}\\Big( \\mathbf A^H \\mathbf A\\Big) +  \\frac{1}{4} \\text{trace}\\Big(\\mathbf A \\mathbf A^H\\Big)  \\leq \\text{trace}\\Big(\\mathbf A^H \\mathbf A\\Big)$\n",
    "\n",
    "$\\text{trace}\\Big(\\big(\\frac{\\mathbf A + \\mathbf A^H }{2}\\big)^2\\Big)= \\frac{1}{4}\\text{trace}\\Big(\\mathbf A^2 + \\big(\\mathbf A^H\\big)^2\\Big) + \\frac{1}{2}\\text{trace}\\Big(\\mathbf A^H \\mathbf A\\Big) \\leq \\text{trace}\\Big(\\mathbf A^H \\mathbf A\\Big)$\n",
    "\n",
    "Thus if we can prove:\n",
    "\n",
    "$\\frac{1}{4}\\text{trace}\\Big(\\mathbf A^2 + \\big(\\mathbf A^H\\big)^2\\Big) + \\frac{1}{2}\\text{trace}\\Big(\\mathbf A^H \\mathbf A\\Big) \\leq \\text{trace}\\Big(\\mathbf A^H \\mathbf A\\Big)$\n",
    "\n",
    "then we are done.  Let's simplify this to\n",
    "\n",
    "$\\frac{1}{4}\\text{trace}\\Big(\\mathbf A^2 + \\big(\\mathbf A^H\\big)^2\\Big) \\leq \\frac{1}{2} \\text{trace}\\Big(\\mathbf A^H \\mathbf A\\Big)$\n",
    "\n",
    "multiply both sides by 2\n",
    "\n",
    "$\\frac{1}{2}\\Big(\\text{trace}\\big(\\mathbf A^2 \\big) + \\text{trace}\\Big(\\big(\\mathbf A^H\\big)^2\\Big)\\Big) \\leq \\text{trace}\\Big(\\mathbf A^H \\mathbf A\\Big)$\n",
    "\n",
    "- - - -\n",
    "note that, except for complex conjugation (which offset each other),\n",
    "\n",
    "$\\text{trace}\\big(\\mathbf A^2 \\big) = \\text{trace}\\Big(\\big(\\mathbf A^H\\big)^2\\Big)$\n",
    "\n",
    "To be exact, we'd say:  \n",
    "\n",
    "$real\\Big(\\text{trace}\\big(\\mathbf A^2 \\big)\\Big) = real\\Big(\\text{trace}\\Big(\\big(\\mathbf A^H\\big)^2\\Big)\\Big)$\n",
    "\n",
    "Thus \n",
    "\n",
    "$real\\Big(\\text{trace}\\big(\\mathbf A^2 \\big) \\Big) +real\\Big( \\text{trace}\\Big(\\big(\\mathbf A^H\\big)^2\\Big)\\Big) = 2*real\\Big(\\text{trace}\\big(\\mathbf A^2 \\big) \\Big) \\leq 2\\big \\vert \\text{trace}\\big(\\mathbf A^2 \\big) \\big \\vert$\n",
    "\n",
    "- - - -\n",
    "we return to our to our main inequality, and apply the above fact (inclusive of the $\\frac{1}{2}$ scaling)\n",
    "\n",
    "$\\frac{1}{2}\\Big(\\text{trace}\\big(\\mathbf A^2 \\big) + \\text{trace}\\Big(\\big(\\mathbf A^H\\big)^2\\Big)\\Big) = \\frac{2}{2} real\\Big(\\text{trace}\\big(\\mathbf A^2 \\big) \\Big) \\leq \\frac{2}{2}\\big \\vert \\text{trace}\\big(\\mathbf A^2 \\big) \\big \\vert = \\big \\vert \\text{trace}\\big(\\mathbf {AA} \\big)\\big \\vert \\leq  \\text{trace}\\Big(\\mathbf A^H \\mathbf A\\Big)$\n",
    " \n",
    "and we confirm that \n",
    "\n",
    "$\\big \\vert \\text{trace}\\big(\\mathbf {AA} \\big) \\big \\vert \\leq  \\text{trace}\\Big(\\mathbf A^H \\mathbf A\\Big)$\n",
    "\n",
    "via the expanded Schur Inequality (see the writeup in \"Schurs_Inequality.ipynb\").  This completes the proof.\n",
    "\n",
    "**extension**\n",
    "\n",
    "The last claim was \n",
    "\n",
    "that \n",
    "\n",
    "$\\text{trace}\\Big(\\big(\\frac{\\mathbf A + \\mathbf A^H }{2}\\big)^2\\Big) \\leq \\text{trace}\\Big(\\mathbf A^H \\mathbf A\\Big)$\n",
    "\n",
    "but what if we wanted the square root of the above, before taking the trace?  \n",
    "\n",
    "where $\\mathbf X$ is the positive square root of the right hand side? i.e.  \n",
    "\n",
    "$\\mathbf X:= \\Big(\\mathbf A^H \\mathbf A\\Big)^{\\frac{1}{2}}$\n",
    "\n",
    "and for convenience\n",
    "\n",
    "$\\mathbf Y:= \\big(\\frac{\\mathbf A + \\mathbf A^H }{2}\\big)$\n",
    "\n",
    "is it true that \n",
    "\n",
    "$\\text{trace}\\Big(\\mathbf Y \\Big) \\leq \\text{trace}\\Big(\\mathbf X\\Big)$\n",
    "\n",
    "yes.  And noting that the eigenvalues and singular values of $\\big(\\mathbf Y\\big)$ are all real, and identical, except that the former may have a negative sign in front of them, we'll prove an even stronger claim:\n",
    "\n",
    "**claim**\n",
    "\n",
    "$\\sigma_{1,Y} + \\sigma_{2,Y} + ... + \\sigma_{n,Y} \\leq \\sigma_{1,X} + \\sigma_{2,X} + ... + \\sigma_{n,X} $\n",
    "\n",
    "or equivalently \n",
    "\n",
    "$\\sum_{k=1}^n \\sigma_k \\Big(\\big(\\frac{\\mathbf A + \\mathbf A^H }{2}\\big)\\Big) \\leq \\sum_{k=1}^n \\sigma_k \\Big(\\mathbf A\\Big) $\n",
    "\n",
    "where $\\sigma_k \\Big(\\mathbf Z\\Big)$ denotes the kth singular value of some matrix $\\mathbf Z$\n",
    "\n",
    "**proof**\n",
    "\n",
    "putting these interesting things together, we have:\n",
    "\n",
    "$\\text{trace}\\Big(\\mathbf Y \\Big) = \\lambda_{1,Y} + \\lambda_{2,Y} + ... + \\lambda_{n,Y} \\leq \\sigma_{1,Y} + \\sigma_{2,Y} + ... + \\sigma_{n,Y} \\leq \\sigma_{1,X} + \\sigma_{2,X} + ... + \\sigma_{n,X} =  \\sum_{k=1}^n \\sigma_k \\Big(\\mathbf A\\Big)= \\text{trace}\\Big(\\mathbf X\\Big)$\n",
    "\n",
    "where $\\lambda_{1,Y} + \\lambda_{2,Y} + ... + \\lambda_{n,Y} \\leq \\big\\vert\\lambda_{1,Y}\\big\\vert + \\big\\vert\\lambda_{2,Y}\\big\\vert + ... + \\big\\vert\\lambda_{n,Y}\\big\\vert = \\sigma_{1,Y} + \\sigma_{2,Y} + ... + \\sigma_{n,Y}$, by the triangle inequality.  \n",
    "\n",
    "The proof is simply that \n",
    "\n",
    "$\\sigma_{1,Y} + \\sigma_{2,Y} + ... + \\sigma_{n,Y} \\leq \\frac{1}{2}\\big(\\sigma_{1,A} + \\sigma_{2,A} + ... + \\sigma_{n,A}\\big) + \\frac{1}{2}\\big(\\sigma_{1,A} + \\sigma_{2,A} + ... + \\sigma_{n,A}\\big)= \\sigma_{1,A} + \\sigma_{2,A} + ... + \\sigma_{n,A} = \\sum_{k=1}^n \\sigma_k \\Big(\\mathbf A\\Big) $\n",
    "\n",
    "where we use the \"L1 norm for singular values\" contained in \"SingularValue_Inequalities.ipynb\"\n",
    "\n",
    "and for avoidance of doubt, our original equation is:\n",
    "\n",
    "$\\mathbf Y = \\frac{1}{2} \\mathbf A + \\frac{1}{2}\\mathbf A^H$\n",
    "\n",
    "but in the format of that proof we'd have $\\mathbf C:= \\mathbf Y$ and $\\mathbf A := \\frac{1}{2} \\mathbf A$ and $\\mathbf B : = \\frac{1}{2}\\mathbf A^H$\n",
    "\n",
    "noting that the singluar values of $\\mathbf B$ (and our 'new' $\\mathbf A$) are exactly half of the singular values of our 'original' $\\mathbf A$.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "*Quadratic form equivalence between Hermitian matrices*   \n",
    "**claim:**   \n",
    "Two $\\text{n}$ x $\\text{n}$  \n",
    "Hermitian matrices $\\mathbf A$ and $\\mathbf B$ are equal *iff* \n",
    "\n",
    "$\\mathbf x^H \\mathbf A\\mathbf x = \\mathbf x^H \\mathbf B\\mathbf x$   \n",
    "for all $\\mathbf x \\in \\mathbb C^n$   \n",
    "\n",
    "**proof:** \n",
    "\n",
    "the first leg is easy:  \n",
    "if $\\mathbf A = \\mathbf B$ then \n",
    "$\\mathbf A\\mathbf x = \\mathbf B\\mathbf x$  \n",
    "and  \n",
    "$\\mathbf x^H \\mathbf A\\mathbf x = \\mathbf x^H \\mathbf B\\mathbf x$   \n",
    "\n",
    "the second leg:  \n",
    "we may proove this two different ways.  First, using extremal characterization and second algebraicly.  \n",
    "\n",
    "(i)  \n",
    "suppose we 'only' know that $\\mathbf x^H \\mathbf A\\mathbf x = \\mathbf x^H \\mathbf B\\mathbf x$   for all  \n",
    "$\\mathbf x \\in \\mathbb C^n$  \n",
    "\n",
    "now consider the Hermitian matrix $\\big(\\mathbf A - \\mathbf B\\big)$.  We'll prove this is the zero matrix.  Using the extremal characterization of eigenvalues (or equivalently: a compactness argument) we have   \n",
    "\n",
    "$\\lambda_1 = \\max_{\\mathbf x} \\mathbf x^H\\big(\\mathbf A - \\mathbf B\\big)\\mathbf x = \\max_{\\mathbf x} \\mathbf x^H\\mathbf A\\mathbf x - \\mathbf x^H\\mathbf B\\mathbf x = 0 $   \n",
    "where $\\big \\Vert \\mathbf x \\big \\Vert_2 = 1$  \n",
    "\n",
    "$\\lambda_n = \\min_{\\mathbf x} \\mathbf x^H\\big(\\mathbf A - \\mathbf B\\big)\\mathbf x = \\min_{\\mathbf x} \\mathbf x^H\\mathbf A\\mathbf x - \\mathbf x^H\\mathbf B\\mathbf x = 0 $  \n",
    "where $\\big \\Vert \\mathbf x \\big \\Vert_2 = 1$  \n",
    "so we have  \n",
    "$0=\\lambda_1 \\geq \\lambda_2 \\geq .... \\geq \\lambda_n = 0$  \n",
    "\n",
    "but this implies \n",
    "$\\mathbf 0 = \\big(\\mathbf A - \\mathbf B\\big)$   \n",
    "because the RHS is diagonalizable, with all eigenvalues equal to zero and is must be the \n",
    "zero matrix, which proves $\\mathbf A = \\mathbf B$  \n",
    "\n",
    "(ii) algebraic proof of the second leg   \n",
    "\n",
    "we can assume WLOG that $\\mathbf B$ is diagonal \n",
    "(if it is not diagonal, collect its eigenvectors in orthogonal matrix $\\mathbf U$ and for ever $\\mathbf x$ of interest, we may instead use $\\mathbf {Ux}$) \n",
    "\n",
    "It is most natural to assume the diagonal elements of $\\mathbf B$ and hence its eigenvalues are ordered \n",
    "$\\lambda_1 \\geq \\lambda_2 \\geq ... \\geq \\lambda_n$  \n",
    "\n",
    "There are slightly faster ways to finish this, but for convenience first consider the over each standard basis vector $\\mathbf e_k$ for $k \\in \\{1, 2, ..., n\\}$  \n",
    "\n",
    "$\\mathbf e_k^H \\mathbf A\\mathbf e_k = \\mathbf e_k^H \\mathbf B\\mathbf e_k = \\lambda_{k}$     \n",
    "\n",
    "which ensures that each diagonal component of $\\mathbf A$ is the same as each diagonal component of $\\mathbf B$\n",
    "\n",
    "we can equivalently write this as  \n",
    "$\\text{trace}\\big(\\mathbf A\\mathbf e_k\\mathbf e_k^H\\big)  = \\text{trace}\\big(\\mathbf B\\mathbf e_k\\mathbf e_k^H\\big) = \\lambda_{k}$     \n",
    "\n",
    "\n",
    "now consider  \n",
    "$\\text{trace}\\big(\\mathbf A\\mathbf e_k\\mathbf e_k^H\\lambda_{k}\\big)  = \\text{trace}\\big(\\mathbf B\\mathbf e_k\\mathbf e_k^H\\lambda_{k}\\big)$     \n",
    "\n",
    "and sum over all $k$ to get \n",
    "\n",
    "$\\text{trace}\\big(\\mathbf {A B}\\big)$  \n",
    "$= \\text{trace}\\big(\\mathbf A \\sum_{k=1}^n \\mathbf e_k\\mathbf e_k^H\\lambda_{k}\\big) $  \n",
    "$= \\sum_{k=1}^n \\text{trace}\\big(\\mathbf A\\mathbf e_k\\mathbf e_k^H\\lambda_{k}\\big)  $  \n",
    "$= \\sum_{k=1}^n \\text{trace}\\big(\\mathbf B\\mathbf e_k\\mathbf e_k^H\\lambda_{k}\\big) $  \n",
    "$= \\sum_{k=1}^n \\lambda_k^2 $  \n",
    "$= \\text{trace}\\big(\\mathbf B^2\\big)$     \n",
    "\n",
    "running the same argument with the mutually orthonormal eigenvectors of $\\mathbf A$ yields  \n",
    "\n",
    "$\\text{trace}\\big(\\mathbf {A B}\\big)$  \n",
    "$= \\text{trace}\\big(\\mathbf {B A}\\big)$  \n",
    "$= \\text{trace}\\big(\\mathbf B \\sum_{k=1}^n \\mathbf v_k\\mathbf v_k^H \\gamma_{k}\\big) $  \n",
    "$= \\sum_{k=1}^n \\text{trace}\\big(\\mathbf B\\mathbf v_k\\mathbf v_k^H\\gamma_{k}\\big) $  \n",
    "$= \\sum_{k=1}^n \\text{trace}\\big(\\mathbf A\\mathbf v_k\\mathbf v_k^H\\gamma_{k}\\big) $  \n",
    "$= \\sum_{k=1}^n \\gamma_k^2 $  \n",
    "$= \\text{trace}\\big(\\mathbf A^2\\big)$   \n",
    "\n",
    "(note the above implies that if either matrix is the zero matrix then both are the zero matrix and we have the desired equality, the below assumes that both matrices are non-zero to finish the proof)  \n",
    "\n",
    "and summing these two gives  \n",
    "\n",
    "$2\\cdot \\text{trace}\\big(\\mathbf {A B}\\big) =  \\text{trace}\\big(\\mathbf A^2\\big) + \\text{trace}\\big(\\mathbf B^2\\big)  = \\big \\Vert \\mathbf A\\big \\Vert_F^2 + \\big \\Vert \\mathbf B\\big \\Vert_F^2 $  \n",
    "\n",
    "we recognize  \n",
    "\n",
    "$0 \\leq \\big \\Vert \\mathbf A -  \\mathbf B\\big \\Vert_F^2 $  \n",
    "with equality *iff* $\\mathbf A =\\mathbf B$  \n",
    "\n",
    "but if we expand this and make use of Hermicity and cyclic property of the trace, we get  \n",
    "\n",
    "$ 2\\cdot \\text{trace}\\big(\\mathbf {A B}\\big) \\leq \\big \\Vert \\mathbf A\\big \\Vert_F^2 + \\big \\Vert \\mathbf B\\big \\Vert_F^2  $  \n",
    "\n",
    "again, with equality *iff* $\\mathbf A =\\mathbf B$  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hermitian Positive Semi Definite Trace Inequalities\n",
    "\n",
    "where $\\mathbf A$ and $\\mathbf B$ are both Hermitian Positive (Semi) Definite Matrices\n",
    "\n",
    "**claim:** \n",
    "\n",
    "$\\text{trace}\\big(\\mathbf{AB}\\big) \\geq 0$ \n",
    "\n",
    "that is, the the above trace is always real and non-negative.  \n",
    "\n",
    "**proof:**\n",
    "\n",
    "$\\text{trace}\\big(\\mathbf{AB}\\big) = \\text{trace}\\big(\\mathbf A^{\\frac{1}{2}}\\mathbf A^{\\frac{1}{2}} \\mathbf B^{\\frac{1}{2}}\\mathbf B^{\\frac{1}{2}}\\big)= \\text{trace}\\big(\\mathbf B^{\\frac{1}{2}}\\mathbf A^{\\frac{1}{2}} \\mathbf A^{\\frac{1}{2}}\\mathbf B^{\\frac{1}{2}}\\big) = \\text{trace}\\Big(\\big(\\mathbf B^{\\frac{1}{2}}\\big)^H \\big(\\mathbf A^{\\frac{1}{2}}\\big)^H \\mathbf A^{\\frac{1}{2}}\\mathbf B^{\\frac{1}{2}}\\Big) = \\text{trace}\\Big(\\big(\\mathbf A^{\\frac{1}{2}} \\mathbf B^{\\frac{1}{2}}\\big)^H \\big( \\mathbf A^{\\frac{1}{2}}\\mathbf B^{\\frac{1}{2}}\\big)\\Big)$ \n",
    "\n",
    "$\\text{trace}\\big(\\mathbf{AB}\\big) = \\big \\Vert \\mathbf A^{\\frac{1}{2}} \\mathbf B^{\\frac{1}{2}}\\big \\Vert_F^2$\n",
    "\n",
    "Alternatively, we could also note that $\\big(\\mathbf{AB}\\big) = \\Big(\\mathbf A^{\\frac{1}{2}}\\big(\\mathbf A^{\\frac{1}{2}} \\mathbf B\\big)\\Big)$  and must have the same eigenvalues as $\\Big(\\big(\\mathbf A^{\\frac{1}{2}} \\mathbf B\\big)\\mathbf A^{\\frac{1}{2}}\\Big)$ which is Hermitian Positive semi-defintie, and hence has all real, non-negative eigenvalues.  Since the trace gives the sum of those eigenvalues, the trace must be real, non-negative.  \n",
    "\n",
    "\n",
    "**claim:**\n",
    "\n",
    "$\\text{trace}\\big(\\mathbf{AB}\\big) \\leq \\text{trace}\\big(\\mathbf A\\big) \\text{trace}\\big(\\mathbf B \\big) \\leq \\frac{1}{2}\\Big(\\text{trace}\\big(\\mathbf A\\big)^2 + \\text{trace}\\big(\\mathbf B\\big)^2\\Big)$\n",
    "\n",
    "**commentary:**\n",
    "\n",
    "The left side of the inequality is of particular interest.  \n",
    "\n",
    "$\\text{trace}\\big(\\mathbf{AB}\\big) \\leq \\text{trace}\\big(\\mathbf A\\big) \\text{trace}\\big(\\mathbf B \\big) $\n",
    "\n",
    "can be rewritten as:\n",
    "\n",
    "$\\text{trace}\\big(\\mathbf{AB}\\big) \\leq \\big(\\sum_{i = 1}^{n} \\mathbf{A}_{i,i}\\big) \\big(\\sum_{i = 1}^{n} \\mathbf{B}_{i,i}\\big)$\n",
    "\n",
    "since $\\mathbf A$ and $\\mathbf B$ are both Hermitian positive semi-definite, we may recall the Hadamard Inequality (see \"HadamardInequality.ipynb\"), and draw an analogy with determinants.\n",
    "\n",
    "noting that $\\text{det}\\big(\\mathbf{AB}\\big) = \\text{det}\\big(\\mathbf{A}\\big) \\text{det}\\big(\\mathbf{B}\\big)$,\n",
    "\n",
    "we use the Hadarmard inequality:  \n",
    "\n",
    "$\\text{det}\\big(\\mathbf{A}\\big) \\leq \\big(\\prod_{i = 1}^{n} \\mathbf{A}_{i,i}\\big)$  \n",
    "$\\text{det}\\big(\\mathbf{B}\\big) \\leq \\big(\\prod_{i = 1}^{n} \\mathbf{B}_{i,i}\\big)$  \n",
    "\n",
    "hence \n",
    "\n",
    "$\\text{det}\\big(\\mathbf{AB}\\big) \\leq \\big(\\prod_{i = 1}^{n} \\mathbf{A}_{i,i}\\big) \\big(\\prod_{i = 1}^{n} \\mathbf{B}_{i,i}\\big)$\n",
    "\n",
    "which seems analogous  to  \n",
    "\n",
    "$\\text{trace}\\big(\\mathbf{AB}\\big) \\leq \\big(\\sum_{i = 1}^{n} \\mathbf{A}_{i,i}\\big) \\big(\\sum_{i = 1}^{n} \\mathbf{B}_{i,i}\\big)$\n",
    "\n",
    "**proof:**  \n",
    "\n",
    "*For the left side of the the inequality:*  \n",
    "\n",
    "$ \\text{trace}\\big(\\mathbf{AB}\\big) = \\big \\Vert \\mathbf A^{\\frac{1}{2}} \\mathbf B^{\\frac{1}{2}}\\big \\Vert_F^2 \\leq \\big \\Vert \\mathbf A^{\\frac{1}{2}}\\big \\Vert_F^2 \\big \\Vert \\mathbf B^{\\frac{1}{2}}\\big \\Vert_F^2 =  \\text{trace}\\Big(\\big(\\mathbf A^{\\frac{1}{2}}\\big)^H \\mathbf A^{\\frac{1}{2}}\\Big) \\text{trace}\\Big(\\big(\\mathbf B^{\\frac{1}{2}}\\big)^H \\mathbf B^{\\frac{1}{2}}\\Big) = \\text{trace}\\big(\\mathbf A\\big) \\text{trace}\\big(\\mathbf B \\big) $\n",
    "\n",
    "where we use the derivation under \"Matrix Norms\" for the proof that \n",
    "\n",
    " $\\big \\Vert \\mathbf A^{\\frac{1}{2}} \\mathbf B^{\\frac{1}{2}}\\big \\Vert_F^2 \\leq \\big \\Vert \\mathbf A^{\\frac{1}{2}}\\big \\Vert_F^2 \\big \\Vert \\mathbf B^{\\frac{1}{2}}\\big \\Vert_F^2 $\n",
    "\n",
    "- - - -\n",
    "*begin alternative proof*\n",
    "\n",
    "Consider that for Hermitian Positive (Semi) Definite matrices, we have strictly real, non-negative values along the diagonal of said matrices and amongst their eigenvalues.  \n",
    "\n",
    "Thus we could interpret this inequality as mutliplying two finite series, and noting that \n",
    "\n",
    "\n",
    "$(\\lambda_1 + \\lambda_2 +... + \\lambda_n)(\\sigma_1 + \\sigma_2 + ... + \\sigma_n) \\geq \\lambda_1 \\sigma_1 + \\lambda_2 \\sigma_2 +... + \\lambda_n \\sigma_n$\n",
    "\n",
    "because every term in the series is real and non-negative\n",
    "\n",
    "To map this to our problem simply let: \n",
    "\n",
    "$\\mathbf A = \\mathbf {Q \\Lambda Q}^H$\n",
    "\n",
    "$\\text{trace}\\big(\\mathbf A\\big) = \\lambda_1 + \\lambda_2 +... + \\lambda_n$\n",
    "\n",
    "$\\text{trace}\\big(\\mathbf B\\big) = \\sigma_1 + \\sigma_2 + ... + \\sigma_n$\n",
    "\n",
    "\n",
    "$\\text{trace}\\big(\\mathbf {AB}\\big) = \\text{trace}\\big(\\mathbf {Q\\Lambda Q}^H \\mathbf B\\big) = \\text{trace}\\big(\\mathbf {\\Lambda Q}^H \\mathbf B\\mathbf Q\\big)$\n",
    "\n",
    "Now define a new matrix $\\mathbf C := \\mathbf Q^H \\mathbf{BQ}$, which is still Hermitian, and similar to $\\mathbf B$, and hence Positive Semi Definite.  We see that \n",
    "\n",
    "$\\text{trace}\\big(\\mathbf {AB}\\big) = \\text{trace}\\big(\\mathbf {\\Lambda C}\\big) $\n",
    "\n",
    "\n",
    "$\\text{trace}\\big(\\mathbf {\\Lambda C}\\big)= \\lambda_1 c_{1,1} + \\lambda_2 c_{2,2} + ... + \\lambda_n c_{n,n} \\leq (\\lambda_1 + \\lambda_2 +... + \\lambda_n)(c_{1,1} + c_{2,2} + ... + c_{n,n}) = \\text{trace}\\big(\\mathbf A\\big) \\text{trace}\\big(\\mathbf C\\big) $\n",
    "\n",
    "Noting that $\\mathbf B$ and $\\mathbf C$ are similar, and hence have the same trace:\n",
    "\n",
    "$\\text{trace}\\big(\\mathbf {AB}\\big) = \\text{trace}\\big(\\mathbf {\\Lambda C}\\big)  \\leq \\text{trace}\\big(\\mathbf A\\big) \\text{trace}\\big(\\mathbf C\\big)= \\text{trace}\\big(\\mathbf A\\big)\\text{trace}\\big(\\mathbf B\\big)$ \n",
    "\n",
    "\n",
    "*end alternative proof*\n",
    "- - - -\n",
    "\n",
    "*For the right hand side of the inequality:*  \n",
    "\n",
    "$\\big(\\mathbf A - \\mathbf B\\big)$ is a Hermitian matrix, and hence its trace given by  \n",
    "\n",
    "$\\text{trace}\\big(\\mathbf A - \\mathbf B \\big) = \\text{trace}\\big(\\mathbf A\\big) - \\text{trace}\\big(\\mathbf B \\big)$\n",
    "\n",
    "is a real number.  Whenever we square a real number, the result must be $\\geq 0$\n",
    "\n",
    "\n",
    "$\\Big(\\text{trace}\\big(\\mathbf A\\big) - \\text{trace}\\big(\\mathbf B \\big)\\Big)^2 \\geq 0 $  \n",
    "$\\text{trace}\\big(\\mathbf A\\big)^2 + \\text{trace}\\big(\\mathbf B \\big)^2 - 2 \\cdot \\text{trace}\\big(\\mathbf A\\big)\\text{trace}\\big(\\mathbf B\\big) \\geq 0$\n",
    "\n",
    "\n",
    "hence we see that \n",
    "\n",
    "$\\frac{1}{2}\\text{trace}\\big(\\mathbf A\\big)^2 + \\frac{1}{2} \\text{trace}\\big(\\mathbf B \\big)^2 \\geq \\text{trace}\\big(\\mathbf A\\big)\\text{trace}\\big(\\mathbf B\\big) $\n",
    "\n",
    "This proves that \n",
    "\n",
    "$\\text{trace}\\big(\\mathbf{AB}\\big) \\leq \\text{trace}\\big(\\mathbf A\\big) \\text{trace}\\big(\\mathbf B \\big) \\leq \\frac{1}{2}\\Big(\\text{trace}\\big(\\mathbf A\\big)^2 + \\text{trace}\\big(\\mathbf B\\big)^2\\Big)$\n",
    "\n",
    "**claim:**  \n",
    "\n",
    "$\\text{trace}\\big(\\mathbf{AB}\\big) \\leq \\lambda_{max}\\big(\\mathbf A\\big) \\text{trace}\\big(\\mathbf B\\big)$\n",
    "\n",
    "**proof:**  \n",
    "\n",
    "for convenience we start by noting $\\text{trace}\\big(\\mathbf{AB}\\big) = \\text{trace}\\big(\\mathbf{BA}\\big)$\n",
    "\n",
    "$\\text{trace}\\big(\\mathbf{BA}\\big) = \\big \\Vert \\mathbf B^{\\frac{1}{2}} \\mathbf A^{\\frac{1}{2}}\\big \\Vert_F^2 \\leq  \\big \\Vert \\mathbf A^{\\frac{1}{2}} \\big \\Vert_2^2 \\big \\Vert\\mathbf B^{\\frac{1}{2}} \\big \\Vert_F^2 = \\sigma_{A,1} \\cdot \\text{trace}\\Big(\\big(\\mathbf B^{\\frac{1}{2}}\\big)^H \\mathbf B^{\\frac{1}{2}} \\Big) = \\lambda_{A,1}\\cdot \\text{trace}\\big(\\mathbf B^{\\frac{1}{2}} \\mathbf B^{\\frac{1}{2}} \\big) = \\lambda_{max}\\big(\\mathbf A\\big) \\text{trace}\\big(\\mathbf B\\big)$  \n",
    "\n",
    "using results from the \"Matrix Norms\" section to justify the inequality, and noting that the largest squared singular value of $\\mathbf A^{\\frac{1}{2}}$ is the largest singular value of $\\mathbf A$ which equals  $\\sigma_{A,1}$.  Also noticing that because $\\mathbf A$ is Hermitian positive semi-definite, its singular values are equal to its eigenvalues.  \n",
    "\n",
    "*begin alternative proof*\n",
    "\n",
    "leveraging the preceding alternative proof, note that\n",
    "\n",
    "$\\text{trace}\\big(\\mathbf {A B}\\big) = \\text{trace}\\big(\\mathbf {\\Lambda C}\\big)= \\lambda_1 c_{1,1} + \\lambda_2 c_{2,2} + ... + \\lambda_n c_{n,n} \\leq \\lambda_1 (c_{1,1} + c_{2,2} + ... + c_{n,n}) = \\lambda_1 \\text{trace}\\big(\\mathbf B\\big) $\n",
    "\n",
    "recalling that the eigenvalues of $\\mathbf A$ are ordered such that $\\lambda_1 \\geq \\lambda_2 \\geq .... \\geq \\lambda_n \\geq 0$, each diagonal element of $\\mathbf C$ is real valued, non-negative, and $\\text{trace}\\big(\\mathbf B\\big) = \\text{trace}\\big(\\mathbf C\\big)$.\n",
    "\n",
    "\n",
    "*end alternative proof*  \n",
    "**claim:**  \n",
    "\n",
    "\n",
    "$\\text{trace}\\big(\\mathbf{AB}\\big) \\leq \\frac{1}{4} \\Big( \\text{trace}\\big(\\mathbf A\\big)+ \\text{trace}\\big(\\mathbf B\\big)\\Big)^2 $\n",
    "\n",
    "**commentary:**  \n",
    "\n",
    "This is a simple extension of an earlier inequality: \n",
    "\n",
    "$\\text{trace}\\big(\\mathbf{AB}\\big) \\leq \\text{trace}\\big(\\mathbf A\\big) \\text{trace}\\big(\\mathbf B \\big) \\leq \\frac{1}{2}\\Big(\\text{trace}\\big(\\mathbf A\\big)^2 + \\text{trace}\\big(\\mathbf B\\big)^2\\Big)$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**proof:**  \n",
    "\n",
    "we start off with \n",
    "\n",
    "\n",
    "$2 \\cdot \\text{trace}\\big(\\mathbf{AB}\\big) \\leq \\text{trace}\\big(\\mathbf A\\big)^2 + \\text{trace}\\big(\\mathbf B\\big)^2$\n",
    "\n",
    "add $2\\cdot \\text{trace}\\big(\\mathbf A\\big)\\text{trace}\\big(\\mathbf B\\big)$ to both sides, noticing that  $\\text{trace}\\big(\\mathbf{AB}\\big) \\leq \\text{trace}\\big(\\mathbf A\\big)\\text{trace}\\big(\\mathbf B\\big)$,  we have   \n",
    "\n",
    "$4 \\cdot \\text{trace}\\big(\\mathbf{AB}\\big) \\leq 2 \\cdot \\text{trace}\\big(\\mathbf{AB}\\big) + 2\\cdot \\text{trace}\\big(\\mathbf A\\big)\\text{trace}\\big(\\mathbf B\\big) \\leq \\text{trace}\\big(\\mathbf A\\big)^2 + \\text{trace}\\big(\\mathbf B\\big)^2 + 2\\cdot \\text{trace}\\big(\\mathbf A\\big)\\text{trace}\\big(\\mathbf B\\big)$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Thus we have \n",
    "\n",
    "\n",
    "$4 \\cdot \\text{trace}\\big(\\mathbf{AB}\\big) \\leq  \\text{trace}\\big(\\mathbf A\\big)^2 + \\text{trace}\\big(\\mathbf B\\big)^2 + 2\\cdot \\text{trace}\\big(\\mathbf A\\big)\\text{trace}\\big(\\mathbf B\\big) = \\Big(\\text{trace}\\big(\\mathbf A\\big) + \\text{trace}\\big(\\mathbf B\\big)\\Big)^2$\n",
    "\n",
    "\n",
    "giving us \n",
    "\n",
    "$\\text{trace}\\big(\\mathbf{AB}\\big) \\leq  \\frac{1}{4}\\Big(\\text{trace}\\big(\\mathbf A\\big) + \\text{trace}\\big(\\mathbf B\\big)\\Big)^2$\n",
    "\n",
    "**claim:**  \n",
    "\n",
    "$\\big \\vert \\text{trace}\\big(\\mathbf{UA}\\big)\\big \\vert \\leq \\text{trace}\\big(\\mathbf A\\big)$ \n",
    "\n",
    "Where both matrices are $n$ x $n$, and $\\mathbf A$ is Hermitian positive semi-definite, and $\\mathbf U$ is unitary.\n",
    "\n",
    "**proof:**  \n",
    "\n",
    "since $\\mathbf U$ is unitary, it is also normal and is unitarily diagonalizable as $\\mathbf U = \\mathbf{VDV}^H$\n",
    "\n",
    "$\\big \\vert \\text{trace}\\big(\\mathbf{UA}\\big)\\big \\vert = \\big \\vert \\text{trace}\\big(\\mathbf{VDV}^H \\mathbf  A\\big)\\big \\vert = \\big \\vert \\text{trace}\\big(\\mathbf{DV}^H \\mathbf  A\\mathbf V \\big)\\big \\vert \\leq \\text{trace}\\big(\\mathbf A\\big)$ \n",
    "\n",
    "let $\\mathbf B := \\mathbf V^H \\mathbf {AV}$\n",
    "\n",
    "$\\big \\vert \\text{trace}\\big(\\mathbf{UA}\\big)\\big \\vert = \\big \\vert \\text{trace}\\big(\\mathbf{DB}\\big)\\big \\vert \\leq  \\text{trace}\\big(\\mathbf B\\big) = \\text{trace}\\big(\\mathbf A\\big)$ \n",
    "\n",
    "here we simply observe, by direct application of triangle inequality:  \n",
    "\n",
    "$\\text{trace}\\big(\\mathbf A\\big) = \\sum_{k=1}^n a_{k,k} = \\sum_{k=1}^n b_{k,k} = \\sum_{k=1}^n \\big \\vert d_{k,k}\\big \\vert \\big \\vert b_{k,k}\\big\\vert = \\sum_{k=1}^n \\big \\vert d_{k,k} b_{k,k}\\big\\vert \\geq \\big \\vert \\big(\\sum_{k=1}^n d_{k,k} b_{k,k}\\big) \\big\\vert$\n",
    "\n",
    "\n",
    "*begin alternative proof*  \n",
    "  \n",
    "since $\\mathbf A$ is Hermitian positive semi-definite, we may unitarily diagonalize it in the form of\n",
    "\n",
    "$\\mathbf A = \\mathbf{QDQ}^H$\n",
    "\n",
    "$\\big \\vert \\text{trace}\\big(\\mathbf{UA}\\big)\\big \\vert = \\big \\vert \\text{trace}\\big(\\mathbf U \\mathbf{QDQ}^H\\big)\\big \\vert = \\big \\vert \\text{trace}\\big(\\mathbf Q^H\\mathbf {UQ} \\mathbf{D}\\big)\\big \\vert  \\leq \\text{trace}\\big(\\mathbf A\\big)$ \n",
    "\n",
    "where $\\mathbf V := \\mathbf Q^H\\mathbf {UQ}$\n",
    "\n",
    "because $\\mathbf V$ is unitary, each column has a length (2 norm) of 1, and hence each diagonal entry has a magnitude in $[0,1]$  \n",
    "\n",
    "$\\big \\vert \\text{trace}\\big(\\mathbf{UA}\\big)\\big \\vert = \\big \\vert \\text{trace}\\big(\\mathbf V \\mathbf{D}\\big)\\big \\vert  \\leq \\text{trace}\\big(\\mathbf A\\big)$ \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "$\\big \\vert \\big(\\sum_{k=1}^n v_{k,k} \\lambda_k \\big)\\big \\vert  \\leq  \\sum_{k=1}^n \\big \\vert v_{k,k} \\lambda_k\\big \\vert = \\sum_{k=1}^n \\big \\vert v_{k,k}\\big \\vert \\lambda_k  \\leq \\sum_{k=1}^n \\lambda_k = \\text{trace}\\big(\\mathbf D\\big) = \\text{trace}\\big(\\mathbf A\\big) $ \n",
    "\n",
    "\n",
    "\n",
    "by fact that each $\\lambda_k\\geq 0$ and hence scaling any entry by some real non-negative number $\\leq 1$ results in a series that is at most the same size, and finally by application of the triangle inequality.  \n",
    "\n",
    "*end alternative proof*  \n",
    "\n",
    "*begin inner product oriented alternative proof:* \n",
    "\n",
    "take the Hermitian postive square root and get \n",
    "\n",
    "$\\mathbf {BB} = \\mathbf B^H \\mathbf B = \\mathbf B \\mathbf B^H = \\mathbf A$ \n",
    "\n",
    "$\\big \\vert \\text{trace}\\big(\\mathbf{UA}\\big)\\big \\vert = \\big \\vert \\text{trace}\\big(\\mathbf{UBB^H}\\big)\\big \\vert =\\big \\vert \\text{trace}\\big(\\mathbf B^H \\mathbf{UB}\\big)\\big \\vert = \\big \\vert \\langle \\mathbf B, \\mathbf {UB} \\rangle \\big \\vert  \\leq \\big \\Vert \\mathbf B \\big \\Vert_F \\big \\Vert \\mathbf {UB} \\big \\Vert_F = \\big \\Vert \\mathbf B \\big \\Vert_F \\big \\Vert \\mathbf {B} \\big \\Vert_F = \\big \\Vert \\mathbf B \\big \\Vert_F^2 =  \\text{trace}\\big(\\mathbf B^H \\mathbf B \\big) = \\text{trace}\\big(\\mathbf A\\big)$ \n",
    "\n",
    "by direct application of Cauchy Schwarz to inner product given by the trace.  (Vec operator can be of help in further interpretting this for some.)  \n",
    "\n",
    "\n",
    "\n",
    "*end second alternative proof*\n",
    "\n",
    "where we recognize that each diagonal element of $\\mathbf A$ and $\\mathbf B$ is real, non-negative (as required by Hermitian positive semi-definite matrices), each diagonal element of $\\mathbf D$ has a magnitude of one (as required for unitary matrices) and the inequality stated is a direct application of triangle inequality. \n",
    "\n",
    "**claim:**  \n",
    "\n",
    "where $\\mathbf Y$ is $n$ x $n$  Hermitian, it is also positive semi-definite, **iff** \n",
    "\n",
    "$\\text{trace}\\big(\\mathbf {YX}\\big) \\geq 0 $ for all Hermitian positive semi definite $\\mathbf X$\n",
    "\n",
    "**remark:  This is something I saw elsewhere (not in Zhang) but it is amenable to the same techniques used there**  \n",
    "\n",
    "**proof:**\n",
    "\n",
    "Let $\\mathbf X = \\sigma_1 \\mathbf x_1 \\mathbf x_1^H + \\sigma_2 \\mathbf x_2 \\mathbf x_2^H + ... + \\sigma_n \\mathbf x_n \\mathbf x_n^H$ \n",
    "\n",
    "where each $\\sigma_k \\geq 0$\n",
    "\n",
    "\n",
    "$\\text{trace}\\Big(\\mathbf {YX}\\Big) = \\text{trace}\\Big(\\mathbf Y \\big(\\sigma_1 \\mathbf x_1 \\mathbf x_1^H + \\sigma_2 \\mathbf x_2 \\mathbf x_2^H + ... + \\sigma_n \\mathbf x_n \\mathbf x_n^H \\big) \\Big) =  \\sigma_1 \\text{trace}\\Big(\\mathbf Y  \\mathbf x_1 \\mathbf x_1^H\\Big)  + \\sigma_2 \\text{trace}\\Big(\\mathbf Y  \\mathbf x_2 \\mathbf x_2^H \\Big) ... + \\sigma_n \\text{trace}\\Big(\\mathbf Y  \\mathbf x_n \\mathbf x_n^H\\Big) \\geq 0 $ \n",
    "\n",
    "$\\text{trace}\\Big(\\mathbf {YX}\\Big) = \\sigma_1 \\Big( \\mathbf x_1^H \\mathbf Y  \\mathbf x_1 \\Big)  + \\sigma_2 \\Big(\\mathbf x_2^H  \\mathbf Y  \\mathbf x_2 \\Big) ... + \\sigma_n \\Big( \\mathbf x_n^H \\mathbf Y  \\mathbf x_n\\Big) \\geq 0$\n",
    "\n",
    "the trace will obviously be $\\geq 0$ if each term in the finite series is $\\geq 0$.  Now we hone in on an important special subset: since this is for *any* $\\mathbf X$, this also includes the special case where $\\sigma_r = 0$ for $r = \\{2, 3, ...,n\\}$ and we select $\\mathbf x_1$ to be any vector we like. Thus the claim reduces to \n",
    "\n",
    "$\\text{trace}\\Big(\\mathbf {YX}\\Big) = \\sigma_1 \\Big( \\mathbf x_1^H \\mathbf Y  \\mathbf x_1 \\Big) \\geq 0$\n",
    "\n",
    "for all $\\mathbf x_1$ and any $\\sigma_1 \\geq 0$, which is the familiar test for positive semi-definiteness of $\\mathbf Y$.  \n",
    "- - - -\n",
    "\n",
    "*alternative proof*  \n",
    "\n",
    "this approach may be more slick, but perhaps less intuitive.\n",
    "\n",
    "$\\mathbf Y = \\mathbf Q \\mathbf \\Lambda \\mathbf Q^H$  \n",
    "$\\mathbf X = \\mathbf{V\\Sigma V^H}$\n",
    "\n",
    "\n",
    "we know that $\\mathbf X$ is any arbitrary Hermitian positive semi-definite matrix, and need to verify that the trace inequality means $\\mathbf Y$ must be Hermitian positive semi-definite as well.  \n",
    "\n",
    "$\\text{trace}\\Big(\\mathbf {YX}\\Big) = \\text{trace}\\Big(\\mathbf Y \\big(\\mathbf{V\\Sigma V^H}\\big) \\Big) = \\text{trace}\\Big(\\big(\\mathbf V^H \\mathbf{Y V}\\big)\\mathbf \\Sigma \\Big) = \\text{trace}\\Big(\\mathbf B \\mathbf \\Sigma \\Big) = \\sum_{i=1}^n b_{i,i}\\sigma_i \\geq 0 $  \n",
    "\n",
    "where $\\mathbf B = \\big(\\mathbf V^H \\mathbf{Y V}\\big)$.  If $\\mathbf Y$ is Hermitian positive semi-definite, then its diagonal elements are real non-negative and thus $\\mathbf B$ (which is also Hermitian positive semi-definite) has real non-negative diagonal elements.  \n",
    "\n",
    "The above should seem intuitve and proves the *if* but not the **iff**.  We now consider the other leg:  \n",
    "\n",
    "\n",
    "$\\text{trace}\\Big(\\mathbf {YX}\\Big) = \\text{trace}\\Big(\\big(\\mathbf Q \\mathbf \\Lambda \\mathbf Q^H\\big) \\mathbf X \\Big) = \\text{trace}\\Big(\\mathbf \\Lambda \\big( \\mathbf Q^H \\mathbf X  \\mathbf Q\\big) \\Big)=  \\text{trace}\\Big(\\mathbf \\Lambda \\big(\\mathbf C \\big) \\Big) = \\sum_{i=1}^n c_{i,i}\\lambda_i \\geq 0$ \n",
    "\n",
    "where $\\big(\\mathbf C \\big) = \\big( \\mathbf Q^H \\mathbf X  \\mathbf Q\\big)$ and $\\mathbf C$ is Hermitian positive semi-definite like $\\mathbf X$, thus each $ c_{i,i} \\geq 0$.  \n",
    "\n",
    "Consider the case where $\\mathbf Y$ has an eigenvalue less than zero.  E.g. suppose $\\lambda_n \\lt 0$ but $\\lambda_r \\geq 0$ for $r = \\{1, 2, 3, ..., n-1\\}$.  If this is true, then \n",
    "\n",
    "$c_{n,n} \\lambda_n + \\sum_{r=1}^{n-1} c_{r,r}\\lambda_r \\ngeq 0$\n",
    "\n",
    "for some large enough $c_{n,n}$.  The easiest approach is to select some $\\mathbf C$ where $c_{n,n} = 1$ and *all* other cells in $\\mathbf C$ are zero.  Such a matrix is still Hermitian positive semi-definite, and from here, for any given $\\mathbf Y$ we can multiply out its eigenvectors and get the appropriate $\\mathbf X = \\big( \\mathbf Q \\mathbf C  \\mathbf Q^H\\big)$, which is Hermitian positive semi-definite, yet violates the above inequality. Hence for the inequality to hold *all* eigenvalues of $\\mathbf X$ must be real non-negative.  And since we have determined that $\\mathbf X$ is Hermitian with strictly non-negative eigenvalues in order for the inequality to always hold, then $\\mathbf X$ must be Hermitian positive semi-definite.  This completes the proof.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**claim:**\n",
    "\n",
    "where $\\mathbf B$ is Hermitian positive semi-definite, and $\\mathbf Q$ has orthonormal columns but generally is **not** square,\n",
    "\n",
    "$\\text{trace}\\big(\\mathbf B\\big) \\geq \\text{trace}\\big(\\mathbf Q^H \\mathbf B \\mathbf Q\\big)$\n",
    "\n",
    "**remark:**\n",
    "I saw this as part of assignment 3, here: http://www4.ncsu.edu/~asaibab/classes/ma723/\n",
    "\n",
    "The suggested approach involves using Cauchy eigenvalue interlacing, and also blocked matrices, in order to prove a special Cauchy interlacing setup, the above trace relation, and also a related claim involving determinants.  Setting aside the determinant, this suggested approach seemed like overkill, except as part of an interlacing or blocked matrix drill.  The below proof makes use of SVD and cyclic property of the trace to prove this claim.  \n",
    "\n",
    "**proof:** \n",
    "\n",
    "let $\\mathbf Q = \\mathbf U \\mathbf \\Sigma \\mathbf V^H$ \n",
    "\n",
    "where $\\mathbf U$ and $\\mathbf V$ are square, but $\\mathbf \\Sigma$ in general is tall and skinny. Note that $\\mathbf Q^H \\mathbf Q = \\mathbf I_k$ where $k$ is the number of columns in $\\mathbf Q$.  Hence we confirm that each singular value $\\{\\sigma_1, \\sigma_2, ..., \\sigma_k\\}$ has a magnitude of one, and hence a value of one (since singular values are, by construction, real, non-negative.) \n",
    "\n",
    "\n",
    "$\\text{trace}\\big(\\mathbf B\\big) \\geq \\text{trace}\\big(\\mathbf Q^H \\mathbf B \\mathbf Q\\big) = \\text{trace}\\big(\\mathbf V \\mathbf \\Sigma^H \\mathbf U^H \\mathbf B \\mathbf U \\mathbf \\Sigma \\mathbf V^H \\big) = \\text{trace}\\big(\\mathbf U^H \\mathbf B \\mathbf U \\mathbf \\Sigma \\mathbf \\Sigma^H \\big)$\n",
    "\n",
    "assign $\\mathbf C := \\mathbf U^H \\mathbf B \\mathbf U$   \n",
    "\n",
    "$\\mathbf C$ is Hermitian positive semi-definite as well  \n",
    "$\\big(\\mathbf \\Sigma \\mathbf \\Sigma^H \\big)$ is a diagonal matrix with $k$ entries equal to one, and all else equal to 0.\n",
    "\n",
    "\n",
    "$\\text{trace}\\big(\\mathbf B\\big) = \\text{trace}\\big(\\mathbf C\\big) = c_{1,1} + c_{2,2} + ... + c_{k,k} + c_{k+1,k+1} + ... + c_{n,n} \\geq c_{1,1} + c_{2,2} + ... + c_{k,k}  = \\text{trace}\\Big(\\mathbf C\\big( \\mathbf \\Sigma \\mathbf \\Sigma^H \\big)\\Big)$\n",
    "\n",
    "because each diagonal entry of $\\mathbf C$ is real non-negative, by virtue of it being Hermitian positive semi-definite.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Begin Interlude involving singular values and ideas from Majorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "consider some Hermitian Positive Definite matrix  \n",
    "$\\mathbf A \\succ 0$   \n",
    "\n",
    "which is unitarily diagonalizable  \n",
    "\n",
    "$\\mathbf A = \\mathbf {Q\\Sigma Q}^*$  \n",
    "\n",
    "with the usual ordering  \n",
    "$\\sigma_1 \\geq \\sigma_2 \\geq ... \\geq \\sigma_n \\gt 0$  \n",
    "\n",
    "$\\mathbf Q = \n",
    "\\bigg[\\begin{array}{c|c|c|c|c}\n",
    "\\mathbf q_1 & \\mathbf q_2 &\\cdots & \\mathbf q_{n-1} & \\mathbf q_{n}\n",
    "\\end{array}\\bigg]$    \n",
    "\n",
    "we prove that  \n",
    "$\\mathbf \\Sigma \\succeq \\text{Diag} \\big(\\mathbf A \\big) = \\mathbf A \\circ \\mathbf I $  \n",
    "(i.e. the diagonal matrix with ordered eigenvalues majorizes $\\mathbf A$.)  \n",
    "\n",
    "$\\mathbf A = \\big(\\mathbf {Q\\Sigma}\\big) \\mathbf Q^* = \\sum_{k=1}^n \\sigma_k \\mathbf q_k \\mathbf q_k^*  $   \n",
    "\n",
    "we now want to look at the diagonal components of that sum of rank one updates (it's not necessary though the hadamard product can be helpful here  \n",
    "\n",
    "$\\big(\\mathbf A \\circ \\mathbf I\\big)\\mathbf 1 = \\big(\\sum_{k=1}^n \\sigma_k \\mathbf q_k \\mathbf q_k^* \\mathbf \\circ \\mathbf I\\big)\\mathbf 1 =  \\sum_{k=1}^n \\sigma_k \\begin{bmatrix}\n",
    "\\big \\vert q_{1}^{(k)} \\big\\vert^2 \\\\\n",
    "\\big\\vert q_{2}^{(k)}\\big\\vert^2 \\\\ \n",
    "\\vdots\\\\ \n",
    "\\big\\vert q_{n-1}^{(k)}\\big\\vert^2 \\\\ \n",
    "\\big\\vert q_{n}^{(k)}\\big\\vert^2\n",
    "\\end{bmatrix}$   \n",
    "\n",
    "if we observe that  \n",
    "$1 = \\big \\Vert \\mathbf q_k \\big \\Vert_2^2 = \\text{trace}\\big(\\mathbf q_k \\mathbf q_k^*\\big) = \\big \\vert q_{1}^{(k)} \\big\\vert^2 + \\big\\vert q_{2}^{(k)}\\big\\vert^2 + ... + \\big\\vert q_{n-1}^{(k)}\\big\\vert^2 + \\big\\vert q_{n}^{(k)}\\big\\vert^2$    \n",
    "\n",
    "then we can create a (doubly) stochastic matrix $D$ (*not a diagonal matrix in general!*)  the $k$th column is given by \n",
    "\n",
    "$\\begin{bmatrix}\n",
    "\\big \\vert q_{1}^{(k)} \\big\\vert^2 \\\\\n",
    "\\big\\vert q_{2}^{(k)}\\big\\vert^2 \\\\ \n",
    "\\vdots\\\\ \n",
    "\\big\\vert q_{n-1}^{(k)}\\big\\vert^2 \\\\ \n",
    "\\big\\vert q_{n}^{(k)}\\big\\vert^2\n",
    "\\end{bmatrix}$  \n",
    "\n",
    "and see that   \n",
    "$\\big(\\mathbf A \\circ \\mathbf I\\big)\\mathbf 1 = D\\mathbf \\Sigma \\mathbf 1$  \n",
    "\n",
    "Based on the above work it is immediate that $ D$ is column stochastic (i.e. real non-negative components that sum to one in each column). To prove it is row stochastic, consider that if we select the $i$th row and sum over all components in said row, row we get  \n",
    "\n",
    "\n",
    "$\\sum_{k=1}^n D_{i,k} = \\sum_{k=1}^n \\big \\vert q_{i}^{(k)} \\big\\vert^2 = 1 $  \n",
    "or in matrix form, if we partition $\\mathbf Q$ by rows \n",
    "\n",
    "$\\mathbf Q= \n",
    "\\begin{bmatrix}\n",
    "\\tilde{ \\mathbf q_1}^T \\\\\n",
    "\\tilde{ \\mathbf q_2}^T \\\\ \n",
    "\\vdots\\\\ \n",
    "\\tilde{ \\mathbf q}_{n-1}^T \\\\ \n",
    "\\tilde{ \\mathbf q_n}^T\n",
    "\\end{bmatrix}$  \n",
    "\n",
    "so e.g. with $i = 2$ we have   \n",
    "$\\sum_{k=1}^n D_{2,k} = \\sum_{k=1}^n\\big \\vert q_{2}^{(k)} \\big\\vert^2 = \\tilde{ \\mathbf q_2}^T \\bar{\\tilde{ \\mathbf q_2}}= \\big \\Vert \\tilde{ \\mathbf q_2}^T\\big \\Vert_2^2= 1 $  \n",
    "\n",
    "which is equivalent to observing that $\\mathbf Q$ has mutually orthonormal rows as well as mutually orthonormal columns, i.e. \n",
    "\n",
    "$\\mathbf Q\\mathbf Q^* = \\mathbf Q\\mathbf Q^{-1} = \\mathbf Q^{-1}\\mathbf Q = \\mathbf I$  \n",
    "\n",
    "This proves $D$ is doubly stochastic.  \n",
    "\n",
    "But since $D$ is bouly stochastic and $\\big(\\mathbf A \\circ \\mathbf I\\big)\\mathbf 1 = D\\big(\\mathbf \\Sigma \\mathbf 1\\big)$  \n",
    "an immediate concusion from the theory of majorization is that that diagonal elements of $\\mathbf A$ are in the convex hull of $\\mathbf \\Sigma$ i.e. that   \n",
    "$\\mathbf \\Sigma \\succeq \\text{Diag} \\big(\\mathbf A \\big) = \\mathbf A \\circ \\mathbf I $  \n",
    "or that, for $r \\in\\{1,2,3...,n\\}$  \n",
    "\n",
    "$\\sum_{i=1}^r \\sigma_i \\geq \\sum_{i=1}^r a_{i,i}$  \n",
    "\n",
    "by linearity, we may nicely extend this to arbitrary hermitian matrices by shifting the eigenvalues of $\\mathbf A$ by some constant real amount $\\alpha$, i.e. considering the matrix \n",
    "\n",
    "$\\big(\\mathbf A + \\alpha\\mathbf I\\big)$  \n",
    "- - - - \n",
    "an *easy corollary* is that every Hermitian matrix is unitarily similar to a matrix with a constant diagonal.  (Indeed every matrix diagonalizable over $\\mathbb C$ is similar to a matrix with a constant diagonal by the below relation.) \n",
    "\n",
    "$\\gamma := \\frac{1}{n}\\text{trace}\\big(\\mathbf A\\big)$    \n",
    "\n",
    "The special case where $\\text{trace}\\big(\\mathbf A\\big) = 0$ is of particular interest -- it tells us that every Hermitian matrix is similar to a matrix with all zeros on the diagonal.  \n",
    "\n",
    "We may intuit this by reconsidering  \n",
    "$\\big(\\mathbf A \\circ \\mathbf I\\big)\\mathbf 1 = D\\big(\\mathbf \\Sigma \\mathbf 1\\big)$  \n",
    "and in particular considering \n",
    "\n",
    "$\\mathbf P=\\frac{1}{2}\\big(D + \\mathbf I\\big)$  \n",
    "(which ensures a limit exist though techincally does deal with reducibility issues which we ignore here), and then consider  \n",
    "\n",
    "$\\big(\\mathbf S \\circ \\mathbf I\\big)\\mathbf 1 = \\lim_{r \\to \\infty} \\mathbf P^r \\big(\\mathbf \\Sigma\\mathbf 1\\big)$  \n",
    "where the diagonal of $\\mathbf S$ is the Hermitian matrix that is (hopefully, barring reducibility issues) in the convex hull of / majorized by all Hermitian matrices with this spectra.  \n",
    "\n",
    "- - - - \n",
    "*remark:*  \n",
    "using induction and more care, the result can be proven somewhat indirectly (making use of intermediate value theorem) to show that an orthogonally similar matrix with constant diagonal exists.  The approach here showing unitary similarity is slightly weaker but easier, more direct and nicely ties in immediately with majorization ideas.  \n",
    "- - - - \n",
    "It is enough to find a matrix that is unitarily similar to $\\mathbf \\Sigma$ but has a constant diagonal.  \n",
    "\n",
    "i.e. to find \n",
    "\n",
    "$\\mathbf S = \\mathbf U \\mathbf \\Sigma \\mathbf U^*$  \n",
    "where $\\mathbf S$ has constants on the diagonal.  \n",
    "\n",
    "To get this 'totally majorized' matrix reconsider that column $k$ of the associated doubly stochastic matrix $D$ is given by  \n",
    "\n",
    "$\\begin{bmatrix}\n",
    "\\big \\vert u_{1}^{(k)} \\big\\vert^2 \\\\\n",
    "\\big\\vert u_{2}^{(k)}\\big\\vert^2 \\\\ \n",
    "\\vdots\\\\ \n",
    "\\big\\vert u_{n-1}^{(k)}\\big\\vert^2 \\\\ \n",
    "\\big\\vert u_{n}^{(k)}\\big\\vert^2\n",
    "\\end{bmatrix}$\n",
    "\n",
    "and the maximally majorized case occurs when $\\mathbf D = \\frac{1}{n} \\mathbf {11}^T$  \n",
    "-- i.e. the uniform distribution case.  Hence we need to find a unitary matrix $\\mathbf U$ where each component has magnitude equal to $\\frac{1}{\\sqrt{n}}$.  \n",
    "\n",
    "The Unitary Vandermonde Matrix works perfectly here (i.e. Discrete Fourier Transform matrix)  \n",
    "\n",
    "$\\mathbf U:=  \\mathbf F = \\frac{1}{\\sqrt n} \\bigg[\\begin{array}{c|c|c|c|c}\n",
    "\\mathbf \\Lambda^0 \\mathbf 1 & \\mathbf \\Lambda^1 \\mathbf 1 & \\mathbf \\Lambda^2 \\mathbf 1 &\\cdots & \\mathbf \\Lambda^{n-1} \\mathbf 1\n",
    "\\end{array}\\bigg] = \\frac{1}{\\sqrt n} \\begin{bmatrix}\n",
    "1 & \\lambda_0 & \\lambda_0^2 & \\dots  & \\lambda_0^{n-1}\\\\ \n",
    "1 & \\lambda_1 & \\lambda_1^2 & \\dots &  \\lambda_1^{n-1} \\\\ \n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\\\ \n",
    "1 & \\lambda_{n-1} & \\lambda_{n-1}^{2} & \\dots  & \\lambda_{n-1}^{n-1}\n",
    "\\end{bmatrix}$  \n",
    "\n",
    "with $\\lambda_k := (\\omega^{k})$ i.e. each of the nth roots of unity.  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# some code to demonstrates this in action  \n",
    "# (small floating point/ rounding issues are to be expected)\n",
    "\n",
    "import numpy as np\n",
    "np.set_printoptions(precision = 2, linewidth=180)\n",
    "\n",
    "n = 20\n",
    "v = np.zeros(n+1)\n",
    "v[0] = 1\n",
    "v[-1] = -1\n",
    "\n",
    "# https://docs.scipy.org/doc/numpy/reference/generated/numpy.roots.html  \n",
    "\n",
    "roots_of_unity = np.roots(v)\n",
    "\n",
    "W = np.zeros((n,n))\n",
    "W = np.array(W, np.complex128)\n",
    "for i in range(n):\n",
    "    W[i] += roots_of_unity**i\n",
    "W *= 1/np.sqrt(n)\n",
    "\n",
    "some_eigs_vec = np.random.random(n)\n",
    "some_eigs_vec[-1] = - np.sum(some_eigs_vec[:-1])\n",
    "some_eigs_vec*= 100\n",
    "# traceless... \n",
    "diag_elements = np.diag(W@ np.diag(some_eigs_vec) @ np.conj(W).T)\n",
    "print(np.round(np.abs(diag_elements),5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**lemma**\n",
    "\n",
    "for *any* Hermitian positive (semi) definite $\\mathbf G$ and some singular values matrix $\\mathbf \\Sigma$  \n",
    "\n",
    "(note: all matrices are $n$ x $n$ )  \n",
    "\n",
    "where $\\lambda_1 \\geq \\lambda_2 \\geq ... \\geq \\lambda_n \\geq 0$ and \n",
    "\n",
    "$\\mathbf G = \\mathbf {U\\Lambda U}^H$ \n",
    "\n",
    "and \n",
    "\n",
    "$\\sigma_1 \\geq \\sigma_2 \\geq ... \\geq \\sigma_n \\geq 0$ \n",
    "\n",
    "and for avoidance of doubt the kth diagonal entry of $\\mathbf \\Sigma $ is $\\sigma_k$ and the kth diagonal entry of  $\\mathbf \\Lambda$ is $ \\lambda_k$  \n",
    "\n",
    "**claim:**  \n",
    "$\\text{trace}\\big(\\mathbf \\Sigma \\mathbf G\\big) = \\sigma_1 g_{1,1} + \\sigma_2 g_{2,2} + ... + \\sigma_n g_{n,n} \\leq \\sigma_1 \\lambda_1 + \\sigma_2 \\lambda_2 + ... + \\sigma_n \\lambda_n = \\text{trace}\\big(\\mathbf \\Sigma \\mathbf \\Lambda\\big)$\n",
    "\n",
    "**proof:**\n",
    "\n",
    "a direct proof can be done by using the fact that the (strictly real non-negative) eigenvalues of $\\mathbf G$ majorize the (strictly real non-negative) diagonal elements of $\\mathbf G$, so there is some doubly stochastic matrix $D$ where \n",
    "\n",
    "$\\Big(\\big(\\mathbf I \\circ \\mathbf G \\big)\\mathbf 1\\Big) = D\\Big(\\mathbf \\Lambda \\mathbf 1\\Big)$  \n",
    "\n",
    "Now, via Birkhoff's Theorem, we can say that $D$ is a convex combination of permutation matrices -- with each unique permutation matrix denoted as $\\mathbf P^{(i)}$ -- giving us \n",
    "\n",
    "\n",
    "$\\big(\\mathbf {\\Sigma 1}\\big)^H \\Big(\\big(\\mathbf I \\circ \\mathbf G \\big)\\mathbf 1\\Big) = \\big(\\mathbf {\\Sigma 1}\\big)^H D\\Big(\\mathbf \\Lambda \\mathbf 1\\Big) = \\big(\\mathbf {\\Sigma 1}\\big)^H\\big(\\sum_i \\alpha_i \\mathbf P^{(i)}\\big)\\Big(\\mathbf \\Lambda \\mathbf 1\\Big)  = \\sum_i \\alpha_i  \\big(\\mathbf {\\Sigma 1}\\big)^H \\mathbf P^{(i)}\\big(\\mathbf \\Lambda \\mathbf 1\\big) \\leq \\big(\\mathbf {\\Sigma 1}\\big)^H \\Big(\\mathbf \\Lambda \\mathbf 1\\Big) = \\text{trace}\\big(\\mathbf{\\Sigma \\Lambda}\\big)$\n",
    "\n",
    "\n",
    "$\\alpha_i \\geq 0$ and $\\sum_i \\alpha_i = 1$ \n",
    "because we are dealing with convex combinations.  \n",
    "\n",
    "and for each $i$ \n",
    "\n",
    "$\\big(\\mathbf {\\Sigma 1}\\big)^H \\mathbf P^{(i)} \\Big(\\mathbf \\Lambda \\mathbf 1\\Big) \\leq \\big(\\mathbf {\\Sigma 1}\\big)^H \\Big(\\mathbf \\Lambda \\mathbf 1\\Big)$ \n",
    "\n",
    "via the re-arrangement inequality.  \n",
    "\n",
    "Hence \n",
    "\n",
    "$\\sum_i \\alpha_i  \\big(\\mathbf {\\Sigma 1}\\big)^H \\mathbf P^{(i)}\\big(\\mathbf \\Lambda \\mathbf 1\\big) \\leq \\sum_i \\alpha_i  \\big(\\mathbf {\\Sigma 1}\\big)^H \\big(\\mathbf \\Lambda \\mathbf 1\\big) =  \\big(\\sum_i \\alpha_i\\big)  \\big(\\mathbf {\\Sigma 1}\\big)^H \\big(\\mathbf \\Lambda \\mathbf 1\\big) =   \\big(\\mathbf {\\Sigma 1}\\big)^H \\big(\\mathbf \\Lambda \\mathbf 1\\big)  $ \n",
    "\n",
    "- - - -  \n",
    "**A refinement of this relationship is given by:**  \n",
    "\n",
    "$\\text{trace}\\Big(\\mathbf \\Sigma \\mathbf J^{H} \\mathbf \\Lambda \\mathbf J \\Big) = \\sigma_1 \\lambda_n + \\sigma_{2} \\lambda_{n-1} + ... + \\sigma_n \\lambda_1 \\leq \\text{trace}\\Big( \\mathbf \\Sigma \\mathbf P^{(k)^H} \\mathbf \\Lambda \\mathbf P^{(k)}\\Big)  = \\big(\\mathbf{\\Sigma 1 }\\big)^H \\mathbf P^{(k)} \\big(\\mathbf{ \\Lambda 1}\\big) \\leq  \\text{trace}\\Big(\\mathbf \\Sigma \\mathbf \\Lambda\\Big)$  \n",
    "\n",
    "\n",
    "Put slightly differently, for Hermitian positive (semi)definite $\\mathbf A$ and $\\mathbf B$, we have the following bounds:  \n",
    "\n",
    "$\\text{trace}\\Big(\\mathbf \\Sigma \\mathbf J^{H} \\mathbf \\Lambda \\mathbf J \\Big) = \\sigma_1 \\lambda_n + \\sigma_{2} \\lambda_{n-1} + ... + \\sigma_n \\lambda_1 \\leq \\text{trace}\\Big( \\mathbf A \\mathbf B \\Big)  = \\text{trace}\\Big(\\mathbf \\Sigma \\mathbf G \\Big) \\leq  \\text{trace}\\Big(\\mathbf \\Sigma \\mathbf \\Lambda\\Big)$  \n",
    "\n",
    "where   \n",
    "$\\mathbf A = \\mathbf U \\mathbf \\Sigma \\mathbf U^*$  \n",
    "$\\mathbf G := \\mathbf U^* \\mathbf B \\mathbf U$  \n",
    "\n",
    "\n",
    "\n",
    "**again, recalling that we have well ordered singular values and real nonnegative eigenvalues** (i.e. the largest are always in the top left corner of the relevant diagonal matrices, and with the next largest in the second to top left corner and then next largest and so on.)  The matrix notation and use of trace is useful in this post, however we must remember that there is an ordering scheme that is being enforced here.  Ultimately much of the interesting implications of (finite) majorization come down to consequences of order.  \n",
    "\n",
    "where $\\mathbf J$ is the reflection matrix, and $\\mathbf P^{(k)}$ is the (kth) Permutation matrix.  The above *is* a direct application of the Re-arrangment inequality.  Note that the above is probably best visualized as diagonal matrices representing a (reducible) graph consisting entirely of self loops with real non-negative weights on the edge.  The Right hand side has a diagonal transition matrix applied to this graph, in each case with the same ordering of their weights.  The Left hand side has all the labelling and hence weights, completely flipped -- this is the most extreme case.  The middle case has some re-labelling done.  We can thus visualize the effect of the permutation matrices on $\\mathbf \\Lambda$ as a graph isomorphism that disturbs the order that we constructed in the edges weights of the graphs.  \n",
    "- - - - - \n",
    "Note: a more exacting interpretation would be to count (weighted) walks in the below bipartite graph \n",
    "\n",
    "$\\mathbf B := \\begin{bmatrix}\n",
    "\\mathbf 0 & \\mathbf \\Lambda\\\\ \n",
    "\\mathbf \\Sigma & \\mathbf 0\n",
    "\\end{bmatrix}$\n",
    "\n",
    "$\\mathbf B^2 = \\begin{bmatrix}\n",
    "\\mathbf {\\Lambda \\Sigma} & \\mathbf 0\\\\ \n",
    "\\mathbf 0 & \\mathbf { \\Sigma \\Lambda}\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "\\mathbf {\\Lambda \\Sigma} & \\mathbf 0\\\\ \n",
    "\\mathbf 0 & \\mathbf { \\Lambda\\Sigma}\n",
    "\\end{bmatrix}$\n",
    "\n",
    "\n",
    "hence $\\frac{1}{2}\\text{trace}\\big( \\mathbf B^2\\big) =  \\text{trace}\\big(\\mathbf{ \\Lambda\\Sigma}\\big)$  \n",
    "\n",
    "each node in the first partition is connected to one node in the second partition, and vice versa.  The maximum walk count comes when we have labelled all nodes in partitions one and two in accordance with how high their edge weight is and connected them to the same ordered node in the other partition.  I.e. we label each node $(1, i), (2,i),...,(n, i)$ for partition $i$ and $(1, ii), (2,ii),...,(n, ii)$ for partition $ii$ and we connect the 1s to each other, 2s to each other and so forth.  In the maximal case it is because $1$ stands for maximal weighted edge, $2$ stands for a weighted edge not larger than $1$ and so forth for a given partition. \n",
    "\n",
    "The minimal case comes into play if we go into the first partition and 'flip' the label of each node so now (1) actually has a minimum weighted, 2 has second minimum, and so on.  We still have 1's connected between the partition, 2's connected between the partition and so on.  \n",
    "\n",
    "The middle portion of the inequality is when we take our maximal case and re-label some (or all) of the nodes of partition i, but in general not in the absolute worst way possible (i.e. we re-lable but don't do the most extreme 'flip' that is possible).  \n",
    "\n",
    "- - - - - \n",
    "\n",
    "But we know that the diagonal elements of $\\mathbf G$ are majorized by $\\mathbf \\Lambda$, and hence via Birkhoff's Theorem are in the convex hull of permutation matrices, which with real non-negative weights $w_k \\geq 0$ such that $\\sum_{k} w_k = 1$, we have \n",
    "\n",
    "\n",
    "$\\big( \\mathbf G \\circ \\mathbf I\\big) = \\sum_{k} w_k \\mathbf P^{(k)^H} \\mathbf \\Lambda \\mathbf P^{(k)}$ \n",
    "\n",
    "so we can take our original inequality and scale by $w_k$ to observe a useful point-wise bound \n",
    "\n",
    "\n",
    "$w_k \\text{trace}\\Big( \\mathbf \\Sigma \\mathbf J^{H} \\mathbf \\Lambda \\mathbf J\\Big) \\leq \\text{trace}\\Big( \\big(\\mathbf \\Sigma\\big) \\big( w_k \\mathbf P^{(k)^H} \\mathbf \\Lambda \\mathbf P^{(k)}\\big) \\Big)  \\leq  w_k \\text{trace}\\Big(\\mathbf \\Sigma \\mathbf \\Lambda\\Big)$  \n",
    "\n",
    "and summing over this bound, we have \n",
    "\n",
    "$\\text{trace}\\Big(\\mathbf \\Sigma \\mathbf J^{H} \\mathbf \\Lambda \\mathbf J \\Big)  \\leq \\sum_k \\text{trace}\\Big( \\big(\\mathbf \\Sigma\\big) \\big( w_k \\mathbf P^{(k)^H} \\mathbf \\Lambda \\mathbf P^{(k)}\\big) \\Big)  =  \\text{trace}\\Big( \\big(\\mathbf \\Sigma\\big) \\sum_k \\big( w_k \\mathbf P^{(k)^H} \\mathbf \\Lambda \\mathbf P^{(k)}\\big) \\Big) = \\text{trace}\\Big( \\big(\\mathbf \\Sigma\\big) \\big( \\mathbf G \\circ \\mathbf I\\big) \\Big)  \\leq  \\text{trace}\\Big(\\mathbf \\Sigma \\mathbf \\Lambda\\Big)   $  \n",
    "\n",
    "\n",
    "Then noticing that   \n",
    "$ \\text{trace}\\Big( \\big(\\mathbf \\Sigma\\big) \\big( \\mathbf G \\circ \\mathbf I\\big) \\Big) = \\text{trace}\\Big( \\mathbf \\Sigma \\mathbf G \\Big) $\n",
    "\n",
    "we have \n",
    "\n",
    "$\\text{trace}\\Big(\\mathbf \\Sigma \\mathbf J^{H} \\mathbf \\Lambda \\mathbf J \\Big)  \\leq \\text{trace}\\Big( \\mathbf \\Sigma \\mathbf G \\Big) \\leq  \\text{trace}\\big(\\mathbf \\Sigma \\mathbf \\Lambda\\big)$  \n",
    "\n",
    "or equivalently \n",
    "\n",
    "$ \\sigma_1 \\lambda_n + \\sigma_{2} \\lambda_{n-1} + ... + \\sigma_n \\lambda_1 \\leq \\sigma_1 g_{1,1} + \\sigma_2 g_{2,2} + ... + \\sigma_n g_{n,n} \\leq \\sigma_1 \\lambda_1 + \\sigma_2 \\lambda_2 + ... + \\sigma_n \\lambda_n  $ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**claim:**  \n",
    "\n",
    "(where $\\mathbf X$ and $\\mathbf Y$ are both $n$ x $n$ with scalars in $\\mathbb C$)  \n",
    "\n",
    "$\\big \\vert \\text{trace}\\big(\\mathbf {XY}\\big) \\big \\vert \\leq \\sigma_{1}\\gamma_{1} + \\sigma_{2}\\gamma_{2} + ... + \\sigma_{n}\\gamma_{n} = \\text{trace}\\big(\\mathbf {\\Sigma \\Gamma}\\big)$  \n",
    "\n",
    "where we have singular values for both on the Right Hand Side, i.e.:  \n",
    "\n",
    "$\\mathbf X = \\mathbf U_X \\mathbf \\Sigma \\mathbf V_X^H = \\mathbf Q_X \\mathbf A$    \n",
    "and   \n",
    "$\\mathbf Y = \\mathbf U_Y \\mathbf \\Gamma \\mathbf V_Y^H = \\mathbf B  \\mathbf Q_Y$ \n",
    "\n",
    "again where where $\\sigma_{1} \\geq \\sigma_{2} \\geq ... \\geq \\sigma_{n} \\geq 0$ and  $\\gamma_{1} \\geq \\gamma_{2} \\geq ... \\geq \\gamma_{n} \\geq 0$  \n",
    "\n",
    "- - - - \n",
    "**commentary**: This inequality comes from von Neumann and is part of problem 139 on page 88 of http://perso.ens-lyon.fr/serre/DPF/exobis.pdf\n",
    "\n",
    "This problem uses slightly different notation here.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**proof :**  \n",
    "   \n",
    "using polar decomposition:  ,\n",
    "    \n",
    "$\\big \\vert \\text{trace}\\big(\\mathbf {XY}\\big) \\big \\vert =  \\big \\vert \\text{trace}\\Big(\\big(\\mathbf Q_X \\mathbf A\\big) \\big(\\mathbf B  \\mathbf Q_Y \\big) \\Big) \\big \\vert =  \\big \\vert \\text{trace}\\Big(\\mathbf Q_Y \\mathbf Q_X \\mathbf A \\mathbf B   \\Big) \\big \\vert = \\big \\vert \\text{trace}\\Big(\\mathbf Q_Y \\mathbf Q_X \\big(\\mathbf V_X \\mathbf \\Sigma \\mathbf V_X^H\\big) \\mathbf B   \\Big) \\big \\vert $ \n",
    "\n",
    "noting that $\\mathbf I = \\mathbf V_X \\mathbf V_X^H$ ,\n",
    "we thus have ,\n",
    "\n",
    "$\\big \\vert \\text{trace}\\big(\\mathbf {XY}\\big) \\big \\vert = \\big \\vert \\text{trace}\\Big(\\mathbf I \\mathbf Q_Y \\mathbf Q_X \\big(\\mathbf V_X \\mathbf \\Sigma \\mathbf V_X^H\\big) \\mathbf B   \\Big) \\big \\vert = \\big \\vert \\text{trace}\\Big(\\mathbf V_X \\mathbf V_X^H \\mathbf Q_Y \\mathbf Q_X \\mathbf V_X \\mathbf \\Sigma \\mathbf V_X^H \\mathbf B   \\Big) \\big \\vert  = \\big \\vert \\text{trace}\\Big( \\mathbf Q \\mathbf \\Sigma \\big(\\mathbf V_X^H \\mathbf B \\mathbf V_X\\big)  \\Big) \\big \\vert  = \\big \\vert \\text{trace}\\big( \\mathbf Q \\mathbf \\Sigma  \\mathbf C \\big) \\big \\vert $,\n",
    "\n",
    "where the unitary matrix ,\n",
    "\n",
    "$\\mathbf Q:= \\big( \\mathbf V_X^H \\mathbf Q_Y \\mathbf Q_X \\mathbf V_X\\big)$ ,\n",
    "\n",
    "and Hermitian Positive (Semi) definite $\\mathbf C$, where    ,\n",
    "$\\mathbf C: = \\big(\\mathbf V_X^H \\mathbf B \\mathbf V_X\\big)$  ,\n",
    "\n",
    "we thus have,\n",
    "\n",
    "$\\big \\vert \\text{trace}\\big(\\mathbf {XY}\\big) \\big \\vert = \\big \\vert \\text{trace}\\big( \\mathbf Q \\mathbf \\Sigma  \\mathbf C \\big) \\big \\vert = \\big \\vert \\text{trace}\\big( \\mathbf Q \\mathbf \\Sigma^\\frac{1}{2} \\mathbf \\Sigma^\\frac{1}{2}  \\mathbf C^\\frac{1}{2} \\mathbf C^\\frac{1}{2} \\big) \\big \\vert = \\big \\vert \\text{trace}\\Big(\\big(\\mathbf \\Sigma^\\frac{1}{2}  \\mathbf C^\\frac{1}{2}\\big)  \\mathbf C^\\frac{1}{2} \\mathbf Q \\mathbf \\Sigma^\\frac{1}{2}  \\Big) \\big \\vert =\\big \\vert \\text{trace}\\Big(\\big(  \\mathbf C^\\frac{1}{2}\\mathbf \\Sigma^\\frac{1}{2} \\big)^H  \\big(\\mathbf C^\\frac{1}{2} \\mathbf Q \\mathbf \\Sigma^\\frac{1}{2} \\Big) \\big \\vert$ ,\n",
    "\n",
    "*all of this has really just been prep work to invoke Cauchy's inequality and the prior lemma.  For the finish:*    ,\n",
    "\n",
    "$\\big \\vert \\text{trace}\\big(\\mathbf {XY}\\big) \\big \\vert  =\\big \\vert \\text{trace}\\Big(\\big(  \\mathbf C^\\frac{1}{2}\\mathbf \\Sigma^\\frac{1}{2} \\big)^H  \\big(\\mathbf C^\\frac{1}{2} \\mathbf Q \\mathbf \\Sigma^\\frac{1}{2} \\Big) \\big \\vert \\leq \\big \\Vert \\mathbf C^\\frac{1}{2}\\mathbf \\Sigma^\\frac{1}{2} \\big \\Vert_F \\big \\Vert \\mathbf C^\\frac{1}{2} \\mathbf Q \\mathbf \\Sigma^\\frac{1}{2} \\big \\Vert_F = \\text{trace}\\big( \\mathbf C \\mathbf \\Sigma\\big)^\\frac{1}{2} \\text{trace}\\big(\\mathbf Q^H \\mathbf C \\mathbf Q \\mathbf \\Sigma \\big)^\\frac{1}{2}$  ,\n",
    "\n",
    "By application of Cauchy Schwarz.  ,\n",
    "\n",
    "Note we have Hermitian positive (semi)definite   \n",
    "\n",
    "$\\mathbf Z:= \\mathbf Q^H \\mathbf C \\mathbf Q$  \n",
    "\n",
    "giving us,  \n",
    "\n",
    "$\\big \\vert \\text{trace}\\big(\\mathbf {XY}\\big) \\big \\vert \\leq \\text{trace}\\big( \\mathbf C \\mathbf \\Sigma\\big)^\\frac{1}{2} \\text{trace}\\big(\\mathbf Q^H \\mathbf C \\mathbf Q \\mathbf \\Sigma \\big)^\\frac{1}{2} = \\text{trace}\\big( \\mathbf C \\mathbf \\Sigma\\big)^\\frac{1}{2} \\text{trace}\\big(\\mathbf Z \\mathbf \\Sigma \\big)^\\frac{1}{2}$    \n",
    "\n",
    "notice that  $\\mathbf Z$ and $\\mathbf C$ are both Hermitian positive (semi)definite and in fact similar to $\\mathbf B$, and hence they both have the same eigenvalues as $\\mathbf B$ which are equivalent to the singular values of $\\mathbf Y$.  \n",
    "\n",
    "Now, by invoking the prior lemma, we have ,\n",
    "\n",
    "$\\text{trace}\\big( \\mathbf C \\mathbf \\Sigma\\big)^\\frac{1}{2} = \\text{trace}\\big(\\mathbf \\Sigma \\mathbf C \\big)^\\frac{1}{2} \\leq \\text{trace}\\big( \\mathbf \\Gamma \\mathbf \\Sigma\\big)^\\frac{1}{2}$,\n",
    "\n",
    "and,\n",
    "\n",
    "$\\text{trace}\\big( \\mathbf Z \\mathbf \\Sigma\\big)^\\frac{1}{2} = \\text{trace}\\big(\\mathbf \\Sigma \\mathbf Z \\big)^\\frac{1}{2} \\leq \\text{trace}\\big( \\mathbf \\Gamma \\mathbf \\Sigma\\big)^\\frac{1}{2}$   \n",
    "\n",
    "putting this together, we have    \n",
    "\n",
    "$\\big \\vert \\text{trace}\\big(\\mathbf {XY}\\big) \\big \\vert \\leq \\text{trace}\\big( \\mathbf C \\mathbf \\Sigma\\big)^\\frac{1}{2} \\text{trace}\\big(\\mathbf Z \\mathbf \\Sigma \\big)^\\frac{1}{2} \\leq \\text{trace}\\big( \\mathbf \\Gamma \\mathbf \\Sigma\\big)^\\frac{1}{2}\\text{trace}\\big( \\mathbf \\Gamma \\mathbf \\Sigma\\big)^\\frac{1}{2} = \\Big(\\text{trace}\\big( \\mathbf \\Gamma \\mathbf \\Sigma\\big)^\\frac{1}{2}\\Big)^2 = \\text{trace}\\big( \\mathbf \\Gamma \\mathbf \\Sigma\\big) = \\text{trace}\\big(  \\mathbf \\Sigma \\mathbf \\Gamma\\big)$  \n",
    "    \n",
    "which completes the proof \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# End Interlude involving singular values and ideas from Majorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Hermitian Positive Semi Definite Inequalities with Hadamard Product \n",
    "\n",
    "*note 1: This apparently is covered under the Schur Product Theorem, which is an interesting albeit esoteric part of matrix theory.* \n",
    "\n",
    "*note 2: The inequalities in here are covered via a diagonalization argument.  For a different proof of these relations see my writeup \"Kronecker_Product.ipynd\" -- specifically the subsection called \"Remark on Kronecker Product vs Hadamard Product\".     *\n",
    "\n",
    "\n",
    "consider Hermitian Positive Semi Definite Matrices $\\mathbf A$ and $\\mathbf B$.  Then consider the matrix $\\mathbf C = \\mathbf A \\circ \\mathbf B$, where $\\circ$ denotes the Hadamard product.    \n",
    "\n",
    "**claim:**\n",
    "$\\mathbf C $ is Hermitian positive semi-definite\n",
    "\n",
    "\n",
    "*Reminder:*\n",
    "Hadamard products distribute accross matrix addition because scalar multiplication distributes across scalar addition. \n",
    "\n",
    "example:\n",
    "\n",
    "$\\mathbf Z = \\big(\\mathbf X + \\mathbf Y\\big) \\circ \\mathbf W = \\mathbf X \\circ \\mathbf W + \\mathbf Y \\circ \\mathbf W$\n",
    "\n",
    "that is  \n",
    "\n",
    "$z_{i,j} = (x_{i,j} + y_{i,j})*w_{i,j} = x_{i,j}w_{i,j} + y_{i,j}w_{i,j}$\n",
    "\n",
    "\n",
    "**proof:**\n",
    "\n",
    "$c_{i,j} = a_{i,j} * b_{i,j} = \\bar{a_{j,i}}\\cdot \\bar{b_{j,i}} = \\bar{c_{j,i}}$\n",
    "\n",
    "By inspection we see that $\\mathbf C$ is Hermitian. \n",
    "\n",
    "Now the claim of positive semi definitiness means $\\mathbf x^H \\mathbf C \\mathbf x \\geq 0 $ for all $\\mathbf x$. \n",
    "\n",
    "next we use the decomposition employed in \"julia_hmm_viterbi_as_qp_and_lp_upload\" contained in the Optimization aka \"markov_optimization\" folder.  \n",
    "\n",
    "$\\mathbf x^H \\mathbf C \\mathbf x  = \\mathbf 1^H \\big(\\mathbf C \\circ \\mathbf x\\mathbf x^H \\big)\\mathbf 1 = \\text{sum}\\big(\\mathbf C \\circ \\mathbf x\\mathbf x^H \\big)$\n",
    "\n",
    "where \"sum\" denotes adding up each scalar entry of the matrix $\\big( \\mathbf C \\circ \\mathbf x\\mathbf x^H \\big)$.\n",
    "\n",
    "Hence we are evaluating:\n",
    "\n",
    "$\\text{sum}\\big(\\mathbf C \\circ \\mathbf x\\mathbf x^H \\big) = \\text{sum}\\big(\\mathbf A \\circ \\mathbf B \\circ \\mathbf x\\mathbf x^H \\big)$\n",
    "\n",
    "\n",
    "using associativity and commutativity of the Hadamard product, we have\n",
    "\n",
    "$\\text{sum}\\big(\\mathbf A \\circ \\mathbf B \\circ \\mathbf x\\mathbf x^H \\big) = \\text{sum}\\Big( \\mathbf A \\circ \\mathbf x\\mathbf x^H \\circ \\mathbf B   \\Big) = \\text{sum}\\Big(\\big(\\mathbf A \\circ \\mathbf x\\mathbf x^H\\big) \\circ \\mathbf B   \\Big)$\n",
    "\n",
    "since $\\mathbf A$ is Hermitiain Positive Semi Definite, we can re-write it as \n",
    "\n",
    "$\\mathbf A = (\\lambda_1 \\mathbf p_1 \\mathbf p_1^H + \\lambda_2 \\mathbf p_2 \\mathbf p_2^H + ... + \\lambda_n \\mathbf p_n \\mathbf p_n^H)$ \n",
    "\n",
    "where each $\\lambda_k$ is real valued and non-negative\n",
    "\n",
    "$\\text{sum}\\Big(\\big(\\mathbf A \\circ \\mathbf x\\mathbf x^H\\big) \\circ \\mathbf B   \\Big) = \\text{sum}\\Big(\\big((\\lambda_1 \\mathbf p_1 \\mathbf p_1^H + \\lambda_2 \\mathbf p_2 \\mathbf p_2^H + ... + \\lambda_n \\mathbf p_n \\mathbf p_n^H) \\circ \\mathbf x\\mathbf x^H\\big) \\circ \\mathbf B   \\Big) $\n",
    "\n",
    "$\\text{sum}\\Big(\\big(\\mathbf A \\circ \\mathbf x\\mathbf x^H\\big) \\circ \\mathbf B   \\Big) = \\text{sum}\\Big(\\big(\\lambda_1 \\mathbf p_1 \\mathbf p_1^H \\circ \\mathbf x\\mathbf x^H + \\lambda_2 \\mathbf p_2 \\mathbf p_2^H \\circ \\mathbf x\\mathbf x^H + ... + \\lambda_n \\mathbf p_n \\mathbf p_n^H\\circ \\mathbf x\\mathbf x^H \\big) \\circ \\mathbf B   \\Big)$\n",
    "\n",
    "re-writing this with sigma notation, we have\n",
    "\n",
    "$\\text{sum}\\Big(\\big( \\sum_{k=1}^n \\lambda_k \\mathbf p_k \\mathbf p_k^H \\circ \\mathbf x\\mathbf x^H \\big) \\circ \\mathbf B   \\Big)$\n",
    "\n",
    "now we definte $\\mathbf y_k$, where\n",
    "\n",
    "$\\mathbf y_k := \\mathbf p_k \\circ \\mathbf x$\n",
    "\n",
    "hence $\\mathbf y_k \\mathbf y_k^H = \\mathbf p_k \\mathbf p_k^H \\circ \\mathbf x\\mathbf x^H $\n",
    "\n",
    "where for avoidance of doubt, we double check the indices:\n",
    "\n",
    "$\\big(\\mathbf y_k \\mathbf y_k^H\\big)_{i,j} = (p_{k,i}  * x_{i}) * (\\bar{p_{k,j}}\\cdot \\bar{x_{j}}) = (p_{k,i} \\cdot \\bar{p_{k,j}})  * (x_{i}  \\cdot \\bar{x_{j}}) = \\big(\\mathbf p_k \\mathbf p_k^H\\big)_{i,j} \\circ \\big(\\mathbf{xx}^H\\big)_{i,j}=   \\big(\\mathbf p_k \\mathbf p_k^H \\circ \\mathbf{xx}^H\\big)_{i,j}$ \n",
    "\n",
    "our expression becomes\n",
    "\n",
    "$\\text{sum}\\Big(\\big( \\sum_{k=1}^n \\lambda_k \\mathbf y_k \\mathbf y_k^H \\big) \\circ \\mathbf B   \\Big) = \\text{sum}\\Big( \\sum_{k=1}^n \\lambda_k \\mathbf y_k \\mathbf y_k^H \\circ \\mathbf B   \\Big)$\n",
    "\n",
    "now, recognizing that when we have a finite number of terms, we can always interchange linear operators like $sum()$ and $\\sum$\n",
    "\n",
    "$\\text{sum}\\Big( \\sum_{k=1}^n \\lambda_k \\mathbf y_k \\mathbf y_k^H \\circ \\mathbf B   \\Big) = \\sum_{k=1}^n  \\text{sum}\\Big( \\lambda_k \\mathbf y_k \\mathbf y_k^H \\circ \\mathbf B   \\Big)$\n",
    "\n",
    "- - - -\n",
    "side note: the above is interchange is equivalent to recognizing that $\\mathbf 1^H \\big( \\mathbf W + \\mathbf X\\big)\\mathbf 1 = \\mathbf 1^H\\big(\\mathbf W\\big)\\mathbf 1 + \\mathbf 1^H\\big(\\mathbf X\\big)\\mathbf 1$\n",
    "- - - -\n",
    "\n",
    "$\\sum_{k=1}^n  \\text{sum}\\big( \\lambda_k \\mathbf y_k \\mathbf y_k^H \\circ \\mathbf B   \\big) = \\sum_{k=1}^n  \\lambda_k \\Big(\\text{sum}\\big(  \\mathbf y_k \\mathbf y_k^H \\circ \\mathbf B   \\big)\\Big) = \\sum_{k=1}^n  \\lambda_k \\Big(\\text{sum}\\big( \\mathbf B \\circ \\mathbf y_k \\mathbf y_k^H \\big)\\Big) = \\sum_{k=1}^n  \\lambda_k \\Big(\\mathbf y_k^H \\mathbf B \\mathbf y_k\\Big)$\n",
    "\n",
    "recognizing our original quadratic form decomposition $\\mathbf y_k^H \\mathbf B \\mathbf y_k = \\text{sum}\\big( \\mathbf B \\circ \\mathbf y_k \\mathbf y_k^H \\big)$ \n",
    "\n",
    "since $\\mathbf B$ is Hermitian positive semi-definite, we know that $\\mathbf y_k^H \\mathbf B \\mathbf y_k \\geq 0$ for any $\\mathbf y_k$, and we recall that $\\lambda_k \\geq 0$ \n",
    "\n",
    "Hence we say \n",
    "\n",
    "$\\sum_{k=1}^n  \\lambda_k \\big(\\mathbf y_k^H \\mathbf B \\mathbf y_k\\big) =  \\lambda_1 \\big(\\mathbf y_1^H \\mathbf B \\mathbf y_1\\big) + \\lambda_2 \\big(\\mathbf y_2^H \\mathbf B \\mathbf y_2\\big) + ... + \\lambda_n \\big(\\mathbf y_n^H \\mathbf B \\mathbf y_n\\big) \\geq 0$\n",
    "\n",
    "because each term in that series is itself $\\geq 0$.\n",
    "\n",
    "This proves that $\\mathbf C = \\mathbf A \\circ \\mathbf B$ is a Hermitian positive semi-definite matrix.\n",
    "\n",
    "**claim:**  \n",
    "\n",
    "$\\lambda_{1, A}\\cdot \\lambda_{1,B} \\geq \\lambda_{1, C}$\n",
    "\n",
    "where $\\mathbf C = \\mathbf A \\circ \\mathbf B$\n",
    "\n",
    "alternatively stated as:  \n",
    "$\\lambda \\big(\\mathbf A\\big)_{max} \\lambda \\big(\\mathbf B\\big)_{max} \\geq \\lambda \\big(\\mathbf A \\circ \\mathbf B\\big)_{max}$\n",
    "\n",
    "and as always eigenvalues are well ordered so that $\\lambda_1 \\geq \\lambda_2 \\geq ... \\geq \\lambda_n \\geq 0$ for any of these Hermitian positive semi-definite matrices\n",
    "\n",
    "The claim is trivially an equality if $\\mathbf A = \\mathbf 0$ or $\\mathbf B = \\mathbf 0$, hence we assume that neither matrix is the zero matrix, i.e. that $\\big \\Vert \\mathbf A \\big \\Vert_F \\gt 0$ and $\\big \\Vert \\mathbf B \\big \\Vert_F \\gt 0$\n",
    "\n",
    "**proof:**  \n",
    "\n",
    "\n",
    "$ \\lambda_{1,A} \\mathbf I - \\mathbf A = \\lambda_{1,A} \\mathbf I - \\mathbf {PDP}^H = \\lambda_{1,A} \\mathbf {PIP}^H - \\mathbf {PDP}^H = \\mathbf P \\big(\\lambda_{1,A} \\mathbf I - \\mathbf D\\big)\\mathbf P^H$\n",
    "\n",
    "when we inspect the diagonal entries of the diagonal matrix given by, we see:\n",
    "\n",
    "$\\big(\\lambda_{1,A} \\mathbf I - \\mathbf D\\big)_{k,k} = \\lambda_{1, A} - \\lambda_{k, A} \\geq 0$\n",
    "\n",
    "hence the matrix $\\big(\\lambda_{1,A} \\mathbf I - \\mathbf A \\big)$ is Hermitian positive semi-definite.  \n",
    "\n",
    "now consider:\n",
    "\n",
    "$\\big(\\lambda_{1,A} \\mathbf I - \\mathbf A \\big)\\circ \\mathbf B$\n",
    "\n",
    "Based on the preceding proof, the matrix that results from this, too, must be Hermitian positive semi-definite.  \n",
    "\n",
    "Hence we say \n",
    "\n",
    "$\\mathbf x^H \\Big(\\big(\\lambda_{1,A} \\mathbf I - \\mathbf A \\big)\\circ \\mathbf B\\Big)\\mathbf x \\geq 0$\n",
    "\n",
    "for any $\\mathbf x$.\n",
    "\n",
    "$\\mathbf x^H \\Big(\\big(\\lambda_{1,A} \\mathbf I - \\mathbf A \\big)\\circ \\mathbf  B\\Big)\\mathbf x = \\mathbf x^H \\Big(\\big(\\lambda_{1,A} \\mathbf I\\big) \\circ \\mathbf B \\Big)\\mathbf x - \\mathbf x^H \\Big(\\mathbf A \\circ \\mathbf B\\Big)\\mathbf x \\geq 0$\n",
    "\n",
    "for any $\\mathbf x$\n",
    "\n",
    "$\\mathbf x^H \\Big(\\big(\\lambda_{1,A} \\mathbf I\\big) \\circ \\mathbf B \\Big)\\mathbf x \\geq \\mathbf x^H \\Big(\\mathbf A \\circ \\mathbf B\\Big)\\mathbf x = \\mathbf x^H \\mathbf C \\mathbf x$\n",
    "\n",
    "for any $\\mathbf x$\n",
    "\n",
    "now set the constraint that $\\Vert \\mathbf x \\Vert_2^2 = 1$.  The right hand side is maximized with $\\mathbf x : = \\mathbf x_1$ which is the eigenvector associated with $\\lambda_{1,C}$\n",
    "\n",
    "$\\mathbf x_1^H \\Big(\\big(\\lambda_{1,A} \\mathbf I\\big) \\circ \\mathbf B \\Big)\\mathbf x_1 = \\lambda_{1,A} \\cdot \\mathbf x_1^H \\Big( \\mathbf I \\circ \\mathbf B \\Big)\\mathbf x_1 \\geq \\lambda_{1,C} = \\mathbf x_1^H \\mathbf C \\mathbf x_1$\n",
    "\n",
    "Hence we know that the Hermitian positive semi-definite matrix given by $\\big(\\mathbf I \\circ \\mathbf B \\big)$ must have at least one eigenvalue that can be scaled by $\\lambda \\big(\\mathbf A\\big)_{max}$ and the resulting product $\\geq \\lambda \\big(\\mathbf C\\big)_{max}$.\n",
    "\n",
    "If we are able to prove that the maximal eigenvalue of $\\mathbf B$ is at least as big as the maximal eigenvalue of $\\big(\\mathbf I \\circ \\mathbf B \\big)$, then we are done.  \n",
    "\n",
    "where $\\mathbf H := \\mathbf I \\circ \\mathbf B$.  \n",
    "\n",
    "Notice that $\\mathbf H$ in effect takes all of the diagonal entries of $\\mathbf B$ (and keeps their ordering intact), and then zeros out all other entries of $\\mathbf B$.\n",
    "\n",
    "What we have proven so far can be re-written as \n",
    "\n",
    "$\\lambda_{1,A} \\cdot \\lambda_{1, H} \\geq \\lambda_{1,C} $\n",
    "\n",
    "Further we now claim:\n",
    "\n",
    "$\\lambda_{1,A} \\cdot \\lambda_{1, B} \\geq \\lambda_{1,A} \\cdot \\lambda_{1, H} \\geq \\lambda_{1,C} $\n",
    "\n",
    "To justify this, consider the following: \n",
    "\n",
    "max $\\mathbf y^H \\mathbf B \\mathbf y = \\lambda_{1, B}$\n",
    "\n",
    "subject to the constraint that $\\Vert \\mathbf y \\Vert_2^2 = 1$\n",
    "\n",
    "we can always choose to restrict ourself to just one standard basis vector $\\mathbf e_k$, for $k = \\{1, 2, ..., n\\}$.  For avoidance of doubt, the standard basis vectors are shown below:\n",
    "\n",
    "$\\bigg[\\begin{array}{c|c|c|c}\n",
    "\\mathbf e_1 & \\mathbf  e_2 &\\cdots &\\mathbf  e_n\n",
    "\\end{array}\\bigg] = \\mathbf I$\n",
    "\n",
    "\n",
    "which makes the optimization\n",
    "\n",
    "max $\\mathbf e_k^H \\mathbf B \\mathbf e_k = \\lambda_{1, H}$\n",
    "\n",
    "Thus\n",
    "\n",
    "max $\\mathbf y^H \\mathbf B \\mathbf y = \\lambda_{1, B}  \\geq$ max $ \\mathbf e_k^H \\mathbf B \\mathbf e_k = \\lambda_{1, H}$, again with the constrain that $\\Vert \\mathbf y \\Vert_2^2 = 1$. \n",
    "\n",
    "Hence $\\lambda_{1, B} \\geq \\lambda_{1, H}$\n",
    "\n",
    "To conclude we have: \n",
    "\n",
    "$\\lambda_{1,A} \\cdot \\lambda_{1, B} \\geq \\lambda_{1,A} \\cdot \\lambda_{1, H} \\geq \\lambda_{1,C} $\n",
    "\n",
    "or more succinctly,\n",
    "\n",
    "$\\lambda \\big(\\mathbf A\\big)_{max} \\lambda \\big(\\mathbf B\\big)_{max} \\geq \\lambda \\big(\\mathbf A\\circ \\mathbf B\\big)_{max}$\n",
    "\n",
    "and the claim is proven\n",
    "\n",
    "- - - -\n",
    "*begin alternative proof*  \n",
    "\n",
    "Another way to prove this, which seems a bit more intuitive, is to leverage the work done in the first part to prove that $\\big(\\mathbf A \\circ \\mathbf B\\big)$ is a  Hermitian positive semi definite matrix.  The final expression we had was\n",
    "\n",
    "$ \\sum_{k=1}^n  \\lambda_k \\big(\\mathbf y_k^H \\mathbf B \\mathbf y_k\\big) = \\lambda_1 \\big(\\mathbf y_1^H \\mathbf B \\mathbf y_1\\big) + \\lambda_2 \\big(\\mathbf y_2^H \\mathbf B \\mathbf y_2\\big) + ... + \\lambda_n \\big(\\mathbf y_n^H \\mathbf B \\mathbf y_n\\big) \\geq 0$\n",
    "\n",
    "Recalling that the $\\lambda_k$'s were the eigenvalues of $\\mathbf A$, and that $\\lambda_1 \\geq \\lambda_2 \\geq ... \\geq \\lambda_n \\geq 0$, we can upper bound this series as follows:\n",
    "\n",
    "$ \\sum_{k=1}^n  \\lambda_1 \\big(\\mathbf y_k^H \\mathbf B \\mathbf y_k\\big) \\geq \\sum_{k=1}^n  \\lambda_k \\big(\\mathbf y_k^H \\mathbf B \\mathbf y_k\\big) \\geq 0$\n",
    "\n",
    "From here we work backward and examine the impact of homegenizing the eigenvalues of $\\mathbf A$, specificially recalling \n",
    "\n",
    "$\\text{sum}\\Big(\\big(\\mathbf A \\circ \\mathbf x\\mathbf x^H\\big) \\circ \\mathbf B   \\Big) = \\text{sum}\\Big(\\big((\\lambda_1 \\mathbf p_1 \\mathbf p_1^H + \\lambda_2 \\mathbf p_2 \\mathbf p_2^H + ... + \\lambda_n \\mathbf p_n \\mathbf p_n^H) \\circ \\mathbf x\\mathbf x^H\\big) \\circ \\mathbf B   \\Big) $\n",
    "\n",
    "now becomes\n",
    "\n",
    "$\\text{sum}\\Big(\\big( \\lambda_1 ( \\mathbf p_1 \\mathbf p_1^H + \\mathbf p_2 \\mathbf p_2^H + ... + \\mathbf p_n \\mathbf p_n^H) \\circ \\mathbf x\\mathbf x^H\\big) \\circ \\mathbf B   \\Big) = \\text{sum}\\Big( \\lambda_1 \\big( \\mathbf P \\mathbf P^H\\big) \\circ \\mathbf x\\mathbf x^H \\circ \\mathbf B   \\Big) = \\text{sum}\\Big(\\lambda \\big(\\mathbf A\\big)_{max} \\big( \\mathbf I\\big) \\circ \\mathbf x\\mathbf x^H \\circ \\mathbf B   \\Big) $\n",
    "\n",
    "which we can re-arrange to\n",
    "\n",
    "$\\lambda \\big(\\mathbf A\\big)_{max} \\cdot \\text{sum}\\Big( \\big(\\mathbf I \\circ \\mathbf B\\big) \\circ \\mathbf x \\mathbf x^H     \\Big) = \\lambda \\big(\\mathbf A\\big)_{max} \\cdot \\mathbf x^H \\big(\\mathbf I \\circ \\mathbf B\\big)\\mathbf x $\n",
    "\n",
    "\n",
    "which tells us that at a minimum $\\lambda \\big(\\mathbf A\\big)_{max} \\cdot \\lambda \\big(\\mathbf I\\circ \\mathbf B\\big)_{max} \\geq \\lambda \\big(\\mathbf A\\circ \\mathbf B\\big)_{max}$\n",
    "\n",
    "because \n",
    "\n",
    "$\\lambda_1 \\big(\\mathbf y_1^H \\mathbf B \\mathbf y_1\\big) + \\lambda_1 \\big(\\mathbf y_2^H \\mathbf B \\mathbf y_2\\big) + ... + \\lambda_1 \\big(\\mathbf y_n^H \\mathbf B \\mathbf y_n\\big) \\geq  \\lambda_1 \\big(\\mathbf y_1^H \\mathbf B \\mathbf y_1\\big) + \\lambda_2 \\big(\\mathbf y_2^H \\mathbf B \\mathbf y_2\\big) + ... + \\lambda_n \\big(\\mathbf y_n^H \\mathbf B \\mathbf y_n\\big)$\n",
    "\n",
    "for any $\\mathbf x$, (subject to $\\Vert \\mathbf x \\Vert_2 = 1$) including $\\mathbf x$ that maximizes the right hand side, which gives an answer $= \\lambda \\big(\\mathbf A\\circ \\mathbf B\\big)_{max}$.  Hence we know that the left hand side has at least one solution (read: eigenvalue associated with $\\lambda \\big(\\mathbf I\\circ \\mathbf B\\big)_{max}$ times $\\lambda \\big(\\mathbf A\\big)_{max}$ ) that is greater than or equal to the right hand side.  \n",
    "\n",
    "from here we again notice that, subject to the constraint $\\Vert \\mathbf z \\Vert_2 = 1$\n",
    "\n",
    "max $\\mathbf z^H \\mathbf B \\mathbf z = \\lambda \\big(\\mathbf B\\big)_{max}  \\geq $ max $\\mathbf e_k^H \\mathbf B \\mathbf e_k = \\lambda \\big(\\mathbf I \\circ \\mathbf B\\big)_{max}$\n",
    "\n",
    "recalling that $\\bigg[\\begin{array}{c|c|c|c}\n",
    "\\mathbf e_1 & \\mathbf  e_2 &\\cdots &\\mathbf  e_n\n",
    "\\end{array}\\bigg] = \\mathbf I$\n",
    "\n",
    "and we conclude with\n",
    "\n",
    "$\\lambda \\big(\\mathbf A\\big)_{max} \\cdot \\lambda \\big(\\mathbf B\\big)_{max}  \\geq \\lambda \\big(\\mathbf A\\big)_{max} \\cdot \\lambda \\big(\\mathbf I\\circ \\mathbf B\\big)_{max} \\geq \\lambda \\big(\\mathbf A\\circ \\mathbf B\\big)_{max}$\n",
    "\n",
    "*end alternative proof*  \n",
    "\n",
    "*commentary*\n",
    "\n",
    "It is interesting to think about the eigenvalues of $\\mathbf B$ and $\\big(\\mathbf I \\circ \\mathbf B\\big)$.  We can use Gerschgorin discs (which we can flatten to line segments if we want because we know the eigenvalues are real) to bound the eigenvalues of $\\mathbf B$, noting that the center point of the discs are given exactly by $\\big(\\mathbf I \\circ \\mathbf B\\big)$.  \n",
    "\n",
    "Let's suppose that $\\mathbf B \\neq \\big(\\mathbf I \\circ \\mathbf B\\big)$ and that there are no zeros along the diagonal.  \n",
    "\n",
    "Then we know that these matrices eigenvalues sum to be the same amount:\n",
    "\n",
    "$\\text{trace}\\big(\\mathbf B\\big) = \\text{trace}\\big(\\mathbf I \\circ \\mathbf B\\big)$\n",
    "\n",
    "since $\\big(\\mathbf I \\circ \\mathbf B\\big)$ is diagonal, we know its determinant is the product of those entries (which is $\\gt 0$ because we assume no zeros on diagonal). From applying the Hadamard Inequality, we know that \n",
    "\n",
    "$\\text{det}\\big(\\mathbf B\\big) \\leq \\text{det}\\big(\\mathbf I \\circ \\mathbf B\\big)$\n",
    "\n",
    "However, we also know that if we square both sides, the left hand side will have a higher trace:\n",
    "\n",
    "$\\text{trace}\\big(\\mathbf B^2\\big) = \\text{trace}\\big(\\mathbf B^H \\mathbf B \\big) = \\Vert \\mathbf B\\Vert_F^2 \\geq \\Vert \\big( \\mathbf I \\circ  \\mathbf B\\big) \\Vert_F^2 = \\text{trace}\\Big(\\big(\\mathbf I \\circ \\mathbf B\\big)^2\\Big)$\n",
    "\n",
    "This would seem to suggest that that $\\mathbf B$ has more extreme eigenvalues than $\\big(\\mathbf I \\circ \\mathbf B\\big)$ (note: this is in fact true and this line of thinking can be considerably refined via some ideas from majorization), that when you add them all up, they are the same, but when you multiply them, you get a smaller product.  However, when you square them, the small ones decrease, but in the spirit of Jensen's Inequality, we'd observe that that $\\frac{1}{2}(\\lambda_1^2 + \\lambda_n^2) \\geq (\\frac{1}{2}(\\lambda_1 + \\lambda_n))^2$\n",
    "\n",
    "And this is what our quadratic form tells us, again, where $\\Vert \\mathbf z \\Vert_2 = 1$  \n",
    "\n",
    "max $\\mathbf z^H \\mathbf B \\mathbf z = \\lambda \\big(\\mathbf B\\big)_{max}  \\geq $ max $\\mathbf e_k^H \\mathbf B \\mathbf e_k = \\lambda \\big(\\mathbf I \\circ \\mathbf B\\big)_{max}$\n",
    "\n",
    "and if we want to minimize the quadratic form, we see,\n",
    "\n",
    "min $\\mathbf z^H \\mathbf B \\mathbf z = \\lambda \\big(\\mathbf B\\big)_{min}  \\leq $ min $\\mathbf e_k^H \\mathbf B \\mathbf e_k = \\lambda \\big(\\mathbf I \\circ \\mathbf B\\big)_{min}$\n",
    "\n",
    "Because $\\mathbf e_k$'s are a proper subset of what we can choose our $\\mathbf z$ from, and hence our optimal $\\mathbf z$ must give a result at least as good as using $\\mathbf e_k$.  \n",
    "\n",
    "\n",
    "- - - - \n",
    "\n",
    "**claim:**  \n",
    "\n",
    "$\\text{trace}\\big(\\mathbf A \\circ \\mathbf B\\big) \\leq \\text{trace}\\big(\\mathbf A\\big) \\text{trace}\\big(\\mathbf B\\big)$\n",
    "\n",
    "**proof:**  \n",
    "\n",
    "$\\text{trace}\\big(\\mathbf A \\circ \\mathbf B\\big) = a_{1,1}*b_{1,1} + a_{2,2}*b_{2,2} +... + a_{n,n}*b_{n,n} \\leq (a_{1,1} + a_{2,2} +... + a_{n,n}) (b_{1,1} + b_{2,2} +... + b_{n,n}) = \\text{trace}\\big(\\mathbf A\\big) \\text{trace}\\big(\\mathbf B\\big)$\n",
    "\n",
    "because each diagonal entry $a_{k,k}$ and $b_{k,k}$ is real valued and non-negative, and hence all cross terms are real valued and non-negative.  \n",
    "\n",
    "\n",
    "**claim:**  \n",
    "\n",
    "$\\text{trace}\\big(\\mathbf A \\circ \\mathbf B\\big) \\leq \\frac{1}{2} \\text{trace}\\big(\\mathbf A \\circ \\mathbf A + \\mathbf B \\circ \\mathbf B\\big)$\n",
    "\n",
    "**proof:**\n",
    "\n",
    "a very simple way to prove this claim is to notice that $\\mathbf C:= \\mathbf A - \\mathbf B$, is a Hermitian matrix.  And $\\mathbf C$, like any Hermitian matrix, must have real valued entries on its diagonal (or else they could not be equal to their conjugate, and we would not have $\\mathbf C = \\mathbf C^H$).  Thus when we square the real valued entries along the diagonal, we see $\\mathbf c_{k,k} \\cdot \\mathbf c_{k,k} \\geq 0$.  \n",
    "\n",
    "Hence we can say:\n",
    "\n",
    "$\\text{trace}\\big(\\mathbf C \\circ \\mathbf C\\big) \\geq 0$\n",
    "\n",
    "$\\text{trace}\\Big(\\big(\\mathbf A - \\mathbf B\\big) \\circ \\big(\\mathbf A - \\mathbf B\\big)\\Big) \\geq 0$\n",
    "\n",
    "$\\text{trace}\\Big( \\mathbf A \\circ \\mathbf A + \\mathbf B \\circ \\mathbf B - 2 \\cdot \\mathbf A \\circ \\mathbf B\\Big) \\geq 0$\n",
    "\n",
    "$\\text{trace}\\Big( \\mathbf A \\circ \\mathbf A + \\mathbf B \\circ \\mathbf B\\Big) - 2\\cdot \\text{trace}\\Big(\\mathbf A \\circ \\mathbf B\\Big) \\geq 0$\n",
    "\n",
    "$\\text{trace}\\Big( \\mathbf A \\circ \\mathbf A + \\mathbf B \\circ \\mathbf B\\Big) \\geq 2\\cdot \\text{trace}\\Big(\\mathbf A \\circ \\mathbf B\\Big) $\n",
    "\n",
    "\n",
    "$\\frac{1}{2} \\text{trace}\\Big( \\mathbf A \\circ \\mathbf A + \\mathbf B \\circ \\mathbf B\\Big) \\geq \\text{trace}\\Big(\\mathbf A \\circ \\mathbf B\\Big) $\n",
    "\n",
    "which completes the proof\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
