{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**notice on notation:**  \n",
    "This posting was written over *many* months by your author.  In some cases $\\mathbf A^H$ is used to denote the conjugate transpose of $\\mathbf A$ and in some other cases $\\mathbf A^*$ is used to denote said conjugate transpose.  On rare occasion the $\\mathbf A^*$ will be used to denote something a bit different in context of quasi-linearization, though this usage is rare and should, hopefully, be clear from context.   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Proof of Cauchy Schwarz\n",
    "\n",
    "the underlying scalars are in $\\mathbb C$ and we have $n $ x $1$ vectors. \n",
    "\n",
    "consider some vector $\\mathbf x =  \\begin{bmatrix}\n",
    "x_1\\\\ \n",
    "x_2\\\\ \n",
    "\\vdots \\\\ \n",
    "x_n\n",
    "\\end{bmatrix}$\n",
    "\n",
    "and some other vector,  $\\mathbf y =  \\begin{bmatrix}\n",
    "y_1\\\\ \n",
    "y_2\\\\ \n",
    "\\vdots \\\\ \n",
    "y_n\n",
    "\\end{bmatrix}$\n",
    "\n",
    "now consider the outer product given by \n",
    "\n",
    "$\\mathbf {xy}^H =  \\begin{bmatrix}\n",
    "x_1 \\bar{y_1} & x_1 \\bar{y_2} &...& x_1 \\bar{y_n}\\\\ \n",
    "x_2 \\bar{y_1} & x_2 \\bar{y_2} & ... & x_2 \\bar{y_n} \\\\ \n",
    "\\vdots & \\vdots & \\ddots &\\vdots \\\\ \n",
    "x_n \\bar{y_1} & x_n \\bar{y_2} &.... & x_n \\bar{y_n}\n",
    "\\end{bmatrix}$\n",
    "\n",
    "\n",
    "From here, consider where we want the squared Frobenius norm of $\\mathbf {xy}^H$ \n",
    "\n",
    "$\\big\\vert\\big\\vert \\mathbf {xy}^H\\big\\vert\\big\\vert_{F}^2 = \\text{trace}\\Big(\\big(\\mathbf {xy}^H\\big)^H \\big( \\mathbf {xy}^H\\big)\\Big) = \\text{trace}\\Big( \\mathbf {yx}^H \\mathbf {xy}^H\\Big) = \\text{trace}\\Big( \\mathbf{y}^H \\mathbf{yx}^H \\mathbf {x}\\Big) = \\mathbf {y}^H\\mathbf y \\mathbf x^H \\mathbf {x}  = \\big\\vert\\big\\vert \\mathbf{y}\\big\\vert\\big\\vert_{2}^2\\big\\vert\\big\\vert \\mathbf {x}\\big\\vert\\big\\vert_{2}^2$\n",
    "\n",
    "where the middle equalities made use of the cyclic property of the trace\n",
    "\n",
    "We conclude the proof with the following:\n",
    "\n",
    "$ \\big\\vert\\big\\vert \\mathbf{y}\\big\\vert\\big\\vert_{2}^2\\big\\vert\\big\\vert \\mathbf {x}\\big\\vert\\big\\vert_{2}^2 =  \\text{trace}\\Big(\\big(\\mathbf {\\mathbf {xy}}^H\\big)^H \\big( \\mathbf {\\mathbf {xy}}^H\\big)\\Big) \n",
    " \\geq \\big \\vert \\text{trace}\\Big( \\big(\\mathbf {xy}^H\\big) \\big(\\mathbf {xy}^H\\big)\\Big)\\big \\vert =  \\big \\vert \\text{trace}\\Big( \\mathbf y^H \\mathbf {xy}^H\\mathbf {x}\\Big)\\big \\vert  = \\big \\vert \\text{trace}\\Big( \\big(\\mathbf y^H \\mathbf {x}\\big) \\big(\\mathbf y^H \\mathbf {x}\\big)\\Big)\\big \\vert = \\big \\vert \\mathbf y^H \\mathbf x \\big \\vert^2 $\n",
    "\n",
    "hence \n",
    "\n",
    "$\\big\\vert\\big\\vert \\mathbf{y}\\big\\vert\\big\\vert_{2}^2\\big\\vert\\big\\vert \\mathbf {x}\\big\\vert\\big\\vert_{2}^2 = \\Big(\\Sigma_{i=1}^{n}\\big\\vert y_i\\big\\vert^2\\Big) \\Big(\\Sigma_{i=1}^{n}\\big\\vert x_i\\big\\vert^2\\Big)  \\geq \\big \\vert\\Big(\\Sigma_{i=1}^{n}x_i \\bar{y_i}\\Big)^2\\big \\vert= \\big \\vert \\mathbf y^H \\mathbf x \\big \\vert^2 $\n",
    "\n",
    "\n",
    "noting that \n",
    "\n",
    "$\\text{trace}\\Big(\\big(\\mathbf {\\mathbf {xy}}^H\\big)^H \\big( \\mathbf {\\mathbf {xy}}^H\\big)\\Big)\n",
    "\\geq \\big \\vert \\text{trace}\\Big( \\big(\\mathbf {xy}^H\\big) \\big(\\mathbf {xy}^H\\big)\\Big)\\big \\vert $\n",
    "\n",
    "via the Schur Inequality in the special case of a rank one matrix (and hence triangle inequality is *not* required here).  \n",
    "\n",
    "- - - - \n",
    "\n",
    "**extension:**  \n",
    "Suppose we we relax the requirement that an inner product is positive definite, and instead allow it to be positive semi-definite.  (Sometimes this may be referred to as a semi inner product.)\n",
    "\n",
    "\n",
    "for some Hermitian positive semi-definite $\\mathbf A$:   \n",
    "$\\langle \\mathbf x, \\mathbf y \\rangle = \\mathbf x^H \\mathbf A \\mathbf y =  \\big(\\mathbf A^{\\frac{1}{2}} \\mathbf x\\big)^H \\big(\\mathbf A^{\\frac{1}{2}}\\mathbf y\\big)$\n",
    "\n",
    "\n",
    "$\\mathbf b:= \\mathbf A^{\\frac{1}{2}} \\mathbf x$  \n",
    "$\\mathbf c:= \\mathbf A^{\\frac{1}{2}} \\mathbf y$  \n",
    "\n",
    "$\\mathbf \\langle \\mathbf x, \\mathbf y \\rangle = \\mathbf b^H \\mathbf c$\n",
    "\n",
    "and we verify that Cauchy's Inequality still holds by repeating the above argument: \n",
    "\n",
    "$\\langle \\mathbf y, \\mathbf y \\rangle \\langle \\mathbf x, \\mathbf x \\rangle =  \\big(\\mathbf{c}^H\\mathbf c\\big)\\big(\\mathbf b^H\\mathbf b\\big) =  \\text{trace}\\Big(\\big(\\mathbf {\\mathbf {bc}}^H\\big)^H \\big( \\mathbf {\\mathbf {bc}}^H\\big)\\Big) \n",
    " \\geq \\big \\vert \\text{trace}\\Big( \\big(\\mathbf {bc}^H\\big)^2 \\Big)\\big \\vert =  \\big \\vert \\text{trace}\\Big( \\mathbf c^H \\mathbf {bc}^H\\mathbf {b}\\Big)\\big \\vert  = \\big \\vert \\mathbf c^H \\mathbf b \\big \\vert^2 = \\big \\vert \\mathbf b^H \\mathbf c \\big \\vert^2  = \\big \\vert \\langle \\mathbf x, \\mathbf y \\rangle \\big \\vert^2 $\n",
    "\n",
    "again by applying Schur's Inequality to the special case of a rank one matrix, we generate the above inequality, i.e. that $\\text{trace}\\Big(\\big(\\mathbf {\\mathbf {bc}}^H\\big)^H \\big( \\mathbf {\\mathbf {bc}}^H\\big)\\Big) \n",
    " \\geq \\big \\vert \\text{trace}\\Big( \\big(\\mathbf {bc}^H\\big)^2 \\Big)\\big \\vert $\n",
    "\n",
    "This extension confirms that Cauchy's Inequality still applies to (finite) inner products that have relaxed the positive definite criterion to positive semi-definite.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Traditional Cauchy Schwarz, over Reals:**  \n",
    "\n",
    "$\\big \\Vert \\mathbf x - \\mathbf y\\big \\Vert_2 \\geq 0$ \n",
    "\n",
    "by construction, the 2 norm add up real non-negative values and as a result is real non-negative\n",
    "\n",
    "$\\big \\Vert \\mathbf x - \\mathbf y\\big \\Vert_2^2 = \\big \\Vert \\mathbf x \\big \\Vert_2^2 - \\mathbf x^T \\mathbf y - \\mathbf y^T \\mathbf x + \\big \\Vert \\mathbf y\\big \\Vert_2^2 \\geq 0$ \n",
    "\n",
    "$\\big \\Vert \\mathbf x \\big \\Vert_2^2 + \\big \\Vert \\mathbf y\\big \\Vert_2^2 \\geq 2 \\mathbf y^T \\mathbf x $ \n",
    "\n",
    "$\\mathbf y^T \\mathbf x \\leq \\frac{1}{2}\\big(\\big \\Vert \\mathbf x \\big \\Vert_2^2 + \\big \\Vert \\mathbf y\\big \\Vert_2^2\\big) $ \n",
    "\n",
    "repeat argument and rescale by $\\alpha, \\gamma$ to get lengths of one for each vector on RHS (assuming each vector is non-zero -- if either vector is zero, the inequality follows by inspection)\n",
    "\n",
    "$\\big \\Vert \\alpha \\mathbf x - \\gamma \\mathbf y\\big \\Vert_2^2 = \\big \\Vert \\alpha^2  \\mathbf x \\big \\Vert_2^2 - \\mathbf \\alpha \\gamma  \\mathbf x^T \\mathbf y - \\gamma \\alpha \\mathbf y^T  \\mathbf x + \\big \\Vert \\mathbf \\gamma^2 \\mathbf y\\big \\Vert_2^2 \\geq 0$ \n",
    "\n",
    "$\\gamma \\alpha \\mathbf y^T \\mathbf x \\leq \\big \\vert \\gamma \\alpha \\mathbf y^T \\mathbf x\\big \\vert = \\gamma \\alpha \\big \\vert  \\mathbf y^T \\mathbf x\\big \\vert  \\leq \\big \\vert\\frac{1}{2}\\big(1 +1 \\big)\\big \\vert = \\frac{1}{2}\\big(1 +1 \\big) =1$ \n",
    "\n",
    "discover that $\\big \\Vert \\alpha^2  \\mathbf x \\big \\Vert_2^2 = \\alpha^2 \\big \\Vert   \\mathbf x \\big \\Vert_2^2 = 1 \\to \\alpha := \\frac{1}{\\big \\Vert   \\mathbf x \\big \\Vert_2}$\n",
    "\n",
    "and by nearly identical argument: \n",
    "\n",
    "$\\gamma := \\frac{1}{\\big \\Vert y\\big \\Vert_2}$\n",
    "\n",
    "$\\gamma \\alpha \\mathbf y^T \\mathbf x  \\leq \\frac{1}{2}\\big(1 +1 \\big) = 1\\to \\mathbf y^T \\mathbf x = \\frac{1}{\\gamma}\\frac{1}{\\alpha}  = \\big \\Vert \\mathbf x \\big \\Vert_2 \\big \\Vert \\mathbf y \\big \\Vert_2 $ \n",
    "\n",
    "some additional clever maneuvering is needed to accommodate complex numbers using this approach, however.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Another Cauchy Schwarz proof, using determinants, over complex scalar field** \n",
    "\n",
    "$\\mathbf Z := \\begin{bmatrix}\n",
    "\\mathbf x & \\mathbf y\n",
    "\\end{bmatrix}$\n",
    "\n",
    "where $\\mathbf Z$ is $n$ x $2$, for some natural number $n \\geq 2$\n",
    "\n",
    "consider the matrix \n",
    "\n",
    "$\\mathbf A := \\mathbf Z^H \\mathbf Z = \\begin{bmatrix}\n",
    "\\mathbf x^H \\mathbf x & \\mathbf x^H\\mathbf y \\\\ \n",
    "\\mathbf y^H\\mathbf x & \\mathbf y^H \\mathbf y\n",
    "\\end{bmatrix}$ \n",
    "\n",
    "*First: Examine the equality conditions:*  \n",
    "\n",
    "note that $\\text{rank}(\\mathbf A) = 2$ unless $\\mathbf x = \\eta \\mathbf y$ -- this immediately gives the required equality conditions -- i.e. if the two vectors aren't scalar multiples of each other then \n",
    "\n",
    "$\\text{det}\\big(\\mathbf A\\big) \\neq 0$, i.e. \n",
    "\n",
    "$\\big(\\mathbf x^H \\mathbf x\\big)\\big(\\mathbf y^H \\mathbf y\\big) -\\big( \\mathbf x^H\\mathbf y \\big) \\big(\\mathbf y^H\\mathbf x\\big) = \\big(\\mathbf x^H \\mathbf x\\big)\\big(\\mathbf y^H \\mathbf y\\big) -\\big( \\mathbf y^H\\mathbf x \\big)^H \\big(\\mathbf y^H\\mathbf x\\big) = \\big \\Vert \\mathbf x \\big \\Vert_2^2 \\big \\Vert \\mathbf y\\big \\Vert_2^2 - \\big \\vert \\mathbf y^H\\mathbf x \\big \\vert^2  \\neq 0 $\n",
    "\n",
    "which can be rewritten as  \n",
    "\n",
    "$\\big \\Vert \\mathbf x \\big \\Vert_2^2 \\big \\Vert \\mathbf y\\big \\Vert_2^2  \\neq \\big \\vert \\mathbf y^H\\mathbf x \\big \\vert^2 = \\big \\vert \\mathbf x^H\\mathbf y \\big \\vert^2 $\n",
    "\n",
    "hence we have an equality if and only if $\\mathbf Z$ has linearly dependent columns aka $\\mathbf A$ is singular, aka when $\\text{det}\\big(\\mathbf A\\big)=0$.  \n",
    "\n",
    "\n",
    "*Second: The strict inequality*  \n",
    "for convenience, we focus on the strict inequality (i.e. non-singular $\\mathbf A$ aka 2 linearly independent columns in $\\mathbf Z$) in what follows:   \n",
    "\n",
    "We are in an inner product space, but can't yet take for granted the triangle inequality (as it implies Cauchy Schwarz).  We can verify by inspection / take for granted the positive definiteness of the 2 norm, i.e. for any $\\mathbf b$ \n",
    "\n",
    "$\\big \\Vert \\mathbf b\\big \\Vert_2^2 = \\mathbf b^H \\mathbf b \\geq 0$ with equality **iff** $\\mathbf b = \\mathbf 0$.  \n",
    "\n",
    "we equivalently confirm that that \n",
    "\n",
    "$\\mathbf c^H \\mathbf A \\mathbf c \\gt 0$ for any $\\mathbf c \\neq \\mathbf 0$\n",
    "\n",
    "- - - -\n",
    "because \n",
    "\n",
    "first for any $\\mathbf c \\neq \\mathbf 0$:  \n",
    "\n",
    "$\\mathbf b:= \\mathbf Z\\mathbf c \\neq \\mathbf 0$\n",
    "\n",
    "Note: given that $\\mathbf Z$ has 2 linearly independent columns, $\\mathbf b = \\mathbf 0$ **iff** $\\mathbf c = \\mathbf 0$\n",
    "\n",
    "second: repeating the argument, using positive definiteness:  \n",
    "\n",
    "$\\mathbf c^H \\mathbf A \\mathbf c = \\mathbf c^H \\mathbf Z^H \\mathbf Z \\mathbf c =  \\big(\\mathbf c^H \\mathbf Z^H\\big) \\big(\\mathbf Z \\mathbf c\\big) =  \\big(\\mathbf {Zc}\\big)^H \\big(\\mathbf Z \\mathbf c\\big)= \\mathbf b^H \\mathbf b = \\big \\Vert \\mathbf b\\big\\Vert_2^2 \\gt 0$ for any $\\mathbf c \\neq \\mathbf 0$\n",
    "\n",
    "note that the above tells us that the quadratic form given by \n",
    "\n",
    "$\\mathbf c^H \\mathbf A \\mathbf c $ is positive for *all* $\\mathbf c \\neq \\mathbf 0$, which means several things, including that the quadratic form is always real valued.  \n",
    "- - - - \n",
    "*why eigenvalues must be positive*  \n",
    "\n",
    "We know that $\\mathbf A$ has eigenvalues given by solving $\\text{det}\\big( \\lambda \\mathbf I-\\mathbf A \\big) = 0$.  Note: we don't need fundamental theorem of algebra or anything particularly sophisticated to know this -- we merely need to know how to use the quadratic formula.  \n",
    "\n",
    "\n",
    "Specifically we know that for each eigen pair $\\lambda_k, \\mathbf v_k$, we have \n",
    "\n",
    "$\\mathbf v_k^H \\mathbf{A v}_k = \\mathbf v_k^H \\big(\\mathbf{A v}_k\\big)  = \\mathbf v_k^H\\big(\\lambda_k  \\mathbf v_k\\big) = \\lambda_k \\mathbf v_k^H \\mathbf v_k = \\lambda_k \\big \\Vert \\mathbf v_k \\big \\Vert_2^2 \\gt 0$ \n",
    "\n",
    "dividing both sides by $\\big \\Vert \\mathbf v_k \\big \\Vert_2^2$ which is positive, since we know that $\\mathbf v_k \\neq \\mathbf 0$ and the positive definiteness of the 2 norm, giving us \n",
    "\n",
    "$\\lambda_k \\gt 0$, i.e. each eigenvalue must be real valued and positive.  \n",
    "- - - -\n",
    "Thus when $rank\\big(\\mathbf A\\big) =2$   \n",
    "\n",
    "$\\text{det}\\big(\\mathbf A\\big) = \\lambda_1 \\lambda_2 \\gt 0$\n",
    "\n",
    "$\\text{det}\\big(\\mathbf A\\big) =\\big(\\mathbf x^H \\mathbf x\\big)\\big(\\mathbf y^H \\mathbf y\\big) -\\big( \\mathbf x^H\\mathbf y \\big) \\big(\\mathbf y^H\\mathbf x\\big) = \\big \\Vert \\mathbf x \\big \\Vert_2^2 \\big \\Vert \\mathbf y\\big \\Vert_2^2 - \\big \\vert \\mathbf x^H\\mathbf y \\big \\vert^2   \\gt 0$\n",
    "\n",
    "which gives us the strict inequality\n",
    "\n",
    "$\\big \\Vert \\mathbf x \\big \\Vert_2^2 \\big \\Vert \\mathbf y\\big \\Vert_2^2 \\gt \\big \\vert \\mathbf x^H\\mathbf y \\big \\vert^2$ \n",
    "\n",
    "when there is no $\\eta$ such that $\\mathbf x = \\eta \\mathbf y$ \n",
    "\n",
    "and putting everything together:  \n",
    "\n",
    "$\\big \\Vert \\mathbf x \\big \\Vert_2^2 \\big \\Vert \\mathbf y\\big \\Vert_2^2 \\geq \\big \\vert \\mathbf x^H\\mathbf y \\big \\vert^2$ \n",
    "\n",
    "or taking advantage of positivity we can square root both sides:  \n",
    "\n",
    "$\\big \\Vert \\mathbf x \\big \\Vert_2 \\big \\Vert \\mathbf y\\big \\Vert_2 \\geq \\big \\vert \\mathbf x^H\\mathbf y \\big \\vert$  \n",
    "\n",
    "with equality **iff**  there is some $\\eta$ such that $\\mathbf x = \\eta \\mathbf y$  \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A better approach to Triangle Inequality:**   \n",
    "a technique discussed in *Cauchy-Schwarz Masterclass* (which easily generalizes to Minkowski Triangle Inequality if we replace the below with H&ouml;lder's Inequality instead of Cauchy-Schwarz)  \n",
    "\n",
    "*define* \n",
    "the 2 norm of some vector $\\mathbf z \\in \\mathbb R^n$ as   \n",
    "$\\big \\Vert \\mathbf z \\big \\Vert_2 =  \\langle \\mathbf w^*, \\mathbf z\\rangle$  \n",
    "where $\\mathbf w^*$ is the vector that maximize the above inner product (given the below constraint)  \n",
    "subject to $\\big(\\sum_{i=1}^n \\big \\vert w_i^*\\big \\vert^2\\big)^\\frac{1}{2} = 1$   \n",
    "(Cauchy-Schwarz tells us the supremum is attainable and that $\\mathbf w^* \\propto \\mathbf z$ for this maximization)\n",
    "\n",
    "now consider the case of \n",
    "$\\mathbf z := \\mathbf x + \\mathbf y$  \n",
    "\n",
    "$\\big \\Vert \\mathbf z \\big \\Vert_2 $  \n",
    "$=\\langle \\mathbf w_0^*, \\mathbf z\\rangle$  \n",
    "$=\\langle \\mathbf w_0^*, \\mathbf x + \\mathbf y\\rangle$  \n",
    "$= \\langle \\mathbf w_0^*, \\mathbf x \\rangle + \\langle \\mathbf w_0^*, \\mathbf y \\rangle$  \n",
    "$\\leq \\langle \\mathbf w_1^*, \\mathbf x \\rangle + \\langle \\mathbf w_2^*, \\mathbf y \\rangle $  \n",
    "$=\\big \\Vert \\mathbf x \\big \\Vert_2 + \\big \\Vert \\mathbf y \\big \\Vert_2$   \n",
    "\n",
    "because 2 choices of maximization are at least as good as one  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This self generalizes to the case of the 2 norm of some vector $\\mathbf z \\in \\mathbb C^n$ as   \n",
    "$\\big \\Vert \\mathbf z \\big \\Vert_2 :=  \\big \\vert \\langle \\mathbf w^*, \\mathbf z\\rangle \\big \\vert$  \n",
    "where $\\mathbf w^*$ is the vector that maximize the magnitude of the above inner product (given the below constraint)  \n",
    "subject to $\\big(\\sum_{i=1}^n \\big \\vert w_i^*\\big \\vert^2\\big)^\\frac{1}{2} = 1$   \n",
    "(Cauchy-Schwarz tells us that $\\mathbf w^* \\propto \\mathbf z$ for this maximization)\n",
    "\n",
    "now consider the case of \n",
    "$\\mathbf z := \\mathbf x + \\mathbf y$  \n",
    "\n",
    "$\\big \\Vert \\mathbf z \\big \\Vert_2 $  \n",
    "$=\\big \\vert \\langle \\mathbf w_0^*, \\mathbf z\\rangle \\big \\vert$  \n",
    "$=\\big \\vert \\langle \\mathbf w_0^*, \\mathbf x + \\mathbf y\\rangle \\big \\vert$  \n",
    "$= \\big \\vert \\langle \\mathbf w_0^*, \\mathbf x \\rangle + \\langle \\mathbf w_0^*, \\mathbf y \\rangle \\big \\vert$  \n",
    "$\\leq \\big \\vert \\langle \\mathbf w_0^*, \\mathbf x \\rangle\\big \\vert  + \\big \\vert \\langle \\mathbf w_0^*, \\mathbf y \\rangle \\big \\vert $  \n",
    "$\\leq \\big \\vert \\langle \\mathbf w_1^*, \\mathbf x \\rangle\\big \\vert  + \\big \\vert \\langle \\mathbf w_2^*, \\mathbf y \\rangle \\big \\vert $  \n",
    "$=\\big \\Vert \\mathbf x \\big \\Vert_2 + \\big \\Vert \\mathbf y \\big \\Vert_2$   \n",
    "\n",
    "by(i) the triangle inequality applied to the addition of two complex scalars and (ii) because 2 choices of maximization are at least as good as one and     \n",
    "\n",
    "For avoidance of doubt, (i) is implied by the previous result on real triangle inequality.  That is, for purposes of addition, we can treat any complex number as a vector space in $\\mathbb R^2$  and using the standard basis vectors observe that \n",
    "\n",
    "$\\big \\vert (a^{(0)} + b^{(0)}i) + (a^{(1)} + b^{(1)}i)\\big \\vert $  \n",
    "$=  \\big \\Vert \\big(a^{(0)}\\mathbf e_1 + b^{(0)} \\mathbf e_2\\big) + \\big(a^{(1)}\\mathbf e_1 + b^{(1)}\\mathbf e_2\\big)\\big \\Vert_2$  \n",
    "$=  \\big \\Vert \\mathbf a^{(0)} + \\mathbf a^{(1)}\\big \\Vert_2$  \n",
    "$\\leq  \\big \\Vert \\mathbf a^{(0)}\\big \\Vert_2 + \\big \\Vert \\mathbf a^{(1)}\\big \\Vert_2$  \n",
    "$= \\big \\vert a^{(0)} + b^{(0)}i \\big \\vert + \\big \\vert a^{(1)} + b^{(1)}i\\big \\vert $   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alternative Approach :  \n",
    "\n",
    "note that instead of using the above methods,  consider the maximum eigenvalue problem, where we have a rank one matrix, $\\mathbf B$.\n",
    "\n",
    "\n",
    "$\\mathbf B = \\mathbf{xy}^H$.  While we know that $\\mathbf B$ is a rank one matrix, the argument to be made is even more general: the magnitude of the largest eigenvalue of $\\big(\\mathbf {BB}\\big)$ is $\\leq$ the largest eigenvalue of $\\mathbf B^H \\mathbf B$, or equivalently, the magnitude of the largest eigenvalue of $\\mathbf B$ ($\\lambda_1$) is $\\leq$ the largest singular value of $\\mathbf B$ ($\\sigma_1$).\n",
    "\n",
    "The approach taken here uses quadratic forms.  So consider the case of maximizing $\\mathbf B^H \\mathbf B$ vs $\\mathbf {BB}$\n",
    "\n",
    "max $\\big \\vert \\mathbf v^H \\mathbf B^H \\mathbf B \\mathbf v \\big \\vert \\geq$ max $\\big \\vert \\mathbf v^H \\mathbf{BB}\\mathbf v \\big \\vert$\n",
    "\n",
    "where we constrain the length (2 norm) of $\\mathbf v$, which for simplicity will be one: $\\mathbf v^H \\mathbf v = 1 = \\big \\vert \\big \\vert \\mathbf v \\big \\vert \\big \\vert_2^{2} = \\big \\vert \\big \\vert \\mathbf v \\big \\vert \\big \\vert_2$ \n",
    "\n",
    "We know via diagonalization arguments (and Lagrange Multipliers), that some quadratic form $\\big \\vert \\mathbf v^H \\mathbf C \\mathbf v\\big \\vert$ subject to $\\mathbf v ^H \\mathbf v = 1$ is maximized when all of $\\mathbf v$ is allocated to the eigenvalue(s) with the largest magnitude of the Hermitian matrix $\\mathbf C$.  Unfortunately, we have no reason to believe $\\mathbf {BB}$ is Hermitian or even non-defective, which complicates things a bit.  But consider having well ordered eigenvalues for $\\mathbf B$ where $\\big \\vert \\lambda_1 \\big \\vert \\geq \\big \\vert\\lambda_2 \\big \\vert\\geq \\big \\vert \\lambda_3 \\big \\vert \\geq ... \\geq \\big \\vert\\lambda_n \\big \\vert$, with associated eigenvectors $\\mathbf v_1, \\mathbf v_2, \\mathbf v_3, ... , \\mathbf v_n$.  Note that there is a simple argument which tells us that allocating to eigenvector $\\mathbf v_k$ where $k \\geq 2$ is (weakly) dominated by $\\mathbf v_1$.  \n",
    "\n",
    "$\\big \\vert \\mathbf v_1^H \\mathbf{BB}\\mathbf v_1 \\big \\vert \\geq \\big \\vert \\mathbf v_k^H \\mathbf{BB}\\mathbf v_k \\big \\vert$\n",
    "\n",
    "$\\big \\vert \\mathbf v_1^H \\mathbf{B}\\big(\\mathbf B \\mathbf v_1\\big) \\big \\vert \\geq \\big \\vert \\mathbf v_k^H \\mathbf{B}\\big( \\mathbf B \\mathbf v_k \\big) \\big \\vert$\n",
    "\n",
    "$\\big \\vert \\mathbf v_1^H \\mathbf{B} \\lambda_1 \\mathbf v_1 \\big \\vert \\geq \\big \\vert\\mathbf v_k^H \\mathbf{B}\\lambda_k \\mathbf v_k \\big \\vert$\n",
    "\n",
    "$\\big \\vert \\lambda_1 \\mathbf v_1^H \\big(\\mathbf{B} \\mathbf v_1\\big) \\big \\vert \\geq \\big \\vert \\lambda_k \\mathbf v_k^H \\big(\\mathbf{B} \\mathbf v_k\\big) \\big \\vert$\n",
    "\n",
    "$\\big \\vert \\lambda_1 \\mathbf v_1^H \\lambda_1 \\mathbf v_1 \\big \\vert \\geq \\big \\vert \\lambda_k \\mathbf v_k^H \\lambda_k \\mathbf v_k \\big \\vert$\n",
    "\n",
    "$\\big \\vert \\lambda_1^2 (\\mathbf v_1^H \\mathbf v_1) \\big \\vert \\geq \\big \\vert \\lambda_k^2 (\\mathbf v_k^H \\mathbf v_k) \\big \\vert$\n",
    "\n",
    "$\\big \\vert \\lambda_1^2 \\cdot1\\big \\vert \\geq \\big \\vert \\lambda_k^2 \\cdot 1 \\big \\vert$  \n",
    "$\\big \\vert \\lambda_1^2\\big \\vert \\geq \\big \\vert\\lambda_k^2 \\big \\vert$  \n",
    "$\\big \\vert \\lambda_1\\big \\vert^2 \\geq \\big \\vert\\lambda_k \\big \\vert^2$  \n",
    "$\\big \\vert \\lambda_1\\big \\vert \\geq \\big \\vert\\lambda_k \\big \\vert$   \n",
    "\n",
    "hence we have a simple exchange argument that tells us any time we allocate to $\\mathbf v_k$ we can get a result greater than or equal to it, by allocating that amount instead to $\\mathbf v_1$.  Thus, with respect to a maximization problem using the eigenvectors of $\\mathbf B$, we can do no better than choosing the eigenpair $\\lambda_1, \\mathbf v_1$.\n",
    "\n",
    "We return to our original equation, with respect to eigenvalues:\n",
    "\n",
    "max $\\big \\vert  \\mathbf v^H \\mathbf B^H \\mathbf B \\mathbf v \\big \\vert \\geq$ max $\\big \\vert \\mathbf v_1^H \\mathbf{BB}\\mathbf v_1 \\big \\vert$\n",
    "\n",
    "max $\\big \\vert \\mathbf v^H \\mathbf B^H \\mathbf B \\mathbf v \\big \\vert = $ max $ \\mathbf v^H \\mathbf B^H \\mathbf B \\mathbf v  \\geq \\big \\vert \\lambda_1^2\\big \\vert $\n",
    "\n",
    "note that we always have the option / backup plan, on the left hand side, of also allocating to $\\mathbf v_1$.  Put differently, if we are lazy, we know that by setting $\\mathbf v := \\mathbf v_1$ we'll always get a 'payoff' with magnitude equal to $\\big\\vert\\lambda_1\\big\\vert^2$ -- thus when maximizing the magnitude of $\\big \\vert \\mathbf v^H \\mathbf B^H \\mathbf B \\mathbf v \\big \\vert$ we'll get a result at least as good as $\\big\\vert\\lambda_1\\big\\vert^2$.  Symbolically, this is shown below.\n",
    "\n",
    "$\\mathbf v_1^H \\mathbf B^H \\mathbf B \\mathbf v_1 = \\big \\vert \\lambda_1\\big \\vert^2 $  \n",
    "\n",
    "$\\big(\\mathbf v_1^H \\mathbf B^H\\big) \\big(\\mathbf B \\mathbf v_1 \\big) = \\big \\vert \\lambda_1\\big \\vert^2 $  \n",
    "\n",
    "$\\big(\\mathbf {B}\\mathbf v_1\\big)^H \\big(\\mathbf B \\mathbf v_1 \\big) = \\big \\vert \\lambda_1 \\big \\vert^2 $  \n",
    "$\\big(\\lambda_1 \\mathbf v_1\\big)^H \\big(\\lambda_1 \\mathbf v_1 \\big) = \\big \\vert \\lambda_1 \\big \\vert^2 $  \n",
    "$\\lambda_1^H \\lambda_1 \\big(\\mathbf v_1^H \\mathbf v_1\\big) = \\big \\vert \\lambda_1 \\big \\vert^2 $  \n",
    "$\\lambda_1^H \\lambda_1 \\cdot 1  = \\big \\vert \\lambda_1\\big \\vert^2 $    \n",
    "$\\lambda_1^H \\lambda_1  = \\big \\vert \\lambda_1 \\big \\vert^2 $  \n",
    "\n",
    "\n",
    "- - - - \n",
    "*begin detour: reminder about complex number maths*   \n",
    "  \n",
    "Consider that we can simply note that $\\big \\vert \\lambda_1^2\\big \\vert = \\big \\vert \\lambda_1 \\big \\vert^2 = \\lambda_1^H \\lambda_1$.\n",
    "\n",
    "Alternatively, for a more granular view, consider the case where:  \n",
    "$\\lambda_1 = \\alpha - \\beta i $, where $\\alpha$ and $\\beta$ are real valued scalars. Accordingly, the magnitude of $\\lambda_1$ is $\\big\\vert \\lambda_1\\big\\vert = \\big(\\alpha^2 + \\beta^2\\big)^\\frac{1}{2}$. Then $\\lambda_1^2 = \\alpha^2 + \\beta^2  i^2 - 2\\alpha\\beta i = \\alpha^2 - \\beta^2 - 2\\alpha\\beta i$, with magnitude of \n",
    "\n",
    "$\\big \\vert \\lambda_1^2\\big \\vert = \\Big(\\big(\\alpha^2 - \\beta^2\\big)^2 + \\big( - 2\\alpha\\beta\\big)^2\\Big)^{\\frac{1}{2}}= \\Big(\\alpha^4 + \\beta^4 - 2 \\alpha^2 \\beta^2 + 4 \\alpha^2\\beta^2)\\Big)^{\\frac{1}{2}} $ \n",
    "\n",
    "$\\big \\vert \\lambda_1^2\\big \\vert = \\Big(\\alpha^4 + \\beta^4 + 2 \\alpha^2 \\beta^2 \\Big)^{\\frac{1}{2}}$\n",
    "\n",
    "and note that $\\lambda_1 ^H \\lambda_1 = \\alpha^2 + \\beta^2$, with magnitude equal to   \n",
    "\n",
    "$\\big \\vert \\lambda_1^H \\lambda_1 \\big \\vert = \\Big(\\big(\\alpha^2 + \\beta^2\\big)^2 + \\big(0\\big)^2 \\Big)^{\\frac{1}{2}} = \\Big(\\alpha^4 + \\beta^4 + 2 \\alpha \\beta \\Big)^{\\frac{1}{2}} = \\big \\vert \\lambda_1^2\\big \\vert  $\n",
    "\n",
    "$ \\big \\vert \\lambda_1^H \\lambda_1 \\big \\vert = \\lambda_1 ^H \\lambda_1 = \\alpha^2 + \\beta^2  = \\Big(\\big(\\alpha^2 + \\beta^2\\big)^\\frac{1}{2}\\Big)^2 = \\big\\vert \\lambda_1\\big\\vert ^2$  \n",
    "  \n",
    "*end detour* \n",
    "- - - - \n",
    "\n",
    "Thus when trying to maximize the magnitude of $\\mathbf v^H \\mathbf B^H \\mathbf B \\mathbf v$, we have a lower bound equal to $ \\big \\vert \\lambda_1 \\big\\vert^2$.  Equivalently, we can say: $\\sigma_1^2 \\geq \\big \\vert \\lambda_1 \\big\\vert^2$ and $\\sigma_1 \\geq \\big \\vert \\lambda_1 \\big\\vert$, where $\\sigma_1$ is the largest singular value of $\\mathbf B$ and thus $\\sigma_1^2$ is the largest eigenvalue of $\\big(\\mathbf B^H \\mathbf B\\big)$\n",
    "\n",
    "For a simple example of this fact, consider:\n",
    "\n",
    "$\\mathbf B = \\left[\\begin{matrix}2 & 3 & 4\\\\4 & 10 & -1\\\\1 & 3 & 4\\end{matrix}\\right]$\n",
    "\n",
    "where $\\lambda_1 \\approx 11.57$, but $\\sigma_1 \\approx 11.87$, hence $\\sigma_1$ exceeds the lower bound set by $\\lambda_1$ \n",
    "- - - -\n",
    "Back to the original problem at hand, in the special case where $\\mathbf B$ is a square, rank one matrix  \n",
    "\n",
    "$\\text{trace}\\big(\\mathbf B ^H \\mathbf B\\big) \\geq \\big \\vert \\text{trace}\\big(\\mathbf{BB}\\big) \\big \\vert$, because  \n",
    "\n",
    "$\\text{trace}\\big(\\mathbf B ^H \\mathbf B\\big) = \\sigma_1^2$ and $\\text{trace}\\big(\\mathbf{BB}\\big) = \\lambda_1^2 $, and we know $\\sigma_1^2 \\geq \\big \\vert \\lambda_1 \\big\\vert^2 = \\big \\vert \\lambda_1^2 \\big\\vert $\n",
    "\n",
    "set: $\\mathbf B:= \\mathbf{xy}^H$ and Cauchy Schwartz simply follows\n",
    "\n",
    "$ \\big\\vert\\big\\vert \\mathbf{y}\\big\\vert\\big\\vert_{2}^2\\big\\vert\\big\\vert \\mathbf {x}\\big\\vert\\big\\vert_{2}^2 = \\big ( \\mathbf y^H \\mathbf y \\big ) \\big ( \\mathbf x^H \\mathbf x \\big ) = \\text{trace}\\Big(\\big(\\mathbf {\\mathbf {xy}}^H\\big)^H \\big( \\mathbf {\\mathbf {xy}}^H\\big)\\Big) \n",
    "\\geq \\big \\vert \\text{trace}\\Big( \\big(\\mathbf {xy}^H\\big) \\big(\\mathbf {xy}^H\\big)\\Big) \\big \\vert =  \\big \\vert \\text{trace}\\Big( \\mathbf y^H \\mathbf {xy}^H\\mathbf {x}\\Big) \\big \\vert = \\big \\vert \\mathbf y^H \\mathbf x \\big \\vert^2 $\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Smallest Eigenvalues and Singular Values\n",
    "\n",
    "Note that the above analysis can be easily extended with respect to the magnitude of the smallest eigenvalue of $\\mathbf B$ and the smallest singular value of $\\mathbf B$, again where $\\mathbf B \\in \\mathbb C^{n x n}$.\n",
    "\n",
    "if we want to minimize $\\big(\\mathbf v_n^H \\mathbf B^H\\big) \\big(\\mathbf B \\mathbf v_n \\big) $ we always have the option of allocating to $\\lambda_n$\n",
    "\n",
    "recall our length constraint:  $\\big \\vert \\big \\vert \\mathbf v \\big \\vert \\big \\vert_2^{2} = 1$ \n",
    "\n",
    "\n",
    "$\\mathbf v_n^H \\mathbf B^H \\mathbf B \\mathbf v_n = \\big \\vert \\lambda_n^2\\big \\vert $  \n",
    "\n",
    "$\\big(\\mathbf v_n^H \\mathbf B^H\\big) \\big(\\mathbf B \\mathbf v_n \\big) = \\big \\vert \\lambda_n^2\\big \\vert $  \n",
    "\n",
    "$\\big(\\mathbf {B}\\mathbf v_n\\big)^H \\big(\\mathbf B \\mathbf v_n \\big) = \\big \\vert \\lambda_n^2 \\big \\vert $  \n",
    "$\\big(\\lambda_n \\mathbf v_n\\big)^H \\big(\\lambda_n \\mathbf v_1 \\big) = \\big \\vert \\lambda_n^2 \\big \\vert $  \n",
    "$\\lambda_n^H \\lambda_n \\big(\\mathbf v_1^H \\mathbf v_1\\big) = \\big \\vert \\lambda_n^2 \\big \\vert $  \n",
    "$\\lambda_n^H \\lambda_n \\cdot 1  = \\big \\vert \\lambda_n^2\\big \\vert $    \n",
    "$\\lambda_n^H \\lambda_n  = \\big \\vert \\lambda_n \\big \\vert^2 $  \n",
    "\n",
    "Since allocating everything to $\\sigma_n^2$ is the (weakly) dominant solution for minimizing $\\mathbf v^H \\mathbf B^H \\mathbf B \\mathbf v$, we can upper bound $\\sigma_n^2 \\leq \\big\\vert \\lambda_n\\big \\vert^2 $ and equivalently say that $\\sigma_n \\leq \\big\\vert \\lambda_n\\big \\vert $\n",
    "\n",
    "\n",
    "# Quadratic Forms and recovering a Frobenius Norm\n",
    "\n",
    "as usual, assume we have well ordered singular values\n",
    "\n",
    "$\\sigma_1 \\geq \\sigma_2 \\geq .... \\geq \\sigma_n \\geq 0$\n",
    "\n",
    "It is worth remarking that if we were to do our optimization problem\n",
    "\n",
    "max $\\mathbf v_1^H \\mathbf B^H \\mathbf B \\mathbf v_1$\n",
    "\n",
    "we recover $\\sigma_1^2$.  Then if we continue doing this optimization problem for $k = \\{2, 3, 4, ... , n\\}$\n",
    "\n",
    "max $\\mathbf v_k^H \\mathbf B^H \\mathbf B \\mathbf v_k$\n",
    "\n",
    "where each $\\big \\Vert \\mathbf v_k\\big \\Vert_2^2 = 1$, **with the added constraint that** $\\mathbf v_k \\perp \\mathbf v_j$, for $j = \\{1, 2, ... , k-1\\}$\n",
    "\n",
    "i.e. each $\\mathbf v_k$ is mutually orthonormal to the $\\mathbf v$'s that come before it,\n",
    "\n",
    "Then we recover \n",
    "\n",
    "$\\mathbf V = \n",
    "\\bigg[\\begin{array}{c|c|c|c}\n",
    "\\mathbf v_1 & \\mathbf v_2 &\\cdots & \\mathbf v_{n}\n",
    "\\end{array}\\bigg]\n",
    "$   \n",
    "\n",
    "where $\\mathbf V$ is a unitary matrix (or in reals, orthogonal).  Put differently, we recover a coordinate system.  \n",
    "\n",
    "And more to the point, we also can collect the 'payoffs': $\\{ \\sigma_1^2, \\sigma_2^2, \\sigma_3^2, ..., \\sigma_n^2 \\}$. from this maximization process.  We could sum up all of these squared singular values, and get\n",
    "\n",
    "$\\sigma_1^2 + \\sigma_2^2+ \\sigma_3^2+ ...+\\sigma_n^2 = \\big \\Vert \\mathbf B \\big \\Vert_F^2$\n",
    "\n",
    "i.e. when we sum up all of these 'payoffs' from our complete quadratic form process, we get the squared Frobenius norm for our matrix $\\mathbf B$. \n",
    "\n",
    "\n",
    "# On Unitary Matrices\n",
    "Note that there is a special case of interest.  Suppose that we have a square unitary matrix $\\mathbf Q$. We know that all eigenvalues of $\\mathbf Q $ have magnitude of 1.  Why? There are multiple approaches, but an elegant one uses the above knowledge with the singular value decomposition:\n",
    "\n",
    "\n",
    "$\\mathbf Q = \\mathbf{U \\Sigma V}^H$\n",
    "\n",
    "$\\mathbf Q^H \\mathbf Q  = \\mathbf I = \\big(\\mathbf{U \\Sigma V}^H\\big)^H \\mathbf{U \\Sigma V}^H  =\\mathbf V \\mathbf\\Sigma^2  \\mathbf V^H$\n",
    "\n",
    "left multiply by $\\mathbf V^H$ and right multiply by $\\mathbf V$, recalling\n",
    "that $\\mathbf V$ is a square, full rank matrix\n",
    "\n",
    "$\\mathbf V^H \\mathbf I \\mathbf V = \\mathbf I = \\mathbf V^H \\big(\\mathbf V \\mathbf\\Sigma^2  \\mathbf V^H\\big) \\mathbf V =\\mathbf \\Sigma^2$\n",
    "\n",
    "$\\mathbf I = \\mathbf \\Sigma^2$\n",
    "\n",
    "recalling that each singular value, by construction, is real and non-negative, we can then determine:\n",
    "\n",
    "$\\mathbf I = \\mathbf \\Sigma$  \n",
    "\n",
    "Thus we know that $\\mathbf \\Sigma$ is itself the Identity matrix (i.e. $\\sigma_1 = \\sigma_2 = ... = \\sigma_n = 1$)\n",
    "\n",
    "When we consider the eigenvalues of $\\mathbf Q$, where $\\big \\vert \\lambda_1 \\big \\vert \\geq \\big \\vert\\lambda_2 \\big \\vert\\geq \\big \\vert \\lambda_3 \\big \\vert \\geq ... \\geq \\big \\vert\\lambda_n \\big \\vert$, we know that $1 = \\sigma_1 \\geq \\big \\vert \\lambda_1 \\big \\vert$ and we know that $\\big \\vert \\lambda_n \\big \\vert \\geq \\sigma_n = 1$.  Every item in our sequence of eigenvalue magnitudes is bounded above and below by one.  Thus all eigenvalues of a unitary (or in Reals, orthogonal) matrix must have magnitude equal to one.\n",
    "\n",
    "$1 = \\sigma_1 \\geq \\big \\vert \\lambda_1 \\big \\vert \\geq \\big \\vert\\lambda_2 \\big \\vert\\geq \\big \\vert \\lambda_3 \\big \\vert \\geq ... \\geq \\big \\vert\\lambda_n \\big \\vert \\geq \\sigma_n = 1$\n",
    "\n",
    "can be re-written as\n",
    "\n",
    "$1 = \\sigma_1 = \\big \\vert \\lambda_1 \\big \\vert = \\big \\vert\\lambda_2 \\big \\vert = \\big \\vert \\lambda_3 \\big \\vert = ... = \\big \\vert\\lambda_n \\big \\vert =\\sigma_n = 1$\n",
    "\n",
    "\n",
    "- - - - \n",
    "\n",
    "# On Nilpotent Matrices\n",
    "\n",
    "The final sequences of inequalities for some arbitrary square matrix:\n",
    "\n",
    "$\\sigma_1 \\geq \\big \\vert \\lambda_1 \\big \\vert \\geq \\big \\vert\\lambda_2 \\big \\vert\\geq \\big \\vert \\lambda_3 \\big \\vert \\geq ... \\geq \\big \\vert\\lambda_n \\big \\vert \\geq \\sigma_n $\n",
    "\n",
    "is quite useful.  \n",
    "\n",
    "A nilpotent matrix $\\mathbf A$ is some $n$ x $n$ matrix where are after finite number of iterations, it becomes the zero matrix.  Thus $\\mathbf A^r = \\mathbf 0$ for some finite, natural number $r$.  (We can tighten the bound and say $r \\leq n$, but this is not really needed here.) \n",
    "\n",
    "**Claim:** a nilpotent matrix has all eigenvalues equal to zero.\n",
    "\n",
    "There are numerous ways to prove this.  The most slick uses the analysis earlier in this post and does the following:\n",
    "\n",
    "**Proof:**  \n",
    "$\\mathbf A$ has eigenvalues of $\\big \\vert \\lambda_1 \\big \\vert \\geq \\big \\vert\\lambda_2 \\big \\vert\\geq \\big \\vert \\lambda_3 \\big \\vert \\geq ... \\geq \\big \\vert\\lambda_n \\big \\vert $\n",
    "\n",
    "and \n",
    "\n",
    "$\\big(\\mathbf A^r\\big)$ has eigenvalues of $\\big \\vert \\lambda_1^r \\big \\vert \\geq \\big \\vert\\lambda_2^r \\big \\vert\\geq \\big \\vert \\lambda_3^r \\big \\vert \\geq ... \\geq \\big \\vert\\lambda_n^r \\big \\vert$\n",
    "\n",
    "or equivalently \n",
    "\n",
    "$\\big(\\mathbf A^r\\big)$ has eigenvalues of $\\big \\vert \\lambda_1 \\big \\vert^r \\geq \\big \\vert\\lambda_2 \\big \\vert^r \\geq \\big \\vert \\lambda_3 \\big \\vert^r \\geq ... \\geq \\big \\vert\\lambda_n \\big \\vert^r $\n",
    "\n",
    "We can do SVD on $\\big(\\mathbf A^r\\big)$ and see  \n",
    "$\\big(\\mathbf A^r\\big) = \\mathbf U \\mathbf \\Sigma \\mathbf V^H = \\mathbf 0$, where $\\mathbf U$ and $\\mathbf V$ are full rank unitary matrices (or orthogonal if dealing with Reals)  \n",
    "\n",
    "Hence:\n",
    "\n",
    "$ \\mathbf \\Sigma = \\mathbf U^H\\big(\\mathbf A^r\\big)\\mathbf V = \\mathbf U^H \\big(\\mathbf 0\\big)\\mathbf V = \\mathbf 0 $\n",
    "\n",
    "That is, all singular values of $\\big(\\mathbf A^r\\big)$  are equal to zero \n",
    "\n",
    "Thus we know that for $ \\big(\\mathbf A^r\\big)$  \n",
    "\n",
    "$0 = \\sigma_1 \\geq \\big \\vert \\lambda_1^r \\big \\vert \\geq \\big \\vert\\lambda_2^r \\big \\vert\\geq \\big \\vert \\lambda_3^r \\big \\vert \\geq ... \\geq \\big \\vert\\lambda_n^r \\big \\vert \\geq \\sigma_n = 0$\n",
    "\n",
    "Since all eigenvalue magnitudes are bounded above and below by zero, we restate this as  \n",
    "$0 = \\lambda_1^r  = \\lambda_2^r = \\lambda_3^r = ... = \\lambda_n^r  = 0$\n",
    "\n",
    "take the $r$th root and we see that all eigenvalues of the nilpotent matrix $\\mathbf A$ must be zero\n",
    "\n",
    "$0 = \\lambda_1  = \\lambda_2 =  \\lambda_3 = ... = \\lambda_n  = 0$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix Norms \n",
    "\n",
    "(this is just a warmup...) \n",
    "\n",
    "consider two n x n matrices $\\mathbf A$ and $\\mathbf B$.  \n",
    "\n",
    "now consider $\\big \\Vert \\mathbf {AB} \\big \\Vert_2 = \\sigma_1$, i.e. the operator norm for $\\big(\\mathbf {AB}\\big)$.  \n",
    "\n",
    "As always we order singular values as $\\sigma_1 \\geq \\sigma_2 \\geq ... \\geq \\sigma_n \\geq 0$.  And again we consider any vectors $\\mathbf x$, $\\mathbf y$, $\\mathbf z$ with the constraint that $\\big \\Vert \\mathbf x\\big \\Vert_2^2 = 1$, $\\big \\Vert \\mathbf y\\big \\Vert_2^2 = 1$, and $\\big \\Vert \\mathbf z\\big \\Vert_2^2 = 1$.\n",
    "\n",
    "The operator norm of $\\big(\\mathbf {AB}\\big)$ can be considered as a quadratic form whereby we look for \n",
    "\n",
    "\n",
    "max $\\mathbf z^H \\big(\\mathbf B^H \\mathbf A^H \\mathbf{AB}\\big) \\mathbf z$\n",
    "\n",
    "we claim that this is upper bounded by maximizing $\\big(\\mathbf x^H \\mathbf A^H\\mathbf A \\mathbf x\\big)\\big(\\mathbf y^H \\mathbf B^H \\mathbf B \\mathbf y\\big)$ which is equivalent to $\\sigma_{1,A}^2 \\sigma_{1,B}^2$\n",
    "\n",
    "We can see this by assuming $\\mathbf {Bz} =\\mathbf x$.  I.e. if $\\mathbf {Bz}$ is equal to our optimal $\\mathbf x$, then we have  $\\big(\\mathbf z^H \\mathbf B^H\\big) \\mathbf A^H \\mathbf{A}\\big( \\mathbf B \\mathbf z \\big)= \\mathbf x^H \\mathbf A^H \\mathbf A \\mathbf x $ and hence we've optimized the internal part of that equation.  But in fact $\\mathbf {Bz} = \\alpha \\mathbf x$.  I.e. $\\mathbf {Bz}$ actually gives us a scaled version of $\\mathbf x$.   So we now consider the fact that we want to maximize $\\big \\Vert \\mathbf {Bz} \\big \\Vert_2$  i.e. maximize the $\\alpha$ in $\\alpha \\mathbf x$.  This is equivalent to maximizing $\\big(\\mathbf y^H \\mathbf B^H \\mathbf B \\mathbf y\\big)$, and we know that this is given by $\\sigma_{B,1}^2$.  Hence maximizing $\\mathbf z^H \\big(\\mathbf B^H \\mathbf A^H \\mathbf{AB}\\big) \\mathbf z$ is upper bounded by maximizing the inside, which returns $\\sigma_{A,1}^2$ and scaling that by a maximal $\\alpha$ which is given by $\\sigma_{B,1}^2$ \n",
    "\n",
    "Thus $\\big \\Vert \\mathbf{AB} \\big \\Vert_2^2 \\leq \\sigma_{A,1}^2\\sigma_{B,1}^2 = \\big \\Vert \\mathbf{A} \\big \\Vert_2^2 \\big \\Vert \\mathbf{B} \\big \\Vert_2^2 $\n",
    "\n",
    "\n",
    "Now if we wanted to maximize $\\big \\Vert \\mathbf{AB} \\big \\Vert_2^2 $ with the constraint that the solution is orthogonal to the (right) singular vectors associated with $\\sigma_{A,1}$, we could upper bound this with $\\sigma_{A,2}^2 \\sigma_{B,1}^2$, and if we wanted to do a maximization that was orthogonal to the (right) singular vectors associated with  $\\{\\big(\\sigma_{A,2}, \\sigma_{B,1}\\big), \\big(\\sigma_{A,1}, \\sigma_{B,1}\\big)\\}$ and we could upper bound that by $\\sigma_{A,3}^2 \\sigma_{B,1}^2$, and so on.  \n",
    "\n",
    "If we were to add all of these upper bounds up, what we'd get is \n",
    "\n",
    "$\\sigma_{B,1}^2 \\big(\\sigma_{A,1}^2 + \\sigma_{A,2}^2 + ... + \\sigma_{A,n}^2\\big) = \\sigma_{B,1}^2 \\text{trace}\\big(\\mathbf A^H \\mathbf A\\big) = \\sigma_{B,1}^2 \\big \\Vert \\mathbf A \\big \\Vert_F^2 = \\big \\Vert \\mathbf B\\big \\Vert_2^2\\big \\Vert \\mathbf A \\big \\Vert_F^2$\n",
    "\n",
    "This process is merely an extension of a preceding section titled \"Quadratic Forms and recovering a Frobenius Norm\"... and we note that if we did this for all $\\mathbf z_k$, we'd recovered a coordinate system $\\mathbf Z$.  If we added up all of the associated 'payoff's we'd recover  $\\big \\Vert \\mathbf{AB} \\big \\Vert_F^2$.  By the above, we have upper bounds for all of those 'payoffs'. \n",
    "\n",
    "Thus we can say \n",
    "\n",
    "$\\big \\Vert \\mathbf{AB} \\big \\Vert_F^2 \\leq \\big \\Vert \\mathbf B\\big \\Vert_2^2 \\big \\Vert\\mathbf A \\big \\Vert_F^2$\n",
    "- - - - \n",
    "**begin subsequent note:**  \n",
    "\n",
    "a *much* better way to prove the above is to consider \n",
    "\n",
    "$\\mathbf B = \n",
    "\\bigg[\\begin{array}{c|c|c|c}\n",
    "\\mathbf b_1 & \\mathbf b_2 &\\cdots & \\mathbf b_{n}\n",
    "\\end{array}\\bigg]\n",
    "$\n",
    "\n",
    "so \n",
    "$\\mathbf {AB} = \n",
    "\\bigg[\\begin{array}{c|c|c|c}\n",
    "\\mathbf A\\mathbf b_1 & \\mathbf A\\mathbf b_2 &\\cdots & \\mathbf A\\mathbf b_{n}\n",
    "\\end{array}\\bigg]\n",
    "$\n",
    "\n",
    "\n",
    "where we recognize the quadratic form argument that   \n",
    "$\\big \\Vert \\mathbf A\\mathbf b_k \\big \\Vert_2^2 \\leq \\sigma_1^2 \\big  \\Vert \\mathbf b_k\\big \\Vert_2^2$ \n",
    "\n",
    "i.e. where $\\sigma_1^2$ is the dominant eigenvalue of $\\mathbf A^* \\mathbf A$  \n",
    "\n",
    "and in effect sum over this bound.  \n",
    "\n",
    "\n",
    "hence  \n",
    "$0 \\leq \\Big \\Vert \\mathbf {AB}\\Big \\Vert_F^2 = \\sum_{k=1}^n \\big \\Vert \\mathbf A\\mathbf b_k\\big \\Vert_2^2 \\leq \\sum_{k=1}^n \\sigma_1^2 \\big \\Vert \\mathbf b_k\\big \\Vert_2^2 = \\sigma_1^2 \\sum_{k=1}^n  \\big \\Vert \\mathbf b_k\\big \\Vert_2^2$  \n",
    "\n",
    "taking square roots gives   \n",
    "$0 \\leq \\Big \\Vert \\mathbf {AB}\\Big \\Vert_F \\leq \\sigma_1 \\big( \\sum_{k=1}^n  \\big \\Vert \\mathbf b_k\\big \\Vert_2^2\\big)^\\frac{1}{2} = \\sigma_1\\Big \\Vert \\mathbf {B}\\Big \\Vert_F = \\Big \\Vert \\mathbf {A}\\Big \\Vert_2\\Big \\Vert \\mathbf {B}\\Big \\Vert_F $  \n",
    "\n",
    "\n",
    "a closely related point is that \n",
    "\n",
    "$\\big \\vert \\mathbf x^* \\mathbf {AB} \\mathbf y \\big \\vert \\leq \\big \\Vert \\mathbf {Ax} \\big \\Vert_2 \\big \\Vert \\mathbf {By} \\big \\Vert_2 \\leq \\sigma_{1,A}\\sigma_{1,B}\\big \\Vert \\mathbf x \\big \\Vert_2 \\big \\Vert \\mathbf y \\big \\Vert_2 $  \n",
    "  \n",
    "\n",
    "by Cauchy Schwarz  \n",
    "\n",
    "**end subsequent note**  \n",
    "- - - - \n",
    "\n",
    "of course we can recall that \n",
    "\n",
    "$\\big \\Vert \\mathbf B\\big \\Vert_F^2 = \\big \\Vert \\mathbf B\\big \\Vert_2^2 + \\sigma_{B,2}^2 + \\sigma_{B,3}^2 + .... + \\sigma_{B,n}^2 = \\sigma_{B,1}^2+ \\sigma_{B,2}^2 + \\sigma_{B,3}^2 + .... + \\sigma_{B,n}^2 $\n",
    "\n",
    "recalling that each singular value $\\sigma_{B, k}$ is real and non-negative.  Thus if we wanted to loosen up the above bound, we could say\n",
    "\n",
    "$\\big \\Vert \\mathbf{AB} \\big \\Vert_F^2 \\leq \\big \\Vert \\mathbf B\\big \\Vert_F^2\\big \\Vert \\mathbf A \\big \\Vert_F^2$\n",
    "\n",
    "or taking the square root of both sides \n",
    "\n",
    "$\\big \\Vert \\mathbf{AB} \\big \\Vert_F \\leq \\big \\Vert \\mathbf B\\big \\Vert_F \\big \\Vert \\mathbf A \\big \\Vert_F$\n",
    "\n",
    "This should jump out at us as being a variant of Cauchy-Schwarz which tells us that \n",
    "\n",
    "$\\big \\vert\\mathbf a^H \\mathbf b\\big \\vert^2 \\leq \\big \\Vert \\mathbf a \\big \\Vert_2^2 \\big \\Vert \\mathbf b\\big \\Vert_2^2$\n",
    "\n",
    "\n",
    "\n",
    "**note: a lot of the above is proved in a different more complete, and much more satisfying way, below, under \"Hermitian Positive Semi Definite Trace Inequalities\"**  \n",
    "\n",
    "E.g. suppose $\\mathbf X^H \\mathbf X = \\mathbf A$ and $\\mathbf Y \\mathbf Y^H = \\mathbf B$,  i.e. both $\\mathbf B$ and $\\mathbf A$ are Hermitian positive semi definite.  Using the above proof with respect to squared Frobenius norms, we see\n",
    "\n",
    "$\\big \\Vert \\mathbf{XY} \\big \\Vert_F^2 \\leq \\big \\Vert \\mathbf X \\big \\Vert_F^2\\big \\Vert \\mathbf Y \\big \\Vert_F^2$\n",
    "\n",
    "but we could also say\n",
    "\n",
    "$\\big \\Vert \\mathbf{XY} \\big \\Vert_F^2 = \\text{trace}\\big(\\mathbf Y^H \\mathbf X^H \\mathbf X \\mathbf Y \\big) =  \\text{trace}\\big(\\mathbf Y \\mathbf Y^H \\mathbf X^H \\mathbf X \\big) = \\text{trace}\\big(\\mathbf {BA}\\big)= \\text{trace}\\big(\\mathbf A \\mathbf B\\big)$\n",
    "\n",
    "then apply the below inequality that if $\\mathbf A$ and $\\mathbf B$ are both Hermitian positive semidefinite,   \n",
    "$\\text{trace}\\big(\\mathbf A \\mathbf B\\big) \\leq \\text{trace}\\big(\\mathbf A\\big) \\text{trace}\\big( \\mathbf B\\big)$\n",
    "\n",
    "The below approaches prove this using spectral methods.  An alternative approach is to recognize the (standard) inner product given by the trace here, so  \n",
    "\n",
    "$\\text{trace}\\big(\\mathbf A \\mathbf B\\big) $  \n",
    "$= \\big \\vert \\text{trace}\\big(\\mathbf A \\mathbf B\\big) \\big \\vert $  \n",
    "$\\leq \\text{trace}\\big(\\mathbf A^2\\big)^\\frac{1}{2} \\text{trace}\\big( \\mathbf B^2\\big)^\\frac{1}{2}$  \n",
    "$\\leq \\text{trace}\\big(\\mathbf A\\big) \\text{trace}\\big( \\mathbf B\\big)$    \n",
    "where the first inequality is Cauchy-Schwarz, and the second in equality is the triangle inequality, i.e. for real non-negative, $\\lambda_i \\geq 0$ it tells us  \n",
    "$\\big(\\sum_{i=1}^n \\lambda_i^2\\big)^\\frac{1}{2} \\leq \\sum_{i=1}^n \\lambda_i$  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Schur's Test (/Schur's 'other' inequality)**   \n",
    "(note: this is an exercise in Cauchy-Schwarz Masterclass, chapter one)  \n",
    "\n",
    "consider  \n",
    "$\\mathbf A \\in \\mathbb C^{\\text{m x n}}$   \n",
    "and \n",
    "$\\mathbf x \\in \\mathbb C^{m}$,    \n",
    "$\\mathbf y \\in \\mathbb C^{n}$   \n",
    "\n",
    "where  \n",
    "$\\big \\Vert \\mathbf x\\big \\Vert_2 = 1$ and $\\big \\Vert \\mathbf y\\big \\Vert_2 = 1$    \n",
    "\n",
    "with  \n",
    "$R := \\max_{i} \\sum_{j=1}^n \\vert a_{i,j}\\vert$  \n",
    "$C := \\max_{j} \\sum_{i=1}^m \\vert a_{i,j}\\vert$  \n",
    "i.e. the maximal magnitude column and row sums   \n",
    "\n",
    "**claim:**  \n",
    "\n",
    "$\\big \\vert \\mathbf x^H \\mathbf A \\mathbf y \\big \\vert  = \\big \\vert \\text{trace}\\big(  \\mathbf A \\mathbf y\\mathbf x^H\\big) \\big \\vert  \\leq \\sqrt{RC}$  \n",
    "\n",
    "**proof:**  \n",
    "$\\big \\vert \\mathbf x^H \\mathbf A \\mathbf y \\big \\vert$  \n",
    "$ = \\big \\vert\\sum_{i=1}^m \\sum_{j=1}^n \\bar{x_i} a_{i,j} y_i\\big \\vert$  \n",
    "$ \\leq \\sum_{i=1}^m \\sum_{j=1}^n \\big \\vert x_i a_{i,j} y_i\\big \\vert$  \n",
    "$= \\sum_{i=1}^m \\sum_{j=1}^n \\vert x_i\\vert \\vert  a_{i,j}\\vert \\vert y_i\\vert$  \n",
    "$= \\sum_{i=1}^m \\sum_{j=1}^n \\big(\\vert x_i\\vert \\vert  a_{i,j}\\vert^\\frac{1}{2}\\big) \\big(\\vert  a_{i,j}\\vert^\\frac{1}{2}\\vert y_i\\vert\\big)$  \n",
    "$\\leq \\Big(\\sum_{i=1}^m \\sum_{j=1}^n \\big(\\vert x_i\\vert \\vert  a_{i,j}\\vert^\\frac{1}{2}\\big)^2\\Big)^\\frac{1}{2} \\Big(\\sum_{i=1}^m \\sum_{j=1}^n \\big(\\vert  a_{i,j}\\vert^\\frac{1}{2}\\vert y_i\\vert\\big)^2\\Big)^\\frac{1}{2}$  \n",
    "$= \\Big(\\sum_{i=1}^m \\vert x_i\\vert^2 \\sum_{j=1}^n \\vert  a_{i,j}\\vert \\Big)^\\frac{1}{2} \\Big(\\sum_{j=1}^n \\vert y_i\\vert^2 \\sum_{i=1}^m  \\vert  a_{i,j}\\vert  \\Big)^\\frac{1}{2}$   \n",
    "$= \\Big(\\sum_{i=1}^m \\vert x_i\\vert^2 \\big(\\sum_{j=1}^n \\vert  a_{i,j}\\vert\\big) \\Big)^\\frac{1}{2} \\Big(\\sum_{j=1}^n \\vert y_i\\vert^2 \\big(\\sum_{i=1}^m  \\vert  a_{i,j}\\vert\\big)  \\Big)^\\frac{1}{2}$   \n",
    "$\\leq \\Big(\\sum_{i=1}^m \\vert x_i\\vert^2 \\cdot R  \\Big)^\\frac{1}{2} \\Big(\\sum_{j=1}^n \\vert y_i\\vert^2 \\cdot C  \\Big)^\\frac{1}{2}$   \n",
    "$=  \\sqrt{RC} \\cdot \\Big(\\sum_{i=1}^m \\vert x_i\\vert^2 \\Big)^\\frac{1}{2} \\Big(\\sum_{j=1}^n \\vert y_i\\vert^2   \\Big)^\\frac{1}{2}$   \n",
    "$=  \\sqrt{RC}\\big \\Vert \\mathbf x \\big \\Vert_2\\big \\Vert \\mathbf y \\big \\Vert_2 $    \n",
    "$=  \\sqrt{RC}  $   \n",
    "\n",
    "where the first inequality is triangle inequality, the second is Cauchy-Schwarz, and the third is by construction     \n",
    "\n",
    "\n",
    "**corollary**  \n",
    "the operator norm of $\\mathbf A$, given by $\\sigma_1$ is bounded above by the square root of the maximal magnitude row sum times the maximal magnitude column sum \n",
    "\n",
    "*proof:*  \n",
    "$\\mathbf A = \\mathbf U \\mathbf \\Sigma \\mathbf V^H$  \n",
    "set $\\mathbf x:= \\mathbf u_1$ and $\\mathbf y := \\mathbf v_1$  \n",
    "\n",
    "using the above we immediately get \n",
    "\n",
    "$\\sigma_1 = \\big \\vert \\mathbf u_1^H \\mathbf A \\mathbf v_1 \\big \\vert \\leq  \\sqrt{RC}  $     \n",
    "\n",
    "**remark**  \n",
    "In some cases, when dealing with normal matrices -- and in particular when dealing with Hermitian matrices-- this estimate on the upper bound of the singular value will coincide with that given by application of Gerschgorin Discs.  For example, on the graph Laplacian, both the Schur Test and Gerschgorin discs tells us that the maximal eigenvalue (where $\\lambda_1 = \\sigma_1$ for real symmetric positive (semi)definite matrices) is bounded above by 2 times the maximal degree  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**remark**  \n",
    "for the case of doubly stochastic matrices, and in particular doubly stochastic markov chains, we have  \n",
    "\n",
    "$1 = \\lambda_1 \\leq \\sigma_1$  \n",
    "(quadratic form argument, where $\\lambda_1$ is the dominant / Perron root)  \n",
    "\n",
    "and by Schur's test  \n",
    "$\\sigma_1 \\leq \\sqrt{RC} =1$  \n",
    "\n",
    "so  \n",
    "$1 = \\lambda_1 \\leq \\sigma_1 \\leq 1$  \n",
    "\n",
    "so $\\lambda_1 = \\sigma_1$  \n",
    "\n",
    "This in fact is an *iff* i.e. for markov chains in general \n",
    "$\\lambda_1 \\leq \\sigma_1$ \n",
    "with equality *iff* the markov chain transition matrix is doubly stochastic  \n",
    "\n",
    "The other leg here that  \n",
    "$\\sigma_1 = \\lambda_1 \\longrightarrow \\mathbf 1 \\text{ is left and right eigenvector}$  \n",
    "is proven elsewhere (e.g. by Schur Triangularization)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**additional remark:**  \n",
    "in the case of any positive matrix $\\mathbf A\\gt 0$ (i.e. a component-wise inequality)\n",
    "(the below holds for *any* such matrix, not just markov chains) \n",
    "\n",
    "$\\lambda_1 = \\sigma_1$ *iff*  \n",
    "$\\mathbf A\\mathbf v_1 = \\lambda_1 \\mathbf v_1$ *and*  $\\mathbf A^T\\mathbf v_1 = \\lambda_1 \\mathbf v_1$  \n",
    "based on the above proved elsewhere item we get the first leg 'free', but there's a more basic point here  \n",
    "\n",
    "from our earlier quadratic form argument, we had, with all vectors having a 2 norm of 1  \n",
    "$\\sigma_1 \\geq \\lambda_1$  or  \n",
    "\n",
    "$\\sigma_1^2 = \\text{max: }\\mathbf x^T \\mathbf A^T\\mathbf A \\mathbf x \\geq \\mathbf v_1^T \\mathbf A^T\\mathbf {Av}_1 =\\lambda_1^2$  \n",
    "\n",
    "however from SVD we know that $\\sigma_1^2$ is the dominant eigenvalue of $\\big( \\mathbf A^T\\mathbf A\\big)$ which is necessarily the Perron root of said matrix, since the matrix is positive and the Perron Root comes with a positive eigenvector, the only positive eigenvector for said matrix and      \n",
    "$\\big(\\mathbf A^T\\mathbf A\\big) \\mathbf v_1 =\\mathbf A^T\\big(\\mathbf A \\mathbf v_1\\big) = \\lambda_1 \\cdot \\mathbf A^T \\mathbf v_1 =\\lambda_1^2 \\cdot \\mathbf v_1$  \n",
    "\n",
    "so $\\mathbf v_1$ *must* be the Perron vector for $\\big(\\mathbf A^T\\mathbf A\\big)$ and $\\lambda_1^2$ is the Perron root for $ \\big( \\mathbf A^T\\mathbf A\\big)$ but that maximal eigenvalue is also equal to $\\sigma_1^2$ so we have  \n",
    "$\\sigma_1^2 = \\lambda_1^2 \\longrightarrow \\sigma_1 = \\lambda_1$  \n",
    "(where both are necessarily positive)  \n",
    "\n",
    "*this then implies the case for merely* $\\mathbf A\\geq 0$ that has a single communicating class and a dominant eigenvalue $\\lambda_1, \\mathbf v_1$ with $\\mathbf A^T\\mathbf v_1 = \\mathbf A\\mathbf v_1 = \\lambda_1$  \n",
    "\n",
    "Consider   \n",
    "$\\mathbf B(\\tau) = \\tau\\cdot \\mathbf A + (1-\\tau)\\cdot \\mathbf v_1 \\mathbf v_1^T$  \n",
    "which is strictly positive for $\\tau \\in [0,1)$  \n",
    "\n",
    "where $\\big \\Vert \\mathbf v_1\\big \\Vert_2 =1$  \n",
    "\n",
    "for any $\\tau \\in [0,1)$   \n",
    "$\\mathbf B(\\tau)\\mathbf v_1 = \\tau\\cdot \\mathbf A\\mathbf v_1 + (1-\\tau)\\cdot \\mathbf v_1 \\mathbf v_1^T\\mathbf v_1 = \\tau\\cdot \\lambda_1 \\mathbf v_1 + (1-\\tau)\\cdot \\mathbf v_1  = \\big(\\tau \\lambda_1 + (1-\\tau)1\\big)\\mathbf v_1 $   \n",
    "and since $\\mathbf A^T \\mathbf v_1 = \\lambda_1 \\mathbf v_1$ we have  \n",
    "$\\mathbf B(\\tau)^T\\mathbf v_1 = \\big(\\tau \\lambda_1 + (1-\\tau)1\\big)\\mathbf v_1= \\mathbf B(\\tau)\\mathbf v_1$   \n",
    "\n",
    "Both the eigenvalues and singular values of $\\mathbf B(\\tau)$ vary continuously with the components in it. So our prior results tell us     \n",
    "$\\sigma_{\\max} - \\lambda_{\\text{Perron}} = 0$ for $\\tau \\in [0,1)\\longrightarrow \\sigma_1 - \\lambda_1 = 0$ for $\\tau =1$ by continuity  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a more general result consider applying Schur Triangularization to $\\mathbf A$ with largest modulus eigenvalue $\\lambda_1 \\neq 0$ selected as the first eigenvector   \n",
    "\n",
    "$\\mathbf T = \\mathbf U^H \\mathbf A \\mathbf U =  \\begin{bmatrix}\\lambda_1 & \\mathbf x^H \\\\ \\mathbf 0& \\mathbf T_1\\end{bmatrix} = \\begin{bmatrix}\\lambda_1 & \\mathbf 0^H \\\\ \\mathbf 0& \\mathbf 0\\end{bmatrix} + \\begin{bmatrix}0 & \\mathbf x^H \\\\ \\mathbf 0&\\mathbf  T_1\\end{bmatrix}$  \n",
    "\n",
    "or  \n",
    "(i)  $\\mathbf U\\mathbf T  = \\mathbf A\\mathbf U$  \n",
    "and  \n",
    "(ii) $\\mathbf T\\mathbf U^H  = \\mathbf U^H\\mathbf A$  \n",
    "\n",
    "specializing to the first column of (i), we have  \n",
    "$\\lambda_1 \\mathbf u_1= \\lambda_1 \\mathbf U\\mathbf e_1=\\mathbf U\\big(\\mathbf T\\mathbf e_1\\big)=\\mathbf U\\mathbf T\\mathbf e_1  = \\mathbf A\\mathbf U\\mathbf e_1 = \\mathbf A\\big(\\mathbf U\\mathbf e_1\\big)= \\mathbf A\\mathbf u_1 = \\lambda_1 \\mathbf u_1$  \n",
    "\n",
    "and specializing to the first row of (ii), we have  \n",
    "$\\lambda_1 \\mathbf u_1^H + \\begin{bmatrix}0 & \\mathbf x^H\\end{bmatrix}\\mathbf U^H = \\Big(\\lambda_1 \\mathbf e_1^H +\\begin{bmatrix}0 & \\mathbf x^H\\end{bmatrix}\\Big)\\mathbf U^H = \\big(\\mathbf e_1^H\\mathbf T\\big)\\mathbf U^H  = \\mathbf e_1^H\\mathbf U^H\\mathbf A = \\big(\\mathbf U\\mathbf e_1\\big)^H \\mathbf A = \\mathbf u_1^H \\mathbf A = \\lambda_1 \\mathbf u_1^H$  \n",
    "\n",
    "subtracting $\\lambda_1 \\mathbf u_1^H $ from each side tells us   \n",
    "$\\begin{bmatrix}0 & \\mathbf x^H\\end{bmatrix}\\mathbf U^H  = \\mathbf 0^H \\longrightarrow \\begin{bmatrix}0 & \\mathbf x^H\\end{bmatrix} = \\mathbf 0^H \\longrightarrow \\mathbf x = \\mathbf 0 $   \n",
    "because $\\mathbf U$ is invertible (and in fact unitary)  \n",
    "\n",
    "so  \n",
    "$\\mathbf T = \\mathbf U^H \\mathbf A \\mathbf U =  \\begin{bmatrix}\\lambda_1 & \\mathbf 0^H \\\\ \\mathbf 0& \\mathbf T_1\\end{bmatrix}$  \n",
    "\n",
    "and $\\big \\vert \\lambda_1\\big \\vert^2$ is an eigenvalue of $\\big(\\mathbf A^H\\mathbf A\\big)$, i.e. it is a singular value of $\\mathbf A$.  Unfortunately, it isn't clear to your author that it is necessarily the *maximal* singular value of $\\mathbf A$  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**extension: Markov Chains must have linearly independent eigenvectors associated with eigenvalues of magnitude 1**\n",
    "\n",
    "The argument is a proof by contradiction using Jordan Forms.  In essence, assume the above statement is not true, and we find it violates the above matrix norms.  This is an extension modified answer for problem 4.15 In Stochastic Processes by Gallagher (problem 3.15 in \"Discrete Stochastic Processes\" in MIT OCW.\n",
    "\n",
    "start by showing a 3 x 3 Jordan Block of the form\n",
    "\n",
    "$\\mathbf J_i = \\begin{bmatrix}\n",
    "\\lambda_i & 1 & 0 \\\\ \n",
    "0 & \\lambda_i & 1\\\\ \n",
    "0 & 0 & \\lambda_i \n",
    "\\end{bmatrix}$\n",
    "\n",
    "then \n",
    "\n",
    "$\\mathbf J_i^n = \\begin{bmatrix}\n",
    "\\lambda_i^n & n \\lambda_i^{n-1} & \\binom{n}{2} \\lambda_i^{n-2} \\\\ \n",
    "0 & \\lambda_i^n & n \\lambda_i^{n-1}\\\\ \n",
    "0 & 0 & \\lambda_i^n \n",
    "\\end{bmatrix}$\n",
    "\n",
    "There are various ways to expand or shrink this, but most succinctly we see that diagonal elements (eigenvalues) multiply exponentially with n, as we'd expect.  The off diagonal element that are non-zero occur when we do not have enough linearly independent eigenvectors for $\\lambda_i$, and as a lower bound, we can say that they too grow exponentially (albeit it at one or two iterations less per 'step' than the diagonals) and are scaled by n. \n",
    "\n",
    "From here, notice that for any $\\big \\vert \\lambda_i \\big\\vert \\lt 1$, that $\\lim_{n \\to \\infty}\\mathbf J_i^n = \\mathbf {0} $.  However if $\\big\\vert \\lambda_i\\big\\vert = 1$, then the off diagonal elements also tend to infinity.  (We don't consider the case of $\\big\\vert \\lambda_i \\big\\vert \\gt 1$, because as noted in the Gerschgorin discs writeup, Markov Chains cannot have eigenvalues with magnitude $\\gt 1$.) \n",
    "\n",
    "\n",
    "Now suppose we have a defective markov chain transition matrix that is $m$ x $m$.  I.e. it factorizes so $\\mathbf A = \\mathbf {PJP}^{-1}$, where $\\mathbf J$ is the jordan form that has non-zero off-diagonal elements because we $\\mathbf A$ is defective.  \n",
    "\n",
    "We also can say:\n",
    "\n",
    "$\\mathbf J^n = \\mathbf P^{-1} \\mathbf A^n \\mathbf P $, or using associativity $\\mathbf J^n = \\mathbf P^{-1} \\big(\\mathbf A^n \\mathbf P \\big)$\n",
    "\n",
    "applying the above Frobenius norm inequality, twice,\n",
    "\n",
    "$ \\big \\Vert \\mathbf J^n \\big \\Vert_F^2=\\big \\Vert \\mathbf P^{-1} \\big(\\mathbf A^n \\mathbf P \\big) \\big \\Vert_F^2 \\leq \\big \\Vert \\mathbf P^{-1} \\big \\Vert_F^2\\big \\Vert \\big(\\mathbf A^n \\mathbf P \\big) \\big \\Vert_F^2 \\leq \\big \\Vert \\mathbf P^{-1} \\big \\Vert_F^2\\big \\Vert \\mathbf A^n   \\big \\Vert_F^2\\big \\Vert  \\mathbf P  \\big \\Vert_F^2$\n",
    "\n",
    "\n",
    "Also recalling that valid transition matrix $\\mathbf A^n$ has all entries as real valued non-negative, and either all columns or all rows sum to one.  Put differently, every value in $\\mathbf A$ is in $[0,1]$, and  thus we can upper bound the squared Frobenius norm of $\\mathbf A^n$ by the ones matrix.\n",
    "\n",
    "$\\big \\Vert \\mathbf A^n  \\big \\Vert_F^2 \\leq \\big \\Vert \\mathbf {11}^T  \\big \\Vert_F^2 = m^2$\n",
    "\n",
    "now multiply both sides by $\\big \\Vert \\mathbf P^{-1} \\big \\Vert_F^2$ and $\\big \\Vert  \\mathbf P  \\big \\Vert_F^2$, which are real valued, positive scalars, and we get:  \n",
    "\n",
    "$ \\big \\Vert \\mathbf J^n \\big \\Vert_F^2 \\leq \\big \\Vert \\mathbf P^{-1} \\big \\Vert_F^2\\big \\Vert \\mathbf A^n  \\big \\Vert_F^2 \\big \\Vert  \\mathbf P  \\big \\Vert_F^2 \\leq m^2 \\big \\Vert \\mathbf P^{-1} \\big \\Vert_F^2  \\big \\Vert  \\mathbf P  \\big \\Vert_F^2 $\n",
    "\n",
    "notice that for any given transition matrix $\\mathbf A$, the right hand side of the equality is some fixed finite number, and it does not vary with respect to $n$.  The contradiction comes by assuming that we do not have enough linearly independent eigenvectors with eigenvalue magnitude of $1$.  Hence for any given $\\mathbf A$ we have a fixed upper bound on the right hand side that does not vary with $n$.  Yet if we have jordan blocks (i.e. super diagonal values of 1) associated with $\\big \\vert\\lambda_i \\big \\vert =1$, then we can find large enough $n$ such that the left hand side has a larger Frobenius norm than the right hand side, which is a contradiction.  Hence we know there cannot be aka a shortage of linearly independent eigenvectors, with respect to eigenvalues of magnitude one in a Markov chain transition matrix. \n",
    "\n",
    "\n",
    "**extension: Projection Matrices must be diagonalizable**  \n",
    "\n",
    "A projector, aka an idempotent matrix is an $n$ x $n$ matrix $\\mathbf A$, where \n",
    "\n",
    "$\\mathbf A = \\mathbf A^2$\n",
    "\n",
    "**claim**  \n",
    "\n",
    "$\\mathbf A$ must diagonalizable.\n",
    "\n",
    "**proof**\n",
    "\n",
    "$\\mathbf A$ has only eigenvalues equal to $0$ and $1$\n",
    "\n",
    "that is, for each eigenvector $\\mathbf x$, we have $\\mathbf A^2 \\mathbf x = \\lambda_k \\mathbf A \\mathbf x = \n",
    "\\lambda_k^2 \\mathbf x = \\lambda_k \\mathbf x = \\mathbf {Ax}$\n",
    "\n",
    "thus $\\lambda_k^2 = \\lambda_k$, which occurs **iff** $\\lambda_k = 0$ or $\\lambda_k = 1$.\n",
    "\n",
    "note: we could rewrite the above as \n",
    "\n",
    "$\\lambda_k^2 = \\lambda_k \\to \\lambda_k^2 - \\lambda_k = 0 \\to \\lambda_k(\\lambda_k - 1) - 0$, hence the roots / $\\lambda_k$ that satisfy this are $1$ and $0$.  \n",
    "\n",
    "\n",
    "now we consider the possibility that $\\mathbf A$ is defective and write out its Jordan Form, similarity transform\n",
    "\n",
    "$\\mathbf P^{-1} \\mathbf A^k \\mathbf P =\\mathbf J^k$ \n",
    "\n",
    "hence we have \n",
    "\n",
    "$\\mathbf P^{-1} \\mathbf A \\mathbf P =\\mathbf J = \\mathbf J^2 = \\mathbf P^{-1} \\mathbf A^2 \\mathbf P $ \n",
    "\n",
    "also notice that since $\\mathbf A = \\mathbf A^2$, then we can left multiply both by $\\mathbf A$ and see that \n",
    "\n",
    "$\\mathbf A^2 = \\mathbf A^3$, hence $\\mathbf A = \\mathbf A^3$. We can further do this process such that $\\mathbf A = \\mathbf A^k$\n",
    "\n",
    "\n",
    "Thus our relationship is $\\mathbf J = \\mathbf J^k$ for any natural number $k$. Recalling that $\\mathbf J$ has eigenvalues equal to zero or one on the diagonal, and at most all 1s in the strictly upper triangular portion, we can upper bound the squared Frobenius norm of $\\mathbf J$ with $\\big \\Vert \\mathbf {11}^T \\big \\Vert_F^2$.  Further, we collect all of the eigenvalues in a diagonal matrix $\\mathbf D$ and remark that $\\mathbf J$ has a Frobenius norm strictly greater than $\\mathbf D$ unless $\\mathbf A$ is diagonalizable. \n",
    "\n",
    "if defective $ \\big \\Vert \\mathbf D \\big \\Vert_F^2  \\lt \\big \\Vert \\mathbf J^k \\big \\Vert_F^2 = \\big \\Vert \\mathbf J \\big \\Vert_F^2 \\lt \\big \\Vert \\mathbf {11}^T \\big \\Vert_F^2$.  \n",
    "\n",
    "if diagonalizable $ \\big \\Vert \\mathbf D \\big \\Vert_F^2  = \\big \\Vert \\mathbf J^k \\big \\Vert_F^2 = \\big \\Vert \\mathbf J \\big \\Vert_F^2 \\lt \\big \\Vert \\mathbf {11}^T \\big \\Vert_F^2$.  \n",
    "\n",
    "We know that the matrix cannot have a shortage of linearly independent eigenvectors for $\\lambda_k = 1$.  Why? Repeat the exact same argument used above for eigenvalues with magnitude 1 in Markov Chains.  The right hand side is fixed in magnitude, but we can find large enough (finite) $k$ that creates a Frobenius norm that exceeds this bound if we have any Jordan blocks (i.e. shortage of linearly independent eigenvectors) with respect to eigenvalues equal to one.  Thus we conclude that the geometric multiplicity = algebraic multiplicity for $\\lambda = 1$\n",
    "\n",
    "Now, with respect to eigenvalues equal to zero, recall that if the matrix is defective, we have Jordan blocks given by: \n",
    "\n",
    "$\\mathbf J_i = \\begin{bmatrix}\n",
    "\\lambda_i & 1 & 0 \\\\ \n",
    "0 & \\lambda_i & 1\\\\ \n",
    "0 & 0 & \\lambda_i \n",
    "\\end{bmatrix}$\n",
    "\n",
    "and, for $k\\gt 2$, we have \n",
    "\n",
    "$\\mathbf J_I^k = \\begin{bmatrix}\n",
    "\\lambda_i^k & k \\lambda_i^{k-1} & \\binom{k}{2} \\lambda_i^{k-2} \\\\ \n",
    "0 & \\lambda_i^k & k \\lambda_i^{k-1}\\\\ \n",
    "0 & 0 & \\lambda_i^k \n",
    "\\end{bmatrix}$\n",
    "\n",
    "Yet this block is nilpotent since $\\lambda = 0$ and thus we can select large enough $k$ (e.g. $k:=n$) and say that $\\mathbf J_I^n = \\mathbf 0$.  But the only way $\\big \\Vert\\mathbf J_I\\big \\Vert_F^2 = \\big \\Vert\\mathbf J_I^n\\big \\Vert_F^2 = \\big \\Vert\\mathbf 0\\big \\Vert_F^2$, is if $\\mathbf J_I$ is itself a zero matrix (i.e. we know the $\\lambda_i = 0$ for this block, and the off diagonal ones cannot exist).\n",
    "\n",
    "since we know that there are no off diagonal elements with respect to $\\lambda = 1$ or with respect to $\\lambda = 0$, we know \n",
    "\n",
    "$\\big \\Vert \\mathbf D \\big \\Vert_F^2 = \\big \\Vert \\mathbf J \\big \\Vert_F^2$\n",
    "\n",
    "which proves that $\\mathbf A$ is diagonalizable (i.e. not defective).  \n",
    "\n",
    "**alternative proof:** \n",
    "\n",
    "A projector, aka an idempotent matrix is an $n$ x $n$ matrix $\\mathbf A$, where \n",
    "(note: we use projector and idempotent interchangeably as is commonly done e.g. in Meyer's *Matrix Analysis*.  Unfortunately it is a common convention to insist on projectors having some additional structure, e.g. being symmetric.  We do not use this other convention.)  \n",
    "\n",
    "\n",
    "our projector obeys:\n",
    "\n",
    "$\\mathbf A = \\mathbf A^2$\n",
    "\n",
    "i.e. \n",
    "\n",
    "$\\mathbf 0 = \\mathbf A^2 - \\mathbf A$  \n",
    "\n",
    "that is, the polynomial \n",
    "\n",
    "$p(x) = x^2 - x$ \n",
    "\n",
    "annihilates our matrix $\\mathbf A$.  \n",
    "\n",
    "In the special case that $\\mathbf A = \\mathbf 0$ or $\\mathbf A = \\mathbf I$ then there is nothing to prove as the zero matrix is already diagonal (alternatively: it it is the only nilpotent matrix that is diagonalizable), and the identity matrix is already diagonal (further it is the only matrix that has every non zero vector as an eigenvector).  \n",
    "\n",
    "So from here, assume $\\mathbf I \\neq \\mathbf A \\neq \\mathbf 0$ \n",
    "\n",
    "revisiting our annihilating polynomial \n",
    "\n",
    "$p(x) = x^2 - x$ \n",
    "\n",
    "$p(\\mathbf A) = \\mathbf A^2 - \\mathbf A = \\mathbf A\\big(\\mathbf A - \\mathbf I\\big)$ \n",
    "\n",
    "we find that is in fact the minimal polynomial of $\\mathbf A$ with no repeated factors and hence $\\mathbf A$ cannot be defective if it is a Projector.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# a better approach \n",
    "\n",
    "consider the involutary matrix \n",
    "\n",
    "$\\mathbf A^2 = \\mathbf I$ \n",
    "\n",
    "such a matrix must be diagonalizable  \n",
    "\n",
    "if $\\mathbf A = \\mathbf I$ or $\\mathbf A = -\\mathbf I$ \n",
    "there is nothing to prove -- it is already diagonalized.  \n",
    "\n",
    "supposing it is not \n",
    "\n",
    "then we have a minimal polynomial given by \n",
    "\n",
    "$\\mathbf A^2 - \\mathbf I = \\big(\\mathbf A - \\mathbf I\\big)\\big(\\mathbf A + \\mathbf I\\big)   = \\mathbf 0$   \n",
    "\n",
    "First this tells us that the above polynomial annihilates $\\mathbf A$ and hence the any eigenvalue of $\\mathbf A$ must be $+1$ or $-1$.  \n",
    "\n",
    "Furthermore, this tells us that for *any* $\\mathbf x \\neq 0$ \n",
    "\n",
    "$\\big(\\mathbf A - \\mathbf I\\big)\\big(\\mathbf A + \\mathbf I\\big)\\mathbf x = \\mathbf 0 $ \n",
    "\n",
    "hence $\\mathbf x$ is equal to a linear combination of vectors in the nullspace of \n",
    "\n",
    "$\\big(\\mathbf A - \\mathbf I\\big)$ and $\\big(\\mathbf A + \\mathbf I\\big)$.  Slightly more carefully, we know we have dimension $n$, and that the matrix $\\big(\\mathbf A + \\mathbf I\\big)$ has $k$ linearly independent vectors in it nullspace / kernel, and $n-k$ linearly independent vectors its column space / image. But each one of those $n-k$ linearly independent vector is then annhilated by $\\big(\\mathbf A - \\mathbf I\\big)$, and hence that means they are all in the nullspace/kernel of said matrix.  We've thus created a basis for dimension $n$ by $k$ + $n-k$ linearly independent vectors in the nullspaces of these respective matrix.  \n",
    "\n",
    "But all non-zero vectors in the nullspaces of these matrices are precisely the eigenvectors corresponding to eigenvalues of $\\mathbf A$.  Hence the eigenvectors of $\\mathbf A$ form a basis and we know that $\\mathbf A$ is diagonalizable.  \n",
    "\n",
    "\n",
    "**now, for any projector/ idempotent matrix**\n",
    "\n",
    "we can use scaling and shifting to write it as a involutory matrix, and hence be diagonalizable.  However a more  direct proof is to simply note that \n",
    "\n",
    "$\\mathbf P = \\mathbf P^2 \\longrightarrow \\mathbf P - \\mathbf P^2 = \\big(\\mathbf P - \\mathbf 0\\big)\\big(\\mathbf I - \\mathbf P\\big) = \\mathbf 0$  \n",
    "\n",
    "hence using the above result, we have a basis formed by the linearly independent vectors in  \n",
    "$\\text{null}\\big(\\mathbf P - \\mathbf 0\\big) \\bigcup \\text{null}\\big(\\mathbf I - \\mathbf P\\big)$   \n",
    "thus $\\mathbf P$ is diagonalizable.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**The bulk of the rest of this posting consists of inequalities from Zhang's** *Linear Algebra: Challenging Problems for Students*, which contains numerous interesting exercises that proceed much like a guided proof.  (Unfortunately, the solutions in the back are frequently some mixture of terse and nearly incomprehensible -- that said your author quite likes the book because of the very high quality exercises.)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Misc notes on Hermitian Matrices\n",
    "\n",
    "if we let $\\mathbf A$ and $\\mathbf B$ both be $n$ x $n$ Hermitian matrices, \n",
    "\n",
    "first, notice $\\mathbf {AB} $ has the same eigenvalues as $\\mathbf {BA}$.  (In general we know that they have the same non-zero eigenvalues with the same Algebraic multiplicities -- and since they have the same dimension $n$ there must be the same number of 'leftover' eigenvalues that are zeros.  A proof is contained in \"Vandermonde_Matrices_Permutations_and_Discrete_Fourier_Transform.ipybn\" )\n",
    "\n",
    "\n",
    "now notice $\\big(\\mathbf{AB}\\big)^H = \\mathbf B^H \\mathbf A^H = \\mathbf {BA}$.  Thus we conclude that $\\mathbf {BA}$ has the same eigenvalues (with same algebraic multipilicities) as $\\mathbf {AB}$ and the same as the transposed conjugate $\\big(\\mathbf {BA}\\big)^H$.   This means that all eigenvalues in $\\mathbf {AB}$ must be either real, or come in conjugate pairs.   \n",
    "\n",
    "This means that $\\text{trace}\\Big(\\big(\\mathbf{AB}\\big)^k\\Big)$ is real valued for any natural number $k$.  \n",
    "\n",
    "- - - - -\n",
    "*begin interlude*  \n",
    "Here is a different take.  The below claims are mostly interested in $\\big(\\mathbf {AB}\\big)$ and $\\big(\\mathbf {AB}\\big)^2$.  For the first case, notice that $\\big( \\mathbf A - \\mathbf B\\big)$ is a Hermitian matrix, and if we square it, it is hermitian positive semi definite matrix-- and hence its trace must be real, non-negative, which we denote as $\\gamma$.  \n",
    "\n",
    "$\\text{trace}\\Big( \\big(\\mathbf A - \\mathbf B\\big)^2 \\Big) = \\text{trace}\\big(\\mathbf A^2\\big) + \\text{trace}\\big(\\mathbf B^2\\big) - \\text{trace}\\big(\\mathbf{AB}\\big) - \\text{trace}\\big(\\mathbf{BA}\\big) = \\gamma$\n",
    "\n",
    "$\\text{trace}\\big(\\mathbf A^2\\big) + \\text{trace}\\big(\\mathbf B^2\\big) - 2\\cdot \\text{trace}\\big(\\mathbf{AB}\\big) = \\gamma$\n",
    "\n",
    "$\\text{trace}\\big(\\mathbf A^2\\big) + \\text{trace}\\big(\\mathbf B^2\\big)  = \\gamma + 2\\cdot \\text{trace}\\big(\\mathbf{AB}\\big)$\n",
    "\n",
    "the left hand side is the sum of traces of 2 Hermitian positive semi-definite matrices and hence is real, non-negative, thus the right hand side is as well. We know that $\\gamma$ is real, thus $\\text{trace}\\big(\\mathbf{AB}\\big)$ must be as well. \n",
    "\n",
    "Also consider \n",
    "\n",
    "$\\Big(\\mathbf{AB} + \\mathbf{BA}\\Big)^2 = \\Big(\\mathbf{AB} + \\big(\\mathbf{AB}\\big)^H\\Big)^2$\n",
    "\n",
    "Where $\\mathbf{AB}$ plus its conjugate transpose creates a new matrix that is Hermitian.  Thus this new matrix given by $\\Big(\\mathbf{AB} + \\mathbf{BA}\\Big)$ has real eigenvalues, and a real trace.  We denote the trace of the square of this new matrix as $\\gamma$.\n",
    "\n",
    "$\\text{trace}\\Big( \\big( \\mathbf{AB} + \\mathbf{BA}\\big)^2 \\Big) = \\gamma = \\text{trace}\\big(\\mathbf{ABAB}\\big) + \\text{trace}\\big(\\mathbf{BABA} \\big) + \\text{trace}\\big(\\mathbf{ABBA}\\big) + \\text{trace}\\big(\\mathbf{BAAB}\\big)$\n",
    "\n",
    "$\\text{trace}\\Big( \\big( \\mathbf{AB} + \\mathbf{BA}\\big)^2 \\Big) = \\gamma  = 2\\cdot \\text{trace}\\big(\\mathbf{ABAB}\\big) + 2\\cdot \\text{trace}\\big(\\mathbf{ABBA}\\big)$\n",
    "\n",
    "$\\frac{1}{2}\\gamma = \\text{trace}\\big(\\mathbf{ABAB}\\big) + \\text{trace}\\big(\\mathbf{ABBA}\\big)$\n",
    "\n",
    "$\\frac{1}{2}\\gamma -  \\text{trace}\\big(\\mathbf{ABBA}\\big) = \\text{trace}\\Big(\\mathbf{\\big(AB\\big)^2}\\Big)$\n",
    "\n",
    "since gamma is real, and $(\\mathbf{ABBA}\\big) $ is Hermitian, and hence has a real trace, then the left hand side has a real trace.  This means that the right hand side must have a real trace as well.  \n",
    "\n",
    "*end interlude*\n",
    "- - - - -\n",
    "\n",
    "\n",
    "**claim:**    \n",
    "\n",
    "$\\text{trace}\\Big(\\big(\\mathbf {AB}\\big)^2\\Big)  \\leq \\text{trace}\\big(\\mathbf A^2 \\mathbf B^2\\big)$\n",
    "\n",
    "**proof:**  \n",
    "\n",
    "using the cyclic property of trace, and the fact that $\\mathbf A$ and $\\mathbf B$ are both Hermitian, we can rewrite the right hand side as:\n",
    "\n",
    "$\\text{trace}\\big(\\mathbf A^2 \\mathbf B^2\\big) = \\text{trace}\\big(\\mathbf {AA} \\mathbf {BB}\\big) = \\text{trace}\\big(\\mathbf A \\mathbf {BBA}\\big) = \\text{trace}\\big(\\mathbf A^H \\mathbf B^H \\mathbf {BA}\\big) = \\text{trace}\\Big(\\big(\\mathbf{BA}\\big)^H \\big(\\mathbf {BA}\\big) \\Big)$\n",
    "\n",
    "Let $\\mathbf C: = \\mathbf {AB}$\n",
    "\n",
    "This means we can rewrite our claim as \n",
    "\n",
    "\n",
    "$\\text{trace}\\big(\\mathbf {C}^2\\big) \\leq \\text{trace}\\big(\\mathbf{C}^H \\mathbf {C}\\big)$\n",
    "\n",
    "and by applying the Schur inequality, we know \n",
    "\n",
    "$\\text{trace}\\big(\\mathbf {C}^2\\big) \\leq \\big \\vert \\text{trace}\\big(\\mathbf {C}^2\\big) \\big \\vert \\leq \\text{trace}\\big(\\mathbf{C}^H \\mathbf {C}\\big)$\n",
    "\n",
    "again, recalling that $\\text{trace}\\big( \\mathbf{C}^k\\big)$ is real valued for any natural number $k$\n",
    "\n",
    "\n",
    "**claim:**  \n",
    " \n",
    "$\\Big(\\text{trace}\\big(\\mathbf {AB}\\big)\\Big)^2 \\leq \\text{trace}\\big(\\mathbf A^2\\big) \\text{trace}\\big(\\mathbf B^2\\big)$\n",
    "\n",
    "- - - - \n",
    "*begin interlude* \n",
    "\n",
    "here we introduce the vec operator (also used in the Kronecker Products writeup) \n",
    "\n",
    "where we have $\\mathbf B = \\bigg[\\begin{array}{c|c|c|c}\n",
    "\\mathbf b_1 & \\mathbf b_2 &\\cdots & \\mathbf b_n\\end{array}\\bigg] $\n",
    "\n",
    "$\\text{vec}\\big(\\mathbf B\\big)  =\\mathbf b= \\begin{bmatrix}\n",
    "\\mathbf b_1 \\\\ \n",
    "\\mathbf b_2\\\\ \n",
    "\\mathbf b_3\\\\ \n",
    "\\vdots \\\\ \n",
    "\\mathbf b_n\n",
    "\\end{bmatrix}$\n",
    "\n",
    "where $\\mathbf b$ has $\\mathbf b_1$ with $\\mathbf b_2$ \"glued\" to the bottom of it, with $\\mathbf b_3$ \"glued\" to the bottom of that ... with $\\mathbf b_n$ glued to the bottom of that.  \n",
    "\n",
    "Thus we see that $\\big \\Vert \\mathbf B \\big \\Vert_F^2 = \\mathbf b ^H \\mathbf b$\n",
    "\n",
    "The same for $\\mathbf A$\n",
    "\n",
    "Note we denote $\\mathbf A = \\bigg[\\begin{array}{c|c|c|c}\n",
    "\\mathbf a_1 & \\mathbf a_2 &\\cdots & \\mathbf a_n\\end{array}\\bigg] $\n",
    "\n",
    "and $\\text{vec}\\big(\\mathbf A\\big)  = \\mathbf a = \\begin{bmatrix}\\mathbf a_1 \\\\ \n",
    "\\mathbf a_2\\\\ \n",
    "\\mathbf a_3\\\\ \n",
    "\\vdots \\\\ \n",
    "\\mathbf a_n\n",
    "\\end{bmatrix}$\n",
    "\n",
    "$\\big \\Vert \\mathbf A \\big \\Vert_F^2 = \\mathbf a^H \\mathbf a$\n",
    "\n",
    "*end interlude*  \n",
    "- - - - \n",
    "\n",
    "**proof:**    \n",
    "\n",
    "because $\\mathbf A = \\mathbf A^H$ we can rewrite the left hand side as  \n",
    "\n",
    "$\\Big(\\text{trace}\\big(\\mathbf {AB}\\big)\\Big)^2  = \\Big(\\text{trace}\\big(\\mathbf {A}^H \\mathbf B \\big)\\Big)^2 = \\Big(\\text{trace}\\big(\\mathbf a^H \\mathbf b \\big)\\Big)^2 = \\big(\\mathbf a^H \\mathbf b\\big)^2$\n",
    "\n",
    "\n",
    "And for the right hand side we can rewrite it as:\n",
    "\n",
    "$\\text{trace}\\big(\\mathbf A^2\\big) \\text{trace}\\big(\\mathbf B^2\\big) = \\text{trace}\\big(\\mathbf {A}^H\\mathbf A\\big) \\text{trace}\\big(\\mathbf B^H \\mathbf B \\big) = \\text{trace}\\big(\\mathbf a^H \\mathbf a\\big) \\text{trace}\\big(\\mathbf b^H \\mathbf b\\big) = \\big(\\mathbf {a}^H \\mathbf a \\big)\\big(\\mathbf b^H \\mathbf b\\big)$\n",
    "\n",
    "hence our claim reduces to a simple application of Cauchy Schwartz:\n",
    "\n",
    "i.e. \n",
    "\n",
    "$\\Big(\\text{trace}\\big(\\mathbf {AB}\\big)\\Big)^2 \\leq \\text{trace}\\big(\\mathbf A^2\\big) \\text{trace}\\big(\\mathbf B^2\\big)$\n",
    "\n",
    "is equivalent to saying \n",
    "\n",
    "$ (\\mathbf a^H \\mathbf b\\big)^2 \\leq \\big(\\mathbf {a}^H \\mathbf a \\big)\\big(\\mathbf b^H \\mathbf b\\big)$\n",
    "\n",
    "recalling the fact that $\\big(\\mathbf {AB}\\big)$ has a real valued trace, thus $\\Big(\\text{trace}\\big(\\mathbf {AB}\\big)\\Big)^2 = \\big(\\mathbf b^H \\mathbf a\\big)^2 = \\big(\\mathbf a^H \\mathbf b\\big)^2$ is real valued as well.  \n",
    "\n",
    "\n",
    "\n",
    "**claim :**\n",
    "\n",
    "$\\text{trace}\\big(\\mathbf{AB}\\big) \\leq \\frac{1}{2}\\Big(\\text{trace}\\big(\\mathbf A^2 \\big) + \\text{trace}\\big(\\mathbf B^2\\big)\\Big)$\n",
    "\n",
    "**proof:**\n",
    "\n",
    "$\\mathbf C := \\mathbf A - \\mathbf B$\n",
    "\n",
    "$\\mathbf C^H = \\mathbf A^H - \\mathbf B^H = \\mathbf A - \\mathbf B = \\mathbf C$\n",
    "\n",
    "hence $\\mathbf C$ is Hermitian\n",
    "\n",
    "$\\mathbf C^2 = \\mathbf C \\mathbf C = \\mathbf C^H \\mathbf C$\n",
    "\n",
    "hence $\\mathbf C^2$ is Hermitian positive semi-definite, thus its trace must be real valued and $\\geq 0$\n",
    "\n",
    "$\\text{trace}\\big(\\mathbf C^2\\big) = \\text{trace}\\Big( \\big(\\mathbf A - \\mathbf B\\big)^2 \\Big) \\geq 0$\n",
    "\n",
    "$\\text{trace}\\Big( \\big(\\mathbf A - \\mathbf B\\big)^2 \\Big) = \\text{trace}\\Big(\\mathbf A^2 + \\mathbf B^2 - \\mathbf {AB} - \\mathbf {BA}\\Big) \\geq 0$  \n",
    "\n",
    "$\\text{trace}\\Big( \\big(\\mathbf A - \\mathbf B\\big)^2 \\Big) = \\text{trace}\\big(\\mathbf A^2 \\big) + \\text{trace}\\big(\\mathbf B^2\\big) - 2\\cdot \\text{trace}\\big(\\mathbf {AB}\\big) \\geq 0$  \n",
    "\n",
    "$\\text{trace}\\big(\\mathbf A^2 \\big) + \\text{trace}\\big(\\mathbf B^2\\big) \\geq 2\\cdot \\text{trace}\\big(\\mathbf {AB}\\big) $\n",
    "\n",
    "which we can re-arrange as \n",
    "\n",
    "$\\text{trace}\\big(\\mathbf{AB}\\big) \\leq \\frac{1}{2}\\Big(\\text{trace}\\big(\\mathbf A^2 \\big) + \\text{trace}\\big(\\mathbf B^2\\big)\\Big)$\n",
    "- - - -\n",
    "(4.38 and 4.39)\n",
    "\n",
    "consider $\\mathbf A$ which is Hermitian positive definite and $\\mathbf B$ which is Hermitian (both are $n$ x $n$).  \n",
    "\n",
    "- The eigenvalues of $\\big(\\mathbf A \\mathbf B\\big)$ are the same as those in $\\big(\\mathbf A^{\\frac{1}{2}} \\mathbf B\\mathbf A^{\\frac{1}{2}}\\big)$, which is Hermitian, and hence both matrices have all real eigenvalues\n",
    "\n",
    "- The eigenvalues of $\\big(\\mathbf A^{-1} \\mathbf B\\big)$ are the same as those in $\\big(\\mathbf A^{\\frac{-1}{2}} \\mathbf B\\mathbf A^{\\frac{-1}{2}}\\big)$, which is Hermitian, and hence both matrices have all real eigenvalues\n",
    "\n",
    "- This is proven as an extension in \"Vandermonde_Matrices_Permutations_and_Discrete_Fourier_Transform.ipynb\" which states that the matrix given by $\\big(\\mathbf G \\mathbf H\\big)$ has the same non-zero eigenvalues as $\\big(\\mathbf H \\mathbf G\\big)$, and if both matrices are square, then they have the same dimension and hence have the same number of zero eigenvalues as well\n",
    "\n",
    "- $\\big(\\mathbf A + \\mathbf B\\big)$ is positive semi definite **iff** each eigenvalue of $\\big(\\mathbf A^{-1}\\mathbf B\\big)\\geq -1$.  Why?  We examine $\\big(\\mathbf A + \\mathbf B\\big)$ then  multiply left side and right side by $\\mathbf A^{\\frac{-1}{2}}$ which gives us another Hermitian matrix $\\mathbf A^{\\frac{-1}{2}}\\big(\\mathbf A + \\mathbf B\\big)   \\mathbf A^{\\frac{-1}{2}} = \\mathbf I + \\mathbf A^{\\frac{-1}{2}} \\mathbf B \\mathbf A^{\\frac{-1}{2}}  $ -- and Sylvester's Law of Inertia tells us that the number of positive eigenvalues, negative eigenvalues and zero eigenvalues of $\\big(\\mathbf A + \\mathbf B\\big)$ is intact when we look instead at $\\mathbf A^{\\frac{-1}{2}}\\big(\\mathbf A + \\mathbf B\\big)   \\mathbf A^{\\frac{-1}{2}}$.  Thus $\\mathbf A^{\\frac{-1}{2}} \\mathbf B \\mathbf A^{\\frac{-1}{2}}$ must have all eigenvalues being $\\geq -1$ for positive semidefiniteness to remain intact (i.e. they must all be $\\geq 0$ once we increase them by one, which comes from adding the identity matrix to this).  Recall that $\\mathbf A^{\\frac{-1}{2}} \\mathbf B \\mathbf A^{\\frac{-1}{2}}$ has the same eigenvalues as $\\mathbf A^{-1} \\mathbf B$, which completes the problem. \n",
    "\n",
    "- With $\\mathbf A$ positive definite, we know that $\\big(\\mathbf {AB}\\big)$ is diagonalizable because it is similar to $\\mathbf A^{\\frac{1}{2}} \\mathbf B \\mathbf A^{\\frac{1}{2}}$ which is Hermitian and thus must be diagonalizable.  Specifically: where $\\mathbf S = \\mathbf A^{\\frac{1}{2}}$, we can see $\\mathbf S^{-1} \\big(\\mathbf {AB}\\big)\\mathbf S= \\mathbf A^{\\frac{1}{2}} \\mathbf B \\mathbf A^{\\frac{1}{2}}$.  However if $\\mathbf A$ is singular, we lose this similarity transform, and hence guaranties about the diagonalizability of $\\big(\\mathbf {AB}\\big)$ go away.  While your author generally creates random matrices to show existence, in this case he consulted the book for a simple example of defectiveness when multiplying Hermitian positive semidefinite $\\mathbf A$ with Hermitian $\\mathbf B$.  The example is:  $\\begin{bmatrix}\n",
    "1 &1 \\\\ \n",
    " 1& 1\n",
    "\\end{bmatrix} \\begin{bmatrix}\n",
    "1 &0 \\\\ \n",
    " 0& -1\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "1 & -1 \\\\ \n",
    " 1& -1\n",
    "\\end{bmatrix}$, where the right hand side is a rank one matrix with zero trace and hence must be defective.  (See 'Diagonalization_of_rank_one_matrices.ipynb' for more information.) note in the above example we can still see that \n",
    "$\\begin{bmatrix}\n",
    "1 &1 \\\\ \n",
    " 1& 1\n",
    "\\end{bmatrix}^\\frac{1}{2} \\begin{bmatrix}\n",
    "1 &0 \\\\ \n",
    " 0& -1\n",
    "\\end{bmatrix}\\begin{bmatrix}\n",
    "1 &1 \\\\ \n",
    " 1& 1\n",
    "\\end{bmatrix}^\\frac{1}{2} = \\begin{bmatrix}\n",
    "0 & 0 \\\\ \n",
    " 0& 0\n",
    "\\end{bmatrix}$  \n",
    "exists, has the same eigenvalues (i.e. both zero), and hence is Hermitian and hence is diagonalizable, and hence is the zero matrix.  \n",
    "\n",
    "\n",
    "- - - - \n",
    "\n",
    "**SPECIAL NOTE: in the below problem** $\\mathbf A$** is a 'regular matrix' i.e. not Hermitian, etc.**\n",
    "\n",
    "We are, in effect, looking at the two different ways to 'force' $\\mathbf A$ to be Hermitian -- by averaging it with its conjugate transpose, or in effect looking at $\\mathbf A^H \\mathbf A$.  \n",
    "\n",
    "all matrices are $n$ x $n$ and the scalar field is $\\mathbb C$\n",
    "\n",
    "\n",
    "(4.40)\n",
    "**claim:**  \n",
    "\n",
    "$\\lambda_{max}\\Big(\\frac{1}{2}\\big(\\mathbf A + \\mathbf A^H \\big)\\Big) \\leq \\sigma_{max}\\Big(\\mathbf A\\Big)$\n",
    "\n",
    "**proof:**  \n",
    "\n",
    "let $\\mathbf B := \\mathbf A^H$\n",
    "\n",
    "$\\mathbf C := \\mathbf A + \\mathbf B  = \\mathbf A + \\mathbf A^H$\n",
    "\n",
    "note that the $\\mathbf C$ is Hermitian, so its eigenvalues are real.  Its maximal eigenvalue *in magnitude* is equal to its largest singular value.  That is, $\\Big \\vert \\big \\vert \\lambda\\big \\vert_{max} \\big(\\mathbf C\\big)\\Big \\vert = \\sigma_{max}\\big(\\mathbf C\\big)$\n",
    "- - - - \n",
    "*technical note:* $\\Big \\vert \\big \\vert \\lambda\\big \\vert_{max} \\big(\\mathbf C\\big)\\Big \\vert$ either refers to $\\lambda_{1,C}$ or $\\big \\vert \\lambda_{n,C}\\big \\vert$.  If the former has largest magnitude it must be positive and if $\\lambda_{n,C}$ has largest magnitude, it must be negative. \n",
    "\n",
    "with ordering, as always: \n",
    "$\\lambda_{1,C} \\geq \\lambda_{2,C} \\geq \\lambda_{3,C} \\geq ... \\geq \\lambda_{n-1,C} \\geq \\lambda_{n,C}$\n",
    "\n",
    "- - - -\n",
    "\n",
    "note that $\\sigma_{max}\\big(\\mathbf B\\big) = \\sigma_{max}\\big(\\mathbf A\\big)$\n",
    "\n",
    "we know from \"SingularValue_Inequalities.ipynb\" that \n",
    "\n",
    "$\\sigma_{A,1} + \\sigma_{B,1} \\geq \\sigma_{C,1}$\n",
    "\n",
    "or equivalently \n",
    "\n",
    "$\\sigma_{max}\\big(\\mathbf A\\big)+ \\sigma_{max}\\big(\\mathbf B\\big) \\geq \\sigma_{max}\\big(\\mathbf C\\big)$\n",
    "\n",
    "which can be re-written as: \n",
    "\n",
    "$2 \\sigma_{max}\\big(\\mathbf A\\big) \\geq \\sigma_{max}\\big(\\mathbf C\\big) = \\Big \\vert \\big \\vert \\lambda\\big \\vert_{max} \\big(\\mathbf C\\big)\\Big \\vert \\geq \\lambda_{1,C} = \\lambda_{max}\\big(\\mathbf C\\big) $ \n",
    "\n",
    "\n",
    "\n",
    "$\\sigma_{max}\\big(\\mathbf A\\big) \\geq \\frac{1}{2} \\lambda_{max}\\big(\\mathbf C\\big)$\n",
    "\n",
    "$\\sigma_{max}\\big(\\mathbf A\\big) \\geq \\frac{1}{2} \\lambda_{max}\\Big(\\big(\\mathbf A + \\mathbf A^H \\big)\\Big) =  \\lambda_{max}\\Big(\\frac{\\mathbf A + \\mathbf A^H }{2}\\Big)$\n",
    "\n",
    "*alternative proof* \n",
    "\n",
    "This is quite similar to the above (though the singular value inequalities proof page only contemplated real matrices so this a touch different)\n",
    "\n",
    "where $\\big \\Vert \\mathbf x \\big \\Vert_2^2 = 1$ and $\\big \\Vert \\mathbf y \\big \\Vert_2^2 = 1$, $\\big \\Vert \\mathbf z \\big \\Vert_2^2 = 1$ \n",
    "\n",
    "maximize $real\\Big(\\mathbf x^H \\mathbf A \\mathbf x \\Big)$ = maximize $real\\Big(\\mathbf x^H \\mathbf A^H \\mathbf x \\Big)$\n",
    "\n",
    "hence \n",
    "\n",
    "maximize:  \n",
    "$real\\Big(\\mathbf x^H \\mathbf A \\mathbf x \\Big)  = \\frac{1}{2}real\\Big(\\mathbf x^H \\big(\\mathbf A\\big) \\mathbf x \\Big) +\\frac{1}{2} real\\Big(\\mathbf x^H \\big( \\mathbf A^H\\big) \\mathbf x \\Big)  =   \\Big(\\mathbf x^H \\big( \\frac{\\mathbf A  + \\mathbf A^H}{2} \\big) \\mathbf x \\Big) = \\lambda_{max}\\Big( \\frac{\\mathbf A  + \\mathbf A^H}{2}\\Big) $\n",
    "\n",
    "recalling that the scalar result of a quadratic form over a Hermitian matrix -- specifically $\\big( \\frac{\\mathbf A  + \\mathbf A^H}{2} \\big)$ -- must be real.  \n",
    "\n",
    "but we know \n",
    "\n",
    "maximize $real\\Big(\\mathbf x^H \\mathbf A \\mathbf x \\Big) \\leq $ maximize $real\\Big(\\mathbf y^H \\mathbf A \\mathbf z \\Big) = \\sigma_{max}\\big(\\mathbf A\\big) $\n",
    "\n",
    "because we can always get the same 'payoff' on the right hand side by setting $\\mathbf y:=\\mathbf x$ and $ \\mathbf z:= \\mathbf x$, thus the right hand side must have a 'payoff' at least as high as the left hand side\n",
    "\n",
    "hence \n",
    "\n",
    "$\\lambda_{max}\\Big( \\frac{\\mathbf A  + \\mathbf A^H}{2}\\Big) = $ max $real\\Big(\\mathbf x^H \\mathbf A \\mathbf x \\Big) \\leq $max $real \\Big(\\mathbf y^H \\mathbf A \\mathbf z \\Big) = \\sigma_{max}\\big(\\mathbf A\\big)$\n",
    "\n",
    "or more succinctly:  \n",
    "$\\lambda_{max}\\Big( \\frac{\\mathbf A  + \\mathbf A^H}{2}\\Big) \\leq \\sigma_{max}\\big(\\mathbf A\\big)$\n",
    "\n",
    "**subsequent note:**   \n",
    "a better approach uses quasi-linearization and Cauchy-Schwarz (ref CS Masterclass notes).  In effect this is a form of triangle inequality.  \n",
    "\n",
    "where for each $k$, we have the constraint $\\big \\Vert \\mathbf x_k \\big \\Vert_2 = 1$   \n",
    "\n",
    "$\\lambda_{max}\\Big(\\frac{1}{2}\\big(\\mathbf A + \\mathbf A^H \\big)\\Big)$  \n",
    "$= \\frac{1}{2} \\text{max}\\Big\\{ \\mathbf x_0^H \\big(\\mathbf A + \\mathbf A^H \\big)\\mathbf x_0\\Big\\}$  \n",
    "$= \\frac{1}{2} \\text{max}\\Big\\{ \\mathbf x_0^H \\mathbf A \\mathbf x_0 + \\mathbf x_0^H \\mathbf A^H \\mathbf x_0\\Big\\}$  \n",
    "$\\leq \\frac{1}{2}\\Big(\\text{max}\\Big\\{ \\mathbf x_1^H \\mathbf A \\mathbf x_1\\Big\\} + \\text{max}\\Big\\{\\mathbf x_2^H \\mathbf A^H \\mathbf x_2\\Big\\}\\Big)$  \n",
    "$ \\leq \\frac{1}{2}\\Big( \\big \\Vert \\mathbf x_1 \\big \\Vert_2  \\big \\Vert \\mathbf A \\mathbf x_1 \\big \\Vert_2 + \\big \\Vert \\mathbf x_2\\big \\Vert_2 \\big \\Vert \\mathbf A^H \\mathbf x_2\\big \\Vert_2\\Big)$  \n",
    "$ = \\frac{1}{2}\\Big( \\big \\Vert \\mathbf A \\mathbf x_1 \\big \\Vert_2 + \\big \\Vert \\mathbf A^H \\mathbf x_2\\big \\Vert_2\\Big)$  \n",
    "$ = \\frac{1}{2}\\Big(\\sigma_{max}\\big(\\mathbf A\\big) + \\sigma_{max}\\big(\\mathbf A^H\\big)\\Big)$  \n",
    "$ = \\frac{1}{2}\\Big(2\\sigma_{max}\\big(\\mathbf A\\big)\\Big)$  \n",
    "$ = \\sigma_{max}\\big(\\mathbf A\\big)$  \n",
    "\n",
    "where the first inequality follows because 2 choices are better than one, and the second inequality is Cauchy Schwarz (though in case of the operator 2 norm / top singular value, this inequality is in fact met with equality).      \n",
    "\n",
    "\n",
    "**end subsequent note:**  \n",
    "\n",
    "\n",
    "**claim:**  \n",
    "\n",
    "$\\text{trace}\\Big(\\big(\\frac{\\mathbf A + \\mathbf A^H }{2}\\big)^2\\Big) \\leq \\text{trace}\\Big(\\mathbf A^H \\mathbf A\\Big)$\n",
    "\n",
    "**proof:**  \n",
    "\n",
    "$\\text{trace}\\Big(\\big(\\frac{\\mathbf A + \\mathbf A^H }{2}\\big)^2\\Big) = \\frac{1}{4} \\text{trace}\\Big(\\mathbf A^2 + \\big(\\mathbf A^H\\big)^2 +\\mathbf A^H \\mathbf A + \\mathbf A \\mathbf A^H\\Big) \\leq \\text{trace}\\Big(\\mathbf A^H \\mathbf A\\Big)$\n",
    "\n",
    "re-write this as\n",
    "\n",
    "$\\text{trace}\\Big(\\big(\\frac{\\mathbf A + \\mathbf A^H }{2}\\big)^2\\Big) = \\frac{1}{4}\\text{trace}\\Big(\\mathbf A^2 + \\big(\\mathbf A^H\\big)^2\\Big) + \\frac{1}{4} \\text{trace}\\Big( \\mathbf A^H \\mathbf A\\Big) +  \\frac{1}{4} \\text{trace}\\Big(\\mathbf A \\mathbf A^H\\Big)  \\leq \\text{trace}\\Big(\\mathbf A^H \\mathbf A\\Big)$\n",
    "\n",
    "$\\text{trace}\\Big(\\big(\\frac{\\mathbf A + \\mathbf A^H }{2}\\big)^2\\Big)= \\frac{1}{4}\\text{trace}\\Big(\\mathbf A^2 + \\big(\\mathbf A^H\\big)^2\\Big) + \\frac{1}{2}\\text{trace}\\Big(\\mathbf A^H \\mathbf A\\Big) \\leq \\text{trace}\\Big(\\mathbf A^H \\mathbf A\\Big)$\n",
    "\n",
    "Thus if we can prove:\n",
    "\n",
    "$\\frac{1}{4}\\text{trace}\\Big(\\mathbf A^2 + \\big(\\mathbf A^H\\big)^2\\Big) + \\frac{1}{2}\\text{trace}\\Big(\\mathbf A^H \\mathbf A\\Big) \\leq \\text{trace}\\Big(\\mathbf A^H \\mathbf A\\Big)$\n",
    "\n",
    "then we are done.  Let's simplify this to\n",
    "\n",
    "$\\frac{1}{4}\\text{trace}\\Big(\\mathbf A^2 + \\big(\\mathbf A^H\\big)^2\\Big) \\leq \\frac{1}{2} \\text{trace}\\Big(\\mathbf A^H \\mathbf A\\Big)$\n",
    "\n",
    "multiply both sides by 2\n",
    "\n",
    "$\\frac{1}{2}\\Big(\\text{trace}\\big(\\mathbf A^2 \\big) + \\text{trace}\\Big(\\big(\\mathbf A^H\\big)^2\\Big)\\Big) \\leq \\text{trace}\\Big(\\mathbf A^H \\mathbf A\\Big)$\n",
    "\n",
    "- - - -\n",
    "note that, except for complex conjugation (which offset each other),\n",
    "\n",
    "$\\text{trace}\\big(\\mathbf A^2 \\big) = \\text{trace}\\Big(\\big(\\mathbf A^H\\big)^2\\Big)$\n",
    "\n",
    "To be exact, we'd say:  \n",
    "\n",
    "$real\\Big(\\text{trace}\\big(\\mathbf A^2 \\big)\\Big) = real\\Big(\\text{trace}\\Big(\\big(\\mathbf A^H\\big)^2\\Big)\\Big)$\n",
    "\n",
    "Thus \n",
    "\n",
    "$real\\Big(\\text{trace}\\big(\\mathbf A^2 \\big) \\Big) +real\\Big( \\text{trace}\\Big(\\big(\\mathbf A^H\\big)^2\\Big)\\Big) = 2\\cdot real\\Big(\\text{trace}\\big(\\mathbf A^2 \\big) \\Big) \\leq 2\\big \\vert \\text{trace}\\big(\\mathbf A^2 \\big) \\big \\vert$\n",
    "\n",
    "- - - -\n",
    "we return to our to our main inequality, and apply the above fact (inclusive of the $\\frac{1}{2}$ scaling)\n",
    "\n",
    "$\\frac{1}{2}\\Big(\\text{trace}\\big(\\mathbf A^2 \\big) + \\text{trace}\\Big(\\big(\\mathbf A^H\\big)^2\\Big)\\Big) = \\frac{2}{2} real\\Big(\\text{trace}\\big(\\mathbf A^2 \\big) \\Big) \\leq \\frac{2}{2}\\big \\vert \\text{trace}\\big(\\mathbf A^2 \\big) \\big \\vert = \\big \\vert \\text{trace}\\big(\\mathbf {AA} \\big)\\big \\vert \\leq  \\text{trace}\\Big(\\mathbf A^H \\mathbf A\\Big)$\n",
    " \n",
    "and we confirm that \n",
    "\n",
    "$\\big \\vert \\text{trace}\\big(\\mathbf {AA} \\big) \\big \\vert \\leq  \\text{trace}\\Big(\\mathbf A^H \\mathbf A\\Big)$\n",
    "\n",
    "via the expanded Schur Inequality (see the writeup in \"Schurs_Inequality.ipynb\").  This completes the proof.\n",
    "\n",
    "**extension**\n",
    "\n",
    "The last claim was \n",
    "\n",
    "that \n",
    "\n",
    "$\\text{trace}\\Big(\\big(\\frac{\\mathbf A + \\mathbf A^H }{2}\\big)^2\\Big) \\leq \\text{trace}\\Big(\\mathbf A^H \\mathbf A\\Big)$\n",
    "\n",
    "but what if we wanted the square root of the above, before taking the trace?  \n",
    "\n",
    "where $\\mathbf X$ is the positive square root of the right hand side? i.e.  \n",
    "\n",
    "$\\mathbf X:= \\Big(\\mathbf A^H \\mathbf A\\Big)^{\\frac{1}{2}}$\n",
    "\n",
    "and for convenience\n",
    "\n",
    "$\\mathbf Y:= \\big(\\frac{\\mathbf A + \\mathbf A^H }{2}\\big)$\n",
    "\n",
    "is it true that \n",
    "\n",
    "$\\text{trace}\\Big(\\mathbf Y \\Big) \\leq \\text{trace}\\Big(\\mathbf X\\Big)$\n",
    "\n",
    "yes.  And noting that the eigenvalues and singular values of $\\big(\\mathbf Y\\big)$ are all real, and identical, except that the former may have a negative sign in front of them, we'll prove an even stronger claim:\n",
    "\n",
    "**claim**\n",
    "\n",
    "$\\sigma_{1,Y} + \\sigma_{2,Y} + ... + \\sigma_{n,Y} \\leq \\sigma_{1,X} + \\sigma_{2,X} + ... + \\sigma_{n,X} $\n",
    "\n",
    "or equivalently \n",
    "\n",
    "$\\sum_{k=1}^n \\sigma_k \\Big(\\big(\\frac{\\mathbf A + \\mathbf A^H }{2}\\big)\\Big) \\leq \\sum_{k=1}^n \\sigma_k \\Big(\\mathbf A\\Big) $\n",
    "\n",
    "where $\\sigma_k \\Big(\\mathbf Z\\Big)$ denotes the kth singular value of some matrix $\\mathbf Z$\n",
    "\n",
    "**proof**\n",
    "\n",
    "putting these interesting things together, we have:\n",
    "\n",
    "$\\text{trace}\\Big(\\mathbf Y \\Big) = \\lambda_{1,Y} + \\lambda_{2,Y} + ... + \\lambda_{n,Y} \\leq \\sigma_{1,Y} + \\sigma_{2,Y} + ... + \\sigma_{n,Y} \\leq \\sigma_{1,X} + \\sigma_{2,X} + ... + \\sigma_{n,X} =  \\sum_{k=1}^n \\sigma_k \\Big(\\mathbf A\\Big)= \\text{trace}\\Big(\\mathbf X\\Big)$\n",
    "\n",
    "where  \n",
    "$\\lambda_{1,Y} + \\lambda_{2,Y} + ... + \\lambda_{n,Y} \\leq \\big\\vert\\lambda_{1,Y}\\big\\vert + \\big\\vert\\lambda_{2,Y}\\big\\vert + ... + \\big\\vert\\lambda_{n,Y}\\big\\vert = \\sigma_{1,Y} + \\sigma_{2,Y} + ... + \\sigma_{n,Y}$  \n",
    "by the triangle inequality.  \n",
    "\n",
    "The proof is simply that \n",
    "\n",
    "$\\sigma_{1,Y} + \\sigma_{2,Y} + ... + \\sigma_{n,Y} \\leq \\frac{1}{2}\\big(\\sigma_{1,A} + \\sigma_{2,A} + ... + \\sigma_{n,A}\\big) + \\frac{1}{2}\\big(\\sigma_{1,A} + \\sigma_{2,A} + ... + \\sigma_{n,A}\\big)= \\sigma_{1,A} + \\sigma_{2,A} + ... + \\sigma_{n,A} = \\sum_{k=1}^n \\sigma_k \\Big(\\mathbf A\\Big) $\n",
    "\n",
    "where we use the \"L1 norm for singular values\" contained in \"SingularValue_Inequalities.ipynb\"\n",
    "\n",
    "and for avoidance of doubt, our original equation is:\n",
    "\n",
    "$\\mathbf Y = \\frac{1}{2} \\mathbf A + \\frac{1}{2}\\mathbf A^H$\n",
    "\n",
    "but in the format of that proof we'd have $\\mathbf C:= \\mathbf Y$ and $\\mathbf A := \\frac{1}{2} \\mathbf A$ and $\\mathbf B : = \\frac{1}{2}\\mathbf A^H$\n",
    "\n",
    "noting that the singular values of $\\mathbf B$ (and our 'new' $\\mathbf A$) are exactly half of the singular values of our 'original' $\\mathbf A$.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "*Quadratic form equivalence between Hermitian matrices*   \n",
    "**claim:**   \n",
    "Two $\\text{n}$ x $\\text{n}$  \n",
    "Hermitian matrices $\\mathbf A$ and $\\mathbf B$ are equal *iff* \n",
    "\n",
    "$\\mathbf x^H \\mathbf A\\mathbf x = \\mathbf x^H \\mathbf B\\mathbf x$   \n",
    "for all $\\mathbf x \\in \\mathbb C^n$   \n",
    "\n",
    "**proof:** \n",
    "\n",
    "the first leg is easy:  \n",
    "if $\\mathbf A = \\mathbf B$ then \n",
    "$\\mathbf A\\mathbf x = \\mathbf B\\mathbf x$  \n",
    "and  \n",
    "$\\mathbf x^H \\mathbf A\\mathbf x = \\mathbf x^H \\mathbf B\\mathbf x$   \n",
    "\n",
    "the second leg:  \n",
    "we may prove this two different ways.  First, using extremal characterization and second algebraicly.  \n",
    "\n",
    "(i)  \n",
    "suppose we 'only' know that $\\mathbf x^H \\mathbf A\\mathbf x = \\mathbf x^H \\mathbf B\\mathbf x$   for all  \n",
    "$\\mathbf x \\in \\mathbb C^n$  \n",
    "\n",
    "now consider the Hermitian matrix $\\big(\\mathbf A - \\mathbf B\\big)$.  We'll prove this is the zero matrix.  Using the extremal characterization of eigenvalues (or equivalently: a compactness argument) we have   \n",
    "\n",
    "$\\lambda_1 = \\max_{\\mathbf x} \\mathbf x^H\\big(\\mathbf A - \\mathbf B\\big)\\mathbf x = \\max_{\\mathbf x} \\big\\{\\mathbf x^H\\mathbf A\\mathbf x - \\mathbf x^H\\mathbf B\\mathbf x\\big\\} = 0 $   \n",
    "where $\\big \\Vert \\mathbf x \\big \\Vert_2 = 1$  \n",
    "\n",
    "$\\lambda_n = \\min_{\\mathbf x} \\mathbf x^H\\big(\\mathbf A - \\mathbf B\\big)\\mathbf x = \\min_{\\mathbf x} \\big\\{\\mathbf x^H\\mathbf A\\mathbf x - \\mathbf x^H\\mathbf B\\mathbf x\\big\\} = 0 $  \n",
    "where $\\big \\Vert \\mathbf x \\big \\Vert_2 = 1$  \n",
    "so we have  \n",
    "$0=\\lambda_1 \\geq \\lambda_2 \\geq .... \\geq \\lambda_n = 0$  \n",
    "\n",
    "but this implies \n",
    "$\\mathbf 0 = \\big(\\mathbf A - \\mathbf B\\big)$   \n",
    "because the RHS is diagonalizable, with all eigenvalues equal to zero, so it must be the zero matrix, which proves $\\mathbf A = \\mathbf B$  \n",
    "\n",
    "(ii) algebraic proof of the second leg   \n",
    "\n",
    "we can assume WLOG that $\\mathbf B$ is diagonal \n",
    "(if it is not diagonal, collect its eigenvectors in orthogonal matrix $\\mathbf U$ and for ever $\\mathbf x$ of interest, we may instead use $\\mathbf {Ux}$) \n",
    "\n",
    "It is most natural to assume the diagonal elements of $\\mathbf B$ and hence its eigenvalues are ordered \n",
    "$\\lambda_1 \\geq \\lambda_2 \\geq ... \\geq \\lambda_n$  \n",
    "\n",
    "There are slightly faster ways to finish this, but for convenience first consider the over each standard basis vector $\\mathbf e_k$ for $k \\in \\{1, 2, ..., n\\}$  \n",
    "\n",
    "$\\mathbf e_k^H \\mathbf A\\mathbf e_k = \\mathbf e_k^H \\mathbf B\\mathbf e_k = \\lambda_{k}$     \n",
    "\n",
    "which ensures that each diagonal component of $\\mathbf A$ is the same as each diagonal component of $\\mathbf B$\n",
    "\n",
    "we can equivalently write this as  \n",
    "$\\text{trace}\\big(\\mathbf A\\mathbf e_k\\mathbf e_k^H\\big)  = \\text{trace}\\big(\\mathbf B\\mathbf e_k\\mathbf e_k^H\\big) = \\lambda_{k}$     \n",
    "\n",
    "\n",
    "now consider  \n",
    "$\\text{trace}\\big(\\mathbf A\\mathbf e_k\\mathbf e_k^H\\lambda_{k}\\big)  = \\text{trace}\\big(\\mathbf B\\mathbf e_k\\mathbf e_k^H\\lambda_{k}\\big)$     \n",
    "\n",
    "and sum over all $k$ to get \n",
    "\n",
    "$\\text{trace}\\big(\\mathbf {A B}\\big)$  \n",
    "$= \\text{trace}\\big(\\mathbf A \\sum_{k=1}^n \\mathbf e_k\\mathbf e_k^H\\lambda_{k}\\big) $  \n",
    "$= \\sum_{k=1}^n \\text{trace}\\big(\\mathbf A\\mathbf e_k\\mathbf e_k^H\\lambda_{k}\\big)  $  \n",
    "$= \\sum_{k=1}^n \\text{trace}\\big(\\mathbf B\\mathbf e_k\\mathbf e_k^H\\lambda_{k}\\big) $  \n",
    "$= \\sum_{k=1}^n \\lambda_k^2 $  \n",
    "$= \\text{trace}\\big(\\mathbf B^2\\big)$     \n",
    "\n",
    "running the same argument with the mutually orthonormal eigenvectors of $\\mathbf A$ yields  \n",
    "\n",
    "$\\text{trace}\\big(\\mathbf {A B}\\big)$  \n",
    "$= \\text{trace}\\big(\\mathbf {B A}\\big)$  \n",
    "$= \\text{trace}\\big(\\mathbf B \\sum_{k=1}^n \\mathbf v_k\\mathbf v_k^H \\gamma_{k}\\big) $  \n",
    "$= \\sum_{k=1}^n \\text{trace}\\big(\\mathbf B\\mathbf v_k\\mathbf v_k^H\\gamma_{k}\\big) $  \n",
    "$= \\sum_{k=1}^n \\text{trace}\\big(\\mathbf A\\mathbf v_k\\mathbf v_k^H\\gamma_{k}\\big) $  \n",
    "$= \\sum_{k=1}^n \\gamma_k^2 $  \n",
    "$= \\text{trace}\\big(\\mathbf A^2\\big)$   \n",
    "\n",
    "(note the above implies that if either matrix is the zero matrix then both are the zero matrix and we have the desired equality, the below assumes that both matrices are non-zero to finish the proof)  \n",
    "\n",
    "and summing these two gives  \n",
    "\n",
    "$2\\cdot \\text{trace}\\big(\\mathbf {A B}\\big) =  \\text{trace}\\big(\\mathbf A^2\\big) + \\text{trace}\\big(\\mathbf B^2\\big)  = \\big \\Vert \\mathbf A\\big \\Vert_F^2 + \\big \\Vert \\mathbf B\\big \\Vert_F^2 $  \n",
    "\n",
    "we recognize  \n",
    "\n",
    "$0 \\leq \\big \\Vert \\mathbf A -  \\mathbf B\\big \\Vert_F^2 $  \n",
    "with equality *iff* $\\mathbf A =\\mathbf B$  \n",
    "\n",
    "but if we expand this and make use of Hermicity and cyclic property of the trace, we get  \n",
    "\n",
    "$ 2\\cdot \\text{trace}\\big(\\mathbf {A B}\\big) \\leq \\big \\Vert \\mathbf A\\big \\Vert_F^2 + \\big \\Vert \\mathbf B\\big \\Vert_F^2  $  \n",
    "\n",
    "again, with equality *iff* $\\mathbf A =\\mathbf B$  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hermitian Positive Semi Definite Trace Inequalities\n",
    "\n",
    "where $\\mathbf A$ and $\\mathbf B$ are both Hermitian Positive (Semi) Definite Matrices\n",
    "\n",
    "**claim:** \n",
    "\n",
    "$\\text{trace}\\big(\\mathbf{AB}\\big) \\geq 0$ \n",
    "\n",
    "that is, the the above trace is always real and non-negative.  \n",
    "\n",
    "**proof:**\n",
    "\n",
    "$\\text{trace}\\big(\\mathbf{AB}\\big) = \\text{trace}\\big(\\mathbf A^{\\frac{1}{2}}\\mathbf A^{\\frac{1}{2}} \\mathbf B^{\\frac{1}{2}}\\mathbf B^{\\frac{1}{2}}\\big)= \\text{trace}\\big(\\mathbf B^{\\frac{1}{2}}\\mathbf A^{\\frac{1}{2}} \\mathbf A^{\\frac{1}{2}}\\mathbf B^{\\frac{1}{2}}\\big) = \\text{trace}\\Big(\\big(\\mathbf B^{\\frac{1}{2}}\\big)^H \\big(\\mathbf A^{\\frac{1}{2}}\\big)^H \\mathbf A^{\\frac{1}{2}}\\mathbf B^{\\frac{1}{2}}\\Big) = \\text{trace}\\Big(\\big(\\mathbf A^{\\frac{1}{2}} \\mathbf B^{\\frac{1}{2}}\\big)^H \\big( \\mathbf A^{\\frac{1}{2}}\\mathbf B^{\\frac{1}{2}}\\big)\\Big)$ \n",
    "\n",
    "$\\text{trace}\\big(\\mathbf{AB}\\big) = \\big \\Vert \\mathbf A^{\\frac{1}{2}} \\mathbf B^{\\frac{1}{2}}\\big \\Vert_F^2$\n",
    "\n",
    "Alternatively, we could also note that $\\big(\\mathbf{AB}\\big) = \\Big(\\mathbf A^{\\frac{1}{2}}\\big(\\mathbf A^{\\frac{1}{2}} \\mathbf B\\big)\\Big)$  and must have the same eigenvalues as $\\Big(\\big(\\mathbf A^{\\frac{1}{2}} \\mathbf B\\big)\\mathbf A^{\\frac{1}{2}}\\Big)$ which is Hermitian Positive semi-definite, and hence has all real, non-negative eigenvalues.  Since the trace gives the sum of those eigenvalues, the trace must be real, non-negative.  \n",
    "\n",
    "\n",
    "**claim:**\n",
    "\n",
    "$\\text{trace}\\big(\\mathbf{AB}\\big) \\leq \\text{trace}\\big(\\mathbf A\\big) \\text{trace}\\big(\\mathbf B \\big) \\leq \\frac{1}{2}\\Big(\\text{trace}\\big(\\mathbf A\\big)^2 + \\text{trace}\\big(\\mathbf B\\big)^2\\Big)$\n",
    "\n",
    "**commentary:**\n",
    "\n",
    "The left side of the inequality is of particular interest.  \n",
    "\n",
    "$\\text{trace}\\big(\\mathbf{AB}\\big) \\leq \\text{trace}\\big(\\mathbf A\\big) \\text{trace}\\big(\\mathbf B \\big) $\n",
    "\n",
    "can be rewritten as:\n",
    "\n",
    "$\\text{trace}\\big(\\mathbf{AB}\\big) \\leq \\big(\\sum_{i = 1}^{n} \\mathbf{A}_{i,i}\\big) \\big(\\sum_{i = 1}^{n} \\mathbf{B}_{i,i}\\big)$\n",
    "\n",
    "since $\\mathbf A$ and $\\mathbf B$ are both Hermitian positive semi-definite, we may recall the Hadamard Inequality (see \"HadamardInequality.ipynb\"), and draw an analogy with determinants.\n",
    "\n",
    "noting that $\\text{det}\\big(\\mathbf{AB}\\big) = \\text{det}\\big(\\mathbf{A}\\big) \\text{det}\\big(\\mathbf{B}\\big)$,\n",
    "\n",
    "we use the Hadarmard inequality:  \n",
    "\n",
    "$\\text{det}\\big(\\mathbf{A}\\big) \\leq \\big(\\prod_{i = 1}^{n} \\mathbf{A}_{i,i}\\big)$  \n",
    "$\\text{det}\\big(\\mathbf{B}\\big) \\leq \\big(\\prod_{i = 1}^{n} \\mathbf{B}_{i,i}\\big)$  \n",
    "\n",
    "hence \n",
    "\n",
    "$\\text{det}\\big(\\mathbf{AB}\\big) \\leq \\big(\\prod_{i = 1}^{n} \\mathbf{A}_{i,i}\\big) \\big(\\prod_{i = 1}^{n} \\mathbf{B}_{i,i}\\big)$\n",
    "\n",
    "which seems analogous  to  \n",
    "\n",
    "$\\text{trace}\\big(\\mathbf{AB}\\big) \\leq \\big(\\sum_{i = 1}^{n} \\mathbf{A}_{i,i}\\big) \\big(\\sum_{i = 1}^{n} \\mathbf{B}_{i,i}\\big)$\n",
    "\n",
    "**proof:**  \n",
    "\n",
    "*For the left side of the the inequality:*  \n",
    "\n",
    "$ \\text{trace}\\big(\\mathbf{AB}\\big) = \\big \\Vert \\mathbf A^{\\frac{1}{2}} \\mathbf B^{\\frac{1}{2}}\\big \\Vert_F^2 \\leq \\big \\Vert \\mathbf A^{\\frac{1}{2}}\\big \\Vert_F^2 \\big \\Vert \\mathbf B^{\\frac{1}{2}}\\big \\Vert_F^2 =  \\text{trace}\\Big(\\big(\\mathbf A^{\\frac{1}{2}}\\big)^H \\mathbf A^{\\frac{1}{2}}\\Big) \\text{trace}\\Big(\\big(\\mathbf B^{\\frac{1}{2}}\\big)^H \\mathbf B^{\\frac{1}{2}}\\Big) = \\text{trace}\\big(\\mathbf A\\big) \\text{trace}\\big(\\mathbf B \\big) $\n",
    "\n",
    "where we use the derivation under \"Matrix Norms\" for the proof that \n",
    "\n",
    " $\\big \\Vert \\mathbf A^{\\frac{1}{2}} \\mathbf B^{\\frac{1}{2}}\\big \\Vert_F^2 \\leq \\big \\Vert \\mathbf A^{\\frac{1}{2}}\\big \\Vert_F^2 \\big \\Vert \\mathbf B^{\\frac{1}{2}}\\big \\Vert_F^2 $\n",
    "\n",
    "- - - -\n",
    "*begin alternative proof*\n",
    "\n",
    "Consider that for Hermitian Positive (Semi) Definite matrices, we have strictly real, non-negative values along the diagonal of said matrices and amongst their eigenvalues.  \n",
    "\n",
    "Thus we could interpret this inequality as multiplying two finite series, and noting that \n",
    "\n",
    "\n",
    "$(\\lambda_1 + \\lambda_2 +... + \\lambda_n)(\\sigma_1 + \\sigma_2 + ... + \\sigma_n) \\geq \\lambda_1 \\sigma_1 + \\lambda_2 \\sigma_2 +... + \\lambda_n \\sigma_n$\n",
    "\n",
    "because every term in the series is real and non-negative\n",
    "\n",
    "To map this to our problem simply let:  \n",
    "$\\mathbf A = \\mathbf {Q \\Lambda Q}^H$   \n",
    "$\\text{trace}\\big(\\mathbf A\\big) = \\lambda_1 + \\lambda_2 +... + \\lambda_n$   \n",
    "$\\text{trace}\\big(\\mathbf B\\big) = \\sigma_1 + \\sigma_2 + ... + \\sigma_n$  \n",
    "\n",
    "$\\text{trace}\\big(\\mathbf {AB}\\big) = \\text{trace}\\big(\\mathbf {Q\\Lambda Q}^H \\mathbf B\\big) = \\text{trace}\\big(\\mathbf {\\Lambda Q}^H \\mathbf B\\mathbf Q\\big)$  \n",
    "\n",
    "Now define a new matrix $\\mathbf C := \\mathbf Q^H \\mathbf{BQ}$, which is still Hermitian, and similar to $\\mathbf B$, and hence Positive Semi Definite.  We see that \n",
    "\n",
    "$\\text{trace}\\big(\\mathbf {AB}\\big) = \\text{trace}\\big(\\mathbf {\\Lambda C}\\big) $\n",
    "\n",
    "\n",
    "$\\text{trace}\\big(\\mathbf {\\Lambda C}\\big)= \\lambda_1 c_{1,1} + \\lambda_2 c_{2,2} + ... + \\lambda_n c_{n,n} \\leq (\\lambda_1 + \\lambda_2 +... + \\lambda_n)(c_{1,1} + c_{2,2} + ... + c_{n,n}) = \\text{trace}\\big(\\mathbf A\\big) \\text{trace}\\big(\\mathbf C\\big) $\n",
    "\n",
    "Noting that $\\mathbf B$ and $\\mathbf C$ are similar, and hence have the same trace:\n",
    "\n",
    "$\\text{trace}\\big(\\mathbf {AB}\\big) = \\text{trace}\\big(\\mathbf {\\Lambda C}\\big)  \\leq \\text{trace}\\big(\\mathbf A\\big) \\text{trace}\\big(\\mathbf C\\big)= \\text{trace}\\big(\\mathbf A\\big)\\text{trace}\\big(\\mathbf B\\big)$ \n",
    "\n",
    "\n",
    "*end alternative proof*\n",
    "- - - -\n",
    "\n",
    "*For the right hand side of the inequality:*  \n",
    "\n",
    "$\\big(\\mathbf A - \\mathbf B\\big)$ is a Hermitian matrix, and hence its trace given by  \n",
    "\n",
    "$\\text{trace}\\big(\\mathbf A - \\mathbf B \\big) = \\text{trace}\\big(\\mathbf A\\big) - \\text{trace}\\big(\\mathbf B \\big)$\n",
    "\n",
    "is a real number.  Whenever we square a real number, the result must be $\\geq 0$\n",
    "\n",
    "\n",
    "$\\Big(\\text{trace}\\big(\\mathbf A\\big) - \\text{trace}\\big(\\mathbf B \\big)\\Big)^2 \\geq 0 $  \n",
    "$\\text{trace}\\big(\\mathbf A\\big)^2 + \\text{trace}\\big(\\mathbf B \\big)^2 - 2 \\cdot \\text{trace}\\big(\\mathbf A\\big)\\text{trace}\\big(\\mathbf B\\big) \\geq 0$\n",
    "\n",
    "hence we see that  \n",
    "$\\frac{1}{2}\\text{trace}\\big(\\mathbf A\\big)^2 + \\frac{1}{2} \\text{trace}\\big(\\mathbf B \\big)^2 \\geq \\text{trace}\\big(\\mathbf A\\big)\\text{trace}\\big(\\mathbf B\\big) $\n",
    "\n",
    "This proves that \n",
    "\n",
    "$\\text{trace}\\big(\\mathbf{AB}\\big) \\leq \\text{trace}\\big(\\mathbf A\\big) \\text{trace}\\big(\\mathbf B \\big) \\leq \\frac{1}{2}\\Big(\\text{trace}\\big(\\mathbf A\\big)^2 + \\text{trace}\\big(\\mathbf B\\big)^2\\Big)$\n",
    "\n",
    "**claim:**  \n",
    "\n",
    "$\\text{trace}\\big(\\mathbf{AB}\\big) \\leq \\lambda_{max}\\big(\\mathbf A\\big) \\text{trace}\\big(\\mathbf B\\big)$\n",
    "\n",
    "**proof:**  \n",
    "\n",
    "for convenience we start by noting $\\text{trace}\\big(\\mathbf{AB}\\big) = \\text{trace}\\big(\\mathbf{BA}\\big)$\n",
    "\n",
    "$\\text{trace}\\big(\\mathbf{BA}\\big) = \\big \\Vert \\mathbf B^{\\frac{1}{2}} \\mathbf A^{\\frac{1}{2}}\\big \\Vert_F^2 \\leq  \\big \\Vert \\mathbf A^{\\frac{1}{2}} \\big \\Vert_2^2 \\big \\Vert\\mathbf B^{\\frac{1}{2}} \\big \\Vert_F^2 = \\sigma_{A,1} \\cdot \\text{trace}\\Big(\\big(\\mathbf B^{\\frac{1}{2}}\\big)^H \\mathbf B^{\\frac{1}{2}} \\Big) = \\lambda_{A,1}\\cdot \\text{trace}\\big(\\mathbf B^{\\frac{1}{2}} \\mathbf B^{\\frac{1}{2}} \\big) = \\lambda_{max}\\big(\\mathbf A\\big) \\text{trace}\\big(\\mathbf B\\big)$  \n",
    "\n",
    "using results from the \"Matrix Norms\" section to justify the inequality, and noting that the largest squared singular value of $\\mathbf A^{\\frac{1}{2}}$ is the largest singular value of $\\mathbf A$ which equals  $\\sigma_{A,1}$.  Also noticing that because $\\mathbf A$ is Hermitian positive semi-definite, its singular values are equal to its eigenvalues.  \n",
    "\n",
    "*begin alternative proof*\n",
    "\n",
    "leveraging the preceding alternative proof, note that\n",
    "\n",
    "$\\text{trace}\\big(\\mathbf {A B}\\big) = \\text{trace}\\big(\\mathbf {\\Lambda C}\\big)= \\lambda_1 c_{1,1} + \\lambda_2 c_{2,2} + ... + \\lambda_n c_{n,n} \\leq \\lambda_1 (c_{1,1} + c_{2,2} + ... + c_{n,n}) = \\lambda_1 \\text{trace}\\big(\\mathbf B\\big) $\n",
    "\n",
    "recalling that the eigenvalues of $\\mathbf A$ are ordered such that $\\lambda_1 \\geq \\lambda_2 \\geq .... \\geq \\lambda_n \\geq 0$, each diagonal element of $\\mathbf C$ is real valued, non-negative, and $\\text{trace}\\big(\\mathbf B\\big) = \\text{trace}\\big(\\mathbf C\\big)$.\n",
    "\n",
    "\n",
    "*end alternative proof*  \n",
    "**claim:**  \n",
    "\n",
    "\n",
    "$\\text{trace}\\big(\\mathbf{AB}\\big) \\leq \\frac{1}{4} \\Big( \\text{trace}\\big(\\mathbf A\\big)+ \\text{trace}\\big(\\mathbf B\\big)\\Big)^2 $\n",
    "\n",
    "**commentary:**  \n",
    "\n",
    "This is a simple extension of an earlier inequality: \n",
    "\n",
    "$\\text{trace}\\big(\\mathbf{AB}\\big) \\leq \\text{trace}\\big(\\mathbf A\\big) \\text{trace}\\big(\\mathbf B \\big) \\leq \\frac{1}{2}\\Big(\\text{trace}\\big(\\mathbf A\\big)^2 + \\text{trace}\\big(\\mathbf B\\big)^2\\Big)$\n",
    "\n",
    "**proof:**  \n",
    "we start off with  \n",
    "$2 \\cdot \\text{trace}\\big(\\mathbf{AB}\\big) \\leq \\text{trace}\\big(\\mathbf A\\big)^2 + \\text{trace}\\big(\\mathbf B\\big)^2$\n",
    "\n",
    "add $2\\cdot \\text{trace}\\big(\\mathbf A\\big)\\text{trace}\\big(\\mathbf B\\big)$ to both sides, noticing that  $\\text{trace}\\big(\\mathbf{AB}\\big) \\leq \\text{trace}\\big(\\mathbf A\\big)\\text{trace}\\big(\\mathbf B\\big)$,  we have   \n",
    "\n",
    "$4 \\cdot \\text{trace}\\big(\\mathbf{AB}\\big) \\leq 2 \\cdot \\text{trace}\\big(\\mathbf{AB}\\big) + 2\\cdot \\text{trace}\\big(\\mathbf A\\big)\\text{trace}\\big(\\mathbf B\\big) \\leq \\text{trace}\\big(\\mathbf A\\big)^2 + \\text{trace}\\big(\\mathbf B\\big)^2 + 2\\cdot \\text{trace}\\big(\\mathbf A\\big)\\text{trace}\\big(\\mathbf B\\big)$\n",
    "\n",
    "\n",
    "Thus we have  \n",
    "$4 \\cdot \\text{trace}\\big(\\mathbf{AB}\\big) \\leq  \\text{trace}\\big(\\mathbf A\\big)^2 + \\text{trace}\\big(\\mathbf B\\big)^2 + 2\\cdot \\text{trace}\\big(\\mathbf A\\big)\\text{trace}\\big(\\mathbf B\\big) = \\Big(\\text{trace}\\big(\\mathbf A\\big) + \\text{trace}\\big(\\mathbf B\\big)\\Big)^2$\n",
    "\n",
    "giving us  \n",
    "$\\text{trace}\\big(\\mathbf{AB}\\big) \\leq  \\frac{1}{4}\\Big(\\text{trace}\\big(\\mathbf A\\big) + \\text{trace}\\big(\\mathbf B\\big)\\Big)^2$\n",
    "\n",
    "**claim:**  \n",
    "\n",
    "$\\big \\vert \\text{trace}\\big(\\mathbf{UA}\\big)\\big \\vert \\leq \\text{trace}\\big(\\mathbf A\\big)$ \n",
    "\n",
    "Where both matrices are $n$ x $n$, and $\\mathbf A$ is Hermitian positive semi-definite, and $\\mathbf U$ is unitary.\n",
    "\n",
    "**proof:**  \n",
    "\n",
    "since $\\mathbf U$ is unitary, it is also normal (see Schur Inequality notebook) and is unitarily diagonalizable as $\\mathbf U = \\mathbf{VDV}^H$\n",
    "\n",
    "$\\big \\vert \\text{trace}\\big(\\mathbf{UA}\\big)\\big \\vert = \\big \\vert \\text{trace}\\big(\\mathbf{VDV}^H \\mathbf  A\\big)\\big \\vert = \\big \\vert \\text{trace}\\big(\\mathbf{DV}^H \\mathbf  A\\mathbf V \\big)\\big \\vert \\leq \\text{trace}\\big(\\mathbf A\\big)$ \n",
    "\n",
    "let $\\mathbf B := \\mathbf V^H \\mathbf {AV}$\n",
    "\n",
    "$\\big \\vert \\text{trace}\\big(\\mathbf{UA}\\big)\\big \\vert = \\big \\vert \\text{trace}\\big(\\mathbf{DB}\\big)\\big \\vert \\leq  \\text{trace}\\big(\\mathbf B\\big) = \\text{trace}\\big(\\mathbf A\\big)$ \n",
    "\n",
    "here we simply observe, by direct application of triangle inequality:  \n",
    "\n",
    "$\\big \\vert \\text{trace}\\big(\\mathbf{UA}\\big)\\big \\vert $  \n",
    "$=\\big \\vert \\text{trace}\\big(\\mathbf{DB}\\big)\\big \\vert $  \n",
    "$= \\big \\vert \\sum_{k=1}^n d_{k,k} b_{k,k} \\big\\vert $  \n",
    "$\\leq  \\sum_{k=1}^n \\big \\vert d_{k,k} b_{k,k}\\big\\vert $  \n",
    "$= \\sum_{k=1}^n \\big \\vert d_{k,k}\\big \\vert \\big \\vert b_{k,k}\\big\\vert $  \n",
    "$= \\sum_{k=1}^n b_{k,k} $  \n",
    "$= \\sum_{k=1}^n a_{k,k}$  \n",
    "$= \\text{trace}\\big(\\mathbf A\\big) $    \n",
    "\n",
    "noting that $b_{k,k} \\geq 0$ (as these are diagonal components of a Hermitian positive semi-definite matrix, and $\\big \\vert d_{k,k}\\big \\vert = 1$ because each eigenvalue for a unitary matrix is on the unit circle.  \n",
    "\n",
    "*begin alternative proof*  \n",
    "  \n",
    "since $\\mathbf A$ is Hermitian positive semi-definite, we may unitarily diagonalize it in the form of\n",
    "\n",
    "$\\mathbf A = \\mathbf{QDQ}^H$  \n",
    "\n",
    "where we use $\\lambda_k := d_{k,k}$  \n",
    "\n",
    "$\\big \\vert \\text{trace}\\big(\\mathbf{UA}\\big)\\big \\vert = \\big \\vert \\text{trace}\\big(\\mathbf U \\mathbf{QDQ}^H\\big)\\big \\vert = \\big \\vert \\text{trace}\\big(\\mathbf Q^H\\mathbf {UQ} \\mathbf{D}\\big)\\big \\vert  \\leq \\text{trace}\\big(\\mathbf A\\big)$ \n",
    "\n",
    "where $\\mathbf V := \\mathbf Q^H\\mathbf {UQ}$\n",
    "\n",
    "because $\\mathbf V$ is unitary, each column has a length (2 norm) of 1, and hence each diagonal entry has a magnitude in $[0,1]$  \n",
    "\n",
    "$\\big \\vert \\text{trace}\\big(\\mathbf{UA}\\big)\\big \\vert$  \n",
    "$= \\big \\vert \\text{trace}\\big(\\mathbf V \\mathbf{D}\\big)\\big \\vert  $    \n",
    "$=\\big \\vert \\sum_{k=1}^n v_{k,k} \\lambda_k \\big \\vert  $  \n",
    "$\\leq  \\sum_{k=1}^n \\big \\vert v_{k,k} \\lambda_k\\big \\vert $    \n",
    "$= \\sum_{k=1}^n \\big \\vert v_{k,k}\\big \\vert \\lambda_k  $    \n",
    "$\\leq \\sum_{k=1}^n \\lambda_k $   \n",
    "$= \\text{trace}\\big(\\mathbf D\\big) $   \n",
    "$= \\text{trace}\\big(\\mathbf A\\big) $   \n",
    "\n",
    "by application of the triangle inequality and then by fact that each $\\lambda_k\\geq 0$ and hence scaling any entry by some real non-negative number $\\leq 1$ results in a series that is at most the same size  \n",
    "\n",
    "*end alternative proof*  \n",
    "\n",
    "*begin inner product oriented alternative proof:* \n",
    "\n",
    "take the Hermitian positive square root and get \n",
    "\n",
    "$\\mathbf {BB} = \\mathbf B^H \\mathbf B = \\mathbf B \\mathbf B^H = \\mathbf A$ \n",
    "\n",
    "$\\big \\vert \\text{trace}\\big(\\mathbf{UA}\\big)\\big \\vert $  \n",
    "$= \\big \\vert \\text{trace}\\big(\\mathbf{UBB^H}\\big)\\big \\vert $  \n",
    "$=\\big \\vert \\text{trace}\\big(\\mathbf B^H \\mathbf{UB}\\big)\\big \\vert $  \n",
    "$= \\big \\vert \\langle \\mathbf B, \\mathbf {UB} \\rangle \\big \\vert  $  \n",
    "$\\leq \\big \\Vert \\mathbf B \\big \\Vert_F \\big \\Vert \\mathbf {UB} \\big \\Vert_F  $  \n",
    "$= \\big \\Vert \\mathbf B \\big \\Vert_F \\big \\Vert \\mathbf {B} \\big \\Vert_F $    \n",
    "$= \\big \\Vert \\mathbf B \\big \\Vert_F^2 $   \n",
    "$=  \\text{trace}\\big(\\mathbf B^H \\mathbf B \\big) $  \n",
    "$= \\text{trace}\\big(\\mathbf A\\big)$   \n",
    "\n",
    "by direct application of Cauchy-Schwarz to the inner product given by the trace.  (Explicit use of the vec operator may be helpful for interpretation purposes for some people. )    \n",
    "\n",
    "\n",
    "\n",
    "*end second alternative proof*\n",
    "\n",
    "where we recognize that each diagonal element of $\\mathbf A$ and $\\mathbf B$ is real, non-negative (as required by Hermitian positive semi-definite matrices), each diagonal element of $\\mathbf D$ has a magnitude of one (as required for unitary matrices) and the inequality stated is a direct application of triangle inequality. \n",
    "\n",
    "**claim:**  \n",
    "\n",
    "where $\\mathbf Y$ is $n$ x $n$  Hermitian, it is also positive semi-definite, **iff** \n",
    "\n",
    "$\\text{trace}\\big(\\mathbf {YX}\\big) \\geq 0 $ for all Hermitian positive semi definite $\\mathbf X$\n",
    "\n",
    "**remark:  This is something I saw elsewhere (not in Zhang) but it is amenable to the same techniques used there**  \n",
    "\n",
    "**proof:**\n",
    "\n",
    "Let $\\mathbf X = \\sigma_1 \\mathbf x_1 \\mathbf x_1^H + \\sigma_2 \\mathbf x_2 \\mathbf x_2^H + ... + \\sigma_n \\mathbf x_n \\mathbf x_n^H$ \n",
    "\n",
    "where each $\\sigma_k \\geq 0$\n",
    "\n",
    "\n",
    "$\\text{trace}\\Big(\\mathbf {YX}\\Big) = \\text{trace}\\Big(\\mathbf Y \\big(\\sigma_1 \\mathbf x_1 \\mathbf x_1^H + \\sigma_2 \\mathbf x_2 \\mathbf x_2^H + ... + \\sigma_n \\mathbf x_n \\mathbf x_n^H \\big) \\Big) =  \\sigma_1 \\text{trace}\\Big(\\mathbf Y  \\mathbf x_1 \\mathbf x_1^H\\Big)  + \\sigma_2 \\text{trace}\\Big(\\mathbf Y  \\mathbf x_2 \\mathbf x_2^H \\Big) ... + \\sigma_n \\text{trace}\\Big(\\mathbf Y  \\mathbf x_n \\mathbf x_n^H\\Big) \\geq 0 $ \n",
    "\n",
    "$\\text{trace}\\Big(\\mathbf {YX}\\Big) = \\sigma_1 \\Big( \\mathbf x_1^H \\mathbf Y  \\mathbf x_1 \\Big)  + \\sigma_2 \\Big(\\mathbf x_2^H  \\mathbf Y  \\mathbf x_2 \\Big) ... + \\sigma_n \\Big( \\mathbf x_n^H \\mathbf Y  \\mathbf x_n\\Big) \\geq 0$\n",
    "\n",
    "the trace will obviously be $\\geq 0$ if each term in the finite series is $\\geq 0$.  Now we hone in on an important special subset: since this is for *any* $\\mathbf X$, this also includes the special case where $\\sigma_r = 0$ for $r = \\{2, 3, ...,n\\}$ and we select $\\mathbf x_1$ to be any vector we like. Thus the claim reduces to \n",
    "\n",
    "$\\text{trace}\\Big(\\mathbf {YX}\\Big) = \\sigma_1 \\Big( \\mathbf x_1^H \\mathbf Y  \\mathbf x_1 \\Big) \\geq 0$\n",
    "\n",
    "for all $\\mathbf x_1$ and any $\\sigma_1 \\geq 0$, which is the familiar test for positive semi-definiteness of $\\mathbf Y$.  \n",
    "- - - -\n",
    "\n",
    "*alternative proof*  \n",
    "\n",
    "this approach may be more slick, but perhaps less intuitive.\n",
    "\n",
    "$\\mathbf Y = \\mathbf Q \\mathbf \\Lambda \\mathbf Q^H$  \n",
    "$\\mathbf X = \\mathbf{V\\Sigma V^H}$\n",
    "\n",
    "\n",
    "we know that $\\mathbf X$ is any arbitrary Hermitian positive semi-definite matrix, and need to verify that the trace inequality means $\\mathbf Y$ must be Hermitian positive semi-definite as well.  \n",
    "\n",
    "$\\text{trace}\\Big(\\mathbf {YX}\\Big) = \\text{trace}\\Big(\\mathbf Y \\big(\\mathbf{V\\Sigma V^H}\\big) \\Big) = \\text{trace}\\Big(\\big(\\mathbf V^H \\mathbf{Y V}\\big)\\mathbf \\Sigma \\Big) = \\text{trace}\\Big(\\mathbf B \\mathbf \\Sigma \\Big) = \\sum_{i=1}^n b_{i,i}\\sigma_i \\geq 0 $  \n",
    "\n",
    "where $\\mathbf B = \\big(\\mathbf V^H \\mathbf{Y V}\\big)$.  If $\\mathbf Y$ is Hermitian positive semi-definite, then its diagonal elements are real non-negative and thus $\\mathbf B$ (which is also Hermitian positive semi-definite) has real non-negative diagonal elements.  \n",
    "\n",
    "The above should seem intuitive and proves the *if* but not the **iff**.  We now consider the other leg:  \n",
    "\n",
    "\n",
    "$\\text{trace}\\Big(\\mathbf {YX}\\Big) = \\text{trace}\\Big(\\big(\\mathbf Q \\mathbf \\Lambda \\mathbf Q^H\\big) \\mathbf X \\Big) = \\text{trace}\\Big(\\mathbf \\Lambda \\big( \\mathbf Q^H \\mathbf X  \\mathbf Q\\big) \\Big)=  \\text{trace}\\Big(\\mathbf \\Lambda \\big(\\mathbf C \\big) \\Big) = \\sum_{i=1}^n c_{i,i}\\lambda_i \\geq 0$ \n",
    "\n",
    "where $\\big(\\mathbf C \\big) = \\big( \\mathbf Q^H \\mathbf X  \\mathbf Q\\big)$ and $\\mathbf C$ is Hermitian positive semi-definite like $\\mathbf X$, thus each $ c_{i,i} \\geq 0$.  \n",
    "\n",
    "Consider the case where $\\mathbf Y$ has an eigenvalue less than zero.  E.g. suppose $\\lambda_n \\lt 0$ but $\\lambda_r \\geq 0$ for $r = \\{1, 2, 3, ..., n-1\\}$.  If this is true, then \n",
    "\n",
    "$c_{n,n} \\lambda_n + \\sum_{r=1}^{n-1} c_{r,r}\\lambda_r \\ngeq 0$\n",
    "\n",
    "for some large enough $c_{n,n}$.  The easiest approach is to select some $\\mathbf C$ where $c_{n,n} = 1$ and *all* other cells in $\\mathbf C$ are zero.  Such a matrix is still Hermitian positive semi-definite, and from here, for any given $\\mathbf Y$ we can multiply out its eigenvectors and get the appropriate $\\mathbf X = \\big( \\mathbf Q \\mathbf C  \\mathbf Q^H\\big)$, which is Hermitian positive semi-definite, yet violates the above inequality. Hence for the inequality to hold *all* eigenvalues of $\\mathbf X$ must be real non-negative.  And since we have determined that $\\mathbf X$ is Hermitian with strictly non-negative eigenvalues in order for the inequality to always hold, then $\\mathbf X$ must be Hermitian positive semi-definite.  This completes the proof.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**claim:**\n",
    "\n",
    "where $\\mathbf B$ is Hermitian positive semi-definite, and $\\mathbf Q$ has orthonormal columns but generally is **not** square,\n",
    "\n",
    "$\\text{trace}\\big(\\mathbf B\\big) \\geq \\text{trace}\\big(\\mathbf Q^H \\mathbf B \\mathbf Q\\big)$\n",
    "\n",
    "**remark:**\n",
    "for more info the spectra of $\\mathbf B$ vs the spectra of $\\big(\\mathbf Q^H \\mathbf B \\mathbf Q\\big)$ see notebook \"eigenvalue_continuity.ipynb\"  \n",
    "\n",
    "\n",
    "**proof:** \n",
    "\n",
    "let $\\mathbf Q = \\mathbf U \\mathbf \\Sigma \\mathbf V^H$ \n",
    "\n",
    "where $\\mathbf U$ and $\\mathbf V$ are square, but $\\mathbf \\Sigma$ in general is tall and skinny. Note that $\\mathbf Q^H \\mathbf Q = \\mathbf I_k$ where $k$ is the number of columns in (and indeed the rank of) $\\mathbf Q$.  Hence we confirm that each singular value $\\{\\sigma_1, \\sigma_2, ..., \\sigma_k\\}$ has a magnitude of one, and hence a value of one (since singular values are, by construction, real, non-negative.) \n",
    "\n",
    "\n",
    "$\\text{trace}\\big(\\mathbf B\\big) \\geq \\text{trace}\\big(\\mathbf Q^H \\mathbf B \\mathbf Q\\big) = \\text{trace}\\big(\\mathbf V \\mathbf \\Sigma^H \\mathbf U^H \\mathbf B \\mathbf U \\mathbf \\Sigma \\mathbf V^H \\big) = \\text{trace}\\big(\\mathbf U^H \\mathbf B \\mathbf U \\mathbf \\Sigma \\mathbf \\Sigma^H \\big)$\n",
    "\n",
    "assign $\\mathbf C := \\mathbf U^H \\mathbf B \\mathbf U$   \n",
    "\n",
    "$\\mathbf C$ is Hermitian positive semi-definite as well  \n",
    "$\\big(\\mathbf \\Sigma \\mathbf \\Sigma^H \\big)$ is a diagonal matrix with $k$ entries equal to one, and all else equal to 0.\n",
    "\n",
    "\n",
    "$\\text{trace}\\big(\\mathbf B\\big) = \\text{trace}\\big(\\mathbf C\\big) = c_{1,1} + c_{2,2} + ... + c_{k,k} + c_{k+1,k+1} + ... + c_{n,n} \\geq c_{1,1} + c_{2,2} + ... + c_{k,k}  = \\text{trace}\\Big(\\mathbf C\\big( \\mathbf \\Sigma \\mathbf \\Sigma^H \\big)\\Big)$\n",
    "\n",
    "because each diagonal entry of $\\mathbf C$ is real non-negative, by virtue of it being Hermitian positive semi-definite.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Begin Interlude involving singular values and ideas from Majorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*note:*  majorization is a simple but sophisticated concept that among many other things, is useful in teasing out various inequalities relating to matrices.  The reader unfamiliar with marjoization may skip this section.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "consider some Hermitian Positive Definite matrix  \n",
    "$\\mathbf A \\succ 0$   \n",
    "\n",
    "which is unitarily diagonalizable  \n",
    "\n",
    "$\\mathbf A = \\mathbf {Q\\Sigma Q}^*$  \n",
    "\n",
    "with the usual ordering  \n",
    "$\\sigma_1 \\geq \\sigma_2 \\geq ... \\geq \\sigma_n \\gt 0$  \n",
    "\n",
    "$\\mathbf Q = \n",
    "\\bigg[\\begin{array}{c|c|c|c|c}\n",
    "\\mathbf q_1 & \\mathbf q_2 &\\cdots & \\mathbf q_{n-1} & \\mathbf q_{n}\n",
    "\\end{array}\\bigg]$    \n",
    "\n",
    "we prove that  \n",
    "$\\mathbf \\Sigma \\succeq \\text{Diag} \\big(\\mathbf A \\big) = \\mathbf A \\circ \\mathbf I $  \n",
    "(i.e. the diagonal matrix with ordered eigenvalues majorizes $\\mathbf A$.)  \n",
    "\n",
    "$\\mathbf A = \\big(\\mathbf {Q\\Sigma}\\big) \\mathbf Q^* = \\sum_{k=1}^n \\sigma_k \\mathbf q_k \\mathbf q_k^*  $   \n",
    "\n",
    "we now want to look at the diagonal components of that sum of rank one updates (it's not necessary though the Hadamard product can be helpful here)  \n",
    "\n",
    "$\\big(\\mathbf A \\circ \\mathbf I\\big)\\mathbf 1 = \\big(\\sum_{k=1}^n \\sigma_k \\mathbf q_k \\mathbf q_k^* \\mathbf \\circ \\mathbf I\\big)\\mathbf 1 =  \\sum_{k=1}^n \\sigma_k \\begin{bmatrix}\n",
    "\\big \\vert q_{1}^{(k)} \\big\\vert^2 \\\\\n",
    "\\big\\vert q_{2}^{(k)}\\big\\vert^2 \\\\ \n",
    "\\vdots\\\\ \n",
    "\\big\\vert q_{n-1}^{(k)}\\big\\vert^2 \\\\ \n",
    "\\big\\vert q_{n}^{(k)}\\big\\vert^2\n",
    "\\end{bmatrix}$   \n",
    "\n",
    "if we observe that  \n",
    "$1 = \\big \\Vert \\mathbf q_k \\big \\Vert_2^2 = \\text{trace}\\big(\\mathbf q_k \\mathbf q_k^*\\big) = \\big \\vert q_{1}^{(k)} \\big\\vert^2 + \\big\\vert q_{2}^{(k)}\\big\\vert^2 + ... + \\big\\vert q_{n-1}^{(k)}\\big\\vert^2 + \\big\\vert q_{n}^{(k)}\\big\\vert^2$    \n",
    "\n",
    "then we can create a (doubly) stochastic matrix $D$ (*not a diagonal matrix in general!*)  the $k$th column is given by \n",
    "\n",
    "$\\begin{bmatrix}\n",
    "\\big \\vert q_{1}^{(k)} \\big\\vert^2 \\\\\n",
    "\\big\\vert q_{2}^{(k)}\\big\\vert^2 \\\\ \n",
    "\\vdots\\\\ \n",
    "\\big\\vert q_{n-1}^{(k)}\\big\\vert^2 \\\\ \n",
    "\\big\\vert q_{n}^{(k)}\\big\\vert^2\n",
    "\\end{bmatrix}$  \n",
    "\n",
    "and see that   \n",
    "$\\big(\\mathbf A \\circ \\mathbf I\\big)\\mathbf 1 = D \\big(\\mathbf \\Sigma \\mathbf 1\\big)$  \n",
    "\n",
    "Based on the above work it is immediate that $ D$ is column stochastic (i.e. real non-negative components that sum to one in each column). To prove it is row stochastic, consider that if we select the $i$th row and sum over all components in said row, row we get  \n",
    "\n",
    "\n",
    "$\\sum_{k=1}^n D_{i,k} = \\sum_{k=1}^n \\big \\vert q_{i}^{(k)} \\big\\vert^2 = 1 $  \n",
    "or in matrix form, if we partition $\\mathbf Q$ by rows \n",
    "\n",
    "$\\mathbf Q= \n",
    "\\begin{bmatrix}\n",
    "\\tilde{ \\mathbf q_1}^T \\\\\n",
    "\\tilde{ \\mathbf q_2}^T \\\\ \n",
    "\\vdots\\\\ \n",
    "\\tilde{ \\mathbf q}_{n-1}^T \\\\ \n",
    "\\tilde{ \\mathbf q_n}^T\n",
    "\\end{bmatrix}$  \n",
    "\n",
    "so e.g. with $i = 2$ we have   \n",
    "$\\sum_{k=1}^n D_{2,k} = \\sum_{k=1}^n\\big \\vert q_{2}^{(k)} \\big\\vert^2 = \\tilde{ \\mathbf q_2}^T \\bar{\\tilde{ \\mathbf q_2}}= \\big \\Vert \\tilde{ \\mathbf q_2}^T\\big \\Vert_2^2= 1 $  \n",
    "\n",
    "which is equivalent to observing that $\\mathbf Q$ has mutually orthonormal rows as well as mutually orthonormal columns, i.e. \n",
    "\n",
    "$\\mathbf Q\\mathbf Q^* = \\mathbf Q\\mathbf Q^{-1} = \\mathbf Q^{-1}\\mathbf Q = \\mathbf Q^*\\mathbf Q = \\mathbf I$  \n",
    "\n",
    "This proves $D$ is doubly stochastic.  \n",
    "\n",
    "But since $D$ is doubly stochastic and $\\big(\\mathbf A \\circ \\mathbf I\\big)\\mathbf 1 = D\\big(\\mathbf \\Sigma \\mathbf 1\\big)$  \n",
    "an immediate conclusion from the theory of majorization is that that diagonal elements of $\\mathbf A$ are in the convex hull of $\\mathbf \\Sigma$ i.e. that   \n",
    "$\\mathbf \\Sigma \\succeq \\text{Diag} \\big(\\mathbf A \\big) = \\mathbf A \\circ \\mathbf I $  \n",
    "or that, for $r \\in\\{1,2,3...,n\\}$  \n",
    "\n",
    "$\\sum_{i=1}^r \\sigma_i \\geq \\sum_{i=1}^r a_{i,i}$  \n",
    "\n",
    "by linearity, we may nicely extend this to arbitrary hermitian matrices by shifting the eigenvalues of $\\mathbf A$ by some constant real amount $\\alpha$, i.e. considering the matrix \n",
    "\n",
    "$\\big(\\mathbf A + \\alpha\\mathbf I\\big)$  \n",
    "- - - - \n",
    "an *easy corollary* is that every Hermitian matrix is unitarily similar to a matrix with a constant diagonal.  (Indeed every matrix diagonalizable over $\\mathbb C$ is similar to a matrix with a constant diagonal by the below relation.) \n",
    "\n",
    "$\\gamma := \\frac{1}{n}\\text{trace}\\big(\\mathbf A\\big)$    \n",
    "\n",
    "The special case where $\\text{trace}\\big(\\mathbf A\\big) = 0$ is of particular interest -- it tells us that every Hermitian matrix is similar to a matrix with all zeros on the diagonal.  \n",
    "\n",
    "We may intuit this by reconsidering  \n",
    "$\\big(\\mathbf A \\circ \\mathbf I\\big)\\mathbf 1 = D\\big(\\mathbf \\Sigma \\mathbf 1\\big)$  \n",
    "and in particular considering \n",
    "\n",
    "$\\mathbf P=\\frac{1}{2}\\big(D + \\mathbf I\\big)$  \n",
    "(which ensures a limit exist though technically does deal with reducibility issues which we ignore here), and then consider  \n",
    "\n",
    "$\\big(\\mathbf S \\circ \\mathbf I\\big)\\mathbf 1 = \\lim_{r \\to \\infty} \\mathbf P^r \\big(\\mathbf \\Sigma\\mathbf 1\\big)$  \n",
    "where the diagonal of $\\mathbf S$ is the Hermitian matrix that is (hopefully, barring reducibility issues) in the convex hull of / majorized by all Hermitian matrices with this spectra.  \n",
    "\n",
    "- - - - \n",
    "*remark:*  \n",
    "using induction and more care, the result can be proven somewhat indirectly (making use of intermediate value theorem) to show that an orthogonally similar matrix with constant diagonal exists.  The approach here showing unitary similarity is slightly weaker but easier, more direct and nicely ties in immediately with majorization ideas.  \n",
    "- - - - \n",
    "It is enough to find a matrix that is unitarily similar to $\\mathbf \\Sigma$ but has a constant diagonal.  \n",
    "\n",
    "i.e. to find \n",
    "\n",
    "$\\mathbf S = \\mathbf U \\mathbf \\Sigma \\mathbf U^*$  \n",
    "where $\\mathbf S$ has constants on the diagonal.  \n",
    "\n",
    "To get this 'totally majorized' matrix reconsider that column $k$ of the associated doubly stochastic matrix $D$ is given by  \n",
    "\n",
    "$\\begin{bmatrix}\n",
    "\\big \\vert u_{1}^{(k)} \\big\\vert^2 \\\\\n",
    "\\big\\vert u_{2}^{(k)}\\big\\vert^2 \\\\ \n",
    "\\vdots\\\\ \n",
    "\\big\\vert u_{n-1}^{(k)}\\big\\vert^2 \\\\ \n",
    "\\big\\vert u_{n}^{(k)}\\big\\vert^2\n",
    "\\end{bmatrix}$\n",
    "\n",
    "and the maximally majorized case occurs when $\\mathbf D = \\frac{1}{n} \\mathbf {11}^T$  \n",
    "-- i.e. the uniform distribution case.  Hence we need to find a unitary matrix $\\mathbf U$ where each component has magnitude equal to $\\frac{1}{\\sqrt{n}}$.  \n",
    "\n",
    "The Unitary Vandermonde Matrix works perfectly here (i.e. Discrete Fourier Transform matrix)  \n",
    "(due to GitHub $\\LaTeX$ rendering issues, the below formula has been inserted as an image)  \n",
    "\n",
    "$\\mathbf U:=  \\mathbf F $  \n",
    "![F_components](images/unitary_vandermondF_components.gif)\n",
    "\n",
    "\n",
    "with $\\lambda_k := (\\omega^{k-1})$ i.e. each of the nth roots of unity.  \n",
    "\n",
    "*remark 1:*  \n",
    "while the approach using the unitary Vandermonde matrix was arrived at via ideas from majorization, none of the ideas are needed to understand the result -- we can directly verify the uniform diagonal of $\\mathbf F \\mathbf \\Sigma \\mathbf F^*$ without knowing anything about majorization.  \n",
    "\n",
    "*remark 2:*  \n",
    "upon inspection and referencing the Vandermonde matrices notebook (in particular the section near the end titled \"A Much simpler look at the Circulant matrices, and the Discrete Fourier Tranform\"), we can see that $\\mathbf S$ is a circulant matrix -- which 'of course' has a uniform diagonal, because circulant matrices have a uniform diagonal.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# some code to demonstrates this in action  \n",
    "# (small floating point/ rounding issues are to be expected)\n",
    "\n",
    "import numpy as np\n",
    "from scipy.linalg import expm\n",
    "np.set_printoptions(precision = 1, linewidth=180)\n",
    "\n",
    "n = 5\n",
    "v = np.zeros(n+1)\n",
    "v[0] = 1\n",
    "v[-1] = -1\n",
    "\n",
    "# https://docs.scipy.org/doc/numpy/reference/generated/numpy.roots.html  \n",
    "\n",
    "roots_of_unity = np.roots(v)\n",
    "\n",
    "W = np.zeros((n,n))\n",
    "W = np.array(W, np.complex128)\n",
    "for i in range(n):\n",
    "    W[i] += roots_of_unity**i\n",
    "W *= 1/np.sqrt(n)\n",
    "\n",
    "some_eigs_vec = np.random.random(n)\n",
    "some_eigs_vec[-1] = - np.sum(some_eigs_vec[:-1])\n",
    "some_eigs_vec*= 100\n",
    "\n",
    "# traceless... \n",
    "diag_elements = np.diag(W@ np.diag(some_eigs_vec) @ np.conj(W).T)\n",
    "print(np.round(np.abs(diag_elements),5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "A = np.random.random((5,5))\n",
    "B = (A + A.T)/2\n",
    "# B = A@A.T\n",
    "B_exp = expm(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15.584269721730422"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.trace(B_exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.7,  8.8, 19.6, 14.7,  6. ])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.diag(B_exp)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.9, 1.3, 2. , 1.6, 1.2],\n",
       "       [1.3, 3. , 2.3, 1.5, 1.7],\n",
       "       [2. , 2.3, 4.4, 2.4, 1.8],\n",
       "       [1.6, 1.5, 2.4, 3.8, 1.6],\n",
       "       [1.2, 1.7, 1.8, 1.6, 2.4]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expm(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.0e+00+0.0e+00j, -3.2e+01-1.6e+00j, -5.0e+01-2.4e+00j, -5.0e+01+2.4e+00j, -3.2e+01+1.6e+00j],\n",
       "       [-3.2e+01+1.6e+00j, -5.0e-14+4.4e-16j, -3.2e+01-1.6e+00j, -5.0e+01-2.4e+00j, -5.0e+01+2.4e+00j],\n",
       "       [-5.0e+01+2.4e+00j, -3.2e+01+1.6e+00j, -9.6e-14+1.8e-15j, -3.2e+01-1.6e+00j, -5.0e+01-2.4e+00j],\n",
       "       [-5.0e+01-2.4e+00j, -5.0e+01+2.4e+00j, -3.2e+01+1.6e+00j, -1.4e-13-1.8e-15j, -3.2e+01-1.6e+00j],\n",
       "       [-3.2e+01-1.6e+00j, -5.0e+01-2.4e+00j, -5.0e+01+2.4e+00j, -3.2e+01+1.6e+00j, -2.0e-13+0.0e+00j]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W@ np.diag(some_eigs_vec) @ np.conj(W).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**lemma**\n",
    "\n",
    "for *any* Hermitian positive (semi) definite $\\mathbf G$ and some singular values matrix $\\mathbf \\Sigma$  \n",
    "\n",
    "(note: all matrices are $n$ x $n$ )  \n",
    "\n",
    "where $\\lambda_1 \\geq \\lambda_2 \\geq ... \\geq \\lambda_n \\geq 0$ and \n",
    "\n",
    "$\\mathbf G = \\mathbf {U\\Lambda U}^H$ \n",
    "\n",
    "and \n",
    "\n",
    "$\\sigma_1 \\geq \\sigma_2 \\geq ... \\geq \\sigma_n \\geq 0$ \n",
    "\n",
    "and for avoidance of doubt the kth diagonal entry of $\\mathbf \\Sigma $ is $\\sigma_k$ and the kth diagonal entry of  $\\mathbf \\Lambda$ is $ \\lambda_k$  \n",
    "\n",
    "**claim:**  \n",
    "$\\text{trace}\\big(\\mathbf \\Sigma \\mathbf G\\big) = \\sigma_1 g_{1,1} + \\sigma_2 g_{2,2} + ... + \\sigma_n g_{n,n} \\leq \\sigma_1 \\lambda_1 + \\sigma_2 \\lambda_2 + ... + \\sigma_n \\lambda_n = \\text{trace}\\big(\\mathbf \\Sigma \\mathbf \\Lambda\\big)$\n",
    "\n",
    "**proof:**\n",
    "\n",
    "a direct proof can be done by using the fact that the (strictly real non-negative) eigenvalues of $\\mathbf G$ majorize the (strictly real non-negative) diagonal elements of $\\mathbf G$, so there is some doubly stochastic matrix $D$ where \n",
    "\n",
    "$\\Big(\\big(\\mathbf I \\circ \\mathbf G \\big)\\mathbf 1\\Big) = D\\Big(\\mathbf \\Lambda \\mathbf 1\\Big)$  \n",
    "\n",
    "Now, via Birkhoff's Theorem, we can say that $D$ is a convex combination of permutation matrices -- with each unique permutation matrix denoted as $\\mathbf P^{(i)}$ -- giving us \n",
    "\n",
    "\n",
    "$\\big(\\mathbf {\\Sigma 1}\\big)^H \\Big(\\big(\\mathbf I \\circ \\mathbf G \\big)\\mathbf 1\\Big) = \\big(\\mathbf {\\Sigma 1}\\big)^H D\\Big(\\mathbf \\Lambda \\mathbf 1\\Big) = \\big(\\mathbf {\\Sigma 1}\\big)^H\\big(\\sum_i \\alpha_i \\mathbf P^{(i)}\\big)\\Big(\\mathbf \\Lambda \\mathbf 1\\Big)  = \\sum_i \\alpha_i  \\big(\\mathbf {\\Sigma 1}\\big)^H \\mathbf P^{(i)}\\big(\\mathbf \\Lambda \\mathbf 1\\big) \\leq \\big(\\mathbf {\\Sigma 1}\\big)^H \\Big(\\mathbf \\Lambda \\mathbf 1\\Big) = \\text{trace}\\big(\\mathbf{\\Sigma \\Lambda}\\big)$\n",
    "\n",
    "\n",
    "$\\alpha_i \\geq 0$ and $\\sum_i \\alpha_i = 1$ \n",
    "because we are dealing with convex combinations.  \n",
    "\n",
    "and for each $i$ \n",
    "\n",
    "$\\big(\\mathbf {\\Sigma 1}\\big)^H \\mathbf P^{(i)} \\Big(\\mathbf \\Lambda \\mathbf 1\\Big) \\leq \\big(\\mathbf {\\Sigma 1}\\big)^H \\Big(\\mathbf \\Lambda \\mathbf 1\\Big)$ \n",
    "\n",
    "via the re-arrangement inequality.  \n",
    "\n",
    "Hence \n",
    "\n",
    "$\\sum_i \\alpha_i  \\big(\\mathbf {\\Sigma 1}\\big)^H \\mathbf P^{(i)}\\big(\\mathbf \\Lambda \\mathbf 1\\big) \\leq \\sum_i \\alpha_i  \\big(\\mathbf {\\Sigma 1}\\big)^H \\big(\\mathbf \\Lambda \\mathbf 1\\big) =  \\big(\\sum_i \\alpha_i\\big)  \\big(\\mathbf {\\Sigma 1}\\big)^H \\big(\\mathbf \\Lambda \\mathbf 1\\big) =   \\big(\\mathbf {\\Sigma 1}\\big)^H \\big(\\mathbf \\Lambda \\mathbf 1\\big)  $ \n",
    "\n",
    "- - - -  \n",
    "**A refinement of this relationship is given by:**  \n",
    "\n",
    "$\\text{trace}\\Big(\\mathbf \\Sigma \\mathbf J^{H} \\mathbf \\Lambda \\mathbf J \\Big) = \\sigma_1 \\lambda_n + \\sigma_{2} \\lambda_{n-1} + ... + \\sigma_n \\lambda_1 \\leq \\text{trace}\\Big( \\mathbf \\Sigma \\mathbf P^{(k)^H} \\mathbf \\Lambda \\mathbf P^{(k)}\\Big)  = \\big(\\mathbf{\\Sigma 1 }\\big)^H \\mathbf P^{(k)} \\big(\\mathbf{ \\Lambda 1}\\big) \\leq  \\text{trace}\\Big(\\mathbf \\Sigma \\mathbf \\Lambda\\Big)$  \n",
    "\n",
    "\n",
    "Put slightly differently, for Hermitian positive (semi)definite $\\mathbf A$ and $\\mathbf B$, we have the following bounds:  \n",
    "\n",
    "$\\text{trace}\\Big(\\mathbf \\Sigma \\mathbf J^{H} \\mathbf \\Lambda \\mathbf J \\Big) = \\sigma_1 \\lambda_n + \\sigma_{2} \\lambda_{n-1} + ... + \\sigma_n \\lambda_1 \\leq \\text{trace}\\Big( \\mathbf A \\mathbf B \\Big)  = \\text{trace}\\Big(\\mathbf \\Sigma \\mathbf G \\Big) \\leq  \\text{trace}\\Big(\\mathbf \\Sigma \\mathbf \\Lambda\\Big)$  \n",
    "\n",
    "where   \n",
    "$\\mathbf A = \\mathbf U \\mathbf \\Sigma \\mathbf U^*$  \n",
    "$\\mathbf G := \\mathbf U^* \\mathbf B \\mathbf U$  \n",
    "\n",
    "\n",
    "\n",
    "**again, recalling that we have well ordered singular values and real nonnegative eigenvalues** (i.e. the largest are always in the top left corner of the relevant diagonal matrices, and with the next largest in the second to top left corner and then next largest and so on.)  The matrix notation and use of trace is useful in this post, however we must remember that there is an ordering scheme that is being enforced here.  Ultimately much of the interesting implications of (finite) majorization come down to consequences of order.  \n",
    "\n",
    "where $\\mathbf J$ is the reflection matrix, and $\\mathbf P^{(k)}$ is the (kth) Permutation matrix.  The above *is* a direct application of the Re-arrangement inequality.  Note that the above is probably best visualized as diagonal matrices representing a (reducible) graph consisting entirely of self loops with real non-negative weights on the edge.  The Right hand side has a diagonal transition matrix applied to this graph, in each case with the same ordering of their weights.  The Left hand side has all the labeling and hence weights, completely flipped -- this is the most extreme case.  The middle case has some re-labeling done.  We can thus visualize the effect of the permutation matrices on $\\mathbf \\Lambda$ as a graph isomorphism that disturbs the order that we constructed in the edges weights of the graphs.  \n",
    "- - - - - \n",
    "Note: a more exacting interpretation would be to count (weighted) walks in the below bipartite graph \n",
    "(**this bipartite interpretation is a work in progress**)  \n",
    "\n",
    "$\\mathbf B := \\begin{bmatrix}\n",
    "\\mathbf 0 & \\mathbf \\Lambda\\\\ \n",
    "\\mathbf \\Sigma & \\mathbf 0\n",
    "\\end{bmatrix}$\n",
    "\n",
    "$\\mathbf B^2 = \\begin{bmatrix}\n",
    "\\mathbf {\\Lambda \\Sigma} & \\mathbf 0\\\\ \n",
    "\\mathbf 0 & \\mathbf { \\Sigma \\Lambda}\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "\\mathbf {\\Lambda \\Sigma} & \\mathbf 0\\\\ \n",
    "\\mathbf 0 & \\mathbf { \\Lambda\\Sigma}\n",
    "\\end{bmatrix}$\n",
    "\n",
    "\n",
    "hence $\\frac{1}{2}\\text{trace}\\big( \\mathbf B^2\\big) =  \\text{trace}\\big(\\mathbf{ \\Lambda\\Sigma}\\big)$  \n",
    "\n",
    "each node in the first partition is connected to one node in the second partition, and vice versa.  The maximum walk count comes when we have labeled all nodes in partitions one and two in accordance with how high their edge weight is and connected them to the same ordered node in the other partition.  I.e. we label each node $(1, i), (2,i),...,(n, i)$ for partition $i$ and $(1, ii), (2,ii),...,(n, ii)$ for partition $ii$ and we connect the 1s to each other, 2s to each other and so forth.  In the maximal case it is because $1$ stands for maximal weighted edge, $2$ stands for a weighted edge not larger than $1$ and so forth for a given partition. \n",
    "\n",
    "The minimal case comes into play if we go into the first partition and 'flip' the label of each node so now (1) actually has a minimum weighted, 2 has second minimum, and so on.  We still have 1's connected between the partition, 2's connected between the partition and so on.  \n",
    "\n",
    "The middle portion of the inequality is when we take our maximal case and re-label some (or all) of the nodes of partition i, but in general not in the absolute worst way possible (i.e. we re-lable but don't do the most extreme 'flip' that is possible).  \n",
    "\n",
    "- - - - - \n",
    "\n",
    "But we know that the diagonal elements of $\\mathbf G$ are majorized by $\\mathbf \\Lambda$, and hence via Birkhoff's Theorem are in the convex hull of permutation matrices, which with real non-negative weights $w_k \\geq 0$ such that $\\sum_{k} w_k = 1$, we have \n",
    "\n",
    "\n",
    "$\\big( \\mathbf G \\circ \\mathbf I\\big) = \\sum_{k} w_k \\mathbf P^{(k)^H} \\mathbf \\Lambda \\mathbf P^{(k)}$ \n",
    "\n",
    "so we can take our original inequality and scale by $w_k$ to observe a useful point-wise bound \n",
    "\n",
    "\n",
    "$w_k \\text{trace}\\Big( \\mathbf \\Sigma \\mathbf J^{H} \\mathbf \\Lambda \\mathbf J\\Big) \\leq \\text{trace}\\Big( \\big(\\mathbf \\Sigma\\big) \\big( w_k \\mathbf P^{(k)^H} \\mathbf \\Lambda \\mathbf P^{(k)}\\big) \\Big)  \\leq  w_k \\text{trace}\\Big(\\mathbf \\Sigma \\mathbf \\Lambda\\Big)$  \n",
    "\n",
    "and summing over this bound, we have \n",
    "\n",
    "$\\text{trace}\\Big(\\mathbf \\Sigma \\mathbf J^{H} \\mathbf \\Lambda \\mathbf J \\Big)  \\leq \\sum_k \\text{trace}\\Big( \\big(\\mathbf \\Sigma\\big) \\big( w_k \\mathbf P^{(k)^H} \\mathbf \\Lambda \\mathbf P^{(k)}\\big) \\Big)  =  \\text{trace}\\Big( \\big(\\mathbf \\Sigma\\big) \\sum_k \\big( w_k \\mathbf P^{(k)^H} \\mathbf \\Lambda \\mathbf P^{(k)}\\big) \\Big) = \\text{trace}\\Big( \\big(\\mathbf \\Sigma\\big) \\big( \\mathbf G \\circ \\mathbf I\\big) \\Big)  \\leq  \\text{trace}\\Big(\\mathbf \\Sigma \\mathbf \\Lambda\\Big)   $  \n",
    "\n",
    "\n",
    "Then noticing that   \n",
    "$ \\text{trace}\\Big( \\big(\\mathbf \\Sigma\\big) \\big( \\mathbf G \\circ \\mathbf I\\big) \\Big) = \\text{trace}\\Big( \\mathbf \\Sigma \\mathbf G \\Big) $\n",
    "\n",
    "we have \n",
    "\n",
    "$\\text{trace}\\Big(\\mathbf \\Sigma \\mathbf J^{H} \\mathbf \\Lambda \\mathbf J \\Big)  \\leq \\text{trace}\\Big( \\mathbf \\Sigma \\mathbf G \\Big) \\leq  \\text{trace}\\big(\\mathbf \\Sigma \\mathbf \\Lambda\\big)$  \n",
    "\n",
    "or equivalently \n",
    "\n",
    "$ \\sigma_1 \\lambda_n + \\sigma_{2} \\lambda_{n-1} + ... + \\sigma_n \\lambda_1 \\leq \\sigma_1 g_{1,1} + \\sigma_2 g_{2,2} + ... + \\sigma_n g_{n,n} \\leq \\sigma_1 \\lambda_1 + \\sigma_2 \\lambda_2 + ... + \\sigma_n \\lambda_n  $ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "an immediate **corollary** is the most basic portion inequality of Thompson that for $n$ x $n$ Hermitian matrices $\\mathbf A$ and $\\mathbf B$  \n",
    "see e.g. page 23 of   \n",
    "http://www.math.uwaterloo.ca/~hwolkowi/henry/reports/thesismingmay613.pdf  \n",
    "\n",
    "(the acutal Thompson Inequality allows for permuting of indices... which is a rather more difficult result)  \n",
    "\n",
    "- - - - -  \n",
    "$\\mathbf C := \\mathbf A + \\mathbf B$  \n",
    "\n",
    "\n",
    "$\\big(\\sum_{i=1}^k \\lambda_i^{(A)}\\big) + \\big(\\sum_{i=1}^{k}\\lambda_{n-k+i}^{(B)}\\big) \\leq \\sum_{i=1}^k \\lambda_i^{(C)} = \\sum_{i=1}^k \\lambda_i^{(A+B)}\\leq \\big(\\sum_{i=1}^k \\lambda_i{(A)}\\big) + \\big(\\sum_{i=1}^k \\lambda_i{(B)}\\big)$   \n",
    "\n",
    "- - - - -  \n",
    "*remarks:*  \n",
    "1.) we can assume WLOG that each $\\mathbf A$ and $\\mathbf B$  are positive (semi)definite-- if not, first add $\\mathbf \\alpha \\mathbf I$ and $\\mathbf \\beta \\mathbf I$, to  $\\mathbf A$ and $\\mathbf B$ respectively, where $\\alpha, \\beta$ are large enough positive scalars (e.g. set at the size of the L1 norm of each column and taking the max for each respective matrix-- i.e. sum of magnitude of components-- the result then follows by application of Gerschgorin discs, *or* the above Schur Test).  We can then deduct $\\mathbf \\alpha \\mathbf I$ and $\\mathbf \\beta \\mathbf I$ respectively at the end without changing the result.  \n",
    "\n",
    "- - - - -   \n",
    "\n",
    "we can immediately extend the lower bound to  \n",
    "\n",
    "$\\big(\\sum_{i=1}^k \\lambda_{\\sigma(i)}^{(A)}\\big) + \\big(\\sum_{i=1}^{k}\\lambda_{n-k+i}^{(B)}\\big) \\leq \\big(\\sum_{i=1}^k \\lambda_i^{(A)}\\big) + \\big(\\sum_{i=1}^{k}\\lambda_{n-k+i}^{(B)}\\big)\\leq \\sum_{i=1}^k \\lambda_i^{(C)} = \\sum_{i=1}^k \\lambda_i^{(A+B)}$  \n",
    "\n",
    "because the eigenvalues of $A$ were sorted from biggest to smallest, so any permutation necessarily cannot increase the size of the sum of the first $k$ of them  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**proof:**  \n",
    "using the above refinement / result and quasilinearization, with rank k (Hermitian, Idempotent) projector $\\mathbf P^{(k)}$, that is unitarily similar to $\\mathbf C$, i.e.  \n",
    "\n",
    "$\\mathbf P^{(k)} = \\mathbf U\\begin{bmatrix} \\mathbf I_k & \\mathbf 0\\mathbf 0^T \\\\ \\mathbf 0\\mathbf 0^T & \\big(\\mathbf 0\\mathbf 0\\big)_{n-k}^T \\end{bmatrix}\\mathbf U^*$  \n",
    "\n",
    "- - - - - \n",
    "**notation overload notice!**  we repeat that here $\\mathbf P^{(k)}$ is a **rank k projector not the kth permutation matrix** as it was in the claim that preceded this corollary.  Unfortunately projectors and permutation matrices are both quite common and useful and denoted by typically denoted as matrix P.    \n",
    "- - - - - \n",
    "\n",
    "$\\big(\\sum_{i=1}^k \\lambda_i^{(A)}\\big) + \\big(\\sum_{i=1}^{k}\\lambda_{n-k+i}^{(B)}\\big)$   \n",
    "$=\\max_{G_A^{(k)}:A} \\Big\\{\\text{trace}\\big( \\mathbf \\Sigma_A\\mathbf G_A^{(k)}\\big)\\Big\\}+\\min_{G_B^{(k)}:B} \\Big\\{\\text{trace}\\big( \\mathbf \\Sigma_B\\mathbf G_B^{(k)}\\big)\\Big\\}$   \n",
    "$=\\max_{G_A^{(k)}:A} \\Big\\{\\text{trace}\\big(\\mathbf G_A^{(k)} \\mathbf \\Sigma_A\\big)\\Big\\}+\\min_{G_B^{(k)}:B} \\Big\\{\\text{trace}\\big(\\mathbf G_B^{(k)} \\mathbf \\Sigma_B\\big)\\Big\\}$   \n",
    "$=\\max_{P^{(k)}:A} \\Big\\{\\text{trace}\\big(\\big(\\mathbf U^*\\mathbf P^{(k)}\\mathbf U\\big) \\mathbf \\Sigma_A\\big)\\Big\\}   +\\min_{P^{(k)}:B} \\Big\\{\\text{trace}\\big(\\big(\\mathbf Q^*\\mathbf P^{(k)}\\mathbf Q \\big)\\mathbf \\Sigma_B\\big)\\Big\\}$   \n",
    "$=\\max_{P^{(k)}:A} \\Big\\{\\text{trace}\\big(\\mathbf P^{(k)}\\mathbf A\\big)\\Big\\} +\\min_{P^{(k)}:B} \\Big\\{\\text{trace}\\big(\\mathbf P^{(k)}\\mathbf B\\big)\\Big\\}$   \n",
    "$\\leq\\max_{P^{(k)}:A} \\Big\\{\\text{trace}\\big(\\mathbf P^{(k)}\\mathbf A\\big)\\Big\\} +\\max_{P^{(k)}:A} \\Big\\{\\text{trace}\\big(\\mathbf P^{(k)}\\mathbf B\\big)\\Big\\}$   \n",
    "$=\\max_{P^{(k)}:A} \\Big\\{\\text{trace}\\big(\\mathbf P^{(k)}\\mathbf A\\big) +\\text{trace}\\big(\\mathbf P^{(k)}\\mathbf B\\big)\\Big\\}$   \n",
    "$=\\max_{P^{(k)}:A} \\text{trace}\\big(\\mathbf P^{(k)}\\mathbf C\\big)$   \n",
    "$\\leq \\max_{P^{(k)}:C} \\text{trace}\\big(\\mathbf P^{(k)}\\mathbf C\\big)$   \n",
    "$\\sum_{j=1}^k \\lambda_i^{(C)}$  \n",
    "$=\\max_{P^{(k)}:C} \\Big\\{\\text{trace}\\big(\\mathbf P^{(k)}\\mathbf A\\big) +\\text{trace}\\big(\\mathbf P^{(k)}\\mathbf B\\big)\\Big\\}$   \n",
    "$\\leq\\max_{P^{(k)}:A} \\Big\\{\\text{trace}\\big(\\mathbf P^{(k)}\\mathbf A\\big)\\Big\\} +\\max_{P^{(k)}:B} \\Big\\{\\text{trace}\\big(\\mathbf P^{(k)}\\mathbf B\\big)\\Big\\}$    \n",
    "$=\\big(\\sum_{i=1}^k \\lambda_i{(A)}\\big) + \\big(\\sum_{i=1}^k \\lambda_i{(B)}\\big)$  \n",
    "\n",
    "\n",
    "(where we could have used sup/inf instead of max /min, but we have inequalities *and* we know the equality conditions of the prior refined inequality that we are applying (again: projectors sharing the same spectrum in an appropriate way), and maximums and minimums are taken with respect to all rank k projectors, so $\\max_{P^{(k)}:A}$  denotes any $\\mathbf P^{(k)}$ is in the set of projectors that maximize the $\\text{trace}\\big(\\mathbf P^{(k)}\\mathbf A\\big)$ (we know the maximum exists but the maximizing projector in general isn't unique, hence there is a set of them.  The reader may re-write this with sup/inf if preferred as the proof of the inequality remains the same.)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**claim:**  \n",
    "\n",
    "(where $\\mathbf X$ and $\\mathbf Y$ are both $n$ x $n$ with scalars in $\\mathbb C$)  \n",
    "\n",
    "$\\big \\vert \\text{trace}\\big(\\mathbf {XY}\\big) \\big \\vert \\leq \\sigma_{1}\\gamma_{1} + \\sigma_{2}\\gamma_{2} + ... + \\sigma_{n}\\gamma_{n} = \\text{trace}\\big(\\mathbf {\\Sigma \\Gamma}\\big)$  \n",
    "\n",
    "where we have singular values for both on the Right Hand Side, i.e.:  \n",
    "\n",
    "$\\mathbf X = \\mathbf U_X \\mathbf \\Sigma \\mathbf V_X^H = \\mathbf Q_X \\mathbf A$    \n",
    "and   \n",
    "$\\mathbf Y = \\mathbf U_Y \\mathbf \\Gamma \\mathbf V_Y^H = \\mathbf B  \\mathbf Q_Y$ \n",
    "\n",
    "again where where $\\sigma_{1} \\geq \\sigma_{2} \\geq ... \\geq \\sigma_{n} \\geq 0$ and  $\\gamma_{1} \\geq \\gamma_{2} \\geq ... \\geq \\gamma_{n} \\geq 0$  \n",
    "\n",
    "- - - - \n",
    "**commentary**: \n",
    "\n",
    "1.) This inequality comes is known as the **von Neumann trace inequality** and is part of problem 139 on page 91 of http://perso.ens-lyon.fr/serre/DPF/exobis.pdf  (over time the problem number and/or page number might change as the document does change)  \n",
    "\n",
    "This problem uses slightly different notation here.  \n",
    "\n",
    "2.) This easily generalizes to $\\text{m x n}$ and $\\text{n x r}$ matrices by padding with appropriate columns and rows of zeros to get two matrices that are both square and of the same dimension.  \n",
    "\n",
    "3.) The above then gives an immediate proof of the H&ouml;lder's Inequality for Schatten norms (this somewhat closely ties in with and generalizes some results in the notebook 'Schurs_Inequality.ipynb').  In particular with \n",
    "\n",
    "$\\Big \\Vert \\mathbf {XY}\\Big \\Vert_{S_1} $  \n",
    "$=\\Big \\vert \\text{trace}\\Big(\\mathbf Q^* \\mathbf {XY}\\Big) \\Big \\vert $  \n",
    "$= \\Big \\vert \\text{trace}\\Big(\\big(\\mathbf Q^* \\mathbf X \\big) \\mathbf Y\\big) \\Big \\vert $  \n",
    "$\\leq \\text{trace}\\Big(\\mathbf {\\Sigma \\Gamma}\\Big) $  \n",
    "$= \\text{vec}\\Big(\\mathbf \\Sigma\\Big)^H  \\text{vec} \\Big(\\mathbf \\Gamma \\Big) $  \n",
    "$\\leq \\Big \\Vert \\text{vec}\\big(\\mathbf \\Sigma\\big)\\Big \\Vert_p \\Big \\Vert \\text{vec} \\big(\\mathbf \\Gamma \\big)\\Big \\Vert_q $  \n",
    "$= \\Big \\Vert \\mathbf X\\Big \\Vert_{S_p}\\Big \\Vert \\mathbf Y\\Big \\Vert_{S_q} $  \n",
    "\n",
    "The key result is that we can define the sum of singular values of a matrix as the product of maximizing unitary matrix $\\mathbf Q^*$ times some matrix $\\mathbf Z$ (use polar form and inequality in this notebook relating to traces of hermitian positive semi definite matrix times unitary matrix) and in this case $\\mathbf Z: =\\mathbf{XY}$).  From here with make use of the fact that multiplication by a unitary matrix does not change singular values, then first inequality is the one to be proven in this section (below), the second one is H&ouml;lder's Inequality (see e.g. Chp_9 notes in the Inequalities / Cauchy Schwarz Masterclass folder) and  as always $p,q$ are H&ouml;lder conjugates (i.e. $p,q \\geq 1$ and $\\frac{1}{p} + \\frac{1}{q} =1$)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**proof :**  \n",
    "   \n",
    "using polar decomposition:  \n",
    "    \n",
    "$\\big \\vert \\text{trace}\\big(\\mathbf {XY}\\big) \\big \\vert $   \n",
    "$=  \\big \\vert \\text{trace}\\Big(\\big(\\mathbf Q_X \\mathbf A\\big) \\big(\\mathbf B  \\mathbf Q_Y \\big) \\Big) \\big \\vert $   \n",
    "$=  \\big \\vert \\text{trace}\\Big(\\mathbf Q_Y \\mathbf Q_X \\mathbf A \\mathbf B   \\Big) \\big\\vert $   \n",
    "$= \\big \\vert \\text{trace}\\Big(\\mathbf Q_Y \\mathbf Q_X \\big(\\mathbf V_X \\mathbf \\Sigma \\mathbf V_X^H\\big) \\mathbf B   \\Big) \\big \\vert $   \n",
    "$= \\big \\vert \\text{trace}\\Big(\\mathbf I \\mathbf Q_Y \\mathbf Q_X \\big(\\mathbf V_X \\mathbf \\Sigma \\mathbf V_X^H\\big) \\mathbf B   \\Big) \\big \\vert $   \n",
    "$= \\big \\vert \\text{trace}\\Big(\\mathbf V_X \\underbrace{\\mathbf V_X^H \\mathbf Q_Y \\mathbf Q_X \\mathbf V_X} \\mathbf \\Sigma \\mathbf V_X^H \\mathbf B   \\Big) \\big \\vert  $  \n",
    "$= \\big \\vert \\text{trace}\\Big( \\mathbf Q \\mathbf \\Sigma \\big(\\mathbf V_X^H \\mathbf B \\mathbf V_X\\big)  \\Big) \\big \\vert  $  with unitary matrix  $\\mathbf Q:= \\underbrace{\\mathbf V_X^H \\mathbf Q_Y \\mathbf Q_X \\mathbf V_X}$  \n",
    "$= \\big \\vert \\text{trace}\\big( \\mathbf Q \\mathbf \\Sigma  \\mathbf C \\big) \\big \\vert $  and Hermitian Positive (semi)definite $\\mathbf C: = \\big(\\mathbf V_X^H \\mathbf B \\mathbf V_X\\big)$   \n",
    "$=\\big \\vert \\text{trace}\\big( \\mathbf Q \\mathbf \\Sigma^\\frac{1}{2} \\mathbf \\Sigma^\\frac{1}{2}  \\mathbf C^\\frac{1}{2} \\mathbf C^\\frac{1}{2} \\big) \\big \\vert $  \n",
    "$= \\big \\vert \\text{trace}\\Big(\\big(\\mathbf \\Sigma^\\frac{1}{2}  \\mathbf C^\\frac{1}{2}\\big)  \\mathbf C^\\frac{1}{2} \\mathbf Q \\mathbf \\Sigma^\\frac{1}{2}  \\Big) \\big \\vert $  \n",
    "$=\\big \\vert \\text{trace}\\Big(\\big(  \\mathbf C^\\frac{1}{2}\\mathbf \\Sigma^\\frac{1}{2} \\big)^H  \\big(\\mathbf C^\\frac{1}{2} \\mathbf Q \\mathbf \\Sigma^\\frac{1}{2} \\Big) \\big \\vert$   \n",
    "\n",
    "*all of this has really just been prep work to invoke Cauchy-Schwarz and the prior lemma.  For the finish:*    \n",
    "$\\big \\vert \\text{trace}\\Big(\\big(  \\mathbf C^\\frac{1}{2}\\mathbf \\Sigma^\\frac{1}{2} \\big)^H  \\big(\\mathbf C^\\frac{1}{2} \\mathbf Q \\mathbf \\Sigma^\\frac{1}{2} \\Big) \\big \\vert \\leq \\big \\Vert \\mathbf C^\\frac{1}{2}\\mathbf \\Sigma^\\frac{1}{2} \\big \\Vert_F \\big \\Vert \\mathbf C^\\frac{1}{2} \\mathbf Q \\mathbf \\Sigma^\\frac{1}{2} \\big \\Vert_F = \\text{trace}\\big( \\mathbf C \\mathbf \\Sigma\\big)^\\frac{1}{2} \\text{trace}\\big(\\mathbf Q^H \\mathbf C \\mathbf Q \\mathbf \\Sigma \\big)^\\frac{1}{2} \\leq \\text{trace}\\big( \\mathbf \\Sigma \\mathbf \\Gamma\\big)^\\frac{1}{2} \\cdot \\text{trace}\\big(  \\mathbf \\Sigma\\mathbf \\Gamma\\big)^\\frac{1}{2} $     \n",
    "by application of Cauchy Schwarz, then expansion of the Frobenius norm (and use of cyclic property of the trace), then the prior lemma  \n",
    "\n",
    "thus  \n",
    "$\\big \\vert \\text{trace}\\big(\\mathbf {XY}\\big) \\big \\vert \\leq \\text{trace}\\big(  \\mathbf \\Sigma \\mathbf \\Gamma\\big)$   \n",
    "- - - - \n",
    "\n",
    "For avoidance of doubt, notice: we have Hermitian positive (semi)definite   \n",
    "$\\mathbf Z:= \\mathbf Q^H \\mathbf C \\mathbf Q$  \n",
    "giving us  \n",
    "$\\big \\vert \\text{trace}\\big(\\mathbf {XY}\\big) \\big \\vert \\leq \\text{trace}\\big( \\mathbf C \\mathbf \\Sigma\\big)^\\frac{1}{2} \\text{trace}\\big(\\mathbf Q^H \\mathbf C \\mathbf Q \\mathbf \\Sigma \\big)^\\frac{1}{2} = \\text{trace}\\big( \\mathbf C \\mathbf \\Sigma\\big)^\\frac{1}{2} \\text{trace}\\big(\\mathbf Z \\mathbf \\Sigma \\big)^\\frac{1}{2}$    \n",
    "\n",
    "but notice that  $\\mathbf Z$ and $\\mathbf C$ are both Hermitian positive (semi)definite and in fact unitarily similar to $\\mathbf B$, and hence they all have the same singular values, which are contained in $\\mathbf \\Gamma$ with the usual ordering.  \n",
    "\n",
    "Now, by invoking the prior lemma, we have  \n",
    "$\\text{trace}\\big( \\mathbf C \\mathbf \\Sigma\\big)^\\frac{1}{2} = \\text{trace}\\big(\\mathbf \\Sigma \\mathbf C \\big)^\\frac{1}{2} \\leq \\ \\text{trace}\\big(  \\mathbf \\Sigma\\mathbf \\Gamma\\big)^\\frac{1}{2}$  and  \n",
    "$\\text{trace}\\big( \\mathbf Z \\mathbf \\Sigma\\big)^\\frac{1}{2} = \\text{trace}\\big(\\mathbf \\Sigma \\mathbf Z \\big)^\\frac{1}{2} \\leq  \\text{trace}\\big(  \\mathbf \\Sigma\\mathbf \\Gamma\\big)^\\frac{1}{2}$   \n",
    "\n",
    "*remark:*  \n",
    "there is a shorter and perhaps conceptually simpler proof of this inequality on pages 6,7 of this document:  \n",
    "https://cs.uwaterloo.ca/~y328yu/mycourses/475/lectures/lec16.pdf  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**corollary: absolute values of eigenvalues of A are weak majorized by singular values of A**  \n",
    "This gives a near immediate proof that the magnitude of the eigenvalues of a square matrix are (weakly) majorized by the singular values of such a matrix.  \n",
    "The technique used below is very similar to and infact a refinement of that used under the heading \"Interesting L1 style extension\" in the notebook \"Schurs_Inequality.ipynb\".  \n",
    "\n",
    "As in that writeup, for any matrix $\\mathbf A \\in \\mathbb C^{n x n}$  \n",
    "consider applying Schur's Triangularization Theorem to get  \n",
    "\n",
    "$ \\mathbf T = \\mathbf U^* \\mathbf A \\mathbf U$  \n",
    "and apply unitary diagonal matrix $\\mathbf D$  so that  \n",
    "$\\mathbf {DT} = \\mathbf R$  \n",
    "where $\\mathbf R$ is upper triangular with all components on the unit circle, and hence has eigenvalues \n",
    "$\\big \\vert \\lambda_1 \\big \\vert\\geq \\big \\vert\\lambda_2 \\big \\vert\\geq .... \\geq \\big \\vert\\lambda_n\\big \\vert \\geq 0$  \n",
    "where $\\lambda_i$ are eigenvalues of $\\mathbf A$ \n",
    "\n",
    "but has the same singular vaues as $\\mathbf A$  \n",
    "$\\sigma_1 \\geq \\sigma_2 \\geq ... \\geq \\sigma_n \\geq 0$   \n",
    "\n",
    "then consider projector $\\mathbf P$ with rank $k$, and in particular given by \n",
    "\n",
    "$\\mathbf P^{(k)} = \\begin{bmatrix} \\mathbf I_k & \\mathbf 0\\mathbf 0^T \\\\ \\mathbf 0\\mathbf 0^T & \\big(\\mathbf 0\\mathbf 0\\big)_{n-k}^T \\end{bmatrix}$  \n",
    "\n",
    " and set $\\mathbf X:= \\mathbf P^{(k)}$ and $\\mathbf Y:= \\mathbf R$, the preceding result gives us   \n",
    "\n",
    "$\\sum_{i=1}^k \\big \\vert\\lambda_i\\big \\vert = \\text{trace}\\big(\\mathbf {XY}\\big) =\\big \\vert \\text{trace}\\big(\\mathbf {XY}\\big)\\big \\vert \\leq \\text{trace}\\big(\\mathbf{\\Gamma \\Sigma}\\big) = \\big(\\sum_{i=1}^k 1 \\cdot \\sigma_i\\big) + \\big(\\sum_{i=k+1}^n 0 \\cdot \\sigma_i\\big) = \\sum_{i=1}^k  \\sigma_i  $  \n",
    "\n",
    "by application of the von-Neumann trace inequality  \n",
    "\n",
    "*remark 0:*  \n",
    "if preferred, we could instead set $\\mathbf X:= \\mathbf U \\mathbf P^{(k)}\\mathbf D\\mathbf U^* $ and $\\mathbf Y:= \\mathbf A$, and we'd have the same inequality:  \n",
    "$\\sum_{i=1}^k \\big \\vert\\lambda_i\\big \\vert = \\text{trace}\\big(\\mathbf {XY}\\big) =\\big \\vert \\text{trace}\\big(\\mathbf {XY}\\big)\\big \\vert \\leq \\text{trace}\\big(\\mathbf{\\Gamma \\Sigma}\\big) = \\sum_{i=1}^k 1 \\cdot \\sigma_i = \\sum_{i=1}^k  \\sigma_i  $  \n",
    "\n",
    "\n",
    "*remark 1:*  \n",
    "the fact that the above has \n",
    "$\\sum_{i=1}^k \\big \\vert\\lambda_i\\big \\vert \\leq \\sum_{i=1}^k 1 \\cdot \\sigma_i = \\sum_{i=1}^k  \\sigma_i  $   \n",
    "shows majorization, and in particular classical or 'strong majorization' when  \n",
    "$\\sum_{i=1}^n \\big \\vert\\lambda_i\\big \\vert = \\sum_{i=1}^n  \\sigma_i  $\n",
    "\n",
    "but in the case where \n",
    "$\\sum_{i=1}^n \\big \\vert\\lambda_i\\big \\vert \\lt \\sum_{i=1}^n  \\sigma_i  $,  \n",
    "this is called 'weak majorization'  and is a slight generalization as in general the sum of alll $n$ eigenvalues will not equal the sum of all $n$ singular values. For our particular problem, there is additional structure here since  \n",
    "$\\prod_{i=1}^n \\big \\vert \\lambda_i\\big \\vert = \\big \\vert \\prod_{i=1}^n \\lambda_i\\big \\vert = \\big \\vert \\det\\big(\\mathbf A\\big)\\big \\vert =    \\big \\vert \\det\\big(\\mathbf U \\mathbf \\Sigma \\mathbf V^*\\big)\\big \\vert = \\big \\vert \\det\\big(\\mathbf U \\mathbf V^*\\big) \\cdot \\det\\big(\\mathbf \\Sigma \\big)\\big \\vert = \\big \\vert \\det\\big(\\mathbf U \\mathbf V^*\\big)\\big \\vert \\cdot \\det\\big(\\mathbf \\Sigma \\big) = \\prod_{i=1}^n \\sigma_i$  \n",
    "this is sometimes called log-majorization (addressed below)  \n",
    "\n",
    "This writeup will in general refer to something as 'weak majorization' if we only know  \n",
    "$\\sum_{i=1}^k \\big \\vert\\lambda_i\\big \\vert \\leq  \\sum_{i=1}^k  \\sigma_i  $   \n",
    "$\\sum_{i=1}^n \\big \\vert\\lambda_i\\big \\vert \\leq\\sum_{i=1}^n  \\sigma_i  $  \n",
    "(i.e. there are some cases under consideration where the $k=n$ has a strict inequality or we otherwise cannot prove that the $k=n$ case is in fact the equality case)  \n",
    "\n",
    "and will refer things proper or strong majorization (or sometimes just majorzation) if we are *certain* that  \n",
    "$\\sum_{i=1}^k \\big \\vert\\lambda_i\\big \\vert \\leq  \\sum_{i=1}^k  \\sigma_i  $   \n",
    "$\\sum_{i=1}^n \\big \\vert\\lambda_i\\big \\vert = \\sum_{i=1}^n  \\sigma_i  $  \n",
    "\n",
    "\n",
    "*remark 2:*  \n",
    "Since there are finitely many real non-negative terms involved in the relationship   \n",
    "$\\sum_{i=1}^k \\big \\vert\\lambda_i\\big \\vert \\leq \\sum_{i=1}^k  \\sigma_i  $ \n",
    "we *could* work backwards to find some intermediate sequence / fundamental bridge that does classically majorize the magnitude of the eigenvalues -- call these quantities $\\phi_i$.  Supposing\n",
    "\n",
    "$\\sum_{i=1}^n \\big \\vert \\lambda_i\\big \\vert  = \\sum_{i=1}^n \\sigma_i$  \n",
    "Then there is nothing to do.  Otherwise compute  \n",
    "$d_n =  \\big(\\sum_{i=1}^n \\sigma_i\\big) - \\sum_{i=1}^n \\big \\vert \\lambda_i\\big \\vert \\gt 0 $   \n",
    "\n",
    "now set \n",
    "$\\phi_n := \\max\\big(0, \\sigma_n -d_n\\big)$  \n",
    "\n",
    "now compute  \n",
    "$d_{n-1} =  \\big(\\sum_{i=1}^{n-1} \\sigma_i\\big) - \\sum_{i=1}^{n-1} \\big \\vert \\lambda_i\\big \\vert$   \n",
    "if $d_{n-1} =0$ then we are done.  Otherwise repeat the above procedure setting    \n",
    "$\\phi_n := \\max\\big(0, \\sigma_{n-1} -d_{n-1}\\big)$  \n",
    "\n",
    "continue onward as needed with for $j = 2,3 ,...,n$  \n",
    "$d_{n-j} =  \\big(\\sum_{i=1}^{n-j} \\sigma_i\\big) - \\sum_{i=1}^{n-j} \\big \\vert \\lambda_i\\big \\vert$   \n",
    "$\\phi_{n-j} := \\max\\big(0, \\sigma_{n-j} -d_{n-j}\\big)$  \n",
    "\n",
    "since  \n",
    "$0\\leq \\sum_{i=1}^n \\big \\vert \\lambda_i\\big \\vert = c$ \n",
    "is fixed and \n",
    "\n",
    "$c\\leq \\sum_{i=1}^n \\sigma_i$   \n",
    "but we may piecewise contract the size of the sum \n",
    "$\\sum_{i=1}^n \\sigma_i \\to \\sum_{i=1}^n \\phi_i$   \n",
    "and we may in fact, if needed, set $\\sum_{i=1}^n \\phi_i =0$  then there must be sequence of $\\phi_i$'s that sum to $c$ (the argument should be cleaned up but it is in effect a piecewise linear intermediate value theorem), and this gives us a proper majorization where \n",
    "\n",
    "if \n",
    "$\\sum_{i=1}^k \\sigma_i \\leq c$ then  \n",
    "$ \\sum_{i=1}^k \\big \\vert \\lambda_i \\big \\vert \\leq  \\sum_{i=1}^k \\sigma_i  = \\sum_{i=1}^k \\phi_i$   \n",
    "by the above construction, and when $\\sum_{i=1}^k \\sigma_i \\geq c$  we have  \n",
    "$\\sum_{i=1}^k \\big \\vert \\lambda_i \\big \\vert \\leq c = \\sum_{i=1}^k \\phi_i \\leq \\sum_{i=1}^k \\sigma_i $   \n",
    "\n",
    "thus for any $k \\in \\{1,2,...,n-1\\}$ we have  \n",
    "$\\sum_{i=1}^k \\big \\vert \\lambda_i \\big \\vert \\leq \\sum_{i=1}^k \\phi_i \\leq \\sum_{i=1}^k \\sigma_i $   \n",
    "and  \n",
    "$\\sum_{i=1}^n \\big \\vert \\lambda_i \\big \\vert = \\sum_{i=1}^n \\phi_i \\leq \\sum_{i=1}^n \\sigma_i $   \n",
    "\n",
    "with the additional (implicit) structure that $\\sigma_i \\geq \\phi_i$ for all i.  Thus $\\phi_i$'s could be viewed as forming a classical/proper majorizing sequence.  \n",
    "\n",
    "The remark from Polya here e.g. in context of Golden-Thompson inequality is that if \n",
    "$\\sum_{i=1}^k \\big \\vert \\lambda_i \\big \\vert \\leq \\sum_{i=1}^k \\sigma_i $   \n",
    "for $k \\in \\{1,2,....,n-1,n\\}$ and the inequality is strict for $k=n$ \n",
    "then we can apply majorization inequalities for any function that is both (Schur) *convex and increasing*.  The creation of $\\phi_i$ makes this clear -- make use of convexity and classical majorization here, then term by term dominance $\\phi_i \\leq \\sigma_i$, which is 'carried over' by virtue of being an increasing function, so $f(\\phi_i)\\leq f(\\sigma_i)$, and summing over this bound then gives \n",
    "\n",
    "$\\sum_{i=1}^n f\\Big(\\big \\vert \\lambda_i \\big \\vert\\Big) \\leq \\sum_{i=1}^n f\\Big(\\phi_i\\Big) \\leq \\sum_{i=1}^n f\\Big(\\sigma_i\\Big) $   \n",
    "\n",
    "or  \n",
    "$\\sum_{i=1}^n f\\Big(\\big \\vert \\lambda_i \\big \\vert\\Big) \\leq \\sum_{i=1}^n f\\Big(\\sigma_i\\Big) $   \n",
    "which makes the role of the fact that we require the function to be convex *and* increasing clear.  \n",
    "\n",
    "*another remark:*  \n",
    "if we were dealing with a nonsingular matrix, and applying a convex, increasing function $f$ that is only defined on positive numbers, we could slightly complicate the above algorithm, by running the same process as above, except at each stage \n",
    "\n",
    "$\\phi_{n-j} := \\max\\big(\\epsilon, \\sigma_{n-j} -d_{n-j}\\big)$  \n",
    "for some sufficiently small $\\epsilon \\gt 0$, perhaps selecting $\\epsilon := \\frac{1}{m}\\big \\vert \\lambda_n\\big \\vert $ where $m$ is the remaining number of $\\phi's$ that need 'filler'-- i.e. that need some de minimis positive amount.  A few more details need to be filled in to make this algorithm.    \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "regarding **log-majorization**  \n",
    "for any $\\mathbf A \\in \\mathbb C^\\text{n x n}$  \n",
    "\n",
    "$\\prod_{j=1}^k \\big \\vert \\lambda_j\\big \\vert \\leq \\prod_{j=1}^k \\sigma_j$  \n",
    "with equality when  $k=n$ \n",
    "\n",
    "because of equality in magnitude in determinants--use polar form or consider taking advantage of real non-negativity and 'squaring', observing that  \n",
    "$\\prod_{j=1}^n \\big \\vert \\lambda_j\\big \\vert^2 = \\big \\vert\\det\\big(\\mathbf A\\big)\\big \\vert^2 = \\big \\vert\\det\\big(\\mathbf A^*\\big)\\big \\vert \\cdot \\big \\vert \\det\\big(\\mathbf A\\big)\\big \\vert  = \\big \\vert\\det\\big(\\mathbf A^*\\mathbf A\\big)\\big \\vert =\\det\\big(\\mathbf A^*\\mathbf A\\big)=\\prod_{j=1}^n \\sigma_j^2 $  \n",
    "\n",
    "taking square roots of each side gives the result (alternatively directly compute SVD of $\\mathbf A$ and compute the determinant two different ways, recalling that unitary matrices have determinant magnitude of 1)   \n",
    "\n",
    "- - -- - \n",
    "the beginning of this posting already proved the case for when $k=1$ (quadratic form argument)  \n",
    "\n",
    "We specialize to **non-singular** $\\mathbf A \\in \\mathbb C^\\text{n x n}$  \n",
    "\n",
    "so we want to prove  \n",
    "$\\prod_{j=1}^k \\big \\vert \\lambda_j\\big \\vert \\leq \\prod_{j=1}^k \\sigma_j$  \n",
    "for $k \\in \\{2, 3, ..., n-1\\}$   \n",
    "\n",
    "(i.e. we get the $k=1$ case for free based on a quadratic form argument at the top of this notebook, or from the weak majorization that we know exists)  \n",
    "\n",
    "- - - - -  \n",
    "and we *know*  \n",
    "$\\prod_{j=1}^n \\big \\vert \\lambda_j\\big \\vert = \\prod_{j=1}^n \\sigma_j$   \n",
    "and per the above we know about the weak majorization of  \n",
    "$\\sum_{j=1}^k \\big \\vert \\lambda_j\\big \\vert \\leq \\sum_{j=1}^k \\sigma_j$  \n",
    "for $k \\in \\{1,2, 3, ..., n-1, n\\}$    \n",
    "- - - - -   \n",
    "\n",
    "note: in log-space our product becomes  \n",
    "\n",
    "$\\sum_{j=1}^k \\log\\Big(\\big \\vert \\lambda_j\\big \\vert\\Big) \\leq \\sum_{j=1}^k \\log\\Big(\\sigma_j\\Big)$  \n",
    "and  \n",
    "$\\sum_{j=1}^n \\log\\Big(\\big \\vert \\lambda_j\\big \\vert\\Big)=\\sum_{j=1}^n \\log\\Big(\\sigma_j\\Big)$  \n",
    "hence the term 'log-majorization'.  \n",
    "- - - - -  \n",
    "\n",
    "**proof**  \n",
    "There a nice recursive proof involved here.  \n",
    "\n",
    "The essence of the proof is that weak majorization with equivalence between product / determinant is enough to prove the desired result in $n-1$ dimensions *and* preserve optimal substructure so that we can re-use / recurse with our original argument.  \n",
    "\n",
    "\n",
    "We start with $n$ then work backwards to $n-1$ then to $n-2$ and so on down to $2$, recalling that we get the $k=1$ case for free.    \n",
    "\n",
    "near the top of this notebook, we proved \n",
    "\n",
    "$\\sigma_n^2 \\leq \\big \\vert \\lambda_n\\big \\vert^2$ or  \n",
    "$\\sigma_n \\leq \\big \\vert \\lambda_n\\big \\vert$  or  \n",
    "$ \\frac{1}{\\big \\vert \\lambda_n\\big \\vert} \\leq \\frac{1}{\\sigma_n}$    \n",
    "via a quadratic form argument  \n",
    "(alternatively consider weak majorization of eigenvalues and singular values of the inverse of the matrix in question, and in particular k=1 case)  \n",
    "\n",
    "so applying the above, we know  \n",
    "$\\prod_{j=1}^{n-1} \\big \\vert \\lambda_j\\big \\vert =  \\frac{1}{\\big \\vert \\lambda_n\\big \\vert}\\prod_{j=1}^n \\big \\vert \\lambda_j\\big \\vert  \\leq \\frac{1}{\\sigma_n}\\prod_{j=1}^n \\big \\vert \\lambda_j\\big \\vert = \\frac{1}{\\sigma_n}\\prod_{j=1}^n \\sigma_j = \\prod_{j=1}^{n-1} \\sigma_j = \\big(\\prod_{j=1}^{n-2} \\sigma_j\\big)\\sigma_{n-1}$  \n",
    "which is step one.  \n",
    "\n",
    "we also know, by weak majorization that  \n",
    "$\\sum_{j=1}^{n-1} \\big \\vert \\lambda_j\\big \\vert \\leq \\sum_{j=1}^{n-1} \\sigma_j = \\big(\\sum_{j=1}^{n-2} \\sigma_j\\big) + \\sigma_{n-1}$  \n",
    "but we can refine this   \n",
    "\n",
    "suppose we set  \n",
    "$\\gamma_{n-1} := \\prod_{j=1}^{n-1} \\big \\vert \\lambda_j\\big \\vert\\big(\\prod_{j=1}^{n-2} \\sigma_j\\big)^{-1} \\gt 0$    \n",
    "\n",
    "it is immediate that  \n",
    "$\\prod_{j=1}^{n-1} \\big \\vert \\lambda_j\\big \\vert = \\big(\\prod_{j=1}^{n-2} \\sigma_j\\big)\\gamma_{n-1} \\leq \\big(\\prod_{j=1}^{n-2} \\sigma_j\\big)\\sigma_{n-1}$  \n",
    "\n",
    "but it further implies   \n",
    "$\\sum_{j=1}^{n-1} \\big \\vert \\lambda_j\\big \\vert\\leq  \\big(\\sum_{j=1}^{n-2} \\sigma_j\\big) + \\gamma_{n-1}  \\leq \\big(\\sum_{j=1}^{n-2} \\sigma_j\\big) + \\sigma_{n-1}$  \n",
    "\n",
    "The RHS inequality is immediate from the above.  \n",
    "The LHS is more subtle, but we inherit this from weak majorization of the first $n-2$ values.  \n",
    "\n",
    "*i.e. suppose for a* **contradiction**  *that we have*  \n",
    "$ \\big(\\sum_{j=1}^{n-2} \\sigma_j\\big) + \\gamma_{n-1} \\lt \\sum_{j=1}^{n-1} \\big \\vert \\lambda_j\\big \\vert = \\Big(\\sum_{j=1}^{n-2} \\big \\vert \\lambda_j\\big \\vert\\Big) + \\big \\vert \\lambda_{n-1}\\big \\vert$   \n",
    "\n",
    "but by weak majorization we still know that  \n",
    "$ \\big(\\sum_{j=1}^{k} \\sigma_j\\big) \\geq  \\sum_{j=1}^{k} \\big \\vert \\lambda_j\\big \\vert$   \n",
    "for $k \\in \\{1, 2, ..., n-2\\}$   \n",
    "\n",
    "and in particular  \n",
    "$ \\Big(\\sum_{j=1}^{n-2} \\sigma_j\\Big) + \\big \\vert \\lambda_{n-1}\\big \\vert \\geq  \\Big(\\sum_{j=1}^{n-2} \\big \\vert \\lambda_j\\big \\vert\\Big) + \\big \\vert \\lambda_{n-1}\\big \\vert$   \n",
    "\n",
    "using the 'spirit' of our preceding argument for weak majorization we can construct $\\phi_{n-1} \\in \\Big(\\gamma_{n-1}, \\big\\vert \\lambda_{n-1}\\big\\vert \\Big]$ and have the below relations      \n",
    "$ \\big(\\sum_{j=1}^{n-2} \\sigma_j\\big) \\lt \\big(\\sum_{j=1}^{n-2} \\sigma_j\\big) + \\gamma_{n-1} \\lt   \\big(\\sum_{j=1}^{n-2} \\sigma_j\\big) + \\phi_{n-1} =   \\Big(\\sum_{j=1}^{n-2} \\big \\vert \\lambda_j\\big \\vert\\Big) + \\big \\vert \\lambda_{n-1}\\big \\vert \\leq \\Big(\\sum_{j=1}^{n-2} \\sigma_j\\Big) + \\big \\vert \\lambda_{n-1}\\big \\vert$   \n",
    "\n",
    "The above equality, combined with weak majorization for the first $n-2$ terms implies  \n",
    "$ \\begin{bmatrix}\n",
    "\\sigma_1\\\\ \n",
    "\\sigma_2 \\\\ \n",
    "\\vdots\\\\ \n",
    "\\sigma_{n-2}\\\\ \n",
    "\\phi_{n-1}\n",
    "\\end{bmatrix} \\succeq \\begin{bmatrix}\n",
    "\\big \\vert \\lambda_{1}\\big \\vert\\\\ \n",
    "\\big \\vert \\lambda_{2}\\big \\vert\\\\ \n",
    "\\vdots\\\\ \n",
    "\\big \\vert \\lambda_{n-2}\\big \\vert\\\\ \n",
    "\\big \\vert \\lambda_{n-1}\\big \\vert\n",
    "\\end{bmatrix}$    \n",
    "where the majorization is strong or \"proper\" (and each term is postive).  \n",
    "\n",
    "But elementary symmetric functions are Schur concave so taking the 'determinant' or $(n-1)$th elementary symmetric function here we have  \n",
    "\n",
    "$\\big(\\prod_{j=1}^{n-2} \\sigma_j \\big)\\gamma_{n-1} \\lt \\big(\\prod_{j=1}^{n-2} \\sigma_j \\big)\\phi_{n-1} \\leq \\prod_{j=1}^{n-1} \\big \\vert \\lambda_{n-1}\\big \\vert$   \n",
    "\n",
    "which contradicts the fact that  \n",
    "$\\big(\\prod_{j=1}^{n-2} \\sigma_j\\big)\\gamma_{n-1} = \\prod_{j=1}^{n-1} \\big \\vert \\lambda_j\\big \\vert $    \n",
    "\n",
    "- - - - - \n",
    "hence we've proven that  \n",
    "\n",
    "$ \\begin{bmatrix}\n",
    "\\big \\vert \\lambda_{1}\\big \\vert\\\\ \n",
    "\\big \\vert \\lambda_{2}\\big \\vert\\\\ \n",
    "\\vdots\\\\ \n",
    "\\big \\vert \\lambda_{n-2}\\big \\vert\\\\ \n",
    "\\big \\vert \\lambda_{n-1}\\big \\vert\n",
    "\\end{bmatrix}\\preceq_w \\begin{bmatrix}\n",
    "\\sigma_1\\\\ \n",
    "\\sigma_2 \\\\ \n",
    "\\vdots\\\\ \n",
    "\\sigma_{n-2}\\\\ \n",
    "\\gamma_{n-1}\n",
    "\\end{bmatrix} $ \n",
    "\n",
    "(where the majorization in general is weak)  \n",
    "but we know    \n",
    "\n",
    "$e_{n-1}\\Big(\\begin{bmatrix}\n",
    "\\big \\vert \\lambda_{1}\\big \\vert\\\\ \n",
    "\\big \\vert \\lambda_{2}\\big \\vert\\\\ \n",
    "\\vdots\\\\ \n",
    "\\big \\vert \\lambda_{n-2}\\big \\vert\\\\ \n",
    "\\big \\vert \\lambda_{n-1}\\big \\vert\n",
    "\\end{bmatrix}\\Big) = \\prod_{j=1}^{n-1} \\big \\vert \\lambda_j\\big \\vert = \\big(\\prod_{j=1}^{n-2} \\sigma_j\\big)\\gamma_{n-1} = e_{n-1}\\Big(\\begin{bmatrix}\n",
    "\\sigma_1\\\\ \n",
    "\\sigma_2 \\\\ \n",
    "\\vdots\\\\ \n",
    "\\sigma_{n-2}\\\\ \n",
    "\\gamma_{n-1}\n",
    "\\end{bmatrix}\\Big) $  \n",
    " \n",
    "which is a replica of our original problem, except 1 dimension smaller.  Now set $n:= n-1$ and recurse. \n",
    "\n",
    "- - - -  \n",
    "For avoidance of doubt, none of the $\\lambda$'s have changed.  We did introduce a slack parameter $\\gamma_{n-1}$ but have not changed any of the $n-2$ first singular values.  We have already proven that   \n",
    "$\\prod_{j=1}^{n-1}\\big \\vert \\lambda_j \\big \\vert \\leq  \\prod_{j=1}^{n-1} \\sigma_j$  \n",
    "\n",
    "and when we recurse once, we will have proven  \n",
    "$\\prod_{j=1}^{n-2}\\big \\vert \\lambda_j \\big \\vert \\leq  \\prod_{j=1}^{n-2} \\sigma_j$  \n",
    "\n",
    "and have in our hands  \n",
    "$ \\begin{bmatrix}\n",
    "\\big \\vert \\lambda_{1}\\big \\vert\\\\ \n",
    "\\big \\vert \\lambda_{2}\\big \\vert\\\\ \n",
    "\\vdots\\\\ \n",
    "\\big \\vert \\lambda_{n-2}\\big \\vert\\\\ \n",
    "\\end{bmatrix}\\preceq_w \\begin{bmatrix}\n",
    "\\sigma_1\\\\ \n",
    "\\sigma_2 \\\\ \n",
    "\\vdots\\\\ \n",
    "\\gamma_{n-2}\n",
    "\\end{bmatrix} $ \n",
    "\n",
    "and the next recursion then gives   \n",
    "$\\prod_{j=1}^{k}\\big \\vert \\lambda_j \\big \\vert \\leq  \\prod_{j=1}^{k} \\sigma_j$  \n",
    "for $k=n-3$  \n",
    "\n",
    "and the next will give   \n",
    "$\\prod_{j=1}^{k}\\big \\vert \\lambda_j \\big \\vert \\leq  \\prod_{j=1}^{k} \\sigma_j$  \n",
    "for $k=n-4$  \n",
    "\n",
    "and so on, recalling that nothing needs proven for the $k=1$ case    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**corollary: singular value of (AB) are weakly majorized by singular values of A times singular values of B** \n",
    "i.e. with the usual ordering  $\\sigma_1 \\geq \\sigma_2 \\geq .... \\geq \\sigma_n$  this reads   \n",
    "$\\Sigma_{AB}\\preceq_w \\Sigma_{A}\\Sigma_{B}$   \n",
    "\n",
    "- - - - -  \n",
    "*technical nits on dimensions*  \n",
    "if A is tall and skinny, we can zero pad its columns so that it is square without impacting its non-zero eigenvalues (consider $AA*$) or those of $AB$.  Similarly we can zero pad rows of $B$ so that it is square if it is short and fat.  **There are still some pending nits to be addressed here but in general we find workarounds to deal with non-square matrices so we can treat them as square**  \n",
    "(with each $\\Sigma$ selected to be square for convenience)  \n",
    "- - - - -  \n",
    "in all cases below   \n",
    "$\\mathbf D_k = \\begin{bmatrix}\n",
    "\\mathbf I_k & \\mathbf 0\\\\ \n",
    "\\mathbf 0 & \\mathbf 0_{n-k}\n",
    "\\end{bmatrix}$  \n",
    "\n",
    "in order to show weak majorization, we need to show \n",
    "\n",
    "$\\text{trace}\\big(\\mathbf D_k\\Sigma_{AB}\\big)\\leq \\text{trace}\\big(\\mathbf D_k\\Sigma_{A}\\Sigma_{B}\\big)$   \n",
    "\n",
    "for $k \\in \\{1,2,3,..., n\\}$  \n",
    "\n",
    "note, by submultiplicativity of the operator norm (Schatten $\\infty$ norm), we already have this for the $k=1$ case.  \n",
    "\n",
    "In general we prove this by showing, with $\\mathbf {AB} = \\mathbf U\\Sigma_{AB}\\mathbf V^*$    \n",
    "\n",
    "$ \\text{trace}\\Big(\\mathbf D_k\\Sigma_{AB}\\Big)$  \n",
    "$= \\text{trace}\\Big( \\mathbf D_k \\mathbf U^* \\mathbf {AB}\\mathbf V\\Big)$  \n",
    "$= \\text{trace}\\Big(\\big(\\mathbf D_k \\mathbf U^* \\mathbf A\\big)\\big( \\mathbf B\\mathbf V\\big) \\Big)$  \n",
    "$\\leq \\text{trace}\\big(\\Sigma_{ D_k U^*A}\\Sigma_{BV}\\big)$  \n",
    "$= \\text{trace}\\big(\\Sigma_{ D_k U^*A}\\Sigma_{B}\\big)$  \n",
    "$\\leq \\text{trace}\\big(\\mathbf D_k\\Sigma_{  A^{(1)}}\\Sigma_{B}\\big)$  \n",
    "$\\leq \\text{trace}\\big(\\mathbf D_k\\Sigma_{  A}\\Sigma_{B}\\big)$  \n",
    "\n",
    "the first inequality is the von-Neumann trace inequality, then we recognize that $\\Sigma_{BV}= \\Sigma_{B}$ because unitary transformations don't change singular values, and finally we observe (proof is at the end) that  \n",
    "\n",
    "$\\Sigma_{D_k QA} \\preceq_w \\mathbf D_k \\Sigma_{A}$  \n",
    "but mimicking the algorithmic approach in *corollary: absolute values of eigenvalues of A are weakly majorized by singular values of A* we can create a fundamental bridge   \n",
    "\n",
    "$\\Sigma_{D_k QA} \\preceq \\mathbf D_k \\Sigma_{A^{(1)}} \\leq \\mathbf D_k \\Sigma_{A}$    \n",
    "where the first \"inequality\" is proper majorization and the second one is a point wise bound  \n",
    "\n",
    "by proper majorization, we have    \n",
    "$\\text{trace}\\big( \\Sigma_{D_k QA}\\big) \\preceq \\text{trace}\\big(\\mathbf D_k \\Sigma_{A^{(1)}}\\Sigma_{B}\\big)$  \n",
    "$\\Sigma_{D_k QA}= \\sum_k w_k\\cdot \\mathbf P_{(k)}\\big(\\mathbf D_k \\Sigma_{A^{(1)}}\\big)\\mathbf P_{(k)}^T$   \n",
    "with $w_k \\geq 0$ and $\\sum_k w_k =1$  \n",
    "and application of the re-arrangement inequality justifies the second to last trace inequality  \n",
    "(i.e. revisit the process at the beginning of the majorization section in this note book)  \n",
    "\n",
    "As for the point-wise bound, it immediately gives us the final trace inequality of   \n",
    "$\\text{trace}\\big(\\mathbf D_k \\Sigma_{A^{(1)}}\\Sigma_{B}\\big)\\leq \\text{trace}\\big(\\mathbf D_k \\Sigma_{A}\\Sigma_{B}\\big)$  \n",
    "\n",
    "in many ways the above is just a re-use of earlier arguments and processes and the proof in this section is *almost* immediate.  What remains to be proven is    \n",
    "\n",
    "$\\Sigma_{D_k QA} \\preceq_w \\mathbf D_k \\Sigma_{A}$    \n",
    "and in particular we need to prove, for $r \\in\\{1,2,..,k\\}$    \n",
    "\n",
    "$\\text{trace}\\Big(\\mathbf D_r\\Sigma_{D_k QA}\\Big)\\leq \\text{trace}\\Big(\\mathbf D_r \\big(\\mathbf D_k\\Sigma_{A}\\big)\\Big)$    \n",
    "\n",
    "we do this by mimicking our beginning argument, this time considering SVD of $\\big(\\mathbf D_k \\mathbf {QA}\\big)$   \n",
    "for $r \\in\\{1,2,..,k\\}$  \n",
    "\n",
    "$\\text{trace}\\Big(\\mathbf D_r\\Sigma_{D_k QA}\\Big)$    \n",
    "$=\\text{trace}\\Big(\\mathbf D_r\\big(\\mathbf U^*\\mathbf D_k \\mathbf {QA}\\mathbf V\\big)\\Big)$  \n",
    "$=\\text{trace}\\Big(\\mathbf V\\mathbf D_r\\mathbf U^*\\mathbf D_k \\mathbf {QA}\\Big)$  \n",
    "$= \\text{trace}\\Big(\\big(\\mathbf D_r\\mathbf U^*\\mathbf D_k\\big) \\big(\\mathbf {QA}\\mathbf V\\big)\\Big)$  \n",
    "$\\leq \\text{trace}\\Big(\\Sigma_{ D_r U^* D_k} \\Sigma_{QA V}\\Big)$  \n",
    "$=\\text{trace}\\Big(\\Sigma_{ D_r U^* D_k} \\Sigma_{A}\\Big)$  \n",
    "$\\leq \\text{trace}\\Big(\\mathbf D_r \\Sigma_{A}\\Big)$  \n",
    "$=\\text{trace}\\Big(\\mathbf D_r \\big(\\mathbf D_k\\Sigma_{A}\\big)\\Big)$  \n",
    "\n",
    "where the first inequality is von-Neumann trace inequality and the second one comes from observing that  \n",
    "\n",
    "$\\text{rank}\\big(\\mathbf D_r\\mathbf U^*\\mathbf D_k\\big) \\leq \\text{rank}\\big(\\mathbf D_r\\big) =r$ and  \n",
    "$\\big\\Vert\\mathbf D_r\\mathbf U^*\\mathbf D_k\\big\\Vert_{S_\\infty}\\leq \\big\\Vert\\mathbf D_r\\big\\Vert_{S_\\infty}\\big \\Vert\\mathbf U^*\\mathbf D_k\\big\\Vert_{S_\\infty}\\leq \\big\\Vert\\mathbf D_r\\big\\Vert_{S_\\infty}\\big\\Vert \\mathbf U^*\\big\\Vert_{S_\\infty}\\big \\Vert \\mathbf D_k\\big\\Vert_{S_\\infty} = 1 \\cdot 1 \\cdot 1 =1$   \n",
    "i.e. submultiplicativity of the operator 2 norm / Schatten $\\infty$ norm  \n",
    "\n",
    "Thus  $\\Sigma_{ D_r U^* D_k}$ has (at most) its first $r$ diagonal components non-zero -- each of which has a value $\\leq 1$  \n",
    "\n",
    "so the final inequality is equivalent to observing thatf or any $\\delta_j \\in [0,1]$     \n",
    "$\\sum_{j=1}^r \\sigma_j^{(A)}\\cdot \\delta_j \\leq \\sum_{j=1}^r \\sigma_j^{(A)}\\cdot 1 = \\sum_{j=1}^r \\sigma_j^{(A)}=\\text{trace}\\Big(\\mathbf D_r \\Sigma_{A}\\Big)$   \n",
    "\n",
    "\n",
    "This proves that  \n",
    "$\\Sigma_{D_k QA} \\preceq_w \\mathbf D_k \\Sigma_{A}$  \n",
    "\n",
    "and completes the proof that  \n",
    "$\\Sigma_{AB}\\preceq_w \\Sigma_{A}\\Sigma_{B}$  \n",
    "- - - - -  \n",
    "\n",
    "\n",
    "*finally*, observing with weak majorization established and observing that if $A$ And $B$ are square  \n",
    "$ \\det\\big(\\Sigma_{AB}\\big) = \\big \\vert\\det\\big(\\mathbf A\\mathbf B\\big)\\big \\vert = \\big \\vert\\det\\big(\\mathbf A\\big)\\big \\vert \\big \\vert \\det\\big(\\mathbf B\\big)\\big \\vert = \\det\\big(\\Sigma_{A}\\big)\\det\\big(\\Sigma_{B}\\big)$   \n",
    "we could run the preceding log-majorization argument for absolute values of eigenvalues and singular values verbatim and see that $\\log\\big(\\Sigma_{AB}\\big) \\preceq \\log\\big(\\Sigma_{A}\\big) +\\log(\\Sigma_{B}\\big)$  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**claim:**  \n",
    "For Hermitian $\\mathbf X$ and Hermitian $\\mathbf Y$, each in $\\mathbb C^{\\text{n x n}}$  and natural number $N$, we have  \n",
    " \n",
    "$\\text{trace}\\Big(\\big(\\mathbf X \\mathbf Y\\big)^{2^N}\\Big) \\leq \\text{trace}\\Big(\\mathbf X^{2^N} \\mathbf Y^{2^N}\\Big)$  \n",
    "\n",
    "*remark:*   this is a key inequality proven on page 3 of Vershynin's proof of the *Golden-Thompson Inequality* using the above relationship of singular values majorizing eigenvalues.  However given that every power is purely a multiple of two, something akin to bisection, using repeated applications of Schur's Inequality seems to be of interest here.  We'll subsequently step through the proof using tools from majorization. The underlying technique reminds your author, in spirit, of Cauchy's leap forward, fall back induction, though technically we are only (backward) inducting and taking advantage of powers of 2 here.  \n",
    "\n",
    "**proof:**  \n",
    "*step one base case (to set up inducting, backwards)*  \n",
    "$\\text{trace}\\Big(\\big(\\mathbf Y^{2^{N-1}}\\mathbf X^{2^{N-1}}\\big)^2\\Big)  \\leq \\Big \\Vert  \\mathbf Y^{2^{N-1}}\\mathbf X^{2^{N-1}}\\Big \\Vert_F^2 = \\text{trace}\\big(\\mathbf X^{2^{N-1}} \\mathbf Y^{2^{N-1}}\\mathbf Y^{2^{N-1}} \\mathbf X^{2^{N-1}}\\Big)=  \\text{trace}\\Big(\\mathbf X^{2^N} \\mathbf Y^{2^N}\\Big) $  \n",
    "\n",
    "via Schur's Inequality  \n",
    "we note that \n",
    "$\\big(\\mathbf Y^{2^{N-1}}\\mathbf X^{2^{N-1}}\\big)$ has the same eigenvalues as   \n",
    "$\\big(\\mathbf X^{2^{N-1}}\\big)^\\frac{1}{2} \\mathbf Y^{2^{N-1}}\\big(\\mathbf X^{2^{N-1}}\\big)^\\frac{1}{2} = \\big(\\mathbf X^{2^{N-2}} \\mathbf Y^{2^{N-2}}\\mathbf Y^{2^{N-2}}\\mathbf X^{2^{N-2}}\\big)$    \n",
    "\n",
    "which is Hermitian (and in fact positive semi definite though we don't need this), hence its eigenvalues are all real, which means squared eigenvalues are all real non-negative and hence the trace gives the sum of the magnitude of the squared eigenvalues  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "*optional step two*  \n",
    "we could head straight to the induction now but this step two may give some additional insight, so we include it as an optional step   \n",
    "\n",
    "$\\mathbf A := \\big(\\mathbf Y^{2^{N-2}}\\mathbf X^{2^{N-2}}\\big)$  \n",
    "$\\text{trace}\\Big(\\big(\\mathbf Y^{2^{N-2}}\\mathbf X^{2^{N-2}}\\big)^{2^2}\\Big) $  \n",
    "$=\\text{trace}\\Big(\\big(\\mathbf Y^{2^{N-2}}\\mathbf X^{2^{N-2}}\\big)^4\\Big) $  \n",
    "$=\\text{trace}\\Big(\\mathbf A^4\\Big) $  \n",
    "$= \\text{trace}\\Big(\\big(\\mathbf A^2\\big)^2\\Big)  $  \n",
    "$\\leq \\Big \\Vert \\mathbf A^2\\Big \\Vert_F^2 $  \n",
    "$= \\text{trace}\\Big(\\big(\\mathbf A^2\\big)^*\\big(\\mathbf A^2\\big) \\Big)  $  \n",
    "$\\leq \\text{trace}\\Big(\\big(\\mathbf A^*\\mathbf A\\big)^2 \\Big)  $  \n",
    "$= \\text{trace}\\Big(\\big(\\mathbf X^{2^{N-2}} \\mathbf Y^{2^{N-2}}\\mathbf Y^{2^{N-2}}\\mathbf X^{2^{N-2}}\\big)^2\\Big)$   \n",
    "$=\\text{trace}\\Big(\\big(\\mathbf Y^{2^{N-1}}\\mathbf X^{2^{N-1}}\\big)^2\\Big) $  \n",
    "where the first inequality is Schur's Inequality, and the second one is implied by it (i.e. see section \"yet another look at at Normal Matrices\" most of the way down in the notebook \"Schurs_Inequality.ipynb\").  \n",
    "\n",
    "*re: the equality case:*  \n",
    "In both cases the inequality is strict unless $\\mathbf A$ is normal (i.e. unitarily diagonalizable). However we know that  \n",
    "$\\text{spec}\\big(\\mathbf A \\big) = \\text{spec}\\big(\\mathbf Y^{2^{N-3}}\\mathbf X^{2^{N-2}}\\big)\\mathbf Y^{2^{N-3}}$  \n",
    "\n",
    "hence $\\mathbf A$ has the same eigenvalues as a Hermitian (positive semidefinite) matrix and thus has entirely real (non-negative) eigenvalues, hence if $\\mathbf A$ is normal it must be Hermitian, which means that \n",
    "\n",
    "$\\mathbf Y^{2^{N-2}}\\mathbf X^{2^{N-2}}= \\mathbf A = \\mathbf A^* = \\big(\\mathbf Y^{2^{N-2}}\\mathbf X^{2^{N-2}}\\big)^* = \\big(\\mathbf X^{2^{N-2}}\\big)^*\\big(\\mathbf Y^{2^{N-2}}\\big)^* = \\mathbf X^{2^{N-2}}\\mathbf Y^{2^{N-2}} $    \n",
    "\n",
    "i.e. the above inequality being non-strict implies $\\mathbf A$ being normal which implies $\\mathbf Y^{2^{N-2}}$ and $\\mathbf X^{2^{N-2}}$  commute.  \n",
    "\n",
    "note that the choice of $N$ is arbitrary and any natural number $N\\geq 4$ will do.  In particular consider selecting $N:=4$ and the above reads:  \n",
    "\n",
    "$\\mathbf Y^{4}\\mathbf X^{4}= \\mathbf A = \\mathbf A^* = \\mathbf X^{2^{4-2}}\\mathbf Y^{2^{4-2}} = \\mathbf X^4 \\mathbf Y^4 $  \n",
    "\n",
    "\n",
    "*this is more than enough to setup an induction*  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*inductive step*  \n",
    "In particular we want to prove  \n",
    "\n",
    "$\\text{trace}\\Big(\\big(\\mathbf Y^{2^{N-i}}\\mathbf X^{2^{N-i}}\\big)^{2^i}\\Big) \\leq \\text{trace}\\Big(\\big(\\mathbf Y^{2^{N-(i-1)}}\\mathbf X^{2^{N-(i-1)}}\\big)^{2^{i-1}}\\Big) = \\text{trace}\\Big(\\big(\\mathbf Y^{2^{N-i+1}}\\mathbf X^{2^{N-i+1}}\\big)^{2^{i-1}}\\Big)$  \n",
    "for $2\\leq i \\leq N$  \n",
    "- - - - \n",
    "where our inductive hypothesis tells us that  \n",
    "$\\text{trace}\\Big(\\big(\\mathbf Y^{2^{N-(i-1)}}\\mathbf X^{2^{N-(i-1)}}\\big)^{2^{i-1}}\\Big) \\leq \\text{trace}\\Big(\\big(\\mathbf Y^{2^{N-(i-2)}}\\mathbf X^{2^{N-(i-2)}}\\big)^{2^{i-2}}\\Big)\\leq ... \\leq \\text{trace}\\Big(\\mathbf X^{2^N} \\mathbf Y^{2^N}\\Big)$   \n",
    "- - - -  \n",
    "as before we remark that  \n",
    "$\\text{spec}\\Big(\\mathbf X^{2^{N-i}}\\mathbf Y^{2^{N-i}} \\mathbf Y^{2^{N-i}}\\mathbf X^{2^{N-i}}\\Big) = \\text{spec}\\Big(\\mathbf Y^{2^{N-i+1}}\\mathbf X^{2^{N-i+1}}\\Big)$  \n",
    "thus  \n",
    "$\\text{trace}\\Big(\\big(\\mathbf X^{2^{N-i}}\\mathbf Y^{2^{N-i}} \\mathbf Y^{2^{N-i}}\\mathbf X^{2^{N-i}}\\big)^{2^{i-1}}\\Big) = \\text{trace}\\Big(\\big(\\mathbf Y^{2^{N-i+1}}\\mathbf X^{2^{N-i+1}}\\big)^{2^{i-1}}\\Big)$  \n",
    "\n",
    "*proof:*  \n",
    "$\\mathbf A := \\big(\\mathbf Y^{2^{N-i}}\\mathbf X^{2^{N-i}}\\big)$  \n",
    "\n",
    "so we need to prove  \n",
    "$\\text{trace}\\Big(\\big(\\mathbf A^2\\big)^{2^{i-1}}\\Big) \\leq \\text{trace}\\Big(\\big(\\mathbf A^*\\mathbf A\\big)^{2^{i-1}}\\Big)$  \n",
    "\n",
    "this is very easy to finish off if we use the above approach involving majorization, i.e.   \n",
    "$\\sum_{j=1}^r \\big \\vert \\lambda_j \\big \\vert \\leq \\sum_{j=1}^r \\phi_i \\leq \\sum_{j=1}^r \\sigma_i$    \n",
    "$\\sum_{j=1}^n \\big \\vert \\lambda_j \\big \\vert = \\sum_{j=1}^n \\phi_i \\leq \\sum_{j=1}^n \\sigma_i$   \n",
    "and  $0\\leq \\phi_i \\leq \\sigma_i$ with the above construction \n",
    "\n",
    "setting $\\lambda_i := \\text{spec}\\big(\\mathbf A\\big)$  and $\\sigma_i$ the singular values of $\\mathbf A$ we notice  \n",
    "$\\text{trace}\\Big(\\big(\\mathbf A^2\\big)^{2^{i-1}}\\Big) = \\text{trace}\\Big(\\big(\\mathbf A\\big)^{2^{i}}\\Big) =  \\sum_{j=1}^n  \\lambda_j^{2^{i}} = \\sum_{j=1}^n \\big \\vert \\lambda_j\\big \\vert^{2^{i}} \\leq  \\sum_{j=1}^n  \\phi_j^{2^{i}} \\leq \\sum_{j=1}^n \\sigma_j^{2^{i}} = \\sum_{j=1}^n \\big(\\sigma_j^2\\big)^{2^{i-1}} = \\text{trace}\\Big(\\big(\\mathbf A^*\\mathbf A\\big)^{2^{i-1}}\\Big)$  \n",
    "\n",
    "where the first inequality  \n",
    "$\\sum_{j=1}^n \\big \\vert \\lambda_j\\big \\vert^{2^{i}} \\leq  \\sum_{j=1}^n  \\phi_j^{2^{i}} $   \n",
    "follows by majorization and the application of a convex function over non-negative reals --i.e. $\\mapsto u^{n}$ \n",
    "\n",
    "- - - - -  \n",
    "and noticing that over $\\mathbb R_{\\geq 0}$  $u\\mapsto u^{n}$ is an increasing function for natural number $n$-- we can verify this via a continuous nonnegative 1st derivative (and right derivative at zero), or more simply we can verify this via the algebra of inequalities, for $0 \\leq a \\leq b \\lt \\infty$, we have \n",
    "\n",
    "$0 \\leq a \\leq b$, and raising each term of the nth power gives  \n",
    "$0 \\leq a^n \\leq b^n$  \n",
    "- - - - -  \n",
    "\n",
    "and the second inequality follows by term by term dominance that we have by construction    \n",
    "$\\phi_j \\leq \\sigma_j $  \n",
    "but we apply an increasing function, which thus preserves this ordering,  \n",
    "$\\phi_j^{2^{i}} \\leq  \\sigma_j^{2^{i}} $  \n",
    "\n",
    "and summing over the bound  \n",
    "$\\sum_{j=1}^n \\phi_j^{2^{i}} \\leq \\sum_{j=1}^n \\sigma_j^{2^{i}} $  \n",
    "\n",
    "*another remark:*    \n",
    "there may be another relatively clean way to finish via repeated application of  H&ouml;lder's Inequality as done in Tao's Blog post here:  \n",
    "https://terrytao.wordpress.com/2010/07/15/the-golden-thompson-inequality/  \n",
    "Unfortunately the matrix form with respect to Schatten norms tends to be proven via majorization techniques and these are the only ones that your author 'gets'.  However other proofs are available on the Internet and worth chewing on... e.g. this one may be of interest  https://arxiv.org/pdf/1106.6189v2.pdf.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now that we have proven  \n",
    "$\\text{trace}\\Big(\\big(\\mathbf X \\mathbf Y\\big)^{2^N}\\Big) \\leq \\text{trace}\\Big(\\mathbf X^{2^N} \\mathbf Y^{2^N}\\Big)$  \n",
    "\n",
    "for any $\\text{n x n}$ Hermitian $\\mathbf X$ and Hermitian $\\mathbf Y$,  if we select \n",
    "\n",
    "$\\mathbf X:= \\exp\\big(\\frac{\\mathbf A}{ 2^N}\\big)$  \n",
    "$\\mathbf Y:= \\exp\\big(\\frac{\\mathbf B}{ 2^N}\\big)$  \n",
    "\n",
    "for Hermitian $\\mathbf A$ and $\\mathbf B$, then this tells us  \n",
    "\n",
    "$\\text{trace}\\Big(\\big(\\exp\\big(\\frac{\\mathbf A}{ 2^N}\\big) \\exp\\big(\\frac{\\mathbf B}{ 2^N}\\big)\\Big)^{2^N}\\Big) =\\text{trace}\\Big(\\big(\\mathbf X \\mathbf Y\\big)^{2^N}\\Big) \\leq \\text{trace}\\Big(\\mathbf X^{2^N} \\mathbf Y^{2^N}\\Big) =\\text{trace}\\Big(\\exp\\big(\\mathbf A\\big)\\exp\\big(\\mathbf B\\big)\\Big)= \\alpha $   \n",
    "\n",
    "where $\\alpha >0$  \n",
    "\n",
    "- - - - \n",
    "*remark:*  \n",
    "This is really is an elaboration / unpacking of Vershynin's proof of Golden-Thompson inequality  \n",
    "- - - -   \n",
    "\n",
    "since this holds for all natural number $N$, using basic results on limits in single variable real analysis, we know  \n",
    "\n",
    "$\\lim_{N\\to \\infty} \\text{trace}\\Big(\\big(\\exp\\big(\\frac{\\mathbf A}{ 2^N}\\big) \\exp\\big(\\frac{\\mathbf B}{ 2^N}\\big)\\Big)^{2^N}\\Big) \\leq \\alpha = \\text{trace}\\Big(\\exp\\big(\\mathbf A\\big)\\exp\\big(\\mathbf B\\big)\\Big)$  \n",
    "\n",
    "what remains then is a claim that is even stronger than what we need  \n",
    "\n",
    "**claim:**  \n",
    "$\\lim_{k\\to \\infty} \\Big(\\exp\\big(\\frac{\\mathbf A}{ k}\\big) \\exp\\big(\\frac{\\mathbf B}{ k}\\big)\\Big)^k = \\exp\\Big(\\mathbf A + \\mathbf B\\Big)$  \n",
    "\n",
    "*This is the Lie product formula*   \n",
    "\n",
    "The claim is proven over natural numbers $k$ and pure powers of 2 are a subset of this, but the limit of the sequence is unique so proving for all $k$ proves the result for the subset.  Since the trace is just summing over the diagonal of a matrix, if the above limit holds, then taking the trace gives the limiting value of the trace (by obvious application of triangle inequality).  \n",
    "\n",
    "**proof:**   \n",
    "The technique is to first homogenize each side in terms of powers of $k$ to get an easy estimate on the difference in powers series between the two sides (the linear terms must agree of course)    \n",
    "\n",
    "so first compare \n",
    "\n",
    "$\\mathbf F_k = \\Big(\\exp\\big(\\frac{\\mathbf A}{ k}\\big) \\exp\\big(\\frac{\\mathbf B}{ k}\\big)\\Big) = \\big(\\mathbf I +\\frac{1}{k}\\mathbf A +O(k^{-2})\\big)\\big(\\mathbf I +\\frac{1}{k}\\mathbf B+ O(k^{-2})\\big) = \\mathbf I+\\frac{1}{k}\\mathbf A +\\frac{1}{k}\\mathbf B+O(k^{-2})$   \n",
    "v.s.  \n",
    "$\\mathbf G_k = \\exp\\Big(\\frac{\\mathbf A + \\mathbf B}{k}\\Big) + \\big(\\mathbf I +\\frac{1}{k}(\\mathbf A + \\mathbf B) +O(k^{-2})\\big)$    \n",
    "\n",
    "\n",
    "hence (where we use the Frobenius norm for convenience)  \n",
    "$\\big \\Vert \\mathbf F_k - \\mathbf G_k\\big \\Vert_F = O(k^{-2})$  \n",
    "\n",
    "but what we actually want to estimate is \n",
    "\n",
    "$\\Big \\Vert \\mathbf F_k^k  -\\mathbf G_k^k \\Big \\Vert_F$  \n",
    "\n",
    "\n",
    "The underlying technique should be familiar from the scalar case (reference e.g. chapter 2 notes in *Cauchy-Schwarz Masterclass*, i.e. in the Inequalities folder), that \n",
    "\n",
    "$x^{k-1}y^0 + x^{k-2}y^{1} + x^{k-3}y^{2} + ... +x^{2}y^{k-3} + x^{1}y^{k-2} + x^{0}y^{k-1} = \\frac{x^{k} - y^{k}}{x - y}$  \n",
    "\n",
    "or  \n",
    "$\\big(x-y\\big)\\big(x^{k-1}y^0 + x^{k-2}y^{1} + x^{k-3}y^{2} + ... +x^{2}y^{k-3} + x^{1}y^{k-2} + x^{0}y^{k-1}\\big) = x^{k} - y^{k}$   \n",
    "\n",
    "(note our matrices are non-singular since they are being exponentiated, so to the zeroth power become the identity matrix, but in general the issue of non-commutativity remains)  \n",
    "\n",
    "To deal with non-commutativity issues, the idea is to once again \"bunch\" the matrices together in terms of powers, so we have, still in scalars:    \n",
    "\n",
    "$ \\big(x-y\\big)\\big(x^{k-1}y^0 + x^{k-2}y^{1} + x^{k-3}y^{2} + ... +x^{2}y^{k-3} + x^{1}y^{k-2} + x^{0}y^{k-1}\\big) $  \n",
    "$=x^{k-1}\\big(x-y\\big)y^0 + x^{k-2}\\big(x-y\\big)y^{1} + x^{k-3}\\big(x-y\\big)y^{2} + ... +x^{2}\\big(x-y\\big)y^{k-3} + x^{1}\\big(x-y\\big)y^{k-2} + x^{0}\\big(x-y\\big)y^{k-1}$   \n",
    "$= x^{k} - y^{k}$   \n",
    "\n",
    "which becomes  \n",
    "$\\mathbf F_k^k  -\\mathbf G_k^k   = \\mathbf F^{k-1}\\big(\\mathbf F-\\mathbf G_k\\big)\\mathbf I + \\mathbf F^{k-2}\\big(\\mathbf F-\\mathbf G_k\\big)\\mathbf G_k^{1} + ... +  \\mathbf F^{1}\\big(\\mathbf F-\\mathbf G_k\\big)\\mathbf G_k^{k-2} + \\mathbf I\\big(\\mathbf F-\\mathbf G_k\\big)\\mathbf G_k^{k-1}$   \n",
    "\n",
    "taking the Frobenius norm of each side, then applying sub-additivity and submultiplicativity gives    \n",
    "$\\Big \\Vert \\mathbf F_k^k  -\\mathbf G_k^k\\Big \\Vert_F$  \n",
    "$= \\Big \\Vert\\mathbf F_k^{k-1}\\big(\\mathbf F_k-\\mathbf G_k\\big) + \\mathbf F_k^{k-2}\\big(\\mathbf F_k-\\mathbf G_k\\big)\\mathbf G_k^{1} + ... +  \\mathbf F_k^{1}\\big(\\mathbf F_k-\\mathbf G_k\\big)\\mathbf G_k^{k-2} + \\big(\\mathbf F_k-\\mathbf G_k\\big)\\mathbf G_k^{k-1}\\Big \\Vert_F$    \n",
    "$\\leq \\Big \\Vert\\mathbf F_k^{k-1}\\big(\\mathbf F_k-\\mathbf G_k\\big)\\Big \\Vert_F + \\Big \\Vert \\mathbf F_k^{k-2}\\big(\\mathbf F_k-\\mathbf G_k\\big)\\mathbf G_k^{1}\\Big \\Vert_F + ... +  \\Big \\Vert\\mathbf F_k^{1}\\big(\\mathbf F_k-\\mathbf G_k\\big)\\mathbf G_k^{k-2}\\Big \\Vert_F + \\Big \\Vert \\big(\\mathbf F_k-\\mathbf G_k\\big)\\mathbf G_k^{k-1}\\Big \\Vert_F$    \n",
    "$\\leq \\Big \\Vert\\mathbf F_k\\Big \\Vert_F^{k-1} \\Big \\Vert\\big(\\mathbf F_k-\\mathbf G_k\\big)\\Big \\Vert_F + \\Big \\Vert \\mathbf F_k\\Big \\Vert_F^{k-2} \\Big \\Vert \\big(\\mathbf F_k-\\mathbf G_k\\big)\\Big \\Vert_F\\Big \\Vert \\mathbf G_k\\Big \\Vert_F + ... +  \\Big \\Vert\\mathbf F_k\\Big \\Vert_F \\Big \\Vert\\big(\\mathbf F_k-\\mathbf G_k\\big)\\Big \\Vert_F \\Big \\Vert\\mathbf G_k\\Big \\Vert_F^{k-2} + \\Big \\Vert \\big(\\mathbf F_k-\\mathbf G_k\\big)\\Big \\Vert_F\\Big \\Vert\\mathbf G_k\\Big \\Vert_F^{k-1}$    \n",
    "$\\leq \\sum_{i=1}^k M(k)^{k-1} \\cdot  \\Big \\Vert \\mathbf F_k-\\mathbf G_k\\Big \\Vert_F $  \n",
    "$ = k \\cdot M(k)^{k-1} \\cdot  \\Big \\Vert \\mathbf F_k-\\mathbf G_k\\Big \\Vert_F $   \n",
    "\n",
    "where  \n",
    "$M(k) := \\max\\Big(\\Big \\Vert\\mathbf F_k\\Big \\Vert_F, \\Big \\Vert\\mathbf G_k\\Big \\Vert_F\\Big)$  \n",
    "\n",
    "thus  \n",
    "$\\Big \\Vert \\mathbf F_k^k  -\\mathbf G_k^k\\Big \\Vert_F \\leq k \\cdot M(k)^{k-1} \\cdot  \\Big \\Vert \\mathbf F_k-\\mathbf G_k\\Big \\Vert_F = k \\cdot M(k)^{k-1} \\cdot O\\big(k^{-2}\\big) = M(k)^{k-1} \\cdot O\\big(k^{-1}\\big) $    \n",
    "so if we can show that $M(k)$ is bounded or at least grows sufficiently slowly with respect to $k$, then we are done.  \n",
    "\n",
    "again making use of submultiplicativity, then homogeneity with respect to positive scaling, and for any selection of matrices, we have some fixed constant \n",
    "$c := \\big \\Vert \\mathbf A\\big\\Vert_F + \\big \\Vert \\mathbf B\\big \\Vert_F$\n",
    "\n",
    "$\\big \\Vert \\mathbf F_k \\big \\Vert_F$  \n",
    "$= \\big \\Vert \\exp\\big(\\frac{\\mathbf A}{ k}\\big) \\exp\\big(\\frac{\\mathbf B}{ k}\\big)\\big \\Vert_F$  \n",
    "$\\leq \\big \\Vert \\exp\\big(\\frac{\\mathbf A}{ k}\\big) \\big \\Vert_F \\big \\Vert \\exp\\big(\\frac{\\mathbf B}{ k}\\big)\\big \\Vert_F$  \n",
    "$\\leq  \\exp\\big(\\big \\Vert\\frac{\\mathbf A}{ k}\\big \\Vert_F \\big) \\exp\\big(\\big \\Vert \\frac{\\mathbf B}{ k}\\big \\Vert_F\\big)$  \n",
    "$= \\exp\\big(\\frac{1}{k} \\big \\Vert \\mathbf A\\big \\Vert_F \\big) \\exp\\big(\\frac{1}{k} \\big \\Vert \\mathbf B\\big \\Vert_F\\big)$   \n",
    "$= \\exp\\Big(\\frac{1}{k}\\big( \\big \\Vert \\mathbf A\\big\\Vert_F + \\big \\Vert \\mathbf B\\big \\Vert_F\\big)\\Big)$   \n",
    "$= \\exp\\Big(\\frac{c}{k}\\Big)$   \n",
    "\n",
    "$\\Big \\Vert \\mathbf G_k\\Big \\Vert_F$  \n",
    "$= \\Big \\Vert \\exp\\Big(\\frac{\\mathbf A + \\mathbf B}{k}\\Big)\\Big \\Vert_F $  \n",
    "$\\leq  \\exp\\Big(\\Big \\Vert \\frac{\\mathbf A + \\mathbf B}{k}\\Big \\Vert_F \\Big) $  \n",
    "$=  \\exp\\Big(\\frac{1}{k} \\Big \\Vert \\mathbf A + \\mathbf B\\Big \\Vert_F \\Big) $  \n",
    "$\\leq   \\exp\\Big(\\frac{1}{k}\\big( \\Big \\Vert \\mathbf A\\Big \\Vert_F + \\Big \\Vert\\mathbf B\\Big \\Vert_F\\big) \\Big) $  \n",
    "$= \\exp\\Big(\\frac{c}{k}\\Big)$   \n",
    "\n",
    "Where the second to last inequality follows by sub-additivity (and the fact that the exponential function is an increasing function).   \n",
    "\n",
    "so we know  \n",
    "$M(k) \\leq  \\exp\\Big(\\frac{c}{k}\\Big)$   \n",
    "  \n",
    "\n",
    "Putting it all together, we have   \n",
    "$\\Big \\Vert \\mathbf F_k^k  -\\mathbf G_k^k\\Big \\Vert_F $  \n",
    "$\\leq M(k)^{k-1} \\cdot O\\big(k^{-1}\\big)$  \n",
    "$\\leq   \\exp\\big(\\frac{c}{k}\\big)^{k-1}\\cdot O\\big(k^{-1}\\big) $  \n",
    "$\\leq   \\exp\\big(\\frac{c(k-1)}{k}\\big)\\cdot O\\big(k^{-1}\\big) $  \n",
    "$\\leq   \\exp\\big(c\\big)\\cdot O\\big(k^{-1}\\big) $  \n",
    "$= O\\big(k^{-1}\\big) $      \n",
    "\n",
    "thus  \n",
    "$\\lim_{k \\to \\infty} \\Big \\Vert \\mathbf F_k^k  -\\mathbf G_k^k\\Big \\Vert_F = 0$  \n",
    "**which proves the Lie Product Formula and in turn proves the Golden-Thompson inequality that**    \n",
    "\n",
    "$\\text{trace}\\Big(e^{\\mathbf A + \\mathbf B}\\Big) \\leq \\text{trace}\\Big(e^{\\mathbf A}e^{\\mathbf B}\\Big)$  \n",
    "\n",
    "for hermitian $\\mathbf A$ and $\\mathbf B$ \n",
    "\n",
    "**corollary**  \n",
    "The Lie Product Formula allows the easiest and most direct proof that if $\\mathbf A$ and $\\mathbf B$ *commute*, then  \n",
    "\n",
    "$e^{\\mathbf A + \\mathbf B} = e^{\\mathbf A}e^{\\mathbf B}$   \n",
    "\n",
    "i.e. supposing that they commute our above result tells us that \n",
    "\n",
    "$\\lim_{k\\to \\infty} \\Big(\\exp\\big(\\frac{\\mathbf A}{ k}\\big) \\exp\\big(\\frac{\\mathbf B}{ k}\\big)\\Big)^k = \\exp\\Big(\\mathbf A + \\mathbf B\\Big)$  \n",
    "\n",
    "and commutativity tells us, for all natural numbers $k$  \n",
    "$\\Big(\\exp\\big(\\frac{\\mathbf A}{ k}\\big) \\exp\\big(\\frac{\\mathbf B}{ k}\\big)\\Big)^k = \\exp\\big(\\frac{\\mathbf A}{ k}\\big)^k \\exp\\big(\\frac{\\mathbf B}{ k}\\big)^k = e^{\\mathbf A}e^{\\mathbf B}$    \n",
    "\n",
    "hence  \n",
    "$e^{\\mathbf A}e^{\\mathbf B} = \\lim_{k\\to \\infty} \\Big(\\exp\\big(\\frac{\\mathbf A}{ k}\\big) \\exp\\big(\\frac{\\mathbf B}{ k}\\big)\\Big)^k = \\exp\\Big(\\mathbf A + \\mathbf B\\Big)$  \n",
    "\n",
    "or  \n",
    "$e^{\\mathbf A}e^{\\mathbf B} = \\exp\\Big(\\mathbf A + \\mathbf B\\Big)$   \n",
    "when they commute.  \n",
    "\n",
    "*note:*  \n",
    "we will, shortly, prove that the the Golden Thompson inequality is strict when $\\mathbf A$ and $\\mathbf B$ do not commute.  This immediately implies  \n",
    "\n",
    "$e^{\\mathbf A}e^{\\mathbf B} \\neq \\exp\\Big(\\mathbf A + \\mathbf B\\Big)$   \n",
    "when $\\mathbf A$ and $\\mathbf B$ don't commute, again for any arbitrary n x n Hermitian matrices.  (i.e. if the equality here did hold, then the sum across their diagonals would hold, but as we'll see the traces cannot be the same here when they don't commute.)    \n",
    "\n",
    "\n",
    "*commentary:*   \n",
    "The Lie Product Formula is sometimes referred to as Trotter's Formula. The result holds in general for n x n complex matrices, however when dealing with non-hermitian matrices, some additional care may be needed (e.g. when dealing with eigenvalues of non-hermitian matrices, they are in general not real, and $e^\\frac{\\lambda}{k} = (e^\\lambda)^\\frac{1}{k}$ but $k$th roots over the complex plane are not continuous over the entire plane (the slit / branch cut on the negative real line is worth recalling) and hence some behavior that we may take for granted does not appl -- interpretting said numbers in polar form works of course, but certain subtleties related to continuity must be kept in mind)  \n",
    "\n",
    "Golden-Thompson is a rather involved inequality and your author does not know of any conceptually easier proofs.  It is peculiar in that it was first published in the 1970s, where the sharpness was immediate -- it was immediate that there was equality if $\\mathbf A$ and $\\mathbf B$ commute, but it took 20 years to verify that this was an *if and only if*.  The reason has to do with the use of limits which blunt the strictness of the inequalities.  The various proofs seen by your author ultimately all seem to use limits of some kind.  The Golden-Thompson inequality itself also is fairly deeply tied into some ideas in Lie theory and typically has applications in statistical mechanics and random matrix theory.   \n",
    "\n",
    "The overall arch of the proof is we know that in general  \n",
    "\n",
    "$e^{\\mathbf X + \\mathbf Y} \\neq e^{\\mathbf X}e^{\\mathbf Y}$   \n",
    "\n",
    "so try to find some case where they can be equivalent -- that occurs in the limit with the Lie Product Formula.  Furthermore while the matrices don't commute, we are dealing with only 2 different matrices, so try to 'bunch them together' -- this technique is used twice, once in trying to estimate   \n",
    "$\\Big \\Vert \\mathbf F_k^k  -\\mathbf G_k^k\\Big \\Vert_F$  \n",
    "via a well known telescoping identity for scalars, and also when we consider another bunching setup  \n",
    "\n",
    "$\\text{trace}\\Big(\\big(\\mathbf X \\mathbf Y\\big)^{2^N}\\Big) \\leq \\text{trace}\\Big(\\mathbf X^{2^N} \\mathbf Y^{2^N}\\Big)$  \n",
    "where the RHS in some sense has maximally bunched $\\mathbf X$ and $\\mathbf Y$ and the LHS has the minimal amount of bunching between like matrices.  \n",
    "\n",
    "Considering trying to prove   \n",
    "$\\text{trace}\\Big(\\big(\\mathbf X \\mathbf Y\\big)^{2^N}\\Big) \\leq \\text{trace}\\Big(\\mathbf X^{2^N} \\mathbf Y^{2^N}\\Big)$  \n",
    "instead of something like  \n",
    "$\\text{trace}\\Big(\\big(\\mathbf X \\mathbf Y\\big)^{k}\\Big) \\text{v.s.} \\text{trace}\\Big(\\mathbf X^{k} \\mathbf Y^{k}\\Big)$   \n",
    "is sensible because we are dealing with two distinct matrices and considering raising them to pure powers of 2 allows us to preserve some symmetrized bunching between the two different matrices as we proceed.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**tightening the equality conditions of Golden-Thompson**  \n",
    "*comment:* the below approach to tease out the fact that there is an equality in Golden-Thompson *iff* $\\mathbf A$ and $\\mathbf B$ commute closely follows part (e) in Serre's link -- problem 364, page 191 at the time of writing, again, here:  \n",
    "http://perso.ens-lyon.fr/serre/DPF/exobis.pdf -- though the application of Schur's Inequality is somewhat different in the below writeup than the inequality suggested in the link  \n",
    "\n",
    "As stated above as a corollary of the Lie Product Formula, when $\\mathbf A$ and $\\mathbf B$ commute, then $e^{\\mathbf A +\\mathbf B} = e^{\\mathbf A}e^{\\mathbf B}$, so of course their traces match.  That is, it's immediate that   \n",
    "\n",
    "$\\text{commutativity of } \\mathbf A \\text{ and } \\mathbf B \\longrightarrow \\text{equality case of Golden-Thompson}$   \n",
    "\n",
    "The below proves the other leg of the *iff*, i.e. that  \n",
    "$\\text{equality case of Golden-Thompson} \\longrightarrow \\text{commutativity of } \\mathbf A \\text{ and } \\mathbf B$   \n",
    "\n",
    "- - - -  \n",
    "re-visiting our work, a simple real scalar result to focus on is the decreasing sequence given by the trace  \n",
    "\n",
    "$\\text{trace}\\Big(\\big(\\mathbf Y\\mathbf X\\big)^{2^{N}}\\Big) \\leq \\text{trace}\\Big(\\big(\\mathbf Y^{2}\\mathbf X^{2}\\big)^{2^{N-1}}\\Big) \\leq  ... \\leq \\text{trace}\\Big(\\big(\\mathbf Y^{2^{N-2}}\\mathbf X^{2^{N-2}}\\big)^4\\Big) \\leq \\text{trace}\\Big(\\big(\\mathbf Y^{2^{N-1}}\\mathbf X^{2^{N-1}}\\big)^2\\Big) \\leq \\text{trace}\\Big(\\mathbf X^{2^N} \\mathbf Y^{2^N}\\Big)$  \n",
    "\n",
    "and when we specialize to  \n",
    "$\\mathbf X:= \\exp\\big(\\frac{\\mathbf A}{ 2^N}\\big)$  \n",
    "$\\mathbf Y:= \\exp\\big(\\frac{\\mathbf B}{ 2^N}\\big)$    \n",
    "\n",
    "this becomes   \n",
    "$u_N = \\text{trace}\\Big(\\big(\\exp\\big(\\frac{\\mathbf B}{ 2^N}\\big)\\exp\\big(\\frac{\\mathbf A}{ 2^N}\\big)\\big)^{2^{N}}\\Big)$  \n",
    "$\\leq u_{N-1}= \\text{trace}\\Big(\\big(\\exp\\big(\\frac{\\mathbf B}{ 2^N}\\big)^{2}\\exp\\big(\\frac{\\mathbf A}{ 2^N}\\big)^{2}\\big)^{2^{N-1}}\\Big) $  \n",
    "$\\vdots$  \n",
    "$\\leq u_{2}= \\text{trace}\\Big(\\big(\\exp\\big(\\frac{\\mathbf B}{ 2^N}\\big)^{2^{N-2}}\\exp\\big(\\frac{\\mathbf A}{ 2^N}\\big)^{2^{N-2}}\\big)^4\\Big)$  \n",
    "$\\leq u_{1}=\\text{trace}\\Big(\\big(\\exp\\big(\\frac{\\mathbf B}{ 2^N}\\big)^{2^{N-1}}\\exp\\big(\\frac{\\mathbf A}{ 2^N}\\big)^{2^{N-1}}\\big)^2\\Big)=\\text{trace}\\Big(\\big(\\exp\\big(\\frac{\\mathbf B}{ 2}\\big)\\exp\\big(\\frac{\\mathbf A}{ 2}\\big)\\big)^2\\Big) $  \n",
    "$\\leq u_{0}= \\text{trace}\\Big(\\exp\\big(\\frac{\\mathbf B}{ 2^N}\\big)^{2^N} \\exp\\big(\\frac{\\mathbf A}{ 2^N}\\big)^{2^N}\\Big)= \\text{trace}\\Big(\\exp\\big(\\mathbf B\\big) \\exp\\big(\\mathbf A\\big)\\Big)$    \n",
    "\n",
    "or more simply we have the monotone non-increasing sequence  \n",
    "$u_{N} \\leq u_{N-1}\\leq ... \\leq u_{2} \\leq u_1 \\leq u_0$    \n",
    "Based on Lie Product Formula, we know  \n",
    "\n",
    "$L = \\lim_{N\\to \\infty} u_{N} = \\text{trace}\\big(e^{\\mathbf A + \\mathbf B}\\big) $  exists, and by the monotone nature of the sequence we know, among other things, that  \n",
    "$L  \\leq u_1 \\leq u_0$  \n",
    "\n",
    "But *in the special case of equality* in the Golden-Thompson inequality, we have $L = u_0$, thus  \n",
    "$L  =  u_1 = u_0$  \n",
    "but by Schur's Inequality (see notebook with that name, in the Linear Algebra folder) we know  \n",
    "\n",
    "$u_1$  \n",
    "$=\\text{trace}\\Big(\\big(\\exp\\big(\\frac{\\mathbf B}{ 2}\\big)\\exp\\big(\\frac{\\mathbf A}{ 2}\\big)\\big)^2\\Big) $  \n",
    "$=\\text{trace}\\Big(\\big(\\exp\\big(\\mathbf B\\big)^\\frac{1}{2}\\exp\\big(\\mathbf A\\big)^\\frac{1}{2}\\big)^2\\Big) $  \n",
    "$\\leq \\Big \\Vert \\exp\\big(\\mathbf B\\big)^\\frac{1}{2}\\exp\\big(\\mathbf A\\big)^\\frac{1}{2}\\Big \\Vert_F^2$  \n",
    "$=\\text{trace}\\Big(\\exp\\big(\\mathbf A\\big)^\\frac{1}{2}\\exp\\big(\\mathbf B\\big)^\\frac{1}{2}\\exp\\big(\\mathbf B\\big)^\\frac{1}{2}\\exp\\big(\\mathbf A\\big)^\\frac{1}{2}\\Big) $  \n",
    "$=\\text{trace}\\Big(\\exp\\big(\\mathbf B\\big) \\exp\\big(\\mathbf A\\big)\\Big) $   \n",
    "$= u_0$  \n",
    "\n",
    "*with equality* **iff**  \n",
    "$\\big(\\exp\\big(\\frac{\\mathbf B}{ 2}\\big)\\exp\\big(\\frac{\\mathbf A}{ 2}\\big)\\big) = \\big(\\exp\\big(\\mathbf B\\big)^\\frac{1}{2}\\exp\\big(\\mathbf A\\big)^\\frac{1}{2}\\big)$  \n",
    "*is normal*   \n",
    "\n",
    "We know said matrix has entirely real (in fact positive) spectrum and hence normality (i.e. being unitarily diagonalizable) implies Hermiticity; this is equivalent to noticing that   \n",
    "$ \\exp\\big(\\mathbf B\\big)^\\frac{1}{2}\\exp\\big(\\mathbf A\\big)^\\frac{1}{2}= \\Big(\\exp\\big(\\mathbf B\\big)^\\frac{1}{2}\\exp\\big(\\mathbf A\\big)^\\frac{1}{2}\\Big)^* = \\Big(\\exp\\big(\\mathbf A\\big)^\\frac{1}{2}\\Big)^* \\Big(\\exp\\big(\\mathbf B\\big)^\\frac{1}{2}\\Big)^* =  \\exp\\big(\\mathbf A\\big)^\\frac{1}{2}\\exp\\big(\\mathbf B\\big)^\\frac{1}{2} $  \n",
    "- - - - \n",
    "so we have  \n",
    "$\\Big(\\exp\\big(\\mathbf B\\big)^\\frac{1}{2}\\exp\\big(\\mathbf A\\big)^\\frac{1}{2}\\Big)=\\Big(\\exp\\big(\\mathbf A\\big)^\\frac{1}{2}\\exp\\big(\\mathbf B\\big)^\\frac{1}{2}\\Big) $   \n",
    "\n",
    "squaring each side gives  \n",
    "$\\exp\\big(\\mathbf B\\big)\\exp\\big(\\mathbf A\\big)$   \n",
    "$=\\exp\\big(\\mathbf B\\big)^\\frac{1}{2}\\underbrace{\\exp\\big(\\mathbf B\\big)^\\frac{1}{2}\\exp\\big(\\mathbf A\\big)^\\frac{1}{2}}\\exp\\big(\\mathbf A\\big)^\\frac{1}{2}$  \n",
    "$=\\exp\\big(\\mathbf B\\big)^\\frac{1}{2}\\underbrace{\\exp\\big(\\mathbf A\\big)^\\frac{1}{2}\\exp\\big(\\mathbf B\\big)^\\frac{1}{2}}\\exp\\big(\\mathbf A\\big)^\\frac{1}{2}$  \n",
    "$ = \\Big(\\exp\\big(\\mathbf B\\big)^\\frac{1}{2}\\exp\\big(\\mathbf A\\big)^\\frac{1}{2}\\Big)^2$  \n",
    "$= \\Big(\\exp\\big(\\mathbf A\\big)^\\frac{1}{2}\\exp\\big(\\mathbf B\\big)^\\frac{1}{2}\\Big)^2 $  \n",
    "$= \\exp\\big(\\mathbf A\\big)^\\frac{1}{2}\\underbrace{\\exp\\big(\\mathbf B\\big)^\\frac{1}{2}\\exp\\big(\\mathbf A\\big)^\\frac{1}{2}}\\exp\\big(\\mathbf B\\big)^\\frac{1}{2} $   \n",
    "$=\\exp\\big(\\mathbf A\\big)^\\frac{1}{2}\\underbrace{\\exp\\big(\\mathbf A\\big)^\\frac{1}{2}\\exp\\big(\\mathbf B\\big)^\\frac{1}{2}}\\exp\\big(\\mathbf B\\big)^\\frac{1}{2} $   \n",
    "$=\\exp\\big(\\mathbf A\\big)\\exp\\big(\\mathbf B\\big)$   \n",
    "\n",
    "so  \n",
    "$e^{\\mathbf A}$ and $e^{\\mathbf B}$ must commute. But both are are both Hermitian so they commute *iff* they are simultaneously diagonalizable; this implies they are simultaneously diagonalizable so there is some $\\mathbf S$ where \n",
    "(ref e.g. the notebook \"simultaneous_diagonalization.ipynb\" in the Linear Algebra folder)  \n",
    "\n",
    "$\\mathbf S^{-1}e^{\\mathbf D_1} \\mathbf S = e^{\\mathbf A}$    \n",
    "$\\mathbf S^{-1}e^{\\mathbf D_2} \\mathbf S = e^{\\mathbf B}$    \n",
    "\n",
    "but since $\\mathbf A$ and $\\mathbf B$ are Hermitian we know that the above exponential function may be chosen to be the real exponential function (hence invertible) and application of the real logarithm gives  \n",
    "\n",
    "$\\mathbf S^{-1}\\mathbf D_1 \\mathbf S= \\mathbf S^{-1}\\log\\big(e^{\\mathbf D_1}\\big) \\mathbf S = \\log\\big(e^{\\mathbf A}\\big) = \\mathbf A$    \n",
    "$\\mathbf S^{-1}\\mathbf D_2 \\mathbf S= \\mathbf S^{-1}\\log\\big(e^{\\mathbf D_2}\\big) \\mathbf S = \\log\\big(e^{\\mathbf B}\\big) = \\mathbf B$    \n",
    "\n",
    "thus $\\mathbf A$ and $\\mathbf B$ are simultaneously diagonalizable and thus they commute.  This completes the proof that  \n",
    "$\\text{equality case of Golden-Thompson} \\longrightarrow \\text{commutativity of } \\mathbf A \\text{ and } \\mathbf B$  \n",
    "- - - -  \n",
    "(an equivalent finish is to notice that if diagonal component $r$ of say $e^{\\mathbf D_1}$ is positive --- i.e. $e^{\\lambda_r} = c \\gt 0 $ -- then the pre-image is given by  \n",
    "$\\lambda_r = \\log(c) + n \\cdot 2\\pi \\cdot i$  \n",
    "for integer $n$.  However since $\\mathbf A$ is Hermitian we know that $\\lambda_r$ is real and thus $n=0$ and we then recover invertibility via the the real (natural) logarithm.)      \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# End Interlude involving singular values and ideas from Majorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Hermitian Positive Semi Definite Inequalities with Hadamard Product \n",
    "\n",
    "*note 1: This apparently is covered under the Schur Product Theorem, which is an interesting albeit esoteric part of matrix theory.* \n",
    "\n",
    "*note 2: The inequalities in here are covered via a diagonalization argument.  For a different proof of these relations see my writeup \"Kronecker_Product.ipynd\" -- specifically the subsection called \"Remark on Kronecker Product vs Hadamard Product\".*\n",
    "\n",
    "*note 3: In effect this tells us that Hermitian Positive Semi Definite matrices are closed under Hadamard/Schur Products*  \n",
    "\n",
    "consider Hermitian Positive Semi Definite Matrices $\\mathbf A$ and $\\mathbf B$.  Then consider the matrix $\\mathbf C = \\mathbf A \\circ \\mathbf B$, where $\\circ$ denotes the Hadamard product.    \n",
    "\n",
    "**claim:**\n",
    "$\\mathbf C $ is Hermitian positive semi-definite\n",
    "\n",
    "\n",
    "*note:* a curious and short proof of this, at least for real symmetric matrices due to Olkin is to note that any real symmetric positive (semi)definite matrix is a covariance matrix for some random variable (which we may choose to be a a multivariate Gaussian --see probability folder notebook \"MGFs_and_multivariate_gaussians.ipynb\" or Feller volume 2 for more information). Thus $\\mathbf A $ and $\\mathbf B$ can be selected as the covariance matrix for zero mean Gaussian random vectors $\\mathbf x$ and $\\mathbf y$, and we can further choose them to be independent vectors.  Direct calculation shows that the random vector $\\mathbf z := \\mathbf x \\circ \\mathbf y$ has covariance matrix $\\mathbf A \\circ \\mathbf B$ which must be real symmetric positive (semi)definite because *all* covariance matrices are (or equivalently all gram matrices are).  Again see  \"MGFs_and_multivariate_gaussians.ipynb\" for more information, or chapter 7 notes to Cauchy-Schwarz Mastersclass in the Inequalities folder.  \n",
    "\n",
    "*Reminder:*\n",
    "Hadamard products distribute across matrix addition because scalar multiplication distributes across scalar addition. \n",
    "\n",
    "example:\n",
    "\n",
    "$\\mathbf Z = \\big(\\mathbf X + \\mathbf Y\\big) \\circ \\mathbf W = \\mathbf X \\circ \\mathbf W + \\mathbf Y \\circ \\mathbf W$\n",
    "\n",
    "that is  \n",
    "\n",
    "$z_{i,j} = (x_{i,j} + y_{i,j})\\cdot w_{i,j} = x_{i,j}w_{i,j} + y_{i,j}w_{i,j}$\n",
    "\n",
    "\n",
    "**proof:**\n",
    "\n",
    "$c_{i,j} = a_{i,j} \\cdot b_{i,j} = \\bar{a_{j,i}}\\cdot \\bar{b_{j,i}} = \\bar{c_{j,i}}$\n",
    "\n",
    "By inspection we see that $\\mathbf C$ is Hermitian. \n",
    "\n",
    "Now the claim of positive semi-definitiness means $\\mathbf x^H \\mathbf C \\mathbf x \\geq 0 $ for all $\\mathbf x$. \n",
    "\n",
    "next we use the decomposition employed in \"julia_hmm_viterbi_as_qp_and_lp_upload\" contained in the Optimization aka \"markov_optimization\" folder.  \n",
    "\n",
    "$\\mathbf x^H \\mathbf C \\mathbf x  = \\mathbf 1^H \\big(\\mathbf C \\circ \\mathbf x\\mathbf x^H \\big)\\mathbf 1 = \\text{sum}\\big(\\mathbf C \\circ \\mathbf x\\mathbf x^H \\big)$\n",
    "\n",
    "where \"sum\" denotes adding up each scalar entry of the matrix $\\big( \\mathbf C \\circ \\mathbf x\\mathbf x^H \\big)$.\n",
    "\n",
    "Hence we are evaluating:\n",
    "\n",
    "$\\text{sum}\\big(\\mathbf C \\circ \\mathbf x\\mathbf x^H \\big) = \\text{sum}\\big(\\mathbf A \\circ \\mathbf B \\circ \\mathbf x\\mathbf x^H \\big)$\n",
    "\n",
    "\n",
    "using associativity and commutativity of the Hadamard product, we have\n",
    "\n",
    "$\\text{sum}\\big(\\mathbf A \\circ \\mathbf B \\circ \\mathbf x\\mathbf x^H \\big) = \\text{sum}\\Big( \\mathbf A \\circ \\mathbf x\\mathbf x^H \\circ \\mathbf B   \\Big) = \\text{sum}\\Big(\\big(\\mathbf A \\circ \\mathbf x\\mathbf x^H\\big) \\circ \\mathbf B   \\Big)$\n",
    "\n",
    "since $\\mathbf A$ is Hermitiain Positive Semi Definite, we can re-write it as \n",
    "\n",
    "$\\mathbf A = (\\lambda_1 \\mathbf p_1 \\mathbf p_1^H + \\lambda_2 \\mathbf p_2 \\mathbf p_2^H + ... + \\lambda_n \\mathbf p_n \\mathbf p_n^H)$ \n",
    "\n",
    "where each $\\lambda_k$ is real valued and non-negative\n",
    "\n",
    "$\\text{sum}\\Big(\\big(\\mathbf A \\circ \\mathbf x\\mathbf x^H\\big) \\circ \\mathbf B   \\Big) = \\text{sum}\\Big(\\big((\\lambda_1 \\mathbf p_1 \\mathbf p_1^H + \\lambda_2 \\mathbf p_2 \\mathbf p_2^H + ... + \\lambda_n \\mathbf p_n \\mathbf p_n^H) \\circ \\mathbf x\\mathbf x^H\\big) \\circ \\mathbf B   \\Big) $\n",
    "\n",
    "$\\text{sum}\\Big(\\big(\\mathbf A \\circ \\mathbf x\\mathbf x^H\\big) \\circ \\mathbf B   \\Big) = \\text{sum}\\Big(\\big(\\lambda_1 \\mathbf p_1 \\mathbf p_1^H \\circ \\mathbf x\\mathbf x^H + \\lambda_2 \\mathbf p_2 \\mathbf p_2^H \\circ \\mathbf x\\mathbf x^H + ... + \\lambda_n \\mathbf p_n \\mathbf p_n^H\\circ \\mathbf x\\mathbf x^H \\big) \\circ \\mathbf B   \\Big)$\n",
    "\n",
    "re-writing this with sigma notation, we have\n",
    "\n",
    "$\\text{sum}\\Big(\\big( \\sum_{k=1}^n \\lambda_k \\mathbf p_k \\mathbf p_k^H \\circ \\mathbf x\\mathbf x^H \\big) \\circ \\mathbf B   \\Big)$\n",
    "\n",
    "now we definite $\\mathbf y_k$, where\n",
    "\n",
    "$\\mathbf y_k := \\mathbf p_k \\circ \\mathbf x$\n",
    "\n",
    "hence $\\mathbf y_k \\mathbf y_k^H = \\mathbf p_k \\mathbf p_k^H \\circ \\mathbf x\\mathbf x^H $\n",
    "\n",
    "where for avoidance of doubt, we double check the indices:\n",
    "\n",
    "$\\big(\\mathbf y_k \\mathbf y_k^H\\big)_{i,j} = (p_{k,i}  \\cdot x_{i}) \\cdot (\\bar{p_{k,j}}\\cdot \\bar{x_{j}}) = (p_{k,i} \\cdot \\bar{p_{k,j}})  \\cdot (x_{i}  \\cdot \\bar{x_{j}}) = \\big(\\mathbf p_k \\mathbf p_k^H\\big)_{i,j} \\circ \\big(\\mathbf{xx}^H\\big)_{i,j}=   \\big(\\mathbf p_k \\mathbf p_k^H \\circ \\mathbf{xx}^H\\big)_{i,j}$ \n",
    "\n",
    "our expression becomes\n",
    "\n",
    "$\\text{sum}\\Big(\\big( \\sum_{k=1}^n \\lambda_k \\mathbf y_k \\mathbf y_k^H \\big) \\circ \\mathbf B   \\Big) = \\text{sum}\\Big( \\sum_{k=1}^n \\lambda_k \\mathbf y_k \\mathbf y_k^H \\circ \\mathbf B   \\Big)$\n",
    "\n",
    "now, recognizing that when we have a finite number of terms, we can always interchange linear operators (that are additive in nature) like $sum()$ and $\\sum$\n",
    "\n",
    "$\\text{sum}\\Big( \\sum_{k=1}^n \\lambda_k \\mathbf y_k \\mathbf y_k^H \\circ \\mathbf B   \\Big) = \\sum_{k=1}^n  \\text{sum}\\Big( \\lambda_k \\mathbf y_k \\mathbf y_k^H \\circ \\mathbf B   \\Big)$\n",
    "\n",
    "- - - -\n",
    "side note: the above is interchange is equivalent to recognizing that $\\mathbf 1^H \\big( \\mathbf W + \\mathbf X\\big)\\mathbf 1 = \\mathbf 1^H\\big(\\mathbf W\\big)\\mathbf 1 + \\mathbf 1^H\\big(\\mathbf X\\big)\\mathbf 1$\n",
    "- - - -\n",
    "\n",
    "$\\sum_{k=1}^n  \\text{sum}\\big( \\lambda_k \\mathbf y_k \\mathbf y_k^H \\circ \\mathbf B   \\big) = \\sum_{k=1}^n  \\lambda_k \\Big(\\text{sum}\\big(  \\mathbf y_k \\mathbf y_k^H \\circ \\mathbf B   \\big)\\Big) = \\sum_{k=1}^n  \\lambda_k \\Big(\\text{sum}\\big( \\mathbf B \\circ \\mathbf y_k \\mathbf y_k^H \\big)\\Big) = \\sum_{k=1}^n  \\lambda_k \\Big(\\mathbf y_k^H \\mathbf B \\mathbf y_k\\Big)$\n",
    "\n",
    "recognizing our original quadratic form decomposition $\\mathbf y_k^H \\mathbf B \\mathbf y_k = \\text{sum}\\big( \\mathbf B \\circ \\mathbf y_k \\mathbf y_k^H \\big)$ \n",
    "\n",
    "since $\\mathbf B$ is Hermitian positive semi-definite, we know that $\\mathbf y_k^H \\mathbf B \\mathbf y_k \\geq 0$ for any $\\mathbf y_k$, and we recall that $\\lambda_k \\geq 0$ \n",
    "\n",
    "Hence we say \n",
    "\n",
    "$\\sum_{k=1}^n  \\lambda_k \\big(\\mathbf y_k^H \\mathbf B \\mathbf y_k\\big) =  \\lambda_1 \\big(\\mathbf y_1^H \\mathbf B \\mathbf y_1\\big) + \\lambda_2 \\big(\\mathbf y_2^H \\mathbf B \\mathbf y_2\\big) + ... + \\lambda_n \\big(\\mathbf y_n^H \\mathbf B \\mathbf y_n\\big) \\geq 0$\n",
    "\n",
    "because each term in that series is itself $\\geq 0$.\n",
    "\n",
    "This proves that $\\mathbf C = \\mathbf A \\circ \\mathbf B$ is a Hermitian positive semi-definite matrix.\n",
    "\n",
    "**claim:**   \n",
    "$\\lambda_{1, A}\\cdot \\lambda_{1,B} \\geq \\lambda_{1, C}$\n",
    "\n",
    "where $\\mathbf C = \\mathbf A \\circ \\mathbf B$\n",
    "\n",
    "alternatively stated as:  \n",
    "$\\lambda \\big(\\mathbf A\\big)_{max} \\lambda \\big(\\mathbf B\\big)_{max} \\geq \\lambda \\big(\\mathbf A \\circ \\mathbf B\\big)_{max}$\n",
    "\n",
    "and as always eigenvalues are well ordered so that $\\lambda_1 \\geq \\lambda_2 \\geq ... \\geq \\lambda_n \\geq 0$ for any of these Hermitian positive semi-definite matrices\n",
    "\n",
    "The claim is trivially an equality if $\\mathbf A = \\mathbf 0$ or $\\mathbf B = \\mathbf 0$, hence we assume that neither matrix is the zero matrix, i.e. that $\\big \\Vert \\mathbf A \\big \\Vert_F \\gt 0$ and $\\big \\Vert \\mathbf B \\big \\Vert_F \\gt 0$\n",
    "\n",
    "**proof:**   \n",
    "$ \\lambda_{1,A} \\mathbf I - \\mathbf A = \\lambda_{1,A} \\mathbf I - \\mathbf {PDP}^H = \\lambda_{1,A} \\mathbf {PIP}^H - \\mathbf {PDP}^H = \\mathbf P \\big(\\lambda_{1,A} \\mathbf I - \\mathbf D\\big)\\mathbf P^H$\n",
    "\n",
    "when we inspect the diagonal entries of the diagonal matrix given by, we see:\n",
    "\n",
    "$\\big(\\lambda_{1,A} \\mathbf I - \\mathbf D\\big)_{k,k} = \\lambda_{1, A} - \\lambda_{k, A} \\geq 0$\n",
    "\n",
    "hence the matrix $\\big(\\lambda_{1,A} \\mathbf I - \\mathbf A \\big)$ is Hermitian positive semi-definite.  \n",
    "\n",
    "now consider:\n",
    "\n",
    "$\\big(\\lambda_{1,A} \\mathbf I - \\mathbf A \\big)\\circ \\mathbf B$\n",
    "\n",
    "Based on the preceding proof, the matrix that results from this, too, must be Hermitian positive semi-definite.  \n",
    "\n",
    "Hence we say \n",
    "\n",
    "$\\mathbf x^H \\Big(\\big(\\lambda_{1,A} \\mathbf I - \\mathbf A \\big)\\circ \\mathbf B\\Big)\\mathbf x \\geq 0$\n",
    "\n",
    "for any $\\mathbf x$.\n",
    "\n",
    "$\\mathbf x^H \\Big(\\big(\\lambda_{1,A} \\mathbf I - \\mathbf A \\big)\\circ \\mathbf  B\\Big)\\mathbf x = \\mathbf x^H \\Big(\\big(\\lambda_{1,A} \\mathbf I\\big) \\circ \\mathbf B \\Big)\\mathbf x - \\mathbf x^H \\Big(\\mathbf A \\circ \\mathbf B\\Big)\\mathbf x \\geq 0$\n",
    "\n",
    "for any $\\mathbf x$\n",
    "\n",
    "$\\mathbf x^H \\Big(\\big(\\lambda_{1,A} \\mathbf I\\big) \\circ \\mathbf B \\Big)\\mathbf x \\geq \\mathbf x^H \\Big(\\mathbf A \\circ \\mathbf B\\Big)\\mathbf x = \\mathbf x^H \\mathbf C \\mathbf x$\n",
    "\n",
    "for any $\\mathbf x$\n",
    "\n",
    "now set the constraint that $\\Vert \\mathbf x \\Vert_2^2 = 1$.  The right hand side is maximized with $\\mathbf x : = \\mathbf x_1$ which is the eigenvector associated with $\\lambda_{1,C}$\n",
    "\n",
    "$\\mathbf x_1^H \\Big(\\big(\\lambda_{1,A} \\mathbf I\\big) \\circ \\mathbf B \\Big)\\mathbf x_1 = \\lambda_{1,A} \\cdot \\mathbf x_1^H \\Big( \\mathbf I \\circ \\mathbf B \\Big)\\mathbf x_1 \\geq \\lambda_{1,C} = \\mathbf x_1^H \\mathbf C \\mathbf x_1$\n",
    "\n",
    "Hence we know that the Hermitian positive semi-definite matrix given by $\\big(\\mathbf I \\circ \\mathbf B \\big)$ must have at least one eigenvalue that can be scaled by $\\lambda \\big(\\mathbf A\\big)_{max}$ and the resulting product $\\geq \\lambda \\big(\\mathbf C\\big)_{max}$.\n",
    "\n",
    "If we are able to prove that the maximal eigenvalue of $\\mathbf B$ is at least as big as the maximal eigenvalue of $\\big(\\mathbf I \\circ \\mathbf B \\big)$, then we are done. (The result is immediate and more refined if the reader knows about majorization-- though most do not and this section of the notebook is not part in the majorization section, so those results are not used.)  \n",
    "\n",
    "where $\\mathbf H := \\mathbf I \\circ \\mathbf B$.  \n",
    "\n",
    "Notice that $\\mathbf H$ in effect takes all of the diagonal entries of $\\mathbf B$ (and keeps their ordering intact), and then zeros out all other entries of $\\mathbf B$.\n",
    "\n",
    "What we have proven so far can be re-written as \n",
    "\n",
    "$\\lambda_{1,A} \\cdot \\lambda_{1, H} \\geq \\lambda_{1,C} $\n",
    "\n",
    "Further we now claim:\n",
    "\n",
    "$\\lambda_{1,A} \\cdot \\lambda_{1, B} \\geq \\lambda_{1,A} \\cdot \\lambda_{1, H} \\geq \\lambda_{1,C} $\n",
    "\n",
    "To justify this, consider the following: \n",
    "\n",
    "max $\\mathbf y^H \\mathbf B \\mathbf y = \\lambda_{1, B}$\n",
    "\n",
    "subject to the constraint that $\\Vert \\mathbf y \\Vert_2^2 = 1$\n",
    "\n",
    "we can always choose to restrict ourself to just one standard basis vector $\\mathbf e_k$, for $k = \\{1, 2, ..., n\\}$.  For avoidance of doubt, the standard basis vectors are shown below:\n",
    "\n",
    "$\\bigg[\\begin{array}{c|c|c|c}\n",
    "\\mathbf e_1 & \\mathbf  e_2 &\\cdots &\\mathbf  e_n\n",
    "\\end{array}\\bigg] = \\mathbf I$\n",
    "\n",
    "\n",
    "which makes the optimization\n",
    "\n",
    "max $\\mathbf e_k^H \\mathbf B \\mathbf e_k = \\lambda_{1, H}$\n",
    "\n",
    "Thus\n",
    "\n",
    "max $\\mathbf y^H \\mathbf B \\mathbf y = \\lambda_{1, B}  \\geq$ max $ \\mathbf e_k^H \\mathbf B \\mathbf e_k = \\lambda_{1, H}$, again with the constrain that $\\Vert \\mathbf y \\Vert_2^2 = 1$. \n",
    "\n",
    "Hence $\\lambda_{1, B} \\geq \\lambda_{1, H}$\n",
    "\n",
    "To conclude we have: \n",
    "\n",
    "$\\lambda_{1,A} \\cdot \\lambda_{1, B} \\geq \\lambda_{1,A} \\cdot \\lambda_{1, H} \\geq \\lambda_{1,C} $\n",
    "\n",
    "or more succinctly,\n",
    "\n",
    "$\\lambda \\big(\\mathbf A\\big)_{max} \\lambda \\big(\\mathbf B\\big)_{max} \\geq \\lambda \\big(\\mathbf A\\circ \\mathbf B\\big)_{max}$\n",
    "\n",
    "and the claim is proven\n",
    "\n",
    "- - - -\n",
    "*begin alternative proof*  \n",
    "\n",
    "Another way to prove this, which seems a bit more intuitive, is to leverage the work done in the first part to prove that $\\big(\\mathbf A \\circ \\mathbf B\\big)$ is a  Hermitian positive semi definite matrix.  The final expression we had was\n",
    "\n",
    "$ \\sum_{k=1}^n  \\lambda_k \\big(\\mathbf y_k^H \\mathbf B \\mathbf y_k\\big) = \\lambda_1 \\big(\\mathbf y_1^H \\mathbf B \\mathbf y_1\\big) + \\lambda_2 \\big(\\mathbf y_2^H \\mathbf B \\mathbf y_2\\big) + ... + \\lambda_n \\big(\\mathbf y_n^H \\mathbf B \\mathbf y_n\\big) \\geq 0$\n",
    "\n",
    "Recalling that the $\\lambda_k$'s were the eigenvalues of $\\mathbf A$, and that $\\lambda_1 \\geq \\lambda_2 \\geq ... \\geq \\lambda_n \\geq 0$, we can upper bound this series as follows:\n",
    "\n",
    "$ \\sum_{k=1}^n  \\lambda_1 \\big(\\mathbf y_k^H \\mathbf B \\mathbf y_k\\big) \\geq \\sum_{k=1}^n  \\lambda_k \\big(\\mathbf y_k^H \\mathbf B \\mathbf y_k\\big) \\geq 0$\n",
    "\n",
    "From here we work backward and examine the impact of homogenizing the eigenvalues of $\\mathbf A$, specifically recalling \n",
    "\n",
    "$\\text{sum}\\Big(\\big(\\mathbf A \\circ \\mathbf x\\mathbf x^H\\big) \\circ \\mathbf B   \\Big) = \\text{sum}\\Big(\\big((\\lambda_1 \\mathbf p_1 \\mathbf p_1^H + \\lambda_2 \\mathbf p_2 \\mathbf p_2^H + ... + \\lambda_n \\mathbf p_n \\mathbf p_n^H) \\circ \\mathbf x\\mathbf x^H\\big) \\circ \\mathbf B   \\Big) $\n",
    "\n",
    "now becomes\n",
    "\n",
    "$\\text{sum}\\Big(\\big( \\lambda_1 ( \\mathbf p_1 \\mathbf p_1^H + \\mathbf p_2 \\mathbf p_2^H + ... + \\mathbf p_n \\mathbf p_n^H) \\circ \\mathbf x\\mathbf x^H\\big) \\circ \\mathbf B   \\Big) = \\text{sum}\\Big( \\lambda_1 \\big( \\mathbf P \\mathbf P^H\\big) \\circ \\mathbf x\\mathbf x^H \\circ \\mathbf B   \\Big) = \\text{sum}\\Big(\\lambda \\big(\\mathbf A\\big)_{max} \\big( \\mathbf I\\big) \\circ \\mathbf x\\mathbf x^H \\circ \\mathbf B   \\Big) $\n",
    "\n",
    "which we can re-arrange to\n",
    "\n",
    "$\\lambda \\big(\\mathbf A\\big)_{max} \\cdot \\text{sum}\\Big( \\big(\\mathbf I \\circ \\mathbf B\\big) \\circ \\mathbf x \\mathbf x^H     \\Big) = \\lambda \\big(\\mathbf A\\big)_{max} \\cdot \\mathbf x^H \\big(\\mathbf I \\circ \\mathbf B\\big)\\mathbf x $\n",
    "\n",
    "\n",
    "which tells us that at a minimum $\\lambda \\big(\\mathbf A\\big)_{max} \\cdot \\lambda \\big(\\mathbf I\\circ \\mathbf B\\big)_{max} \\geq \\lambda \\big(\\mathbf A\\circ \\mathbf B\\big)_{max}$\n",
    "\n",
    "because \n",
    "\n",
    "$\\lambda_1 \\big(\\mathbf y_1^H \\mathbf B \\mathbf y_1\\big) + \\lambda_1 \\big(\\mathbf y_2^H \\mathbf B \\mathbf y_2\\big) + ... + \\lambda_1 \\big(\\mathbf y_n^H \\mathbf B \\mathbf y_n\\big) \\geq  \\lambda_1 \\big(\\mathbf y_1^H \\mathbf B \\mathbf y_1\\big) + \\lambda_2 \\big(\\mathbf y_2^H \\mathbf B \\mathbf y_2\\big) + ... + \\lambda_n \\big(\\mathbf y_n^H \\mathbf B \\mathbf y_n\\big)$\n",
    "\n",
    "for any $\\mathbf x$, (subject to $\\Vert \\mathbf x \\Vert_2 = 1$) including $\\mathbf x$ that maximizes the right hand side, which gives an answer $= \\lambda \\big(\\mathbf A\\circ \\mathbf B\\big)_{max}$.  Hence we know that the left hand side has at least one solution (read: eigenvalue associated with $\\lambda \\big(\\mathbf I\\circ \\mathbf B\\big)_{max}$ times $\\lambda \\big(\\mathbf A\\big)_{max}$ ) that is greater than or equal to the right hand side.  \n",
    "\n",
    "from here we again notice that, subject to the constraint $\\Vert \\mathbf z \\Vert_2 = 1$\n",
    "\n",
    "max $\\mathbf z^H \\mathbf B \\mathbf z = \\lambda \\big(\\mathbf B\\big)_{max}  \\geq $ max $\\mathbf e_k^H \\mathbf B \\mathbf e_k = \\lambda \\big(\\mathbf I \\circ \\mathbf B\\big)_{max}$\n",
    "\n",
    "recalling that $\\bigg[\\begin{array}{c|c|c|c}\n",
    "\\mathbf e_1 & \\mathbf  e_2 &\\cdots &\\mathbf  e_n\n",
    "\\end{array}\\bigg] = \\mathbf I$\n",
    "\n",
    "and we conclude with\n",
    "\n",
    "$\\lambda \\big(\\mathbf A\\big)_{max} \\cdot \\lambda \\big(\\mathbf B\\big)_{max}  \\geq \\lambda \\big(\\mathbf A\\big)_{max} \\cdot \\lambda \\big(\\mathbf I\\circ \\mathbf B\\big)_{max} \\geq \\lambda \\big(\\mathbf A\\circ \\mathbf B\\big)_{max}$\n",
    "\n",
    "*end alternative proof*  \n",
    "\n",
    "*commentary*\n",
    "\n",
    "It is interesting to think about the eigenvalues of $\\mathbf B$ and $\\big(\\mathbf I \\circ \\mathbf B\\big)$.  We can use Gerschgorin discs (which we can flatten to line segments if we want because we know the eigenvalues are real) to bound the eigenvalues of $\\mathbf B$, noting that the center point of the discs are given exactly by $\\big(\\mathbf I \\circ \\mathbf B\\big)$.  \n",
    "\n",
    "Let's suppose that $\\mathbf B \\neq \\big(\\mathbf I \\circ \\mathbf B\\big)$ and that there are no zeros along the diagonal.  \n",
    "\n",
    "Then we know that these matrices eigenvalues sum to be the same amount:\n",
    "\n",
    "$\\text{trace}\\big(\\mathbf B\\big) = \\text{trace}\\big(\\mathbf I \\circ \\mathbf B\\big)$\n",
    "\n",
    "since $\\big(\\mathbf I \\circ \\mathbf B\\big)$ is diagonal, we know its determinant is the product of those entries (which is $\\gt 0$ because we assume no zeros on diagonal). From applying the Hadamard Inequality, we know that \n",
    "\n",
    "$\\text{det}\\big(\\mathbf B\\big) \\leq \\text{det}\\big(\\mathbf I \\circ \\mathbf B\\big)$\n",
    "\n",
    "However, we also know that if we square both sides, the left hand side will have a higher trace:\n",
    "\n",
    "$\\text{trace}\\big(\\mathbf B^2\\big) = \\text{trace}\\big(\\mathbf B^H \\mathbf B \\big) = \\Vert \\mathbf B\\Vert_F^2 \\geq \\Vert \\big( \\mathbf I \\circ  \\mathbf B\\big) \\Vert_F^2 = \\text{trace}\\Big(\\big(\\mathbf I \\circ \\mathbf B\\big)^2\\Big)$\n",
    "\n",
    "This would seem to suggest that that $\\mathbf B$ has more extreme eigenvalues than $\\big(\\mathbf I \\circ \\mathbf B\\big)$ (note: this is in fact true and this line of thinking can be considerably refined via some ideas from majorization), that when you add them all up, they are the same, but when you multiply them, you get a smaller product.  However, when you square them, the small ones decrease, but in the spirit of Jensen's Inequality, we'd observe that that $\\frac{1}{2}(\\lambda_1^2 + \\lambda_n^2) \\geq (\\frac{1}{2}(\\lambda_1 + \\lambda_n))^2$\n",
    "\n",
    "And this is what our quadratic form tells us, again, where $\\Vert \\mathbf z \\Vert_2 = 1$  \n",
    "\n",
    "max $\\mathbf z^H \\mathbf B \\mathbf z = \\lambda \\big(\\mathbf B\\big)_{max}  \\geq $ max $\\mathbf e_k^H \\mathbf B \\mathbf e_k = \\lambda \\big(\\mathbf I \\circ \\mathbf B\\big)_{max}$\n",
    "\n",
    "and if we want to minimize the quadratic form, we see,\n",
    "\n",
    "min $\\mathbf z^H \\mathbf B \\mathbf z = \\lambda \\big(\\mathbf B\\big)_{min}  \\leq $ min $\\mathbf e_k^H \\mathbf B \\mathbf e_k = \\lambda \\big(\\mathbf I \\circ \\mathbf B\\big)_{min}$\n",
    "\n",
    "Because $\\mathbf e_k$'s are a proper subset of what we can choose our $\\mathbf z$ from, and hence our optimal $\\mathbf z$ must give a result at least as good as using $\\mathbf e_k$.  \n",
    "\n",
    "\n",
    "- - - - \n",
    "\n",
    "**claim:**  \n",
    "\n",
    "$\\text{trace}\\big(\\mathbf A \\circ \\mathbf B\\big) \\leq \\text{trace}\\big(\\mathbf A\\big) \\text{trace}\\big(\\mathbf B\\big)$\n",
    "\n",
    "**proof:**  \n",
    "\n",
    "$\\text{trace}\\big(\\mathbf A \\circ \\mathbf B\\big) = a_{1,1}\\cdot b_{1,1} + a_{2,2}\\cdot b_{2,2} +... + a_{n,n}\\cdot b_{n,n} \\leq (a_{1,1} + a_{2,2} +... + a_{n,n}) (b_{1,1} + b_{2,2} +... + b_{n,n}) = \\text{trace}\\big(\\mathbf A\\big) \\text{trace}\\big(\\mathbf B\\big)$\n",
    "\n",
    "because each diagonal entry $a_{k,k}$ and $b_{k,k}$ is real valued and non-negative, and hence all cross terms are real valued and non-negative.  \n",
    "\n",
    "\n",
    "**claim:**  \n",
    "\n",
    "$\\text{trace}\\big(\\mathbf A \\circ \\mathbf B\\big) \\leq \\frac{1}{2} \\text{trace}\\big(\\mathbf A \\circ \\mathbf A + \\mathbf B \\circ \\mathbf B\\big)$\n",
    "\n",
    "**proof:**\n",
    "\n",
    "a very simple way to prove this claim is to notice that $\\mathbf C:= \\mathbf A - \\mathbf B$, is a Hermitian matrix.  And $\\mathbf C$, like any Hermitian matrix, must have real valued entries on its diagonal (or else they could not be equal to their conjugate, and we would not have $\\mathbf C = \\mathbf C^H$).  Thus when we square the real valued entries along the diagonal, we see $\\mathbf c_{k,k} \\cdot \\mathbf c_{k,k} \\geq 0$.  \n",
    "\n",
    "Hence we can say:\n",
    "\n",
    "$\\text{trace}\\big(\\mathbf C \\circ \\mathbf C\\big) \\geq 0$\n",
    "\n",
    "$\\text{trace}\\Big(\\big(\\mathbf A - \\mathbf B\\big) \\circ \\big(\\mathbf A - \\mathbf B\\big)\\Big) \\geq 0$\n",
    "\n",
    "$\\text{trace}\\Big( \\mathbf A \\circ \\mathbf A + \\mathbf B \\circ \\mathbf B - 2 \\cdot \\mathbf A \\circ \\mathbf B\\Big) \\geq 0$\n",
    "\n",
    "$\\text{trace}\\Big( \\mathbf A \\circ \\mathbf A + \\mathbf B \\circ \\mathbf B\\Big) - 2\\cdot \\text{trace}\\Big(\\mathbf A \\circ \\mathbf B\\Big) \\geq 0$\n",
    "\n",
    "$\\text{trace}\\Big( \\mathbf A \\circ \\mathbf A + \\mathbf B \\circ \\mathbf B\\Big) \\geq 2\\cdot \\text{trace}\\Big(\\mathbf A \\circ \\mathbf B\\Big) $\n",
    "\n",
    "\n",
    "$\\frac{1}{2} \\text{trace}\\Big( \\mathbf A \\circ \\mathbf A + \\mathbf B \\circ \\mathbf B\\Big) \\geq \\text{trace}\\Big(\\mathbf A \\circ \\mathbf B\\Big) $\n",
    "\n",
    "which completes the proof\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "suppose we have real symmetric positive semi definite matrix $Z$ and  real symmetric positive semi definite matrix $C$ that we know is rank one, i.e. $\\mathbf X = \\mathbf x\\mathbf x^*$  \n",
    "\n",
    "*lemma:*    \n",
    "$\\mathbf x\\mathbf x^* \\circ B= \\text{diag}(\\mathbf {x}) B\\text{diag}(\\mathbf {\\bar{x}})$  \n",
    "\n",
    "*proof:*  \n",
    "consider for any coordinate i,j  \n",
    "$\\mathbf e_i^*\\big(\\mathbf x\\mathbf x^* \\circ B\\big)\\mathbf e_j = x_{i}\\cdot\\bar{x_j}\\cdot b_{i,j}=  x_i\\cdot \\mathbf e_i^* B \\mathbf e_j \\cdot \\bar{x_j} = \\mathbf e_i^*\\text{diag}(\\mathbf x) B\\text{diag}(\\mathbf {\\bar{x}})\\mathbf e_j$  \n",
    "\n",
    "*remark:*\n",
    "in part to emphasize probabilistic interpretations which are reasonably nice in the case of the Hadamard prodcuct, \n",
    "**the below are shown for the case of real symmetric postive (semi) definite matrices.**  However as shown/noted in the final section, these results also apply to Hermitian symmetric positive (semi)definite matrices with scalars in $\\mathbb C$.  \n",
    "\n",
    "\n",
    "*next: Introduce random vectors*    \n",
    "Let $\\mathbf y$  be a vector of iid Rademacher r.v.'s.  This implies $\\mathbb E\\big[\\mathbf y\\big] =\\mathbf 0$.  \n",
    "now for the real symmetric positive semi definite matrix $\\mathbf \\Gamma$  we have $\\mathbf \\Gamma = \\mathbf L\\mathbf L^T$ via Cholesky decomposition.  Now define     \n",
    "$\\mathbf x:= \\mathbf L\\mathbf y$  \n",
    "so  \n",
    "$\\mathbb E\\Big[\\mathbf x\\mathbf x^T\\Big] = \\text{var}\\Big(\\mathbf x\\Big)= \\mathbb E\\Big[\\big(\\mathbf L\\mathbf y\\big)\\big(\\mathbf L\\mathbf y\\big)^T\\Big]=\\mathbb E\\Big[\\mathbf L\\mathbf y\\mathbf y^T \\mathbf L^T\\Big]= \\mathbf L\\mathbb E\\Big[\\mathbf y\\mathbf y^T \\Big]\\mathbf L^T = \\mathbf L \\mathbf I \\mathbf L^T =\\mathbf L\\mathbf L^T = \\mathbf\\Gamma$  \n",
    "\n",
    "i.e. $\\mathbf \\Gamma$ is the (co)variance matrix for random vector $\\mathbf x$  \n",
    "\n",
    "note: the more typical choice for $\\mathbf y$ would be a vector of iid $N(0,1)$ r.v.s -- that is to use Gaussians.  However using a Rademacher, means that $\\mathbf x$ is a vector of *simple* random variables which is expedient and is perhaps more fitting for algebraic arguments.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "\n",
    "**claim**  \n",
    "$\\Big \\Vert \\mathbf \\Gamma \\circ \\mathbf Z \\Big \\Vert_{\\text{maximal eigenvalue}} \\leq \\max_k \\gamma_{k,k}\\cdot \\lambda_1(\\mathbf Z)$   \n",
    "\n",
    "*remark:*  \n",
    "this comes down to using \n",
    "$\\text{trace}\\big(\\mathbf{AB}\\big) \\leq \\lambda_{max}\\big(\\mathbf A\\big) \\text{trace}\\big(\\mathbf B\\big)$  \n",
    "which was proven much earlier in this notebook under Hermitian Positive (semi) definite trace inequalities  \n",
    "\n",
    "as well using as our lemma and the fact that trace and expectations commmute, and finally that $E\\big[\\mathbf x \\mathbf x^T \\big]=\\mathbf \\Gamma$  \n",
    "i.e. a real symmetric positive-semi definite matrix is a covariance matrix for some \\mathbf Zero mean r.v. \n",
    "\n",
    "In the below, $P$ is a rea symmetric projector, $P:=\\mathbf u\\mathbf u^T$, with trace one    \n",
    "\n",
    "\n",
    "**proof:**  \n",
    "$\\Big \\Vert \\mathbf \\Gamma \\circ \\mathbf Z \\Big \\Vert_{\\text{maximal eigenvalue}}$  \n",
    "$=\\Big \\Vert \\mathbf \\Gamma \\circ \\mathbf Z \\Big \\Vert_{S_\\infty}$  \n",
    "$=\\Big \\Vert \\mathbb E\\big[\\mathbf x \\mathbf x^T\\big] \\circ \\mathbf Z \\Big \\Vert_{S_\\infty}$  \n",
    "$=\\Big \\Vert \\mathbb E\\big[\\mathbf x \\mathbf x^T\\circ \\mathbf Z \\big] \\Big \\Vert_{S_\\infty}$  \n",
    "$=\\Big\\Vert \\mathbb E\\big[\\text{diag}(\\mathbf x) \\mathbf Z\\text{diag}(\\mathbf x) \\big]\\Big\\Vert_{S_\\infty}$  \n",
    "$= \\max_{P=uu^T}\\text{: } \\text{trace}\\Big(P\\mathbb E\\big[\\text{diag}(\\mathbf x) \\mathbf Z\\text{diag}(\\mathbf x) \\big]\\Big)$  \n",
    "$=\\max_{P=uu^T}\\text{: } \\mathbb E\\Big[\\text{trace}\\big(P\\text{diag}(\\mathbf x) \\mathbf Z\\text{diag}(\\mathbf x) \\big)\\Big]$  \n",
    "$=\\max_{P=uu^T}\\text{: } \\mathbb E\\Big[\\text{trace}\\big(\\big(\\text{diag}(\\mathbf x) P\\text{diag}(\\mathbf x)\\big) \\mathbf Z \\big)\\Big]$  \n",
    "$=\\max_{P=uu^T}\\text{: } \\mathbb E\\Big[\\text{trace}\\Big(\\big(\\mathbf x\\mathbf x^T \\circ P\\big)\\mathbf Z\\Big)\\Big]$  \n",
    "$=\\max_{P=uu^T}\\text{: } \\text{trace}\\Big(\\mathbb E\\Big[\\big(\\mathbf x\\mathbf x^T \\circ  \\mathbf {uu}^T\\big) \\mathbf Z\\Big] \\Big)$  \n",
    "$=\\max_{P=uu^T}\\text{: } \\text{trace}\\Big(\\mathbb E\\Big[\\big(\\mathbf x\\mathbf x^T \\circ  \\mathbf {uu}^T\\big)\\Big] \\mathbf Z \\Big)$  \n",
    "$=\\max_{P=uu^T}\\text{: } \\text{trace}\\Big(\\big(\\mathbb E\\big[\\mathbf x\\mathbf x^T\\big] \\circ  \\mathbf {uu}^T\\big) \\mathbf Z \\Big)$  \n",
    "$=\\max_{P=uu^T}\\text{: } \\text{trace}\\Big(\\big(\\mathbf \\Gamma \\circ  \\mathbf {uu}^T\\big) \\mathbf Z \\Big)$  \n",
    "$=\\max_{P=uu^T}\\text{: } \\text{trace}\\Big( \\big(\\mathbf \\Gamma \\circ  \\mathbf {uu}^T\\big)  Q \\mathbf \\Lambda Q^T \\Big)$  \n",
    "$=\\max_{P=uu^T}\\text{: } \\text{trace}\\Big(\\big( Q^T \\big(\\mathbf \\Gamma \\circ  \\mathbf {uu}^T\\big)  Q\\big)\\mathbf \\Lambda \\Big)$  \n",
    "$\\leq \\lambda_1 \\cdot \\max_{P=uu^T}\\text{: } \\text{trace}\\Big( Q^T\\big(\\mathbf \\Gamma \\circ  \\mathbf {uu}^T\\big)  Q \\Big)$  \n",
    "$= \\lambda_1 \\cdot \\max_{P=uu^T}\\text{: } \\text{trace}\\Big(\\big(\\mathbf \\Gamma \\circ  \\mathbf {uu}^T\\big)  \\Big)$  \n",
    "$= \\lambda_1\\cdot \\max_{P=uu^T}\\text{: } \\sum_{k=1}^n  \\gamma_{k,k} u_k^2$   \n",
    "$\\leq \\lambda_1 \\cdot\\max_k \\gamma_{k,k} \\cdot  \\sum_{k=1}^n  u_k^2$   \n",
    "$= \\lambda_1 \\cdot\\max_k \\mathbf \\gamma_{k,k} \\cdot 1$   \n",
    "$= \\Big \\Vert  \\mathbf Z\\Big \\Vert_2 \\cdot \\max_{i,j}\\mathbf \\Gamma_{i,j}$   \n",
    "\n",
    "*follow-up*  \n",
    "suppose $\\mathbf \\Gamma \\succeq 0$ and $\\mathbf Z \\succ 0$ then we can prove that $\\mathbf \\Gamma \\circ \\mathbf Z \\succ 0$ (i.e. non-singularity, which isn't obvious by the Schur Product Theorem proofs)  by essentially replicating the above argument, again with a real symmetric projector $P$ with rank 1, this time using  \n",
    "$\\text{trace}\\big(\\mathbf{AB}\\big) \\geq \\lambda_{min}\\big(\\mathbf A\\big) \\text{trace}\\big(\\mathbf B\\big)$  \n",
    "\n",
    "**claim:**  \n",
    "$\\Big \\Vert \\mathbf \\Gamma \\circ \\mathbf Z \\Big \\Vert_{\\text{minimal eigenvalue}} \\geq \\min_k \\gamma_{k,k}\\cdot \\lambda_n(\\mathbf Z)$   \n",
    "\n",
    "**proof:**  \n",
    "$\\Big \\Vert \\mathbf \\Gamma \\circ \\mathbf Z \\Big \\Vert_{\\text{minimal eigenvalue}}$  \n",
    "$= \\min_{P=uu^T}\\text{: } \\text{trace}\\Big(P\\mathbb E\\big[\\text{diag}(\\mathbf x) \\mathbf Z\\text{diag}(\\mathbf x) \\big]\\Big)$  \n",
    "$=\\min_{P=uu^T}\\text{: } \\text{trace}\\Big(\\big(\\mathbf \\Gamma \\circ  \\mathbf {uu}^T\\big) \\mathbf Z \\Big)$  \n",
    "$=\\min_{P=uu^T}\\text{: } \\text{trace}\\Big(\\big( Q^T \\big(\\mathbf \\Gamma \\circ  \\mathbf {uu}^T\\big)  Q\\big)\\mathbf \\Lambda \\Big)$  \n",
    "$\\geq \\lambda_n \\cdot \\max_{P=uu^T}\\text{: } \\text{trace}\\Big( Q^T\\big(\\mathbf \\Gamma \\circ  \\mathbf {uu}^T\\big)Q \\Big)$      \n",
    "$=\\lambda_n \\cdot \\max_{P=uu^T}\\text{: } \\text{trace}\\Big(\\mathbf \\Gamma \\circ  \\mathbf {uu}^T \\Big)$      \n",
    "$ =\\lambda_n \\cdot \\max_{P=uu^T} \\sum_{k=1}^n \\gamma_{k,k} u_k^2$  \n",
    "$ \\geq \\lambda_n \\cdot \\min\\big(\\gamma_{k,k}\\big) \\cdot \\sum_{k=1}^n  u_k^2$  \n",
    "$ = \\lambda_n \\cdot \\min\\big(\\gamma_{k,k}\\big) \\cdot 1 $  \n",
    "$ =\\lambda_n \\cdot \\min\\big(\\gamma_{k,k}\\big) $  \n",
    "\n",
    "so we can conclude that the Hadamard product of two real symmetric matrices, where one is positive definite, and the other is positive semidefinite, but has no \\mathbf Zeros on the diagonal, is necessarily non-singular.  In fact requiring the semidefinite matrix to have no \\mathbf Zeros on the diagonal is both sufficient (shown above) and necessary for the resulting matrix to be positive definite -- e.g. for necessity apply Hadamard determinant inequality for positive semidefinite matrices or basic results about $0$'s  on the diagonal of a symmetric positive semidefinite matrix (i.e. they imply the entire associated row and column is $\\mathbf 0$ which implies non-trivial nullspace).  \n",
    "\n",
    "note: if we didn't want to do the probabilistic interpretation and/or wanted to introduce complex numbers, we could also observe  \n",
    "$\\text{trace}\\Big(P\\big(\\mathbf \\Gamma \\circ \\mathbf Z\\big) \\Big)$   \n",
    "$=\\text{trace}\\Big(P\\big(\\sum_{k=1}^n \\sigma_k \\mathbf v_k \\mathbf v_k^*\\big) \\circ \\mathbf Z \\Big)$   \n",
    "$=\\text{trace}\\Big(P\\big(\\sum_{k=1}^n \\sigma_k \\mathbf v_k \\mathbf v_k^* \\circ \\mathbf Z\\big) \\Big)$   \n",
    "$=\\text{trace}\\Big(P\\big(\\sum_{k=1}^n \\sigma_k \\text{diag}\\big( \\mathbf v_k\\big)\\mathbf Z\\text{diag}\\big( \\mathbf{ \\bar{v}}_k\\big)\\big) \\Big)$   \n",
    "$=\\sum_{k=1}^n \\sigma_k\\cdot\\text{trace}\\Big(P\\text{diag}\\big( \\mathbf v_k\\big) \\mathbf Z\\text{diag}\\big(\\mathbf{ \\bar{v}}_k\\big) \\Big)$   \n",
    "$=\\sum_{k=1}^n \\sigma_k\\cdot\\text{trace}\\Big(\\text{diag}\\big( \\mathbf{ \\bar{v}}_k\\big)P\\text{diag}\\big( \\mathbf v_k\\big) \\mathbf Z \\Big)$   \n",
    "$=\\sum_{k=1}^n  \\sigma_k\\cdot \\text{trace}\\Big(\\big(\\mathbf {\\bar{v}}_k\\mathbf {\\bar{v}}_k^* \\circ P\\big) \\mathbf Z \\Big)$   \n",
    "$=\\text{trace}\\Big(\\big(\\big\\{\\sum_{k=1}^n  \\sigma_k \\mathbf {\\bar{v}}_k\\mathbf {\\bar{v}}_k^*\\big\\} \\circ P\\big) \\mathbf Z \\Big)$   \n",
    "$=\\text{trace}\\Big( \\big(\\mathbf {\\bar{\\Gamma}} \\circ P\\big) \\mathbf Z \\Big)$   \n",
    "\n",
    "where $\\mathbf {\\bar{\\Gamma}}$ is perhaps a bit awkward, but its eigenvectors $\\mathbf {\\bar{v}}_k$ are still mutually orthonormal because    \n",
    "\n",
    "$\\mathbf {\\bar{v}}_k^* \\mathbf {\\bar{v}}_j = \\mathbf {v}_k^T \\mathbf {\\bar{v}}_j  = \\big(\\mathbf {v}_k^T \\mathbf {\\bar{v}}_j\\big)^T = \\mathbf {v}_j^*\\mathbf {v}_k = 0^T = 0 \\text{ if } j \\neq k $   \n",
    "(and is $=1$ when $j=k$)  \n",
    "\n",
    "since transposing a scalar doesn't change the scalar.  We can then see that the above results apply to Hermitian positive (semi)definite matrices as well.  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
