{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This posting has two different looks at how we can minimize the length of some vector $\\mathbf x$ in an underdetermined (but full row rank) system of equations $\\mathbf{Ax} = \\mathbf b$.\n",
    "\n",
    "The underlying scalars are in $\\mathbb R$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "consider the equation $\\mathbf {A x} = \\mathbf b$, where we know $\\mathbf A$ and $\\mathbf b$, and need to solve for $\\mathbf x$. For avoidance of doubt $\\mathbf b \\neq \\mathbf 0$.  $\\mathbf A$ is an m x n matrix.\n",
    "\n",
    "In the case where $\\mathbf A$ is tall and skinny (and with noise in the data, this should equate to an overdetermined system of equations, with full column rank), we can use ordinary least squares to solve for the $\\mathbf x$ that minimizes the squared length (2 norm), of $\\mathbf v = \\mathbf {Ax} - \\mathbf b$, thus we are minimizing $\\mathbf v^T \\mathbf v$.  We may may use the Normal Equations, or QR factorization, or many other tools at our disposal.  \n",
    "\n",
    "If $\\mathbf A$ is square and of full rank, then we can directly invert $\\mathbf A$, or use Gaussian elimination or whatever tool we want. \n",
    "\n",
    "Now consider the case where the $\\mathbf A$ has more columns than rows -- i.e. n > m-- (and again there is noise in the data, so we have full row rank), this means that there are *many* solutions to $\\mathbf {A x} = \\mathbf b$, because $\\mathbf A$ has a non-trivial nullspace.  In this case, we first will want to question why we have this situation, and perhaps gather more data. If we still want to 'solve' this equation, what form might we take?  We have many solutions at our disposal, so perhaps one that minimizes the length (2 norm) of $\\mathbf x$ is the one we want. \n",
    "\n",
    "For avoidance of doubt we have $\\mathbf A \\in \\mathbb R^{\\text{m x n}}$ where $m \\lt n$.  This also means that $\\mathbf x \\in \\mathbb R^{\\text{n}}$ and  $\\mathbf b\\in \\mathbb R^{\\text{m}} $ .  This means we need at most $m$ linearly independent vectors to generate *any* given $\\mathbf b$.  Suppose we choose said linearly independent vectors to be mutually orthonormal, and we purge any vectors not in that set from the solution.  In such a case we would have a solution $\\mathbf x$ satisfying the equation $\\mathbf{Ax} = \\mathbf b$.  We'd also clearly have a smaller solution (L2 norm) solution than any one using the above solution plus additional mutually orthonormal vectors, which must in some sense be in the nullspace... \n",
    "\n",
    "There are basically two approaches to solving this.  \n",
    "\n",
    "First the algebraic one.\n",
    "\n",
    "$\\mathbf {A x} =\\big( \\mathbf {U \\Sigma V}^T\\big)\\mathbf x = \\mathbf b$, using the Singular Value Decomposition, where $\\mathbf U$ and $\\mathbf V$ are both rull rank, square, orthogonal matrices, but because $\\mathbf A$ is not square, $\\mathbf \\Sigma$ is a diagonal matrix that has more columns than rows.  \n",
    "\n",
    "That is $\\mathbf A$ is an m x n matrix with rank m (meaning that each singular value > 0)\n",
    "\n",
    "$\\mathbf A =\n",
    "\\bigg[\\begin{array}{c|c|c|c}\n",
    "\\mathbf u_1 & \\mathbf u_2 &\\cdots & \\mathbf u_{m}\n",
    "\\end{array}\\bigg] \\begin{bmatrix}\n",
    "\\sigma_1 & 0 &0  &0 & ... &0 \\\\ \n",
    "0 & \\sigma_2& 0 & 0& ... &0\\\\ \n",
    "0 & 0 &  \\ddots & 0& ... &0 \\\\ \n",
    "0 & 0 & 0 & \\sigma_m& ... &0  \n",
    "\\end{bmatrix} \\bigg[\\begin{array}{c|c|c|c}\n",
    "\\mathbf v_1 & \\mathbf v_2 &\\cdots & \\mathbf v_{n}\n",
    "\\end{array}\\bigg]^T\n",
    "$   \n",
    "\n",
    "if we left multiply both sides of $\\mathbf {A x} $ by $\\mathbf U^T$, we get \n",
    "\n",
    "$\\mathbf {\\Sigma V}^T \\mathbf x = \\begin{bmatrix}\n",
    "\\sigma_1 & 0 &0  &0 & ... &0 \\\\ \n",
    "0 & \\sigma_2& 0 & 0& ... &0\\\\ \n",
    "0 & 0 &  \\ddots & 0& ... &0 \\\\ \n",
    "0 & 0 & 0 & \\sigma_m& ... &0  \n",
    "\\end{bmatrix} \\bigg[\\begin{array}{c|c|c|c}\n",
    "\\mathbf v_1 & \\mathbf v_2 &\\cdots & \\mathbf v_{n}\n",
    "\\end{array}\\bigg]^T \\mathbf x = \\mathbf U^T \\mathbf b$\n",
    "\n",
    "now with an **abuse of notation**, consider left multiplying by $\\mathbf \\Sigma^{-1}$:\n",
    "\n",
    "where $\\mathbf \\Sigma^{-1} = \\Big(\\begin{bmatrix}\n",
    "\\frac{1}{\\sigma_1} & 0 &0  &0 &  ... &0 \\\\ \n",
    "0 & \\frac{1}{\\sigma_2}& 0 & 0& ... &0\\\\ \n",
    "0 & 0 &  \\ddots & 0& ... &0 \\\\ \n",
    "0 & 0 & 0 & \\frac{1}{\\sigma_m}& ... &0  \n",
    "\\end{bmatrix}^T$ \n",
    "\n",
    "Thus it is not technically an inverse or a left inverse... $\\mathbf \\Sigma^{-1}$ **is actually a right inverse** but we ultimately are multiplying on the left because that is all we can do here -- hence this is an abuse of notation.\n",
    "\n",
    "$\\mathbf {(D)V}^T \\mathbf x = \n",
    "\\Big(\\begin{bmatrix}\n",
    "\\frac{1}{\\sigma_1} & 0 &0  &0 &  ... &0 \\\\ \n",
    "0 & \\frac{1}{\\sigma_2}& 0 & 0& ... &0\\\\ \n",
    "0 & 0 &  \\ddots & 0& ... &0 \\\\ \n",
    "0 & 0 & 0 & \\frac{1}{\\sigma_m}& ... &0  \n",
    "\\end{bmatrix}^T \\begin{bmatrix}\n",
    "\\sigma_1 & 0 &0  &0 & ... &0 \\\\ \n",
    "0 & \\sigma_2& 0 & 0& ... &0\\\\ \n",
    "0 & 0 &  \\ddots & 0& ... &0 \\\\ \n",
    "0 & 0 & 0 & \\sigma_m& ... &0  \n",
    "\\end{bmatrix}\\Big) \\bigg[\\begin{array}{c|c|c|c}\n",
    "\\mathbf v_1 & \\mathbf v_2 &\\cdots & \\mathbf v_{n}\n",
    "\\end{array}\\bigg]^T \\mathbf x =  \\mathbf \\Sigma^{-1}\\mathbf U^T \\mathbf b$\n",
    "\n",
    "\n",
    "$\\mathbf {(D)V}^T \\mathbf x = \\begin{bmatrix}\n",
    "1 & 0 &0  &0 & \\mathbf 0^T \\\\ \n",
    "0 & 1 & 0 & 0& \\mathbf 0^T\\\\ \n",
    "0 & 0 &  \\ddots & 0& \\mathbf 0^T \\\\ \n",
    "0 & 0 & 0 & 1 & \\mathbf 0^T  \\\\\n",
    "\\mathbf 0 & \\mathbf 0 & \\mathbf 0 & \\mathbf 0 & \\mathbf 0\\mathbf 0^T  \\\\ \n",
    "\\end{bmatrix} \\bigg[\\begin{array}{c|c|c|c}\n",
    "\\mathbf v_1 & \\mathbf v_2 &\\cdots & \\mathbf v_{n}\n",
    "\\end{array}\\bigg]^T \\mathbf x = \\begin{bmatrix}\n",
    "\\mathbf I & \\mathbf{00}^T \\\\ \n",
    "\\mathbf {00}^T & \\mathbf {00}^T  \\\\ \n",
    "\\end{bmatrix} \\bigg[\\begin{array}{c|c|c|c}\n",
    "\\mathbf v_1 & \\mathbf v_2 &\\cdots & \\mathbf v_{n}\n",
    "\\end{array}\\bigg]^T \\mathbf x = \\mathbf \\Sigma^{-1}\\mathbf U^T \\mathbf b$\n",
    "\n",
    "Which is to say that $\\mathbf D$ is the spectra for an idempotent matrix (i.e. a projection matrix's eigenvalues).  (Note that to deal with notational overload, $\\mathbf {0}$ is to be the appropriately sized zero vector, and $\\mathbf {00}^T$ is the appropriately sized zero matrix.)  \n",
    "\n",
    "From here multiply both sides by $\\mathbf V$, and we get \n",
    "\n",
    "$\\mathbf {VDV}^T \\mathbf x =  \\bigg[\\begin{array}{c|c|c|c}\n",
    "\\mathbf v_1 & \\mathbf v_2 &\\cdots & \\mathbf v_{n}\n",
    "\\end{array}\\bigg] \\begin{bmatrix}\n",
    "\\mathbf I & \\mathbf{00}^T \\\\ \n",
    "\\mathbf {00}^T & \\mathbf {00}^T  \\\\ \n",
    "\\end{bmatrix}\\bigg[\\begin{array}{c|c|c|c}\n",
    "\\mathbf v_1 & \\mathbf v_2 &\\cdots & \\mathbf v_{n}\n",
    "\\end{array}\\bigg]^T \\mathbf x= \\mathbf {V\\Sigma}^{-1}\\mathbf U^T \\mathbf b$\n",
    "\n",
    "\n",
    "note that \n",
    "\n",
    "$\\mathbf P = \\mathbf {VDV}^T$  \n",
    "\n",
    "obeys \n",
    "\n",
    "$\\mathbf P^2 = \\mathbf P$  \n",
    "\n",
    "hence $\\mathbf P$ is idempotent, and indeed it is orthgonally diagonalizable -- i.e. it is a projection matrix.  (Note: there are some different conventions -- some texts refer to all idempotent matrices as projection matrices, while others only refer to real symmetric -- or Hermitian -- ones such as this as projection matrices.)  \n",
    "\n",
    "Note that if $\\mathbf A$ was full rank, $\\mathbf D = \\mathbf I$, and $\\mathbf \\Sigma^{-1}$ would be an actual inverse, not an abuse of notation (right inverse in this case), and hence we would have solved our equation.  \n",
    "\n",
    "That is, if $\\mathbf A$ was full rank, we would have had:\n",
    "\n",
    "\n",
    "$\\mathbf {VIV}^T \\mathbf x = \\mathbf {VV}^T \\mathbf x = \\big(\\mathbf{v_1 v_1}^T + \\mathbf{v_2 v_2}^T +... + \\mathbf{v_n v_n}^T\\big) \\mathbf x = \\mathbf {I} \\mathbf x = \\mathbf x = \\mathbf {V\\Sigma}^{-1}\\mathbf U^T \\mathbf b$\n",
    "\n",
    "but instead what we have is\n",
    "\n",
    "$\\mathbf P \\mathbf x =  \\Big(\\big(\\mathbf{v_1 v_1}^T + \\mathbf{v_2 v_2}^T +... + \\mathbf{v_m v_m}^T\\big) + \\big(0\\mathbf{v_{m+1} v_{m+1}}^T + 0 \\mathbf{v_{m+2} v_{m+2}}^T + ... + 0\\mathbf{v_n v_n}^T\\big)\\Big) \\mathbf x = \\mathbf {V\\Sigma}^{-1}\\mathbf U^T \\mathbf b$\n",
    "\n",
    "or more simply \n",
    "\n",
    "$\\mathbf {Px}= \\big(\\mathbf{v_1 v_1}^T + \\mathbf{v_2 v_2}^T +... + \\mathbf{v_m v_m}^T\\big) \\mathbf x =  \\mathbf {V\\Sigma}^{-1}\\mathbf U^T \\mathbf b $\n",
    "\n",
    "Now recall that $\\mathbf V = \\bigg[\\begin{array}{c|c|c|c}\n",
    "\\mathbf v_1 & \\mathbf v_2 &\\cdots & \\mathbf v_{n}\n",
    "\\end{array}\\bigg]$, which is an n x n orthogonal matrix -- that is $\\mathbf V$ can be thought of as a coordinate system.  Thus our solution vector $\\mathbf x$ can be a linear combination of the columns of $\\mathbf V$.  We can write this as \n",
    "\n",
    "$\\mathbf x = \\mathbf{Vy} = y_1\\mathbf v_1 + y_2\\mathbf v_2 + ... + y_m\\mathbf v_m +  y_{m+1}\\mathbf v_{m+1} + ... + y_{n}\\mathbf v_{n}$  \n",
    "\n",
    "and recalling that since $\\mathbf V$ is orthogonal, it is length (2 norm) preserving, so:  \n",
    "\n",
    "$\\big \\vert\\big \\vert \\mathbf x \\big \\vert \\big \\vert_2^{2} =  \\big \\vert\\big \\vert \\mathbf{Vy} \\big \\vert\\big \\vert_2^{2} = \\big \\vert \\big \\vert \\mathbf y \\big \\vert\\big \\vert_2^{2} = \\mathbf y^T \\mathbf y = y_1^2 + y_2^2 + ... + y_m^2 + y_{m+1}^2 + ... + y_n^2$\n",
    "\n",
    "we substitute in and get \n",
    "\n",
    "$\\big(\\mathbf{v_1 v_1}^T + \\mathbf{v_2 v_2}^T +... + \\mathbf{v_m v_m}^T\\big) \\big(y_1\\mathbf v_1 + y_2\\mathbf v_2 + ... + y_m\\mathbf v_m +  y_{m+1}\\mathbf v_{m+1} + ... + y_{n}\\mathbf v_{n}\\big) = \\mathbf {V\\Sigma}^{-1}\\mathbf U^T \\mathbf b$\n",
    "\n",
    "which by the orthogonality of the columns in $\\mathbf V$ gives us:\n",
    "\n",
    "$\\mathbf x = y_1  \\mathbf{v_1} + y_2\\mathbf v_2 +... + y_m  \\mathbf{v_m} = \\mathbf {V\\Sigma}^{-1}\\mathbf U^T \\mathbf b $\n",
    "\n",
    "From here we notice that any $y_k$, for $k \\gt m$ contributes to the length of $\\mathbf x$ but does not contribute to the solution of the problem (i.e. they are in the null space).  \n",
    "\n",
    "Thus the minimal length solution to the underdetermined $\\mathbf {Ax} = \\mathbf b$ comes in the form of a solution to the equation that has $\\mathbf x $ written purely as a linear combination of $\\{\\mathbf v_1, \\mathbf v_2, ..., \\mathbf v_m \\}$\n",
    "\n",
    "The way to interpret this, then, is we solve for any legal $\\mathbf x$ that is a valid solution, and then project such a solution down to a subspace that only is spanned by $m$ mutually orthonormal vectors ($\\mathbf v_1, ..., \\mathbf v_m$)  \n",
    "\n",
    "**a nice insight**   \n",
    "suppose, for instance we solve a Linear Program and compute \n",
    "\n",
    "$\\mathbf x_{\\text{L1 norm minimized}}$  \n",
    "\n",
    "if we project it down to the subspace of $\\{\\mathbf v_1, \\mathbf v_2, ...., \\mathbf v_m\\}$ using, of course, our projector $\\mathbf P$, to do so, we see \n",
    "\n",
    "\n",
    "$\\mathbf P \\mathbf x_{\\text{L1 norm minimized}} = \\mathbf x_{\\text{L2 norm minimized}} = \\mathbf x$  \n",
    "\n",
    "of course $\\mathbf P$ is not invertible (unless $\\mathbf A$ is square) so this is a one way relation, but nevertheless interesting.  \n",
    "\n",
    "- - - - - \n",
    "\n",
    "**alternative takes:  **  \n",
    "\n",
    "1.)  \n",
    "some math text, and some computational solvers (e.g. Julia's) will do SVD for short fat matrices in the form of \n",
    "\n",
    "$\\mathbf A = \\mathbf U \\mathbf \\Sigma_{\\text{square}}\\mathbf V_{\\text{non-square}}^T$   \n",
    "\n",
    "$\\mathbf V^T$ is $m$ x $n$, or equivalently, $\\mathbf V$ is $n$ x $m$, hence it has mutually orthonormal vectors of dimension $n$, but only has $m$ of them and hence is not an orthgonal matrix, just a matrix with mutually orthnormal columns -- but not enough to form a basis.  \n",
    "\n",
    "\n",
    "To verify that the end results are unchanged, we can see \n",
    "\n",
    "\n",
    "$\\mathbf A = \\mathbf U \\big(\\mathbf \\Sigma_{\\text{square}}\\mathbf V_{\\text{non-square}}^T\\big) = \\big(\\mathbf \\Sigma\\mathbf V^T\\big)$  \n",
    "\n",
    "where $\\mathbf \\Sigma_{\\text{square}}$ is $m$ x $m$ \n",
    "\n",
    "\n",
    "\n",
    "because \n",
    "\n",
    "$\\mathbf \\Sigma_{\\text{square}}\\mathbf V_{\\text{non-square}}^T = \\mathbf \\Sigma\\mathbf V^T = \n",
    "\\begin{bmatrix}\n",
    "\\sigma_1 \\mathbf v_1^T \\\\\n",
    " \\sigma_2 \\mathbf v_2^T \\\\ \n",
    "\\vdots\\\\ \n",
    "\\sigma_{m-1} \\mathbf v_{m-1}^T \\\\ \n",
    "\\sigma_m \\mathbf v_m^T\n",
    "\\end{bmatrix}$ \n",
    "\n",
    "and as before \n",
    "\n",
    "$\\mathbf P = \\big(\\mathbf{v_1 v_1}^T + \\mathbf{v_2 v_2}^T +... + \\mathbf{v_m v_m}^T\\big) = \\mathbf V_{\\text{non-square}} \\mathbf V_{\\text{non-square}}^T$  \n",
    "\n",
    "2.) *An alternative computationally efficient approach:*  \n",
    "\n",
    "While the SVD approach allows us to compute $\\mathbf P$ and get insights into the geometry of the minimum costs L2 norm solution to our system of equations, what the below section (using Lagrange multipliers) shows is that the solution is given by \n",
    "\n",
    "$\\mathbf x = \\mathbf A^T \\big(\\mathbf{AA}^T\\big)^{-1} \\mathbf b$ \n",
    "\n",
    "as with ordinary least squares, we generally don't want to actually compute $\\big(\\mathbf{AA}^T\\big)$ for cost and numeric stability reasons.  But the reader may also recall that computing the SVD is the most expensive of operation of the typical matrix factorizations.  The nice middle ground here (much like in least squares) is to use QR factorization.  An outline of the computational approach using QR factorization is shown below.  \n",
    "\n",
    "\n",
    "$\\mathbf A^T = \\mathbf {QR}$\n",
    "\n",
    "where $\\mathbf Q$ is tall and skinny and $\\mathbf R$ is a square upper triangular matrix.  Since $\\mathbf A$ has full row rank this means $\\mathbf A^T$ has full column rank, which mean $\\mathbf R$ has no zeros along its diagonal, which means $\\mathbf R^{-1}$ exists.  \n",
    "\n",
    "If you did this, you'd get \n",
    "\n",
    "$\\mathbf x = \\mathbf A^T\\big(\\mathbf {AA}^T\\big)^{-1}\\mathbf b =\\big(\\mathbf {QR}\\big)\\Big(\\big(\\mathbf Q \\mathbf R\\big)^T\\big(\\mathbf Q\\mathbf R\\big)\\Big)^{-1}\\mathbf b = \\mathbf {QR}\\Big(\\big(\\mathbf R^T \\mathbf Q^T\\big)\\big(\\mathbf Q\\mathbf R\\big)\\Big)^{-1}\\mathbf b $ \n",
    "\n",
    "$ = \\mathbf {QR}\\big(\\mathbf R^T\\mathbf R \\big)^{-1}\\mathbf b = \\mathbf {QR}\\big(\\mathbf R \\big)^{-1}\\big(\\mathbf R^T\\big)^{-1}\\mathbf b = \\mathbf Q\\big(\\mathbf R^T\\big)^{-1}\\mathbf b$  \n",
    "\n",
    "hence we have \n",
    "\n",
    "$\\mathbf x = \\mathbf x_{\\text{L2 norm minimized}} = \\mathbf Q\\big(\\mathbf R^T\\big)^{-1}\\mathbf b$ \n",
    "\n",
    "which is to say that the solution is equivalent to running QR factorization on $\\mathbf A^T$  \n",
    "\n",
    "and then solving the lower triangular system of equations for $\\mathbf y$ in \n",
    "\n",
    "$\\mathbf R^T \\mathbf y = \\mathbf b$ \n",
    "\n",
    "and then after solving for $\\mathbf y$, left multiplying by $\\mathbf Q$\n",
    "\n",
    "\n",
    "$\\mathbf x = \\mathbf Q\\mathbf y =\\mathbf Q\\Big(\\big(\\mathbf R^T\\big)^{-1}\\mathbf b\\Big)$   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can streamline the above by considering that \n",
    "\n",
    "$\\mathbf A^T = \\mathbf {QR}$  is equivalent to  \n",
    "$\\mathbf A = \\mathbf {R}^T\\mathbf Q^T =  \\mathbf {L}\\mathbf Q^T$   \n",
    "\n",
    "where $\\mathbf Q$ is tall and skinny, so $\\mathbf Q^T \\in \\mathbb R^{\\text{m x n}}$ is short and fat.   \n",
    "Now consider the projector given by   \n",
    "$\\mathbf P:= \\mathbf Q \\mathbf Q^T $  \n",
    "\n",
    "$\\mathbf P^2:= \\mathbf Q \\mathbf Q^T\\mathbf Q \\mathbf Q^T =  \\mathbf Q\\big(\\mathbf Q^T\\mathbf Q\\big) \\mathbf Q^T = \\mathbf Q\\big(\\mathbf I_m \\big) \\mathbf Q^T = \\mathbf P$  \n",
    "\n",
    "and $\\mathbf P = \\mathbf P^T$ so it is a projector  \n",
    "\n",
    "note: $\\mathbf P$ has rank $m$ -- given by its trace.    \n",
    "\n",
    "\n",
    "note that \n",
    "\n",
    "$\\mathbf {AP} = \\mathbf {LQ}^T\\mathbf Q \\mathbf Q^T = \\mathbf {LQ}^T = \\mathbf A$  \n",
    "\n",
    "so when we consider any satisfying $\\mathbf x$ where \n",
    "$\\mathbf {Ax} = \\mathbf b$ \n",
    "\n",
    "and we consider its squared legnth, we have, in effect via Pythagorean theorem    \n",
    "\n",
    "$\\Big \\Vert \\mathbf x\\Big \\Vert_2^2$  \n",
    "$= \\Big \\Vert\\big(\\mathbf I\\big)\\mathbf x\\Big \\Vert_2^2$  \n",
    "$= \\Big \\Vert\\Big(\\mathbf P +\\big( \\mathbf I - \\mathbf P\\big)\\Big)\\mathbf x\\Big \\Vert_2^2$  \n",
    "$= \\Big \\Vert\\mathbf {Px} +\\big( \\mathbf I - \\mathbf P\\big)\\mathbf x\\Big \\Vert_2^2$  \n",
    "$= \\Big \\Vert\\mathbf {Px}\\Big \\Vert_2^2 + \\Big \\Vert \\big( \\mathbf I - \\mathbf P\\big)\\mathbf x\\Big \\Vert_2^2$  \n",
    "$\\geq \\Big \\Vert\\mathbf {Px}\\Big \\Vert_2^2 $  \n",
    "\n",
    "with equality **iff**   \n",
    "$\\Big \\Vert \\big( \\mathbf I - \\mathbf P\\big)\\mathbf x\\Big \\Vert_2^2 = 0$ \n",
    "\n",
    "which we get by selecting our optimal $\\mathbf x^* := \\mathbf {Px}$, and of course  \n",
    "$\\Big \\Vert \\big( \\mathbf I - \\mathbf P\\big)\\mathbf x^*\\Big \\Vert_2^2= \\Big \\Vert \\big( \\mathbf I - \\mathbf P\\big)\\mathbf P \\mathbf x\\Big \\Vert_2^2 = \\Big \\Vert  \\mathbf {Px} - \\mathbf P^2 \\mathbf x \\Big \\Vert_2^2 = \\Big \\Vert  \\mathbf {Px} - \\mathbf P \\mathbf x \\Big \\Vert_2^2= 0$ \n",
    "\n",
    "so we are able to achieve this lower bound.  Furthermore, we know $\\mathbf {x}^*$ still satisfies the original equation because  \n",
    "\n",
    "$\\mathbf {Ax}^* $  \n",
    "$=\\mathbf {A}\\big(\\mathbf {Px}\\big)$  \n",
    "$=\\big(\\mathbf {AP}\\big)\\mathbf {x}$  \n",
    "$=\\big(\\mathbf {A}\\big)\\mathbf {x}$  \n",
    "$=\\mathbf {A}\\mathbf {x}$  \n",
    "$= \\mathbf b$  \n",
    "\n",
    "and of course with  \n",
    "$\\mathbf{R}^T\\mathbf Q^T\\mathbf x = \\mathbf{Ax} = \\mathbf b$  \n",
    "$\\mathbf Q^T\\mathbf x = \\big(\\mathbf{R}^T\\big)^{-1}\\mathbf b$  \n",
    "where we recall that $\\det\\big(\\mathbf R\\big) \\neq 0$   \n",
    "so the right hand side is uniquely specified by the problem at this point.  From here we multiply each side by $\\mathbf Q$, which is tall and skinny and full rank    \n",
    "\n",
    "$\\mathbf Q \\mathbf Q^T\\mathbf x = \\mathbf {Px} = \\mathbf x^* = \\mathbf Q\\big(\\mathbf{R}^T\\big)^{-1}\\mathbf b $  \n",
    " \n",
    "this is enough to confirm the uniqueness of $\\mathbf x^*$... \n",
    "\n",
    "suppose we have two distinct (non-optimized) solution vectors $\\mathbf x_1$ and $\\mathbf x_2$  \n",
    "\n",
    "Then  \n",
    "$\\mathbf A\\big(\\mathbf x_1 - \\mathbf x_2 \\big) = \\mathbf A\\mathbf x_1 - \\mathbf A \\mathbf x_2 = \\mathbf b - \\mathbf b = \\mathbf 0$  \n",
    "\n",
    "left multiplying each side by \n",
    "$\\big(\\mathbf R^T\\big)^{-1}$  gives  \n",
    "\n",
    "$\\mathbf Q^T \\big(\\mathbf x_1 - \\mathbf x_2 \\big) = \\mathbf 0$  \n",
    "and left multiplying each side by $\\mathbf Q$ gives  \n",
    "$\\mathbf Q \\mathbf Q^T \\big(\\mathbf x_1 - \\mathbf x_2 \\big) = \\mathbf P \\big(\\mathbf x_1 - \\mathbf x_2 \\big) = \\mathbf 0$  \n",
    "or  \n",
    "$\\mathbf P \\mathbf x_1 = \\mathbf P \\mathbf x_2 = \\mathbf x^*$  \n",
    "the contrapositive is if we think we find an even better minimal solution vector $\\mathbf x^{**}$ where \n",
    "$\\Big \\Vert\\mathbf {Px}\\Big \\Vert_2^2 \\gt \\Big \\Vert\\mathbf {x^{**}}\\Big \\Vert_2^2 $  \n",
    "then we know $\\mathbf {Px} \\neq \\mathbf x^{**}$ and in fact that $\\mathbf {A}\\mathbf x^{**} \\neq \\mathbf b$   \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For avoidance of doubt we have $\\mathbf A \\in \\mathbb R^{\\text{m x n}}$ where $m \\lt n$. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Radical streamlining of the above:*  \n",
    "The idea, at its core is that \n",
    "\n",
    "$\\mathbf A$ has rows that span a portion of the vector space that $\\mathbf x$ lives in.  Extend this to make a basis, and write all legal solutions $\\mathbf x$ as a linear combination of these vectors.  Then consider that $\\mathbf{Ax}$ is equivalent to our original solution, except each 'basis vector' not in the span of $\\mathbf A$'s rows.  With respect to minimizing a 2 norm, we can make this argument crisp if we first make our basis consist of mutually orthonormal vectors, and of course this is exactly what we did whether via QR factorization or SVD.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "consider invertible matrix $\\mathbf B$ given by  \n",
    "$\\mathbf B = \n",
    "\\bigg[\\begin{array}{c|c|c|c|c|c} \n",
    "\\mathbf a_1  &\\cdots & \\mathbf a_{m} & \\mathbf b_{m+1} & \\cdots & \\mathbf b_p \n",
    "\\end{array}\\bigg]$  \n",
    "\n",
    "$\\underbrace{\\{\\mathbf a_1,...,\\mathbf a_m\\}}_{\\text{m linearly independent columns}}$  \n",
    "$\\underbrace{\\{\\mathbf b_{m+1},...,\\mathbf b_p\\}}_{\\text{p - m linearly independent vectors from left nullspace}}$  \n",
    "\n",
    "i.e. \n",
    "$\\mathbf b_k^* \\mathbf A^*  = \\mathbf 0^*$  \n",
    "\n",
    "we know   \n",
    "$\\mathbf P \\mathbf B = \\bigg[\\begin{array}{c|c|c|c|c|c} \n",
    "\\mathbf P \\mathbf a_1  &\\cdots & \\mathbf P \\mathbf a_{m} & \\mathbf P \\mathbf b_{m+1} & \\cdots & \\mathbf P \\mathbf b_p \n",
    "\\end{array}\\bigg] = \\bigg[\\begin{array}{c|c|c|c|c|c} \n",
    "\\mathbf a_1  &\\cdots & \\mathbf a_{m} & \\mathbf 0 & \\cdots & \\mathbf 0 \n",
    "\\end{array}\\bigg]$  \n",
    "\n",
    "- - - - \n",
    "and if we find some other projector $\\mathbf S$ that has $\\mathbf {SA} = \\mathbf A$ where    \n",
    "\n",
    "$\\text{rank}\\big(\\mathbf P\\big)= \\text{rank}\\big(\\mathbf S\\big)$    \n",
    "(or what is equivalent $\\text{trace}\\big(\\mathbf P\\big)= \\text{trace}\\big(\\mathbf S\\big)$ )   \n",
    "\n",
    "then  \n",
    "$\\mathbf P = \\mathbf S$    \n",
    "\n",
    "- - - - \n",
    "First consider \n",
    "$\\mathbf B$  \n",
    "\n",
    "$\\text{rank}\\big(\\mathbf {PB}\\big)= \\text{rank}\\big(\\mathbf {SB}\\big)$  \n",
    "because multiplication by an inveritble matrix preserves rank  \n",
    "\n",
    "hence  \n",
    "$\\mathbf {SB} = \\bigg[\\begin{array}{c|c|c|c|c|c} \n",
    "\\mathbf a_1  &\\cdots & \\mathbf a_{m} & \\mathbf S \\mathbf b_{m+1} & \\cdots & \\mathbf S \\mathbf b_p \n",
    "\\end{array}\\bigg] = \\bigg[\\begin{array}{c|c|c|c|c|c} \n",
    "\\mathbf a_1  &\\cdots & \\mathbf a_{m} & \\sum_{i=1}^m c_i^{(m+1)} \\mathbf a_i & \\cdots & \\sum_{i=1}^m c_i^{(p)} \\mathbf a_i\n",
    "\\end{array}\\bigg]$  \n",
    "\n",
    "$= \\bigg[\\begin{array}{c|c|c|c|c|c} \n",
    "\\mathbf a_1  &\\cdots & \\mathbf a_{m} & \\mathbf 0 & \\cdots & \\mathbf 0 \n",
    "\\end{array}\\bigg]$  \n",
    "\n",
    "we prove the second line as follows:  \n",
    "\n",
    "a projector satisfies $\\mathbf S = \\mathbf S^2$ or equivalently is anhilated by  \n",
    "$\\mathbf S - \\mathbf S^2 = \\big(\\mathbf I - \\mathbf S\\big)\\mathbf S = \\mathbf 0$    \n",
    "\n",
    "hence  \n",
    "$\\big(\\mathbf I - \\mathbf S\\big)\\mathbf S\\mathbf B = \\mathbf 0$    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "we claim that for each $\\mathbf b_k$ we have  \n",
    "$\\mathbf S \\mathbf b_{k} = \\sum_{i=1}^m c_i^{(k)} \\mathbf a_i = \\mathbf 0$   \n",
    "(i.e. each coefficient $c_i^{(k)}$ since $\\mathbf a_i$ are linearly independen). \n",
    "\n",
    "\n",
    "This is most easily confirmed by running gramm schmidt on $\\{\\mathbf a_1,...,\\mathbf a_m\\}$ and decomposing into linear combinations mutually orthonormal vectors $\\{\\mathbf u_1,...,\\mathbf u_m\\}$ (note we may do this in triangular manner similar to QR decomposition -- which immediately implies $\\mathbf a_1 = \\mathbf u_1^* \\mathbf b_k = 0 $, and for larger $j$ \n",
    "\n",
    "$0 = \\mathbf a_j^*\\mathbf b_k = \\mathbf u_j^* \\mathbf b_k  + \\sum_{r =1 }^{j-1} \\mathbf u_r^* \\mathbf b_k  = \\mathbf u_j^* \\mathbf b_k $  \n",
    "\n",
    "where $\\mathbf u_r^* \\mathbf b_k = 0$ by induction hypothesis   \n",
    "\n",
    "\n",
    "So examine the above, decompose into $\\mathbf u_r$'s and left multiply by $\\mathbf u_j$\n",
    "\n",
    "We confirm this by left multiplying by any $\\mathbf u_j^*$ to see \n",
    "\n",
    "$0 = \\mathbf u_j^* \\mathbf b_{k}  = \\big(\\mathbf u_j^* \\mathbf S\\big) \\mathbf b_{k} = \\mathbf u_j^* \\big(\\mathbf S \\mathbf b_{k}\\big) =\\mathbf u_j^* \\sum_{i=1}^m c_i^{(k)}\\mathbf a_j  =\\mathbf u_j^* \\sum_{i=1}^m c_i^{(k)}\\sum_{t=1}^m \\gamma_t \\mathbf u_i = \\alpha_j \\big \\Vert\\mathbf u_j \\big \\Vert_2^2  + 0 = \\alpha_j$   \n",
    "\n",
    "where $\\alpha_j = c_i^{(k)} \\cdot \\gamma_t$ and $\\gamma_t \\neq 0\\longrightarrow c_i^{(k)} = 0$   \n",
    "\n",
    "\n",
    "where the right hand side follows (i.e. each coefficient must be zero) by orthgonality  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** additional look at projection brought to us by SVD**  \n",
    "In some sense this may be the cleanest approach, though it is overkill to read this *and* the above  \n",
    "\n",
    "note: remember that $\\mathbf A$ is assumed to be full row rank, which means it has column rank of $m$.  We want to have a linear combination of $\\mathbf A$'s column such that said combination is equal to $\\mathbf b\\in \\mathbb R^{\\text{m}}$.  That is, we know we can cleverly select from $\\mathbf A$'s columns to form a basis.  The goal, then is to select said basis in such a way that we can isolate the vectors which are in the (right) nullspace of $\\mathbf A$ and remove them from our solution.  We use orthogonality to get a 'clean look' at the various vectors being used to construct a solution and the ones that only contribute to the length of $\\mathbf x$ but do not contribute to the actual solution in $\\mathbf b$.  Given that we are using orthogonality for a non square matrix, a very natural question is: what are the implications of using SVD on $\\mathbf A$ in order to (attempt) to solve this problem.  \n",
    "\n",
    "- - - - \n",
    "Each row vector in $\\mathbf V^T $ must be $n$ dimensional, like $\\mathbf x$.  \n",
    "\n",
    "Let use consider the orthonormal basis given by the columns of $\\mathbf V$.  \n",
    "\n",
    "However, for our short fat $\\mathbf A$, we may choose to have $\\mathbf V_{\\text{non-square}}^T$ (i.e. short and fat), with the other two matrices in the SVD being square, as given below  \n",
    "\n",
    "$\\mathbf A = \\mathbf U \\mathbf \\Sigma_{\\text{square}}\\mathbf V_{\\text{non-square}}^T$  \n",
    "\n",
    "- - - - -  \n",
    "*note on dimensions*  \n",
    "i.e. $\\mathbf V_{\\text{non-square}} \\in \\mathbb R^{n x m}$  (i.e. the column vectors are n dimensional but there are only $m \\lt n$ of them)   \n",
    "\n",
    "equivalently, \n",
    "\n",
    "$\\mathbf V_{\\text{non-square}} \\bigg[\\begin{array}{c|c|c|c}\n",
    "\\mathbf v_1 & \\mathbf v_2 &\\cdots & \\mathbf v_{m}\n",
    "\\end{array}\\bigg]$  \n",
    "\n",
    "where $m \\lt n$  \n",
    "- - - - -  \n",
    "*The key insight:*  we know that we can solve for a satisfying $\\mathbf x$ given $\\mathbf A$, $\\mathbf b$, exactly -- why? Because the columns of $\\mathbf A$ form a basis (this is implied by full row rank).  However there are just too many columns relative to the dimension so at least one column is linearly dependent. But since we have a basis in $\\mathbf A$'s columns, why not choose a smart basis and write any satisfying $\\mathbf x$ in terms of it?  \n",
    "\n",
    "Furthermore, because  $\\mathbf U \\mathbf \\Sigma_{\\text{square}}$ are both invertible we can simplify the results with a change of variables and assume WLOG that we are solving for a linear transofmration of $\\mathbf x$ that gives $\\mathbf c$ for some $\\mathbf c \\neq 0$ where $\\mathbf c: = \\big(\\mathbf U \\mathbf \\Sigma_{\\text{square}}\\big)^{-1}\\mathbf b$.  \n",
    "\n",
    "But this tells us the the smart basis to write our solution in terms of is $\\mathbf V_{\\text{non-square}}$ -- except first well extend this to an orthonormal basis, by including $\\mathbf v_{m+1}, \\mathbf v_{m+2}, ..., \\mathbf v_{n}$  \n",
    "\n",
    "- - - - -   \n",
    "\n",
    "So we write $\\mathbf x$ as a linear combination of $\\mathbf v_k$'s     \n",
    "\n",
    "$\\mathbf x = \\mathbf{Vy} = y_1\\mathbf v_1 + y_2\\mathbf v_2 + ... + y_m\\mathbf v_m +  y_{m+1}\\mathbf v_{m+1} + ... + y_{n}\\mathbf v_{n}$\n",
    "\n",
    "but this makes our problem:  \n",
    "\n",
    "$\\mathbf V_{\\text{non-square}}^T  \\mathbf x  = \\mathbf V_{\\text{non-square}}^T\\mathbf{Vy}= \\mathbf c_{\\in \\mathbb R^m}$  \n",
    "\n",
    "\n",
    "now, we know \n",
    "\n",
    "$\\mathbf V_{\\text{non-square}}^T\\big( y_{m+1}\\mathbf v_{m+1} + ... + y_{n}\\mathbf v_{n}\\big) = \\mathbf A\\big( y_{m+1}\\mathbf v_{m+1} + ... + y_{n}\\mathbf v_{n}\\big) = \\mathbf 0_{\\in \\mathbb R^m} $  \n",
    "\n",
    "because  \n",
    "\n",
    "$\\mathbf V_{\\text{non-square}}^T \\mathbf v_j = \\mathbf 0$  for $j \\gt m$   \n",
    "\n",
    "but we also know \n",
    "\n",
    "$\\mathbf V_{\\text{non-square}}^T\\big( y_{1}\\mathbf v_{1} + y_{2}\\mathbf v_{2} + ... + y_{m}\\mathbf v_{m}\\big) =  y_{1}\\mathbf e_{1\\in \\mathbb R^m} + y_{2}\\mathbf e_{2\\in \\mathbb R^m} + ... + y_m\\mathbf e_{m\\in \\mathbb R^m} = \\begin{bmatrix}\n",
    "y_1\\\\ \n",
    "y_2\\\\ \n",
    "\\vdots \\\\ \n",
    "y_m\\\\\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "c_1\\\\ \n",
    "c_2\\\\ \n",
    "\\vdots \\\\ \n",
    "c_m\\\\\n",
    "\\end{bmatrix} = \\mathbf c $  \n",
    "\n",
    "\n",
    "hence each $y_k$ for $1 \\leq k \\leq m$ is uniquely specified by $c_k$.  \n",
    "\n",
    "equivalently, we know, that for any given $\\mathbf c$, the $\\mathbf x$ must at least include the linear combination of \n",
    "\n",
    "$\\mathbf x = \\mathbf{Vy} = y_1\\mathbf v_1 + y_2\\mathbf v_2 + ... + y_m\\mathbf v_m  =  c_1\\mathbf v_1 + c_2\\mathbf v_2 + ... + c_m\\mathbf v_m = \\sum_{k=1}^m c_k \\mathbf v_k = \\mathbf V_{\\text{non-square}}\\mathbf{c}$\n",
    "\n",
    "If any proprosed solution does not include the above linear combination of $\\mathbf V_{\\text{non-square}}\\mathbf{c}$ for any $\\mathbf c \\neq \\mathbf 0$ then it is not an accurate solution.  This solution has length (squared 2 norm) given by $\\big \\Vert \\mathbf V_{\\text{non-square}}\\mathbf{c}\\big \\Vert_2^2 = \\big \\Vert \\mathbf{c}\\big \\Vert_2^2 = \\sum_{k=1}^m c_k^2 = \\sum_{k=1}^m y_k^2 $.  Taking advantage of orthogonality, we can see that including any additional $y_j$  for $m \\lt j \\leq n$  does not change the configuration of $y_k$ but does necessarily increases the length of the solution (by positive definiteness of the 2 norm).   Hence we deem $\\mathbf V_{\\text{non-square}}\\mathbf{c}$ as necessarily the valid and minimal length solution.  \n",
    "\n",
    "- - - - \n",
    "\n",
    "\n",
    "This gives a nice interpretation to our projection matrix $\\mathbf P$.  We have \n",
    "\n",
    "\n",
    "$\\mathbf P\\mathbf x^{(0)} =  \\mathbf V \\begin{bmatrix}\n",
    "\\mathbf I_m & \\mathbf{00}^T \\\\ \n",
    "\\mathbf {00}^T & \\mathbf {00}^T  \\\\ \n",
    "\\end{bmatrix} \\mathbf V^T \\mathbf x^{(0)} = \\mathbf V \\big(\\mathbf {DV}^T \\mathbf x^{(0)}\\big) = \\mathbf V \\big(\\mathbf D\\mathbf y^{(0)}\\big) = \\mathbf V \\big(\\begin{bmatrix}\n",
    "\\mathbf I_m & \\mathbf{00}^T \\\\ \n",
    "\\mathbf {00}^T & \\mathbf {00}^T  \\\\ \n",
    "\\end{bmatrix}\\begin{bmatrix}\n",
    "y_1\\\\ \n",
    "y_2\\\\ \n",
    "\\vdots \\\\ \n",
    "y_m\\\\\n",
    "y_{m+1}\\\\\n",
    "\\vdots\\\\\n",
    "y_n\n",
    "\\end{bmatrix}\\big) = \\mathbf V \\begin{bmatrix}\n",
    "y_1\\\\ \n",
    "y_2\\\\ \n",
    "\\vdots \\\\ \n",
    "y_m\\\\\n",
    "0\\\\\n",
    "\\vdots\\\\\n",
    "0\n",
    "\\end{bmatrix} = \\sum_{k=1}^m y_m \\mathbf v_k = \\mathbf x_{\\text{optimal}}$  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "That is we know there is only one $\\mathbf x$ that will solve for any $\\mathbf {Ax} = \\mathbf b \\neq \\mathbf 0$, so long as we confine ourselves to writing $\\mathbf x$ as a linear combination of $\\mathbf v_k$'s for $1 \\leq k \\leq n$.  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# alternative approach using Lagrange multipliers\n",
    "\n",
    "to derive the minimum cost (L2 norm) solution\n",
    "\n",
    "- - - -\n",
    "\n",
    "In this case, let \n",
    "\n",
    "$\\mathbf d  =  \\begin{bmatrix}\n",
    "\\lambda_1\\\\ \n",
    "\\lambda_2\\\\ \n",
    "\\vdots \\\\ \n",
    "\\lambda_m\n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "i.e. $\\mathbf d$ is a vector containing the Lagrange multipliers\n",
    "\n",
    "we setup the Lagrangian we want to minimize as\n",
    "\n",
    "$L(\\mathbf x) = \\mathbf x^T \\mathbf x + \\mathbf d^T  \\big(\\mathbf{Ax} - \\mathbf b \\big)$\n",
    "\n",
    "$\\nabla_{\\mathbf x} = 2 \\mathbf x + \\mathbf A^T \\mathbf d := \\mathbf 0$\n",
    "\n",
    "Note: check dimensions to see why it is not $\\mathbf d^T \\mathbf A$\n",
    "\n",
    "$\\nabla_{\\mathbf d } = \\mathbf{Ax} - \\mathbf b := \\mathbf 0$\n",
    "\n",
    "solving $\\nabla_{\\mathbf x}$ first, we see:\n",
    "\n",
    "$\\mathbf x = \\frac{-1}{2} \\mathbf{A}^T \\mathbf d$\n",
    "\n",
    "now substitute into the second equations $\\nabla_{\\mathbf d }$, we see\n",
    "\n",
    "$\\mathbf{A}\\big(\\frac{-1}{2} \\mathbf{A}^T \\mathbf d \\big) - \\mathbf b := \\mathbf 0$\n",
    "\n",
    "hence $\\mathbf d = -2\\big(\\mathbf{AA}^T\\big)^{-1} \\mathbf b$\n",
    "\n",
    "and here we plug this back into $\\nabla_{\\mathbf x}$  \n",
    "\n",
    "$\\mathbf 0 = 2 \\mathbf x + \\mathbf A^T \\mathbf d  = 2 \\mathbf x + \\mathbf A^T \\Big(-2\\big(\\mathbf{AA}^T\\big)^{-1} \\mathbf b\\Big)$\n",
    "\n",
    "\n",
    "$-2 \\mathbf x = -2\\mathbf A^T \\big(\\mathbf{AA}^T\\big)^{-1} \\mathbf b$ \n",
    "\n",
    "or \n",
    "\n",
    "$\\mathbf x = \\mathbf A^T \\big(\\mathbf{AA}^T\\big)^{-1} \\mathbf b $\n",
    "\n",
    "and of course, if we used the SVD on $\\mathbf A$, we'd see\n",
    "\n",
    "$\\mathbf x = \\mathbf{V \\Sigma}^T \\mathbf U^T \\big(\\mathbf{U \\Sigma \\Sigma }^T \\mathbf U^T\\big)^{-1} \\mathbf b = \\mathbf{V \\Sigma}^T \\mathbf U^T \\mathbf{U \\big(\\Sigma \\Sigma }^T\\big)^{-1} \\mathbf U^T \\mathbf b = \\mathbf{V \\Sigma}^T \\big(\\mathbf{\\Sigma \\Sigma }^T\\big)^{-1} \\mathbf U^T \\mathbf b = \\mathbf{V \\Sigma}^{-1} \\mathbf U^T \\mathbf b $\n",
    "\n",
    "where we recover the equation from the earlier approach, and as before \n",
    "\n",
    "where $\\mathbf \\Sigma^{-1} = \\begin{bmatrix}\n",
    "\\frac{1}{\\sigma_1} & 0 &0  &0 &  ... &0 \\\\ \n",
    "0 & \\frac{1}{\\sigma_2}& 0 & 0& ... &0\\\\ \n",
    "0 & 0 &  \\ddots & 0& ... &0 \\\\ \n",
    "0 & 0 & 0 & \\frac{1}{\\sigma_m}& ... &0  \n",
    "\\end{bmatrix}^T$, \n",
    "\n",
    "which is to say that $\\mathbf \\Sigma^{-1}$ is the right inverse of the $\\mathbf \\Sigma$ matrix\n",
    "\n",
    "- - - - \n",
    "\n",
    "it is also, of course, easy to verify that the proposed solution is valid (ignoring cost) without knowing anything about Lagrange Multipliers.  I.e. we can verify \n",
    "\n",
    "$\\mathbf A\\Big(\\mathbf x\\Big) = \\mathbf A\\Big(\\mathbf A^T \\big(\\mathbf{AA}^T\\big)^{-1} \\mathbf b\\Big) = \\big(\\mathbf A\\mathbf A^T\\big) \\big(\\mathbf{AA}^T\\big)^{-1} \\mathbf b = \\mathbf b$    \n",
    "\n",
    "and hence this choice of $\\mathbf x$ is valid  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "note that converting the final equation to the SVD equivalent one may be easier to intperet with the decomposition\n",
    "\n",
    "$\\mathbf A = \\mathbf U \\mathbf \\Sigma_{\\text{square}}\\mathbf V_{\\text{non-square}}^T$  \n",
    "\n",
    "in which case we'd have \n",
    "\n",
    "\n",
    "$\\mathbf x = \\mathbf A^T \\big(\\mathbf{AA}^T\\big)^{-1} \\mathbf b = \\big(\\mathbf U \\mathbf \\Sigma_{\\text{square}}\\mathbf V_{\\text{non-square}}^T\\big)^T \\Big(\\mathbf U \\mathbf \\Sigma_{\\text{square}}\\mathbf V_{\\text{non-square}}^T \\big(\\mathbf U \\mathbf \\Sigma_{\\text{square}}\\mathbf V_{\\text{non-square}}^T\\big)^T\\Big)^{-1}\\mathbf b $    \n",
    "\n",
    "$= \\big( \\mathbf V_{\\text{non-square}} \\mathbf \\Sigma_{\\text{square}} \\mathbf U^T\\big) \\Big(\\mathbf U \\mathbf \\Sigma_{\\text{square}}\\mathbf V_{\\text{non-square}}^T \\mathbf V_{\\text{non-square}} \\mathbf \\Sigma_{\\text{square}}\\mathbf U^T \\Big)^{-1}\\mathbf  = \\big( \\mathbf V_{\\text{non-square}} \\mathbf \\Sigma_{\\text{square}} \\mathbf U^T\\big) \\Big(\\mathbf U \\mathbf \\Sigma_{\\text{square}}^2 \\mathbf U^T \\Big)^{-1}\\mathbf b $    \n",
    "\n",
    "$ = \\big( \\mathbf V_{\\text{non-square}} \\mathbf \\Sigma_{\\text{square}} \\mathbf U^T\\big)\\big(\\mathbf U \\mathbf \\Sigma_{\\text{square}}^{-2} \\mathbf U^T\\big) \\mathbf b  = \\mathbf V_{\\text{non-square}} \\mathbf \\Sigma_{\\text{square}}^{-1} \\mathbf U^T \\mathbf b $  \n",
    "\n",
    "\n",
    "and a quick associativity check tells us \n",
    "\n",
    "$\\mathbf{V \\Sigma}^{-1} = \\mathbf V_{\\text{non-square}} \\mathbf \\Sigma_{\\text{square}}^{-1}$   \n",
    "\n",
    "hence this solution agrees with the earlier one \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
