{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Proof \n",
    "\n",
    "For simplicity the field will be reals, though the results can be easily extended to complex numbers.  \n",
    "\n",
    "Note: I was originally looking for a different approach to proving Cauchy-Schwartz, using something simple and powerful with the trace operation, but I somehow ended up using eigenvalues and the Schur Inequality... which is a lot heavier duty machinery than I had wanted... sigh...\n",
    "\n",
    "consider some vector $\\mathbf x =  \\begin{bmatrix}\n",
    "x_1\\\\ \n",
    "x_2\\\\ \n",
    "\\vdots \\\\ \n",
    "x_n\n",
    "\\end{bmatrix}$\n",
    "\n",
    "and some other vector,  $\\mathbf y =  \\begin{bmatrix}\n",
    "y_1\\\\ \n",
    "y_2\\\\ \n",
    "\\vdots \\\\ \n",
    "y_n\n",
    "\\end{bmatrix}$\n",
    "\n",
    "now consider the outer product given by \n",
    "\n",
    "$\\mathbf {xy}^T =  \\begin{bmatrix}\n",
    "x_1 y_1 & x_1 y_2 &...& x_1 y_n\\\\ \n",
    "x_2 y_1 & x_2 y_2 & ... & x_2 y_n \\\\ \n",
    "\\vdots & \\vdots & \\ddots &\\vdots \\\\ \n",
    "x_n y_1 & x_n y_2 &.... & x_n y_n\n",
    "\\end{bmatrix}$\n",
    "\n",
    "\n",
    "From here, consider where we want the squared Frobenius norm of $\\mathbf {xy}^T$ \n",
    "\n",
    "$\\big\\vert\\big\\vert \\mathbf {xy}^T\\big\\vert\\big\\vert_{F}^2 = trace\\Big(\\big(\\mathbf {xy}^T\\big)^T \\big( \\mathbf {xy}^T\\big)\\Big) = trace\\Big( \\mathbf {yx}^T \\mathbf {xy}^T\\Big) = trace\\Big( \\mathbf{y}^T \\mathbf{yx}^T \\mathbf {x}\\Big) = \\mathbf {y}^T\\mathbf y \\mathbf x^T \\mathbf {x}  = \\big\\vert\\big\\vert \\mathbf{y}\\big\\vert\\big\\vert_{2}^2\\big\\vert\\big\\vert \\mathbf {x}\\big\\vert\\big\\vert_{2}^2$\n",
    "\n",
    "where the middle equalities made use of the cyclic property of the trace\n",
    "\n",
    "Unfortunately, this seems to be going off in the deep end.  We conclude the proof with the following:\n",
    "\n",
    "$ \\big\\vert\\big\\vert \\mathbf{y}\\big\\vert\\big\\vert_{2}^2\\big\\vert\\big\\vert \\mathbf {x}\\big\\vert\\big\\vert_{2}^2 = trace\\Big(\\big(\\mathbf {\\mathbf {xy}}^T\\big)^T \\big( \\mathbf {\\mathbf {xy}}^T\\big)\\Big) \n",
    "\\geq trace\\Big( \\big(\\mathbf {xy}^T\\big) \\big(\\mathbf {xy}^T\\big)\\Big) =  trace\\Big( \\mathbf y^T \\mathbf {xy}^T\\mathbf {x}\\Big) = trace\\Big( \\big(\\mathbf y^T \\mathbf {x}\\big) \\big(\\mathbf y^T \\mathbf {x}\\big)\\Big) = \\big(\\mathbf y^T \\mathbf x\\big)^2$\n",
    "\n",
    "hence \n",
    "\n",
    "$\\big\\vert\\big\\vert \\mathbf{y}\\big\\vert\\big\\vert_{2}^2\\big\\vert\\big\\vert \\mathbf {x}\\big\\vert\\big\\vert_{2}^2 = \\Big(\\Sigma_{i=1}^{n}y_i^2\\Big) \\Big(\\Sigma_{i=1}^{n}x_i^2\\Big)  \\geq \\Big(\\Sigma_{i=1}^{n}x_iy_i\\Big)^2= \\big(\\mathbf y^T \\mathbf x\\big)^2 $\n",
    "\n",
    "\n",
    "noting that \n",
    "\n",
    "$trace\\Big(\\big(\\mathbf {\\mathbf {xy}}^T\\big)^T \\big( \\mathbf {\\mathbf {xy}}^T\\big)\\Big)\n",
    "\\geq trace\\Big( \\big(\\mathbf {xy}^T\\big) \\big(\\mathbf {xy}^T\\big)\\Big) $\n",
    "\n",
    "via the Schur Inequality.  (Also see the cell below -- we can ignore Schur Inequality and directly interpret this in terms of maximal eigenvalues.) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alternative Approach that does not use Schur Inequality:\n",
    "\n",
    "note that instead of using the Schur Inequality (which strictly speaking should have had an absolute value / magnitude included in there),  consider the maximum eigenvalue problem, where we have a rank one matrix, $\\mathbf B$, this time over a complex numbers field\n",
    "\n",
    "\n",
    "$\\mathbf B = \\mathbf{xy}^H$.  While we know that $\\mathbf B$ is a rank one matrix, the argument to be made is even more general: the magnitude of the largest eigenvalue of $\\big(\\mathbf {BB}\\big)$ is $\\leq$ the largest eigenvalue of $\\mathbf B^H \\mathbf B$, or equivalently, the magnitude of the largest eigenvalue of $\\mathbf B$ ($\\lambda_1$) is $\\leq$ the largest singular value of $\\mathbf B$ ($\\sigma_1$).\n",
    "\n",
    "The approach taken here uses quadratic forms.  So consider the case of maximizing $\\mathbf B^H \\mathbf B$ vs $\\mathbf {BB}$\n",
    "\n",
    "max $\\big \\vert \\mathbf v^H \\mathbf B^H \\mathbf B \\mathbf v \\big \\vert \\geq$ max $\\big \\vert \\mathbf v^H \\mathbf{BB}\\mathbf v \\big \\vert$\n",
    "\n",
    "where we constrain the length (2 norm) of $\\mathbf v$, which for simplicity will be one: $\\mathbf v^H \\mathbf v = 1 = \\big \\vert \\big \\vert \\mathbf v \\big \\vert \\big \\vert_2^{2} = \\big \\vert \\big \\vert \\mathbf v \\big \\vert \\big \\vert_2$ \n",
    "\n",
    "We know via diagonalization arguments (and Lagrange Multipliers), that some quadratic form $\\big \\vert \\mathbf v^H \\mathbf C \\mathbf v\\big \\vert$ subject to $\\mathbf v ^H \\mathbf v = 1$ is maximized when all of $\\mathbf v$ is allocated to the eigenvalue(s) with the largest magnitude of the Hermitian matrix $\\mathbf C$.  Unfortunately, we have no reason to believe $\\mathbf {BB}$ is Hermitian or even non-defective, which complicates things a bit.  But consider having well ordered eigenvalues for $\\mathbf B$ where $\\big \\vert \\lambda_1 \\big \\vert \\geq \\big \\vert\\lambda_2 \\big \\vert\\geq \\big \\vert \\lambda_3 \\big \\vert \\geq ... \\geq \\big \\vert\\lambda_n \\big \\vert$, with associated eigenvectors $\\mathbf v_1, \\mathbf v_2, \\mathbf v_3, ... , \\mathbf v_n$.  Note that there is a simple argument which tells us that allocating to eigenvector $\\mathbf v_k$ where $k \\geq 2$ is (weakly) dominated by $\\mathbf v_1$.  \n",
    "\n",
    "$\\big \\vert \\mathbf v_1^H \\mathbf{BB}\\mathbf v_1 \\big \\vert \\geq \\big \\vert \\mathbf v_k^H \\mathbf{BB}\\mathbf v_k \\big \\vert$\n",
    "\n",
    "$\\big \\vert \\mathbf v_1^H \\mathbf{B}\\big(\\mathbf B \\mathbf v_1\\big) \\big \\vert \\geq \\big \\vert \\mathbf v_k^H \\mathbf{B}\\big( \\mathbf B \\mathbf v_k \\big) \\big \\vert$\n",
    "\n",
    "$\\big \\vert \\mathbf v_1^H \\mathbf{B} \\lambda_1 \\mathbf v_1 \\big \\vert \\geq \\big \\vert\\mathbf v_k^H \\mathbf{B}\\lambda_k \\mathbf v_k \\big \\vert$\n",
    "\n",
    "$\\big \\vert \\lambda_1 \\mathbf v_1^H \\big(\\mathbf{B} \\mathbf v_1\\big) \\big \\vert \\geq \\big \\vert \\lambda_k \\mathbf v_k^H \\big(\\mathbf{B} \\mathbf v_k\\big) \\big \\vert$\n",
    "\n",
    "$\\big \\vert \\lambda_1 \\mathbf v_1^H \\lambda_1 \\mathbf v_1 \\big \\vert \\geq \\big \\vert \\lambda_k \\mathbf v_k^H \\lambda_k \\mathbf v_k \\big \\vert$\n",
    "\n",
    "$\\big \\vert \\lambda_1^2 (\\mathbf v_1^H \\mathbf v_1) \\big \\vert \\geq \\big \\vert \\lambda_k^2 (\\mathbf v_k^H \\mathbf v_k) \\big \\vert$\n",
    "\n",
    "$\\big \\vert \\lambda_1^2 *1\\big \\vert \\geq \\big \\vert \\lambda_k^2 *1 \\big \\vert$  \n",
    "$\\big \\vert \\lambda_1^2\\big \\vert \\geq \\big \\vert\\lambda_k^2 \\big \\vert$  \n",
    "$\\big \\vert \\lambda_1\\big \\vert^2 \\geq \\big \\vert\\lambda_k \\big \\vert^2$  \n",
    "$\\big \\vert \\lambda_1\\big \\vert \\geq \\big \\vert\\lambda_k \\big \\vert$   \n",
    "\n",
    "hence we have a simple exchange argument that tells us any time we allocate to $\\mathbf v_k$ we can get a result greater than or equal to it, by allocating that amount instead to $\\mathbf v_1$.  Thus we can do no better than allocating everything in $\\mathbf v$ to the eigenpair $\\lambda_1, \\mathbf v_1$\n",
    "\n",
    "We return to our original equation, with respect to eigenvalues:\n",
    "\n",
    "max $\\big \\vert  \\mathbf v^H \\mathbf B^H \\mathbf B \\mathbf v \\big \\vert \\geq$ max $\\big \\vert \\mathbf v_1^H \\mathbf{BB}\\mathbf v_1 \\big \\vert$\n",
    "\n",
    "max $\\big \\vert \\mathbf v^H \\mathbf B^H \\mathbf B \\mathbf v \\big \\vert = $ max $ \\mathbf v^H \\mathbf B^H \\mathbf B \\mathbf v  \\geq \\big \\vert \\lambda_1^2\\big \\vert $\n",
    "\n",
    "note that we always have the option / backup plan, on the left hand side, of also allocating to $\\mathbf v_1$.  Put differently, if we are lazy, we know that by setting $\\mathbf v := \\mathbf v_1$ we'll always get a 'payoff' with magnitude equal to $\\big\\vert\\lambda_1^2\\big\\vert$ -- thus when maximizing the magnitude of $\\big \\vert \\mathbf v^H \\mathbf B^H \\mathbf B \\mathbf v \\big \\vert$ we'll get a result at least as good as $\\big\\vert\\lambda_1^2\\big\\vert$.  Symbolically, this is shown below.\n",
    "\n",
    "$\\mathbf v_1^H \\mathbf B^H \\mathbf B \\mathbf v_1 = \\big \\vert \\lambda_1^2\\big \\vert $  \n",
    "\n",
    "$\\big(\\mathbf v_1^H \\mathbf B^H\\big) \\big(\\mathbf B \\mathbf v_1 \\big) = \\big \\vert \\lambda_1^2\\big \\vert $  \n",
    "\n",
    "$\\big(\\mathbf {B}\\mathbf v_1\\big)^H \\big(\\mathbf B \\mathbf v_1 \\big) = \\big \\vert \\lambda_1^2 \\big \\vert $  \n",
    "$\\big(\\lambda_1 \\mathbf v_1\\big)^H \\big(\\lambda_1 \\mathbf v_1 \\big) = \\big \\vert \\lambda_1^2 \\big \\vert $  \n",
    "$\\lambda_1^H \\lambda_1 \\big(\\mathbf v_1^H \\mathbf v_1\\big) = \\big \\vert \\lambda_1^2 \\big \\vert $  \n",
    "$\\lambda_1^H \\lambda_1 *1  = \\big \\vert \\lambda_1^2\\big \\vert $    \n",
    "$\\lambda_1^H \\lambda_1  = \\big \\vert \\lambda_1^2 \\big \\vert $  \n",
    "\n",
    "Where the final statement is true by the math of complex numbers. A quick detour justifying this is shown below.\n",
    "\n",
    "- - - - \n",
    "To justify this, consider that we can simply note that $\\big \\vert \\lambda_1^2\\big \\vert = \\big \\vert \\lambda_1 \\big \\vert^2 = \\lambda_1^H \\lambda_1$.\n",
    "\n",
    "Alternatively, for a more granular view, consider the case where:  \n",
    "$\\lambda_1 = \\alpha - \\beta i $, where $\\alpha$ and $\\beta$ are real valued scalars. Accordingly, the magnitude of $\\lambda_1$ is $\\big\\vert \\lambda_1\\big\\vert = \\big(\\alpha^2 + \\beta^2\\big)^\\frac{1}{2}$. Then $\\lambda_1^2 = \\alpha^2 + \\beta^2  i^2 - 2\\alpha\\beta i = \\alpha^2 - \\beta^2 - 2\\alpha\\beta i$, with magnitude of \n",
    "\n",
    "$\\big \\vert \\lambda_1^2\\big \\vert = \\Big(\\big(\\alpha^2 - \\beta^2\\big)^2 + \\big( - 2\\alpha\\beta\\big)^2\\Big)^{\\frac{1}{2}}= \\Big(\\alpha^4 + \\beta^4 - 2 \\alpha^2 \\beta^2 + 4 \\alpha^2\\beta^2)\\Big)^{\\frac{1}{2}} $ \n",
    "\n",
    "$\\big \\vert \\lambda_1^2\\big \\vert = \\Big(\\alpha^4 + \\beta^4 + 2 \\alpha^2 \\beta^2 \\Big)^{\\frac{1}{2}}$\n",
    "\n",
    "and note that $\\lambda_1 ^H \\lambda_1 = \\alpha^2 + \\beta^2$, with magnitude equal to   \n",
    "\n",
    "$\\big \\vert \\lambda_1^H \\lambda_1 \\big \\vert = \\Big(\\big(\\alpha^2 + \\beta^2\\big)^2 + \\big(0\\big)^2 \\Big)^{\\frac{1}{2}} = \\Big(\\alpha^4 + \\beta^4 + 2 \\alpha \\beta \\Big)^{\\frac{1}{2}} = \\big \\vert \\lambda_1^2\\big \\vert  $\n",
    "\n",
    "$ \\big \\vert \\lambda_1^H \\lambda_1 \\big \\vert = \\lambda_1 ^H \\lambda_1 = \\alpha^2 + \\beta^2  = \\Big(\\big(\\alpha^2 + \\beta^2\\big)^\\frac{1}{2}\\Big)^2 = \\big\\vert \\lambda_1\\big\\vert ^2$\n",
    "- - - - \n",
    "\n",
    "Thus when trying to maximize the magnitude of $\\mathbf v^H \\mathbf B^H \\mathbf B \\mathbf v$, we have a lower bound equal to $ \\big \\vert \\lambda_1^2 \\big\\vert$.  Equivalently, we can say: $\\sigma_1^2 \\geq \\big \\vert \\lambda_1^2 \\big\\vert$ and $\\sigma_1 \\geq \\big \\vert \\lambda_1 \\big\\vert$, where $\\sigma_1$ is the largest singular value of $\\mathbf B$ and thus $\\sigma_1^2$ is the largest eigenvalue of $\\mathbf B^H \\mathbf B$\n",
    "\n",
    "For a simple example of this fact, consider:\n",
    "\n",
    "$\\mathbf B = \\left[\\begin{matrix}2 & 3 & 4\\\\4 & 10 & -1\\\\1 & 3 & 4\\end{matrix}\\right]$\n",
    "\n",
    "where $\\lambda_1 \\approx 11.57$, but $\\sigma_1 \\approx 11.87$, hence $\\sigma_1$ exceeds the lower bound set by $\\lambda_1$ \n",
    "- - - -\n",
    "Back to the original problem at hand, in the special case where $\\mathbf B$ is a square, rank one matrix  \n",
    "\n",
    "$trace \\big(\\mathbf B ^H \\mathbf B\\big) \\geq \\big \\vert trace\\big(\\mathbf{BB}\\big) \\big \\vert$, because  \n",
    "\n",
    "$trace \\big(\\mathbf B ^H \\mathbf B\\big) = \\sigma_1^2$ and $trace\\big(\\mathbf{BB}\\big) = \\lambda_1^2 $, and we know $\\sigma_1^2 \\geq \\big \\vert \\lambda_1^2 \\big\\vert$\n",
    "\n",
    "set: $\\mathbf B:= \\mathbf{xy}^H$ and Cauchy Schwartz simply follows\n",
    "\n",
    "$ \\big\\vert\\big\\vert \\mathbf{y}\\big\\vert\\big\\vert_{2}^2\\big\\vert\\big\\vert \\mathbf {x}\\big\\vert\\big\\vert_{2}^2 = \\big ( \\mathbf y^H \\mathbf y \\big ) \\big ( \\mathbf x^H \\mathbf x \\big ) = trace\\Big(\\big(\\mathbf {\\mathbf {xy}}^H\\big)^H \\big( \\mathbf {\\mathbf {xy}}^H\\big)\\Big) \n",
    "\\geq \\big \\vert trace\\Big( \\big(\\mathbf {xy}^H\\big) \\big(\\mathbf {xy}^H\\big)\\Big) \\big \\vert =  \\big \\vert trace\\Big( \\mathbf y^H \\mathbf {xy}^H\\mathbf {x}\\Big) \\big \\vert = \\big \\vert \\mathbf y^H \\mathbf x \\big \\vert^2 $\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Smallest Eigenvalues and Singular Values\n",
    "\n",
    "Note that the above analysis can be easily extended with respect to the magnitude of the smallest eigenvalue of $\\mathbf B$ and the smallest singular value of $\\mathbf B$, again where $\\mathbf B \\in \\mathbb C^{n x n}$.\n",
    "\n",
    "for $k \\lt n$\n",
    "\n",
    "(i.e. $\\big \\vert \\lambda_k \\big \\vert \\geq \\big \\vert \\lambda_n \\big \\vert$)\n",
    "\n",
    "thus if we want to minimize $\\big(\\mathbf v_1^H \\mathbf B^H\\big) \\big(\\mathbf B \\mathbf v_1 \\big) $ we always have the option of allocating to $\\lambda_n$\n",
    "\n",
    "$\\mathbf v_n^H \\mathbf B^H \\mathbf B \\mathbf v_n = \\big \\vert \\lambda_n^2\\big \\vert $  \n",
    "\n",
    "$\\big(\\mathbf v_n^H \\mathbf B^H\\big) \\big(\\mathbf B \\mathbf v_n \\big) = \\big \\vert \\lambda_n^2\\big \\vert $  \n",
    "\n",
    "$\\big(\\mathbf {B}\\mathbf v_n\\big)^H \\big(\\mathbf B \\mathbf v_n \\big) = \\big \\vert \\lambda_n^2 \\big \\vert $  \n",
    "$\\big(\\lambda_n \\mathbf v_n\\big)^H \\big(\\lambda_n \\mathbf v_1 \\big) = \\big \\vert \\lambda_n^2 \\big \\vert $  \n",
    "$\\lambda_n^H \\lambda_n \\big(\\mathbf v_1^H \\mathbf v_1\\big) = \\big \\vert \\lambda_n^2 \\big \\vert $  \n",
    "$\\lambda_n^H \\lambda_n *1  = \\big \\vert \\lambda_n^2\\big \\vert $    \n",
    "$\\lambda_n^H \\lambda_n  = \\big \\vert \\lambda_n \\big \\vert^2 $  \n",
    "\n",
    "Since allocating everything to $\\sigma_n^2$ is the (weakly) dominant solution for minimizing $\\mathbf v^H \\mathbf B^H \\mathbf B \\mathbf v$, we can upper bound $\\sigma_n^2 \\leq \\big\\vert \\lambda_n\\big \\vert^2 $ and equivalently say that $\\sigma_n \\leq \\big\\vert \\lambda_n\\big \\vert $\n",
    "\n",
    "# Thoughts on Unitary Matrices\n",
    "Note that there is a special case of interest.  Suppose that we have a square unitary matrix $\\mathbf Q$. We know that all eigenvalues of $\\mathbf Q $ have magnitude of 1.  Why? There are multiple approaches, but an elegant one uses the above knowledge with the singular value decomposition:\n",
    "\n",
    "\n",
    "$\\mathbf Q = \\mathbf{U \\Sigma V}^H$\n",
    "\n",
    "$\\mathbf Q^H \\mathbf Q  = \\mathbf I = \\big(\\mathbf{U \\Sigma V}^H\\big)^H \\mathbf{U \\Sigma V}^H  =\\mathbf V \\mathbf\\Sigma^2  \\mathbf V^H$\n",
    "\n",
    "left multiply by $\\mathbf V^H$ and right multiply by $\\mathbf V$, recalling\n",
    "that $\\mathbf V$ is a square, full rank matrix\n",
    "\n",
    "$\\mathbf V^H \\mathbf I \\mathbf V = \\mathbf I = \\mathbf V^H \\big(\\mathbf V \\mathbf\\Sigma^2  \\mathbf V^H\\big) \\mathbf V =\\mathbf \\Sigma^2$\n",
    "\n",
    "$\\mathbf I = \\mathbf \\Sigma^2$\n",
    "\n",
    "hence \n",
    "\n",
    "$\\mathbf I = \\mathbf \\Sigma$  \n",
    "\n",
    "Thus we know that $\\mathbf \\Sigma$ is itself the Identity matrix (i.e. $\\sigma_1 = \\sigma_2 = ... = \\sigma_n = 1$)\n",
    "\n",
    "When we consider the eigenvalues of $\\mathbf Q$, where $\\big \\vert \\lambda_1 \\big \\vert \\geq \\big \\vert\\lambda_2 \\big \\vert\\geq \\big \\vert \\lambda_3 \\big \\vert \\geq ... \\geq \\big \\vert\\lambda_n \\big \\vert$, we know that $1 = \\sigma_1 \\geq \\big \\vert \\lambda_1 \\big \\vert$ and we know that $\\big \\vert \\lambda_n \\big \\vert \\geq \\sigma_n = 1$.  Every item in our sequence of eigenvalue magnitudes is bounded above and below by one.  Thus all eigenvalues of a unitary (or in Reals, othogonal) matrix must have magnitue equal to one.\n",
    "\n",
    "$1 = \\sigma_1 \\geq \\big \\vert \\lambda_1 \\big \\vert \\geq \\big \\vert\\lambda_2 \\big \\vert\\geq \\big \\vert \\lambda_3 \\big \\vert \\geq ... \\geq \\big \\vert\\lambda_n \\big \\vert \\geq \\sigma_n = 1$\n",
    "\n",
    "can be re-written as\n",
    "\n",
    "$1 = \\sigma_1 = \\big \\vert \\lambda_1 \\big \\vert = \\big \\vert\\lambda_2 \\big \\vert = \\big \\vert \\lambda_3 \\big \\vert = ... = \\big \\vert\\lambda_n \\big \\vert =\\sigma_n = 1$\n",
    "\n",
    "\n",
    "- - - - \n",
    "\n",
    "# Thoughts on Nilpotent Matrices\n",
    "\n",
    "The final sequences of inequalities for some arbitary square matrix:\n",
    "\n",
    "$\\sigma_1 \\geq \\big \\vert \\lambda_1 \\big \\vert \\geq \\big \\vert\\lambda_2 \\big \\vert\\geq \\big \\vert \\lambda_3 \\big \\vert \\geq ... \\geq \\big \\vert\\lambda_n \\big \\vert \\geq \\sigma_n $\n",
    "\n",
    "is quite useful.  \n",
    "\n",
    "A nilpotent matrix $\\mathbf A$ is some $n$ x $n$ matrix where are after finite number of iterations, it becomes the zero matrix.  Thus $\\mathbf A^r = \\mathbf 0$ for some finite, natural number $r$.  (We can tighten the bound and say $r \\leq n$, but this is not really needed here.) \n",
    "\n",
    "**Claim:** a nilpotent matrix has all eigenvalues equal to zero.\n",
    "\n",
    "There are numerous ways to prove this.  The most slick uses the analysis earlier in this post and does the following:\n",
    "\n",
    "**Proof: **  \n",
    "$\\mathbf A$ has eigenvalues of $\\big \\vert \\lambda_1 \\big \\vert \\geq \\big \\vert\\lambda_2 \\big \\vert\\geq \\big \\vert \\lambda_3 \\big \\vert \\geq ... \\geq \\big \\vert\\lambda_n \\big \\vert $\n",
    "\n",
    "and \n",
    "\n",
    "$\\big(\\mathbf A^r\\big)$ has eigenvalues of $\\big \\vert \\lambda_1^r \\big \\vert \\geq \\big \\vert\\lambda_2^r \\big \\vert\\geq \\big \\vert \\lambda_3^r \\big \\vert \\geq ... \\geq \\big \\vert\\lambda_n^r \\big \\vert$\n",
    "\n",
    "or equivalently \n",
    "\n",
    "$\\big(\\mathbf A^r\\big)$ has eigenvalues of $\\big \\vert \\lambda_1 \\big \\vert^r \\geq \\big \\vert\\lambda_2 \\big \\vert^r \\geq \\big \\vert \\lambda_3 \\big \\vert^r \\geq ... \\geq \\big \\vert\\lambda_n \\big \\vert^r $\n",
    "\n",
    "We can do SVD on $\\big(\\mathbf A^r\\big)$ and see  \n",
    "$\\big(\\mathbf A^r\\big) = \\mathbf U \\mathbf \\Sigma \\mathbf V^H = \\mathbf 0$, where $\\mathbf U$ and $\\mathbf V$ are full rank unitary matrices (or orthogonal if dealing with Reals)  \n",
    "\n",
    "Hence:\n",
    "\n",
    "$ \\mathbf \\Sigma = \\mathbf U^H\\big(\\mathbf A^r\\big)\\mathbf V = \\mathbf U^H \\big(\\mathbf 0\\big)\\mathbf V = \\mathbf 0 $\n",
    "\n",
    "That is, all singular values of $\\big(\\mathbf A^r\\big)$  are equal to zero \n",
    "\n",
    "Thus we know that for $ \\big(\\mathbf A^r\\big)$  \n",
    "\n",
    "$0 = \\sigma_1 \\geq \\big \\vert \\lambda_1^r \\big \\vert \\geq \\big \\vert\\lambda_2^r \\big \\vert\\geq \\big \\vert \\lambda_3^r \\big \\vert \\geq ... \\geq \\big \\vert\\lambda_n^r \\big \\vert \\geq \\sigma_n = 0$\n",
    "\n",
    "Since all eigenvalue magnitudes are bounded above and below by zero, we restate this as  \n",
    "$0 = \\lambda_1^r  = \\lambda_2^r = \\lambda_3^r = ... = \\lambda_n^r  = 0$\n",
    "\n",
    "take the $r$th root and we see that all eigenvalues of the nilpotent matrix $\\mathbf A$ must be zero\n",
    "\n",
    "$0 = \\lambda_1  = \\lambda_2 =  \\lambda_3 = ... = \\lambda_n  = 0$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [py35]",
   "language": "python",
   "name": "Python [py35]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
