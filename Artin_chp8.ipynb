{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sympy as sp\n",
    "import numpy as np\n",
    "np.set_printoptions(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = sp.Symbol('a', real =True)\n",
    "b = sp.Symbol('b', real =True )\n",
    "c = sp.Symbol('c', real =True)\n",
    "d = sp.Symbol('d', real =True )\n",
    "\n",
    "x = 1 + 2* sp.I\n",
    "x /= 2\n",
    "x = sp.sqrt(x)  \n",
    "\n",
    "# to make well defined  \n",
    "# select top right quadrant  \n",
    "# https://www.wolframalpha.com/input/?i=sqrt%281%2F2+%2B+i%29\n",
    "# though either root works  \n",
    "\n",
    "y = 1 - 2* sp.I\n",
    "y /= 2\n",
    "y = sp.sqrt(y)  \n",
    "# similar idea with y  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "# w = sp.Matrix([3,4])\n",
    "# w /= 5\n",
    "# w = sp.Matrix([5, sp.sqrt(24)*sp.I])\n",
    "w = sp.Matrix([sp.sqrt(24)*sp.I,5])\n",
    "\n",
    "D = sp.Matrix([[1,0],[0,-1]])\n",
    "\n",
    "print(sp.trace(w@w.T))\n",
    "\n",
    "P = sp.Matrix([[1,0],[0,1]]) - 2*w @ w.T\n",
    "Q = D@P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}49 & - 20 \\sqrt{6} i\\\\20 \\sqrt{6} i & 49\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "Matrix([\n",
       "[          49, -20*sqrt(6)*I],\n",
       "[20*sqrt(6)*I,            49]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sympy as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}1 & \\frac{1}{2}\\\\\\frac{1}{2} & -1\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "Matrix([\n",
       "[  1, 1/2],\n",
       "[1/2,  -1]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = sp.Matrix([[1,1],[0,-1]])\n",
    "A = A + A.T\n",
    "A /=2\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle - \\frac{5}{4}$"
      ],
      "text/plain": [
       "-5/4"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.det()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{-sqrt(5)/2: 1, sqrt(5)/2: 1}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.eigenvals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}1 & -1\\\\-1 & 0\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "Matrix([\n",
       "[ 1, -1],\n",
       "[-1,  0]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B = sp.Matrix([[1,-2],[0,0]])\n",
    "B = B + B.T\n",
    "B /=2\n",
    "B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle -1$"
      ],
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B.det()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1/2 - sqrt(5)/2: 1, 1/2 + sqrt(5)/2: 1}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B.eigenvals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{49 - 20*sqrt(6): 1, 20*sqrt(6) + 49: 1}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q.eigenvals()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}\\sqrt{\\frac{1}{2} + i} & - \\sqrt{\\frac{1}{2} - i}\\\\\\sqrt{\\frac{1}{2} - i} & \\sqrt{\\frac{1}{2} + i}\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "Matrix([\n",
       "[sqrt(1/2 + I), -sqrt(1/2 - I)],\n",
       "[sqrt(1/2 - I),  sqrt(1/2 + I)]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = sp.Matrix([[x,-y], [y, x]])\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\operatorname{PurePoly}{\\left( \\lambda^{2} -  \\sqrt{2 + 4 i} \\lambda + 1, \\lambda, domain=EX \\right)}$"
      ],
      "text/plain": [
       "PurePoly(lambda**2 - sqrt(2 + 4*I)*lambda + 1, lambda, domain='EX')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.charpoly()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}1 & 0\\\\0 & 1\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "Matrix([\n",
       "[1, 0],\n",
       "[0, 1]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.T*A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ex **8.2.1**  \n",
    "$A:=  \\displaystyle \\left[\\begin{matrix}\\sqrt{\\frac{1}{2} + i} & - \\sqrt{\\frac{1}{2} - i}\\\\\\sqrt{\\frac{1}{2} - i} & \\sqrt{\\frac{1}{2} + i}\\end{matrix}\\right]$  \n",
    "\n",
    "(The choice of root is arbitrary and does not affect the outcome.  For concreteness in each case assume it is the root in the right quadrant.)  \n",
    "\n",
    "$A^T A = I$  \n",
    "$\\det\\big(A\\big) = 1$  and  \n",
    "$\\text{trace}\\big(A^2\\big) = 4i$  \n",
    "and this tells us one eigenvalue, $\\lambda_1$ must have modulus of $1 + \\delta$ for some $\\delta \\gt 0$  \n",
    "and given the determinant we know the other eigenvalue, $\\lambda_2$ has modulus $(1+\\delta)^{-1}\\lt 1$  \n",
    "\n",
    "where for $k$ large enough $\\big \\vert \\lambda_2 \\big \\vert^k  \\lt \\epsilon$ for any $\\epsilon \\in (0,1)$  \n",
    "or  $\\big(\\big \\vert \\lambda_2 \\big \\vert^k\\big)^{-1} = c \\gt \\epsilon^{-1}$  \n",
    "but    \n",
    "$\\lambda_1 \\lambda_2 = 1\\longrightarrow  \\big\\vert\\lambda_1\\big\\vert^k = \\big(\\big \\vert \\lambda_2 \\big \\vert^k\\big)^{-1} = c \\gt \\epsilon^{-1}$  \n",
    "i.e. $\\big\\vert\\lambda_1\\big\\vert^k$  may be made arbitrarily large by selecting large enough $k$  \n",
    "\n",
    "recalling that orthogonal matrices, even with scalars in $\\mathbb C$ *still* form a group, we see  \n",
    "$A^k$ is unbounded-- e.g. check the Frobenius norm or even the trace   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ex 8.1.2**  \n",
    "\n",
    "if $A \\in SP_n(\\mathbb R)$ then  \n",
    "$A^T\\left[\\begin{matrix}0 & -1\\\\1 & 0\\end{matrix}\\right]A=\\left[\\begin{matrix}0 & -1\\\\1 & 0\\end{matrix}\\right]$  \n",
    "taking determinants tells us  \n",
    "$\\det\\big(A\\big) \\in \\{-1,+1\\}$  \n",
    "(The starred ex 8.1.13 aims to show us that it in fact must be $+1$)  \n",
    "\n",
    "ex 8.1.11 tells us that in the special case of 2x2 matrices, matrices in $SP_2\\big(\\mathbb R\\big)$ are closed under multiplication by elementary matrices of the first type.  \n",
    "But elementary matrices of the first type generate all of $SL_2\\big(\\mathbb R\\big)$\n",
    "(see e.g. 'Artin_chp2_SLN_subgroup_generators.ipynb'), so we know  \n",
    "\n",
    "$SL_2\\big(\\mathbb R\\big) \\subset SP_2\\big(\\mathbb R\\big)$   \n",
    "\n",
    "Elementary matrices of the 3rd type, and in particular  \n",
    "$D: = \\left[\\begin{matrix}-1 & 0\\\\0 & 1\\end{matrix}\\right]$ \n",
    "generate the coset to $SL_2(\\mathbb R)$  with determinant $-1$ -- i.e. all 2 x 2 matrices with determinant -1.  Now suppose for a contradiction that $\\det(A) = -1$ and $A \\in SP_2(\\mathbb R)$, this means  \n",
    "\n",
    "$A^T\\left[\\begin{matrix}0 & -1\\\\1 & 0\\end{matrix}\\right]A=\\left[\\begin{matrix}0 & -1\\\\1 & 0\\end{matrix}\\right]$ \n",
    "\n",
    "and  \n",
    "$B: = DA \\in SL_2(\\mathbb R) \\subset SP_2(\\mathbb R)\\longrightarrow B \\in SP_2(\\mathbb R)$  \n",
    "but  \n",
    "$B^T\\left[\\begin{matrix}0 & -1\\\\1 & 0\\end{matrix}\\right]B=D\\left[\\begin{matrix}0 & -1\\\\1 & 0\\end{matrix}\\right]D = \\left[\\begin{matrix}0 & 1\\\\-1 & 0\\end{matrix}\\right]\\neq \\left[\\begin{matrix}0 & -1\\\\1 & 0\\end{matrix}\\right]$  \n",
    "\n",
    "which would be a contradiction. So for the 2x2 case we know that it is also the case    \n",
    "$ SP_2\\big(\\mathbb R\\big) \\subset SL_2\\big(\\mathbb R\\big)$   \n",
    "i.e.  $ SP_2\\big(\\mathbb R\\big) = SL_2\\big(\\mathbb R\\big)$   \n",
    "\n",
    "as for the 4x4 case, consider  \n",
    "\n",
    "$A=\\left[\\begin{matrix}1 & 0 & 2 & 0\\\\0 & 1 & 0 & 0\\\\0 & 0 & 1 & 0\\\\0 & 0 & 0 & 1\\end{matrix}\\right]$  \n",
    "and $A\\in SL_4(\\mathbb R) \\not \\in SP_4(\\mathbb R)$  since   \n",
    "$A^TJA = \\left[\\begin{matrix}0 & 0 & 0 & 1\\\\0 & 0 & 1 & 0\\\\0 & -1 & 0 & 2\\\\-1 & 0 & -2 & 0\\end{matrix}\\right]$  \n",
    "\n",
    "**open problem / bug**  \n",
    "is the 4x4 counterexample $A$ not in the form  \n",
    "\n",
    "$A=\\left[\\begin{matrix} I_2 &  B \\\\ \\mathbf 0 &  I_2\\end{matrix}\\right]$  \n",
    "where  \n",
    "$B = \\left[\\begin{matrix}2 & 0 \\\\  0 & 0\\end{matrix}\\right]$  \n",
    "so $B = B^T$ and $A$ is in the form given by ex 8.1.11 so it should be in $SP_4(\\mathbb R)$  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = sp.Matrix([[1,0,2,0],[0,1,0,0],[0,0,1,0], [0,0,0,1]])  \n",
    "# A = sp.Matrix([[1,0,0,2],[0,1,0,0],[0,0,1,0], [0,0,0,1]])  \n",
    "J = sp.Matrix([[0,0,0,1],[0,0,1,0],[0,-1,0,0], [-1,0,0,0]])  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}0 & 0 & 0 & 1\\\\0 & 0 & 1 & 0\\\\0 & -1 & 0 & 2\\\\-1 & 0 & -2 & 0\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "Matrix([\n",
       "[ 0,  0,  0, 1],\n",
       "[ 0,  0,  1, 0],\n",
       "[ 0, -1,  0, 2],\n",
       "[-1,  0, -2, 0]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.T @J@ A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}1 & 0 & 2 & 0\\\\0 & 1 & 0 & 0\\\\0 & 0 & 1 & 0\\\\0 & 0 & 0 & 1\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "Matrix([\n",
       "[1, 0, 2, 0],\n",
       "[0, 1, 0, 0],\n",
       "[0, 0, 1, 0],\n",
       "[0, 0, 0, 1]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}0 & 0 & 0 & 1\\\\0 & 0 & 1 & 0\\\\0 & -1 & 0 & 0\\\\-1 & 0 & 0 & 0\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "Matrix([\n",
       "[ 0,  0, 0, 1],\n",
       "[ 0,  0, 1, 0],\n",
       "[ 0, -1, 0, 0],\n",
       "[-1,  0, 0, 0]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ex 8.1.5**  \n",
    "\n",
    "a matrix $P$ is orthogonal iff its columns form an orthonormal basis, i.e. \n",
    "\n",
    "$\\langle \\mathbf p_i, \\mathbf p_j\\rangle = \\mathbf p_i^T\\left[\\begin{matrix}\\mathbf I_{n-1} & 0\\\\0 & 1\\end{matrix}\\right]\\mathbf p_j= 1$ if $i=j$ and zero otherwise.  \n",
    "\n",
    "Putting all these relationships together, this reads  \n",
    "\n",
    "$P^TP = P^TIP =P^T \\left[\\begin{matrix}\\mathbf I_{n-1} & 0\\\\0 & 1\\end{matrix}\\right] P =  \\left[\\begin{matrix}\\mathbf I_{n-1} & 0\\\\0 & 1\\end{matrix}\\right] = I$  \n",
    "\n",
    "the idea is essentially the same with respect to the Lorentz Form, except we have an 'almost orthonormal' basis and we are no longer using the standard Euclidean (scalar) product but instead the Lorentz bilinear form  \n",
    "\n",
    "$\\langle \\mathbf p_i, \\mathbf p_j\\rangle = \\mathbf p_i^T\\left[\\begin{matrix}\\mathbf I_{n-1} & 0\\\\0 & -1\\end{matrix}\\right]\\mathbf p_j$  \n",
    "$= 1$ if $i=j\\leq n-1$,  \n",
    "$=-1$ if $i =j=n$  \n",
    "and zero otherwise.  \n",
    "\n",
    "Collecting all these relationships, this reads   \n",
    "$P^T  \\left[\\begin{matrix}\\mathbf I_{n-1} & 0\\\\0 & -1\\end{matrix}\\right] P =    \\left[\\begin{matrix}\\mathbf I_{n-1} & 0\\\\0 & -1\\end{matrix}\\right]$  \n",
    "\n",
    "note we can make an analogous claim for 'almost orthonormal' rows in $P$ as the above implies    \n",
    "$\\Big(P^T  \\left[\\begin{matrix}\\mathbf I_{n-1} & 0\\\\0 & -1\\end{matrix}\\right]\\Big)\\Big( P  \\left[\\begin{matrix}\\mathbf I_{n-1} & 0\\\\0 & -1\\end{matrix}\\right]\\Big) = \\mathbf I =  \\Big(P  \\left[\\begin{matrix}\\mathbf I_{n-1} & 0\\\\0 & -1\\end{matrix}\\right]\\Big)  \\Big(P^T\\left[\\begin{matrix}\\mathbf I_{n-1} & 0\\\\0 & -1\\end{matrix}\\right]\\Big)$  \n",
    "\n",
    "which implies  \n",
    "\n",
    "$P \\left[\\begin{matrix}\\mathbf I_{n-1} & 0\\\\0 & -1\\end{matrix}\\right] P^T =    \\left[\\begin{matrix}\\mathbf I_{n-1} & 0\\\\0 & -1\\end{matrix}\\right]$  \n",
    "\n",
    "where we make use of the fact that inverses commute and that  $\\left[\\begin{matrix}\\mathbf I_{n-1} & 0\\\\0 & -1\\end{matrix}\\right]$ is involutive  \n",
    "\n",
    "**remark:**  \n",
    "another way to view this problem is to start with a simple coordinate system for the Lorenz Form, using standard basis vectors    \n",
    "$\\langle \\mathbf e_i, \\mathbf e_j\\rangle = \\mathbf e_i^T\\left[\\begin{matrix}\\mathbf I_{n-1} & 0\\\\0 & -1\\end{matrix}\\right]\\mathbf e_j$   \n",
    "\n",
    "and any $ P$ that satisfies the stabilizer equation obeys   \n",
    "\n",
    "$\\langle P\\mathbf e_i, P\\mathbf e_j\\rangle = \\langle \\mathbf p_i, \\mathbf p_j\\rangle = \\mathbf p_i^T\\left[\\begin{matrix}\\mathbf I_{n-1} & 0\\\\0 & -1\\end{matrix}\\right]\\mathbf p_j = \\mathbf e_i^T\\mathbf P^T\\left[\\begin{matrix}\\mathbf I_{n-1} & 0\\\\0 & -1\\end{matrix}\\right]\\mathbf P\\mathbf e_j = \\mathbf e_i^T\\left[\\begin{matrix}\\mathbf I_{n-1} & 0\\\\0 & -1\\end{matrix}\\right]\\mathbf e_j= \\langle \\mathbf e_i, \\mathbf e_j\\rangle$     \n",
    "i.e.    \n",
    "$\\langle P\\mathbf e_i, P\\mathbf e_j\\rangle  = \\langle \\mathbf e_i, \\mathbf e_j\\rangle$  \n",
    "\n",
    "and the result then hold for arbitrary $\\mathbf v, \\mathbf w \\in V$ by making use of standard basis vectors, then change of basis via $ P$ and and pulling out the summations via bilinearity -- but cross terms are annihilated by orthogonality    \n",
    "\n",
    "This can be seen as a generalization of orthonormal change of basis (e.g. ref p.253, 5.3b in particular)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ex 8.1.6**  \n",
    "prove that $O_4$ and $O_{3,1}$ don't have a continuous isomorphism between the two   \n",
    "\n",
    "$D := \\left[\\begin{matrix}\\mathbf I_{n-1} & 0\\\\0 & -1\\end{matrix}\\right] $  \n",
    "\n",
    "in particular with $n=4$  \n",
    "\n",
    "\n",
    "**note:**  \n",
    "Since Householder matrices generate $O_n$ and are orthogonally similar to  $D$, it seems like there should be a more succinct proof of this result using conjugation  \n",
    "\n",
    "**remark:**  \n",
    "by focusing our line of attack on Householder matrices, which we know generate $O_n$ (see ex 7.3.9 in \"Artin_chp7.ipynb\") we can show a stronger claim: there can be no surjective homomorphism from $O_4$ to $O_{3,1}$ \n",
    "\n",
    "i.e. we don't need injectivity or continuity of the homomorphism that a continuous isomorphism implies-- these are used in optional finishes (2) and (3), but not used in finish (1)     \n",
    "\n",
    "**proof:**  \n",
    "select some Householder matrix $ H\\in O_n$ (again with $n =4$ for our problem) and   \n",
    "*suppose for a contradiction that a valid surjective homomorphism* $\\phi$ exists  \n",
    "\n",
    "$\\phi\\big(H\\big) = M$  \n",
    "where $M$ is involutive because  \n",
    "$M^2 = \\phi\\big(H\\big)^2  = \\phi\\big(H^2\\big) = \\phi\\big(I_n\\big)=I_n$   \n",
    "and   \n",
    "$MDM^T=D  $  \n",
    "Left multiplying by $M$, and right multiplying by $D$ we see      \n",
    "$DM^TD=M  $  \n",
    "\n",
    "this implies for $1\\leq j \\leq n-1$   \n",
    "$-m_{n,j}= m_{j,n}$ \n",
    "\n",
    "in blocked form we have  \n",
    "$M=\\left[\\begin{matrix} M'& \\mathbf m_n\\\\-\\mathbf m_n^T & \\alpha\\end{matrix}\\right]$  \n",
    "\n",
    "then  \n",
    "$ \\left[\\begin{matrix}\\mathbf I_{n-1} & 0\\\\0 & 1\\end{matrix}\\right] = I =M^2 = \\left[\\begin{matrix} (M')^2-\\mathbf m_n\\mathbf m_n^T & M'\\mathbf m_n\\\\-\\mathbf m_n^T M' & -\\mathbf m_n^T\\mathbf m_n + \\alpha^2\\end{matrix}\\right]$  \n",
    "\n",
    "however, since $\\mathbf m_n$ is in the kernel of $M'$ (as a bonus, it is the left nullspace too though only one of these is needed), then consider the quadratic form \n",
    "$\\mathbf x^T\\big((M')^2-\\mathbf m_n\\mathbf m_n^T\\big)\\mathbf x$  and set $\\mathbf x:=\\mathbf m_n$  giving  \n",
    "$\\mathbf m_n^T\\big((M')^2-\\mathbf m_n\\mathbf m_n^T\\big)\\mathbf m_n $  $ =0 - \\mathbf m_n^T\\mathbf m_n\\mathbf m_n^T\\mathbf m_n = -\\big\\Vert \\mathbf m_n\\big \\Vert_2^2\\cdot \\big\\Vert \\mathbf m_n\\big \\Vert_2^2 \\leq 0$   \n",
    "and the upper bound is met with equality only when $\\mathbf m_n = \\mathbf 0$   \n",
    "\n",
    "We also know  \n",
    "$(M')^2-\\mathbf m_n\\mathbf m_n^T=\\mathbf I_{n-1}\\succeq \\mathbf 0\\longrightarrow \\mathbf m_n = \\mathbf 0$  \n",
    "\n",
    "Thus  \n",
    "\n",
    "$\\phi\\big(H\\big) = M=\\left[\\begin{matrix} M'& \\mathbf 0\\\\\\mathbf 0 & \\alpha\\end{matrix}\\right]$    \n",
    "\n",
    "*various ways to finish*  \n",
    "\n",
    "\n",
    "1.) *generating a counter-example*  \n",
    "since the homomorphism is surjective and Householder matrices generate all of $O_n$, this combined with blocked multiplication implies  *all* of the Lorenz Group is in this block diagonal form, yet consider the matrix $B$, below (the matrix below was created by use of the equations from ex *8.1.7*)  \n",
    "$A: = \\left[\\begin{matrix}2 & \\sqrt{3}\\\\\\sqrt{3} & 2\\end{matrix}\\right]$  \n",
    "\n",
    "$B:=  \\left[\\begin{matrix}\\mathbf I_2 &  \\mathbf 0\\\\ \\mathbf 0 &A\\end{matrix}\\right]= \\left[\\begin{matrix}1 &  0&  0& 0\\\\ 0&1 & 0& 0 &\\\\  0 &  0&2 & \\sqrt{3} \\\\ 0 & 0& \\sqrt{3} & 2 \\end{matrix}\\right]$  \n",
    "and $B^TD B = D$  \n",
    "so $B \\in I_{3,1}$  \n",
    "but $B$ does not follow this blocked diagonal form, which is a contradiction  \n",
    "\n",
    "\n",
    "\n",
    "2.) *using injectivity*  \n",
    "$MDM^T = D$  \n",
    "evaluation of the blocked structure tells us that $M'$ is orthogonal, and $\\alpha \\in \\{-1,1\\}$  i.e. $M$ is orthogonal -- and since we are using generators, and orthogonal matrices form a group, we see that *all* matrices in the Lorenz Group are orthogonal in this blocked structure -- i.e. the Lorenz Group must be a subgroup of the Orthogonal Group.  A possible finish would be to use the First Isomorphism Theorem (p. 68 - 69) which implies that more than the Identity Matrix is in the kernel of our homomorphism, which contradicts the assumption that our homomorphism is injective.  \n",
    "\n",
    "\n",
    "3.) *using continuity*  \n",
    "using a hybrid of insights from (1) and (2), we can see that the Lorenz group has 4 connected components ($+1$ and $-1$ for $a$ and determinant of +1 and -1 for $M'$ where we inherit standard results about path connectivity for $\\text{n-1 x n-1}$ orthogonal matrices like $M'$) but $O_n$ has 2 connected components, hence hence our hypothesis that there is a continuous map (continuous homomorphism) from $O_n$ to the Lorenz Group generates a contradiction.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8.1.7**  \n",
    "Describe by equations the group $O_{1,1}$ -- i.e. 2x2 reflection group-- and show that it has 4 connected components  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = sp.Matrix([[a,b],[c,d]])\n",
    "D = sp.Matrix([[1,0],[0,-1]])\n",
    "# A.T*D*A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\left[\\begin{matrix}1 & 0\\\\0 & -1\\end{matrix}\\right] = \\left[\\begin{matrix}a^{2} - c^{2} & a b - c d\\\\a b - c d & b^{2} - d^{2}\\end{matrix}\\right]$  \n",
    "\n",
    "**solution set:**    \n",
    "\n",
    "$a\\geq 1, a\\leq -1, d\\geq 1, d\\leq -1$  \n",
    "these are the 4 distinct connected components (2 choices for a, 2 choices for b).  \n",
    "sketching in the plane for various choices of $a$ and $d$ as domain choices, and the implied values of $b$ and $c$ is of interest    \n",
    "\n",
    "And in particular focus on a strict inequality case of the above. e.g. $a\\gt 1, d\\lt 1$  \n",
    "This would appear to allow exactly 2 choices for $b$ and 2 choices for $c$ (i.e. the above choices fix the modulus for b and c)   \n",
    "\n",
    "focusing on the off-diagonal, with fixed selections of $a$ and $d$ we see  \n",
    "$ab-cd =0 \\longrightarrow \\frac{ab}{c} =d$   \n",
    "which is to say for any selection of $b$ there is exactly one value of $d$\n",
    "(and we know $c\\neq 0$ since we focus on $\\vert a\\vert \\gt 1$-- the case when $\\vert a\\vert =1$ is immediate)  \n",
    "\n",
    "(there is probably a bit of work remaining to clarify that the determinant value of +1 or -1 is in fact maintained)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}1 & 0\\\\0 & -1\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "Matrix([\n",
       "[1,  0],\n",
       "[0, -1]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example calculations  \n",
    "B = sp.Matrix([[a,b],[c,d]])\n",
    "\n",
    "a_value = 2\n",
    "d_value = 2\n",
    "# if a and d do not have the same modulus... the result seems to break  \n",
    "\n",
    "B = B.subs(a, a_value)\n",
    "B = B.subs(d, d_value)\n",
    "# B[-1,-1] \n",
    "B\n",
    "# so  \n",
    "# ab - cd = 0 = 3b - 5c -> b= (5/3)c  and  using the negative determinant case  \n",
    "# -1 = ad - bc = 15- (5/3)c^2 -> 5/3 c^2 = 16,\n",
    "c_value_sq =  a_value**2 -1 \n",
    "# c_value = -sp.sqrt(c_value_sq)\n",
    "c_value = sp.sqrt(c_value_sq)\n",
    "\n",
    "b_value = c_value * d_value / a_value\n",
    "\n",
    "\n",
    "B = B.subs(c,c_value)\n",
    "B = B.subs(b, b_value)\n",
    "B * D * B.T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}2 & \\sqrt{3}\\\\\\sqrt{3} & 2\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "Matrix([\n",
       "[      2, sqrt(3)],\n",
       "[sqrt(3),       2]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sympy as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = sp.Symbol('n', integer = True)\n",
    "\n",
    "# sympy has a lot of trouble with the zero matrix  \n",
    "\n",
    "A = sp.MatrixSymbol('A', n, n)\n",
    "M_prime = sp.MatrixSymbol('M_p' , n-1,n-1)\n",
    "m = sp.MatrixSymbol('m', n-1, n)\n",
    "M = sp.BlockMatrix([[ M_prime, m], [-m.T, 0]]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}M_{p}^{2} - m m^{T} & M_{p} m\\\\- m^{T} M_{p} & - m^{T} m\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "Matrix([\n",
       "[M_p**2 - m*m.T,  M_p*m],\n",
       "[      -m.T*M_p, -m.T*m]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp.block_collapse(M*M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = sp.Symbol('n', integer = True)\n",
    "\n",
    "# sympy has a lot of trouble with the zero matrix  \n",
    "\n",
    "A = sp.MatrixSymbol('A', n, n)\n",
    "B = sp.MatrixSymbol('B', n,n)\n",
    "C = sp.MatrixSymbol('C', n, n)\n",
    "D = sp.MatrixSymbol('D', n,n)\n",
    "\n",
    "Z = sp.MatrixSymbol('Z', n,n)\n",
    "# symplectic\n",
    "\n",
    "Zero = sp.ZeroMatrix(n,n)\n",
    "S = sp.BlockMatrix([[ sp.ZeroMatrix(n,n), sp.Identity(n)], [-sp.Identity(n), sp.ZeroMatrix(n,n)]])\n",
    "# S = sp.BlockMatrix([[ Z, sp.Identity(n)], [-sp.Identity(n), Z ]])\n",
    "S_t = sp.BlockMatrix([[ Z, -sp.Identity(n)], [sp.Identity(n), Z]])\n",
    "# sympy throws errors if we use S_t = S.T \n",
    "# some issue with the ZeroMatrix() and transposition "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}\\left(\\mathbb{0}\\right)^{T} - \\mathbb{0} & \\mathbb{I} + \\mathbb{0} \\left(\\mathbb{0}\\right)^{T}\\\\- \\mathbb{I} - \\mathbb{0} & \\left(\\mathbb{0}\\right)^{T} - \\mathbb{0}\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "Matrix([\n",
       "[   0.T - 0, I + 0*0.T],\n",
       "[-I - 0*0.T,   0.T - 0]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M = S_t*S*S_t.T\n",
    "M_simplified = sp.block_collapse(M)\n",
    "M_simplified = M_simplified.subs(Z, Zero)\n",
    "sp.simplify(M_simplified)\n",
    "\n",
    "# note the result should be obvious, since they are all the same matrix except for rescaling by minus 1, and \n",
    "#S^2 = -I, so -S^3 = -S (-I) = S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}A \\mathbb{0} - \\mathbb{0} & \\mathbb{I} - \\mathbb{0}\\\\- \\mathbb{I} + \\left(\\mathbb{0}\\right)^{T} \\mathbb{0} & - \\left(A^{T}\\right)^{-1} \\mathbb{0} + \\left(\\mathbb{0}\\right)^{T} A^{-1}\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "Matrix([\n",
       "[A*0 - 0.T*A.T,                  I - 0.T*0],\n",
       "[   -I + 0.T*0, -A.T**(-1)*0 + 0.T*A**(-1)]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = sp.BlockMatrix([[A.T,Z],\n",
    "                    [Z, A.inv()]])\n",
    "M = X.T*S*X\n",
    "M_simplified = sp.block_collapse(M)\n",
    "M_simplified = M_simplified.subs(Z, Zero)\n",
    "sp.simplify(M_simplified)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}- \\left(\\mathbb{0}\\right)^{T} + \\mathbb{0} & \\mathbb{I} - \\mathbb{0}\\\\- \\mathbb{I} + B^{T} \\mathbb{0} & - B + B^{T}\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "Matrix([\n",
       "[  -0.T + 0, I - 0.T*B],\n",
       "[-I + B.T*0,  -B + B.T]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = sp.BlockMatrix([[sp.Identity(n),B],\n",
    "                    [Z, sp.Identity(n)]])\n",
    "M = X.T*S*X\n",
    "M_simplified = sp.block_collapse(M)\n",
    "M_simplified = M_simplified.subs(Z, Zero)\n",
    "sp.simplify(M_simplified)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}\\mathbb{I} & B\\\\Z & \\mathbb{I}\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "Matrix([\n",
       "[I, B],\n",
       "[Z, I]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*additional misc calculations are below*  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}\\left(\\mathbb{I} - \\mathbb{0}\\right)^{-1} & - B \\left(\\mathbb{I} - \\mathbb{0}\\right)^{-1}\\\\- \\left(\\mathbb{I} - \\mathbb{0}\\right)^{-1} \\mathbb{0} & \\left(\\mathbb{I} - \\mathbb{0}\\right)^{-1}\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "Matrix([\n",
       "[   (I - B*0)**(-1), -B*(I - 0*B)**(-1)],\n",
       "[-(I - 0*B)**(-1)*0,    (I - 0*B)**(-1)]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M = X.inv()\n",
    "M_simplified = sp.block_collapse(M)\n",
    "\n",
    "M_simplified = M_simplified.subs(Z, Zero)\n",
    "sp.simplify(M_simplified)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\left[\\begin{matrix}\\left(\\mathbb{0}\\right)^{T} - \\mathbb{0} & \\mathbb{I} + \\mathbb{0} \\left(\\mathbb{0}\\right)^{T}\\\\- \\mathbb{I} - \\mathbb{0} & \\left(\\mathbb{0}\\right)^{T} - \\mathbb{0}\\end{matrix}\\right]\n",
      "\\left[\\begin{matrix}A \\mathbb{0} - \\mathbb{0} & \\mathbb{I} - \\mathbb{0}\\\\- \\mathbb{I} + \\left(\\mathbb{0}\\right)^{T} \\mathbb{0} & - \\left(A^{T}\\right)^{-1} \\mathbb{0} + \\left(\\mathbb{0}\\right)^{T} A^{-1}\\end{matrix}\\right]\n",
      "\\left[\\begin{matrix}- \\left(\\mathbb{0}\\right)^{T} + \\mathbb{0} & \\mathbb{I} - \\mathbb{0}\\\\- \\mathbb{I} + B^{T} \\mathbb{0} & - B + B^{T}\\end{matrix}\\right]\n"
     ]
    }
   ],
   "source": [
    "x_list = [S_t.T, sp.BlockMatrix([[A.T,Z], [Z, A.inv()]]) , sp.BlockMatrix([[sp.Identity(n),B],\n",
    "                    [Z, sp.Identity(n)]])]\n",
    "y_list = x_list[:]\n",
    "\n",
    "result_list = []\n",
    "for X in x_list:                                 \n",
    "    M = X.T*S*X\n",
    "    M_simplified = sp.block_collapse(M)\n",
    "    M_simplified = M_simplified.subs(Z, Zero)\n",
    "    print(sp.latex(sp.simplify(M_simplified)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\left[\\begin{matrix}\\left(\\mathbb{0}\\right)^{T} - \\mathbb{0} & \\mathbb{I} + \\mathbb{0} \\left(\\mathbb{0}\\right)^{T}\\\\- \\mathbb{I} - \\mathbb{0} & \\left(\\mathbb{0}\\right)^{T} - \\mathbb{0}\\end{matrix}\\right]\n",
    "\\left[\\begin{matrix}A \\mathbb{0} - \\mathbb{0} & \\mathbb{I} - \\mathbb{0}\\\\- \\mathbb{I} + \\left(\\mathbb{0}\\right)^{T} \\mathbb{0} & - \\left(A^{T}\\right)^{-1} \\mathbb{0} + \\left(\\mathbb{0}\\right)^{T} A^{-1}\\end{matrix}\\right]\n",
    "\\left[\\begin{matrix}- \\left(\\mathbb{0}\\right)^{T} + \\mathbb{0} & \\mathbb{I} - \\mathbb{0}\\\\- \\mathbb{I} + B^{T} \\mathbb{0} & - B + B^{T}\\end{matrix}\\right]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\left[\\begin{matrix}A^{T} \\left(\\mathbb{0}\\right)^{T} - \\mathbb{0} & A^{T} + \\mathbb{0} \\left(\\mathbb{0}\\right)^{T}\\\\- A^{-1} + \\mathbb{0} \\left(\\mathbb{0}\\right)^{T} & A^{-1} \\left(\\mathbb{0}\\right)^{T} + \\mathbb{0}\\end{matrix}\\right]\n",
      "\\left[\\begin{matrix}- B + \\left(\\mathbb{0}\\right)^{T} & \\mathbb{I} + B \\left(\\mathbb{0}\\right)^{T}\\\\- \\mathbb{I} + \\mathbb{0} \\left(\\mathbb{0}\\right)^{T} & \\left(\\mathbb{0}\\right)^{T} + \\mathbb{0}\\end{matrix}\\right]\n",
      "\\left[\\begin{matrix}\\left(\\mathbb{0}\\right)^{T} A^{T} + \\mathbb{0} & A^{-1} + \\left(\\mathbb{0}\\right)^{T} \\mathbb{0}\\\\- A^{T} + \\left(\\mathbb{0}\\right)^{T} \\mathbb{0} & \\left(\\mathbb{0}\\right)^{T} A^{-1} - \\mathbb{0}\\end{matrix}\\right]\n",
      "\\left[\\begin{matrix}B \\mathbb{0} + A^{T} & B A^{-1} + \\mathbb{0}\\\\\\mathbb{0} + \\mathbb{0} A^{T} & A^{-1} + \\left(\\mathbb{0}\\right)^{2}\\end{matrix}\\right]\n",
      "\\left[\\begin{matrix}\\left(\\mathbb{0}\\right)^{T} + \\mathbb{0} & \\mathbb{I} + \\left(\\mathbb{0}\\right)^{T} B\\\\- \\mathbb{I} + \\left(\\mathbb{0}\\right)^{T} \\mathbb{0} & - B + \\left(\\mathbb{0}\\right)^{T}\\end{matrix}\\right]\n",
      "\\left[\\begin{matrix}\\left(\\mathbb{0}\\right)^{2} + A^{T} & A^{T} B + \\mathbb{0}\\\\A^{-1} \\mathbb{0} + \\mathbb{0} & A^{-1} + \\mathbb{0} B\\end{matrix}\\right]\n"
     ]
    }
   ],
   "source": [
    "for X in x_list:\n",
    "    for Y in y_list:\n",
    "        if X == Y:\n",
    "            continue\n",
    "        M = Y*X\n",
    "        M_simplified = sp.block_collapse(M)\n",
    "        M_simplified = M_simplified.subs(Z, Zero)\n",
    "        print(sp.latex(sp.simplify(M_simplified)))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\left[\\begin{matrix}A^{T} \\left(\\mathbb{0}\\right)^{T} - \\mathbb{0} & A^{T} + \\mathbb{0} \\left(\\mathbb{0}\\right)^{T}\\\\- A^{-1} + \\mathbb{0} \\left(\\mathbb{0}\\right)^{T} & A^{-1} \\left(\\mathbb{0}\\right)^{T} + \\mathbb{0}\\end{matrix}\\right]\n",
    "\\left[\\begin{matrix}- B + \\left(\\mathbb{0}\\right)^{T} & \\mathbb{I} + B \\left(\\mathbb{0}\\right)^{T}\\\\- \\mathbb{I} + \\mathbb{0} \\left(\\mathbb{0}\\right)^{T} & \\left(\\mathbb{0}\\right)^{T} + \\mathbb{0}\\end{matrix}\\right]\n",
    "\\left[\\begin{matrix}\\left(\\mathbb{0}\\right)^{T} A^{T} + \\mathbb{0} & A^{-1} + \\left(\\mathbb{0}\\right)^{T} \\mathbb{0}\\\\- A^{T} + \\left(\\mathbb{0}\\right)^{T} \\mathbb{0} & \\left(\\mathbb{0}\\right)^{T} A^{-1} - \\mathbb{0}\\end{matrix}\\right]\n",
    "\\left[\\begin{matrix}B \\mathbb{0} + A^{T} & B A^{-1} + \\mathbb{0}\\\\\\mathbb{0} + \\mathbb{0} A^{T} & A^{-1} + \\left(\\mathbb{0}\\right)^{2}\\end{matrix}\\right]\n",
    "\\left[\\begin{matrix}\\left(\\mathbb{0}\\right)^{T} + \\mathbb{0} & \\mathbb{I} + \\left(\\mathbb{0}\\right)^{T} B\\\\- \\mathbb{I} + \\left(\\mathbb{0}\\right)^{T} \\mathbb{0} & - B + \\left(\\mathbb{0}\\right)^{T}\\end{matrix}\\right]\n",
    "\\left[\\begin{matrix}\\left(\\mathbb{0}\\right)^{2} + A^{T} & A^{T} B + \\mathbb{0}\\\\A^{-1} \\mathbb{0} + \\mathbb{0} & A^{-1} + \\mathbb{0} B\\end{matrix}\\right]\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}- B + Z & \\mathbb{I} - B^{2}\\\\- \\mathbb{I} + Z^{2} & - B + Z\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "Matrix([\n",
       "[   -B + Z, I - B**2],\n",
       "[-I + Z**2,   -B + Z]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp.block_collapse(x_list[2] * S * x_list[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**note on transposes and symplectic group**   \n",
    "where $J=J_{2n}$ equals the standard symplectic basis.  While not involutive, $J$ does have period 4, i.e.  \n",
    "\n",
    "$J^2 = -I$    \n",
    "$J^3=-J$  \n",
    "$J^4=I$   \n",
    "much in the way that $i$ and $-i$ do (and $J$ is skew symmetric so diagonalizable with purely imaginary eigenvalues, but also orthogonal so all eigenvalues must be on the unit circle-- putting this together we can see that all eigenvalues of $J$ are $i$ and $-i$, coming in conjugate pairs.  This tells us $J^2=-I$ and the rest follows from associativity.)     \n",
    "\n",
    "\n",
    "*lemma*  \n",
    "in a spirit similar to what was shown for the Lorenz form (mid way through ex 8.1.5), consider \n",
    "\n",
    "$P^T JP = J \\longrightarrow PJP^T = J$   \n",
    "\n",
    "*proof*  \n",
    "$P^T JP = J$  \n",
    "$\\longrightarrow P^T JPJ = -I$  \n",
    "$\\longrightarrow (-P^T J)(PJ) = I $   \n",
    "$\\longrightarrow (PJ)(-P^T J) = I $   \n",
    "$\\longrightarrow  PJP^T J = -I $   \n",
    "$\\longrightarrow  PJP^T (-I) = PJP^T J^2  = -J $   \n",
    "$\\longrightarrow  PJP^T = J $   \n",
    "as desired  \n",
    "\n",
    "in other words, with respect to the standard skew form, \n",
    "if  for all $j, k \\in\\{1,2,...,n\\}$  \n",
    "$\\langle C\\mathbf e_j , C\\mathbf e_k \\rangle = \\langle \\mathbf e_j, \\mathbf e_k \\rangle \\longrightarrow  \\langle C^T\\mathbf e_j , C^T\\mathbf e_k \\rangle = \\langle \\mathbf e_j , \\mathbf e_k \\rangle  $   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8.1.12**  \n",
    "Prove that the symplectic group $SP_{2n}\\big(\\mathbb R\\big)$ operates transitively on $\\mathbb R^{2n} - \\{\\mathbf 0\\}$  \n",
    "\n",
    "*note:*    \n",
    "the stated problem in the book does not carve out the zero vector though it needs to  \n",
    "\n",
    "*lemma:*  \n",
    "nonzero $\\mathbf x \\in \\mathbb R^{2n}$, where the 'bottom half' is zero i.e. $x_i=0$ for all $i\\in \\{n+1,n+2,...,2n\\}$  \n",
    "then $C \\in SP_{2n}\\big(\\mathbb R\\big)$ exists such that \n",
    "\n",
    "$C\\mathbf x = \\mathbf e_1$  \n",
    "\n",
    "*proof:*  \n",
    "set  \n",
    "$C =\\displaystyle \\left[\\begin{matrix}A^T & \\mathbf 0\\\\\\mathbf 0 & A^{-1}\\end{matrix}\\right]$  \n",
    "where  \n",
    "\n",
    "$\\displaystyle \\left[\\begin{matrix}A^{-T} & \\mathbf 0\\\\\\mathbf 0 & A\\end{matrix}\\right]   = C^{-1}$ has its first column given by $\\mathbf x$  \n",
    "\n",
    "\n",
    "*proof of the main claim:*  \n",
    "suppose $x_i\\neq 0$ for at least one $i\\in \\{n+1,n+2,...,2n\\}$  \n",
    "(if not, call on the lemma and we are done)  \n",
    "  \n",
    "i.e.  \n",
    "$\\mathbf x = \\displaystyle \\left[\\begin{matrix}\\mathbf y^{(U)}\\\\ \\mathbf y^{(L)}\\end{matrix}\\right]$   \n",
    "and $\\mathbf y^{(L)}\\neq \\mathbf 0$  \n",
    "\n",
    "we may assume WLOG that $\\mathbf y^{(L)} = \\mathbf e_1 \\in \\mathbb R^n$  \n",
    "if not, first multiply by well chosen  \n",
    "$\\displaystyle \\left[\\begin{matrix}A^{-T} & \\mathbf 0\\\\\\mathbf 0 & A\\end{matrix}\\right]$  \n",
    "\n",
    "so $A\\mathbf y^{(L)} = \\mathbf e_1$  \n",
    "\n",
    "\n",
    "*corner case*   \n",
    "if there is are no zeros in the upper half then first multiply by  \n",
    "\n",
    "$Y = \\displaystyle \\left[\\begin{matrix}I & B'\\\\\\mathbf 0 & I\\end{matrix}\\right]$  \n",
    "where $B'$ is symmetric and non-singular....  \n",
    "then afterward, the resulting vector after matrix vector multiplication must have a non-zero component in the top half  \n",
    "\n",
    "\n",
    "now we have WLOG  \n",
    "\n",
    "$\\mathbf x$  \n",
    "with at least one nonzero component in top half and bottom half   \n",
    "\n",
    "\n",
    "Now select  \n",
    "\n",
    "$Z = \\displaystyle \\left[\\begin{matrix}I & \\mathbf 0\\\\B & I\\end{matrix}\\right]$  \n",
    "where $B=B^T$ and per *ex 11* $Z$ is in the group   \n",
    "such that \n",
    "so  \n",
    "\n",
    "$Z\\mathbf x=\\displaystyle \\left[\\begin{matrix}\\mathbf y^{(U)}\\\\\\mathbf 0\\end{matrix}\\right]$  \n",
    "gives a vector with its bottom half all zero  \n",
    "now call on the lemma, which complete the proof    \n",
    "\n",
    "i.e. we need \n",
    "\n",
    "$B\\mathbf y^{(U)} = - \\mathbf y^{(L)} = -\\mathbf e_1$  \n",
    "\n",
    "as long as the top component of \n",
    "$\\mathbf y^{(U)} $  is not zero, then setting \n",
    "\n",
    "then write $\\mathbf y^{(U)}$ as a linear combination of n dim standard basis vectors, and have $B\\propto \\mathbf e_1\\mathbf e_1^T$  \n",
    "(i.e. all other standard basis vectors are in the kernel of B$)  \n",
    "\n",
    "and that gets the result.... \n",
    "\n",
    "*ugly finishing note:*  \n",
    "\n",
    "if the first component of $\\mathbf y^{(L)}$ is zero then we need another multiplication first by   \n",
    "$\\displaystyle \\left[\\begin{matrix}I & B''\\\\\\mathbf 0 & I\\end{matrix}\\right]$     \n",
    "so that we can be sure that the component is nonzero  \n",
    "\n",
    "**the above argument should be radically streamlined**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8.1.13**  \n",
    "Prove that $SP_{2n}(\\mathbb R)$ is path connected and conclude that every symplectic matrix has determinant 1  \n",
    "\n",
    "\n",
    "\n",
    "**main proof**  \n",
    "the idea is to mimic as closely as we can the approach taken in \"Artin_chp2_SLN_subgroup_generators.ipynb\" and to adapt it to the nuances of the Symplectic group, in particular using a composition of matrices like \n",
    "$\\left[\\begin{matrix}I_n  & B_n\\\\ \\mathbf{0} & I_n\\end{matrix}\\right]$, which by analogy we call type 1 matrices (since they are analogous to elementary type 1 matrices), to reduce $P$ to the identity matrix; these matrices are easily path connected to the identity. We may also use e.g. $\\left[\\begin{matrix}A^T  & \\mathbf 0\\\\ \\mathbf{0} & A^{-1}\\end{matrix}\\right]$ which by analogy we call 'type 3' matrices, **so long as we ensure that** $A$ has positive determinant -- i.e. the chapter 2 results tells us that $A$ is path connected to the identity in such a case (and so is its inverse). Other than the 2x2 Base Case (covered in ex 8.1.3) any time during this recursive proof that we need do use  \n",
    "$\\left[\\begin{matrix}A^T  & \\mathbf 0\\\\ \\mathbf{0} & A^{-1}\\end{matrix}\\right]$  \n",
    "in the event that $\\mathbf A^T$ has negative determinant, we can instead use  \n",
    "\n",
    "$A' := AD $\n",
    "\n",
    "with  \n",
    "$D := \\left[\\begin{matrix}\\mathbf I_{n-1} & 0\\\\0 & -1\\end{matrix}\\right]$  \n",
    "(as in the Lorenz Form proof)  \n",
    "\n",
    "so  \n",
    "$\\left[\\begin{matrix}A^T  & \\mathbf 0\\\\ \\mathbf{0} & A^{-1}\\end{matrix}\\right] \\mapsto \\left[\\begin{matrix}DA^T  & \\mathbf 0\\\\ \\mathbf{0} & DA^{-1}\\end{matrix}\\right]$  \n",
    "\n",
    "and then each block has positive determinant  \n",
    "\n",
    "In some sense this 'kicks the can forward' where we have issues to sort through eventually in the 2x2 case, but that is easily dealt with (ref ex 8.1.3).  \n",
    "\n",
    "\n",
    "\n",
    "**a little care and cleanup needed around A**  \n",
    "\n",
    "\n",
    "*sketch of the proof*  \n",
    "from ex 8.1.12 we know there is some \n",
    "$Q \\in SP_{2n}(\\mathbb R)$  \n",
    "where \n",
    "\n",
    "such that  \n",
    "\n",
    "$B= QP = \\bigg[\\begin{array}{c|c|c|c|c}\n",
    "\\mathbf e_1 & * &\\cdots & * &\\mathbf x\n",
    "\\end{array}\\bigg]$  \n",
    "\n",
    "making use of the lemma, we know  \n",
    "\n",
    "$B^T J B = J$  and in particular  \n",
    "\n",
    "$\\langle \\mathbf b_1 , \\mathbf b_n \\rangle = \\mathbf e_1 ^T B^T J B\\mathbf e_n = \\big(\\mathbf e_1 ^T B^T\\big) J B\\mathbf e_n  = \\big(\\mathbf e_1^T J\\big)\\big(B\\mathbf e_n\\big) = \\langle B^T\\mathbf e_1 , B^T\\mathbf e_n \\rangle  =  \\langle \\mathbf e_1 , \\mathbf e_n \\rangle  = \\mathbf e_n^T  \\mathbf b_n  = \\mathbf e_1^T J\\mathbf e_n = 1$\n",
    "\n",
    "so $b_{n,n}=1$  \n",
    "\n",
    "now by preserving orthogonality with respect to our skew form we know that \n",
    "$\\mathbf b_1 = \\mathbf e_1$  \n",
    "for $k\\neq n$  \n",
    "$\\langle \\mathbf e_1 , \\mathbf b_k \\rangle=\\langle \\mathbf b_1 , \\mathbf b_k \\rangle = 0$  and  \n",
    "so for $k\\in\\{2,3,...,n-1\\}$  \n",
    "we see that  \n",
    "$b_n^{(k)}= \\mathbf e_n^T \\mathbf b_k = 0$   \n",
    "\n",
    "i.e. we now have  \n",
    "$B = \\begin{bmatrix}  1 & ? &\\cdots & ? & ? \\\\ \\mathbf 0 & * &\\cdots & * & * \\\\ 0 & 0 &\\cdots & 0 & 1\\end{bmatrix}$  \n",
    "\n",
    "we now have isolated the top and bottom row and may recurse on a 2n-2 subproblem with blocked multiplication  \n",
    "*fill in the details*   \n",
    "after recursing $n-1$ times this gives us   \n",
    "\n",
    "$B' = I + R$  \n",
    "for some strictly upper triangular matrix $R$  \n",
    "\n",
    "At this point we have accomplished the main goal, since $\\det\\big(B'\\big) =1$ so we've proven path connectivity to a determinant 1 matrix which implies our original matrix has determinant 1.  \n",
    "\n",
    "However for well chosen $A$ we have  \n",
    "\n",
    "$\\left[\\begin{matrix}A^T  & \\mathbf 0\\\\ \\mathbf{0} & A^{-1}\\end{matrix}\\right]B = \\left[\\begin{matrix}I_n  & *\\\\ \\mathbf{0} & *\\end{matrix}\\right]= \\left[\\begin{matrix}I_n  & C\\\\ \\mathbf{0} & I_n\\end{matrix}\\right]$   \n",
    "\n",
    "where $C$ is a real symmetric matrix.  \n",
    "\n",
    "We know first that the bottom right corner must be $I_n$ because if not, we'd have  \n",
    "\n",
    "$\\langle \\mathbf e_k, \\mathbf v_r\\rangle \\neq 0$  \n",
    "for some $k\\in\\{1,2,...,n\\}$ and for some $r\\in\\{n+1,n+2,..., 2n\\}$    \n",
    "\n",
    "\n",
    "The goal is to prove that we know considerably more, in fact that $B'$ is in the form of a type 1 matrix, i.e.  \n",
    "\n",
    "$B' = \\left[\\begin{matrix}I_n & C\\\\\\mathbf 0 & I_n\\end{matrix}\\right]$  \n",
    "for some real $C=C^T$  \n",
    "(where prior problems would have it as real symmetric $C$ though we've overloaded notation unfortunately)  \n",
    "\n",
    "and  \n",
    "$(B')^{-1} = \\left[\\begin{matrix}I_n & -C\\\\\\mathbf 0 & I_n\\end{matrix}\\right] $  \n",
    "where \n",
    "\n",
    "$(B')^{-1}$ is somewhat obviously path connected to the identity i.e. setting    \n",
    "$(B')^{-1}(\\tau) = \\big(1-\\tau\\big) \\cdot \\left[\\begin{matrix}I_n & -C\\\\\\mathbf 0 & I_n\\end{matrix}\\right] + \\tau \\cdot I_{2n} $  \n",
    "for $\\tau\\in[0,1]$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**remark:**  \n",
    "Showing path connectivity via generators is something of an interesting side goal.  If we take the above approach, what we've is that any matrix in the symplectic group may be reduced to the identity by application of finitely many matrices from ex 8.1.11, each having determinant 1, so our matrix necessarily has determinant 1 as well for algebraic reasons -- and this algebraic approach then tells us that $P \\in SP_{2n}\\big(\\mathbb F\\big)\\to \\det\\big(P\\big)=1$   implies that the result holds over any field.  \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a shorter look at path connectivity  \n",
    "consider $P \\in SP_{2n}\\big(\\mathbb R\\big)$\n",
    "\n",
    "$B(\\tau) :=  \\sqrt{\\big(1-\\tau\\big)} \\cdot P  + \\sqrt{\\tau} \\cdot I_{2n} $  \n",
    "for $\\tau \\in[0,1]$  \n",
    "\n",
    "another problem with this would be verifying that it is non-singular over the path  \n",
    "\n",
    "and  \n",
    "$B(\\tau)^TJ B(\\tau) = J$  \n",
    "clearly holds for $\\tau \\in\\{0,1\\}$.  Now specialize to $\\tau \\in (0,1)$  \n",
    "\n",
    "we now wish to prove  \n",
    "$J = B(\\tau)^TJ B(\\tau) = \\big(1-\\tau\\big) \\cdot P^TJP  +{\\tau} J +  \\sqrt{\\tau \\big(1-\\tau\\big)} \\cdot P^T J +    \\sqrt{\\tau \\big(1-\\tau\\big)}JP  =  J +  \\sqrt{\\tau \\big(1-\\tau\\big)} \\cdot P^T J +    \\sqrt{\\tau \\big(1-\\tau\\big)}JP $  \n",
    "\n",
    "i.e. that  \n",
    "$\\sqrt{\\tau \\big(1-\\tau\\big)} \\cdot P^T J +    \\sqrt{\\tau \\big(1-\\tau\\big)}JP = \\mathbf 0\\longrightarrow \\mathbf 0 =  P^T J + JP = P^T J + \\big(P^T J\\big)^T$    \n",
    "i.e. we need to prove that \n",
    "$P^T J$  is skew symmetric \n",
    "\n",
    "# BROKEN\n",
    "if lower bound is met with equality... i think that would do it... need a think to show not symmetric  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\Big\\vert\\text{trace}\\big( (P^T J P^T J)^T(P^T J P^T J)\\big)\\Big\\vert$    \n",
    "$= \\Big\\vert\\text{trace}\\big(  J^T P J^TP P^T J P^T J\\big)\\Big\\vert$    \n",
    "$= \\Big\\vert\\text{trace}\\big(   J^TP P^T J P^T P\\big)\\Big\\vert$    \n",
    "$= \\Big\\vert\\text{trace}\\big(   JP P^T J P^T P\\big)\\Big\\vert$    \n",
    "$= \\Big\\vert\\text{trace}\\big(   J^TP J P^{-1} P^T P\\big)\\Big\\vert$    \n",
    "$= \\Big\\vert\\text{trace}\\big(  P^{-T} P^{-1} P^T P\\big)\\Big\\vert$    \n",
    "$= \\Big\\vert\\text{trace}\\big(  (P^{-T} P^{-1})^T (P^T P)\\big)\\Big\\vert$    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\big\\Vert P^T J\\big\\Vert_F^2 = \\text{trace}\\Big( J^TPP^TJ\\Big)= \\text{trace}\\Big( PP^T\\Big)\\geq \\Big\\vert\\text{trace}\\big( P^T J P^T J\\big)\\Big\\vert = \\Big\\vert\\text{trace}\\big(-P^{-1}  P^T \\big)\\Big\\vert = \\Big\\vert\\text{trace}\\big(ZU^{-1} P^{-1}  P^T \\big)\\Big\\vert$     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "can't we directly show commuting with conjugate transpose, i.e \n",
    "\n",
    "\n",
    "$(P^TJ)^T(P^TJ) = J^TP(P^TJ) = (J^T P J)(J^TP^TJ)= P^{-T}P^{-1}$  \n",
    "\n",
    "\n",
    "$P = UZ$  \n",
    "polar decomposition  \n",
    "\n",
    "$P^{T}= ZU^{-1}$  \n",
    "$P^{-T}= U Z^{-1}$  \n",
    "  \n",
    "$P^{-1} = Z^{-1}U^{-1}$  \n",
    "\n",
    "or maybe not ?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$P^TJP = J\\longrightarrow JP = P^{-T}J\\longrightarrow -P^T J = -JP^{-1}$  \n",
    "$P^TJ = JP^{-1}$  \n",
    "$J^TP^TJ = P^{-1}$  \n",
    "and  \n",
    "$PJP^T = J$  \n",
    "$PJ = JP^{-T}$  \n",
    "$J^TPJ = P^{-T}$  \n",
    "\n",
    "\n",
    "since $P^T$ is (orthogonally) similar to $P^{-1}$  and $P$ and $P^T$ have the same eigenvalues... it immediately follows that $P$ has a positive determinant -- so long as we can be sure that $-1$ is not an eigenvalue of $P$ (and/or equivalently +1 is not an eigenvalue of P given negation effects) but a block diagonal matrix with involutive $D$ on diagonals is in the group   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "trying to apply this inequality:  \n",
    "$\\text{trace}\\Big(\\big(\\mathbf A^H \\mathbf A\\big)^2 \\Big) = \\text{trace}\\Big(\\big(\\mathbf A \\mathbf A^H\\big)^2 \\Big)^\\frac{1}{2} \\text{trace}\\Big(\\big(\\mathbf A^H \\mathbf A\\big)^2 \\Big)^\\frac{1}{2}  \\geq  \\text{trace}\\Big(\\big(\\mathbf A^2\\big)^H \\big(\\mathbf A^2\\big)  \\Big) = \\text{trace}\\Big(\\big(\\mathbf A^H \\mathbf A\\big)^H\\big( \\mathbf A\\mathbf A^H\\big)  \\Big)$  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8.3.6**  \n",
    "extend the homorphism from  $\\Phi: SU_2\\longrightarrow SO_3$  developed in the chapter to    \n",
    "$\\Phi: U_2\\longrightarrow SO_3$  \n",
    "\n",
    "for $V \\in U_2$  with $\\det\\big(V\\big) = \\beta$ on the unit circle  \n",
    "select $\\alpha = e^{i\\theta}$ for $\\theta \\in [0,\\pi)$\n",
    "such that $\\alpha^2 \\cdot \\beta=1$ and of course  \n",
    "$\\det\\big(\\alpha I \\big)= \\det\\big(\\alpha I_2 \\big)=\\alpha^2$  \n",
    "(i.e. $\\alpha$ is in the upper half of the unit circle)  \n",
    "\n",
    "thus we have  \n",
    "$\\big(\\alpha I V \\big)= Q \\in SU_2$  \n",
    "so we define $\\Phi$ such that \n",
    "$\\phi\\big(\\alpha I  \\big) = I_3$ and   \n",
    "\n",
    "$\\phi\\big(Q\\big)$ acts as before thus  \n",
    "\n",
    "$\\phi\\big(Q\\big) = \\big(\\alpha I V \\big) = \\phi\\big(\\alpha I\\big)\\phi\\big( V \\big) = \\phi\\big( V \\big)$  \n",
    "for arbitrary $V\\in U_2$  \n",
    "\n",
    "in the case of $SU_2$ we had the kernel of $\\phi$ given by $\\{-I,I\\}$  \n",
    "now we have the kernel as given by   $\\{-\\alpha I, \\alpha I\\}$ for arbitrary in the upper half unit circle, and its negative is in complementary lower half unit circle, thus the kernel is   \n",
    "$\\{\\gamma I\\}$ for arbitrary $\\gamma$ on the unit circle   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8.4.3**  \n",
    "$P \\in SO_3\\big(\\mathbb C\\big)$  \n",
    "*(a)* prove 1 is an eigenvalue  \n",
    "$P^T P = I \\longrightarrow P^T = P^{-1}$  \n",
    "thus $P^{-1}$ has same charpoly as $P^T$ which has same charpoly as $P$  \n",
    "\n",
    "so if $\\lambda_2 \\neq \\lambda_2^{-1}$ is and eigenvalue of $P$, then $\\lambda_2$ is an eigenvalue of $P^{-1}$ and $\\lambda_2^{-1}$ is an eigenvalue of $P$. Thus  \n",
    "$1=\\det\\big(P\\big) = \\lambda_1 \\cdot \\lambda_2 \\cdot \\lambda_3 =  \\lambda_1 \\cdot \\lambda_2 \\cdot \\lambda_2^{-1} = \\lambda_1$  \n",
    "\n",
    "to deal with the special case of involutions (no work needed for eigenvalue of 1), consider if $\\lambda_1=-1$ is an eigenvalue, then the product of the remaining two eigenvalues must be $\\lambda_2 \\cdot \\lambda_3 =-1$ given that the determinant is 1.   But the above argument tells us that if $\\lambda_2 \\not \\in \\{-1,1\\}$ then $\\lambda_2 \\cdot \\lambda_3 = \\lambda_2 \\cdot \\lambda_2^{-1}=1 \\longrightarrow \\lambda_2 \\in \\{-1,1\\}$ and thus one of $\\lambda_2$ and $\\lambda_3$ must be -1 and the other 1.    \n",
    "\n",
    "\n",
    "*(b)*  let $\\mathbf x_1$, $\\mathbf x_2$ be eigenvectors with $\\lambda_1,\\lambda_2$ respectively.  Prove that $\\mathbf x_1^T\\mathbf x_2 =0$ unless $\\lambda_1 = \\lambda_2^{-1}$   \n",
    "(the spirit of the approach to b and c is that taken in ex 7.5.14)  \n",
    "\n",
    "*note:*  \n",
    "*in the spirit of chapter 7, for non-involutive eigenvalues, this has a similar structural feel to the proof involving the symplectic form.  However it does not seem that $P$ must be diagonalizable.*    \n",
    "\n",
    "we introduce the *symmetric bilinear form* given by the dot product  (where we implicitly have chosen the standard basis)  \n",
    "$\\langle \\mathbf v, \\mathbf w\\rangle = \\mathbf v^T  P^TP \\mathbf w = \\mathbf v^T \\mathbf w$   \n",
    "\n",
    "for (b) then  \n",
    "$\\langle \\mathbf x_1, \\mathbf x_2\\rangle = \\lambda_1\\lambda_2  \\mathbf v^T \\mathbf w = \\mathbf v^T \\mathbf w$  \n",
    "so if  $\\mathbf v^T \\mathbf w = 0$ then $\\lambda_1\\lambda_2 = 1\\longrightarrow \\lambda_1 = \\lambda_2^{-1}$  \n",
    "(note that all eigenvalues are necessarily nonzero since $P$ is invertible)   \n",
    "\n",
    "*(c)*  Prove that that for the eigenpair $\\mathbf x_1, \\lambda_1=1$, if $P\\neq I$ then $\\mathbf x_1^T \\mathbf x_1 \\neq 0$  i.e. the the eigenvector is not self orthogonal  \n",
    "*remark:* as of chapter 8, your author does not know the proof for the case where $P$ has minimal polynomial  $\\big(P-I\\big)^3$ i.e. a maximally defective case.  \n",
    "\n",
    "we prove this in two stages\n",
    "\n",
    "(i) consider the easy case where $P$ is diagonalizable  \n",
    "if all eigenvalues were the same we'd have $P=I$ and $\\mathbf x = \\mathbf e_1 + \\mathbf e_2$ would be a self-orthogonal eigenvector, which is accordingly carved out of the problem\n",
    "\n",
    "if there are 2 or 3 distinct eigenvalues, in either case this implies that $\\lambda_1=1$ has algebraic multiplicity 1 (check the determinant)  \n",
    "\n",
    "so collect all 3 eigenvectors as columns in a matrix $X$ and consider since $X$ is invertible    \n",
    "\n",
    "$3 = \\text{rank}\\big(X^T X\\big)$  \n",
    "\n",
    "and computing the result:  \n",
    "\n",
    "$X^TX = X^TP^TPX = \\begin{bmatrix}  \\langle \\mathbf x_1, \\mathbf x_1\\rangle &\\langle \\mathbf x_2, \\mathbf x_1\\rangle &\\langle \\mathbf x_3, \\mathbf x_1\\rangle\\\\  \\langle \\mathbf x_1, \\mathbf x_2\\rangle &\\langle \\mathbf x_2, \\mathbf x_2\\rangle & \\langle \\mathbf x_3, \\mathbf x_2\\rangle \\\\ \\langle \\mathbf x_1, \\mathbf x_3\\rangle & \\langle \\mathbf x_2, \\mathbf x_3\\rangle & \\langle \\mathbf x_3, \\mathbf x_3\\rangle\\end{bmatrix} =  \\begin{bmatrix}  \\langle \\mathbf x_1, \\mathbf x_1\\rangle &0&0\\\\ 0 &\\langle \\mathbf x_2, \\mathbf x_2\\rangle & \\langle \\mathbf x_3, \\mathbf x_2\\rangle \\\\ 0 & \\langle \\mathbf x_2, \\mathbf x_3\\rangle & \\langle \\mathbf x_3, \\mathbf x_3\\rangle\\end{bmatrix}$    \n",
    "\n",
    "now if $\\langle \\mathbf x_1, \\mathbf x_1\\rangle =0$, then we'd have the first column be all zeros, so rank at most 2, and  \n",
    "$3 =\\text{rank}\\big(X^T X\\big) \\leq 2$  \n",
    "which would be a contradiction  \n",
    "\n",
    "(note: this is equivalent to the concept of a null vector from chapter 7-- the set $\\{\\mathbf x_1, \\mathbf x_2, \\mathbf x_3\\}$ forms a basis, and by (b) we know $\\mathbf x_1$ is orthogonal to the 2nd and 3rd of these, so if it was self orthogonal it would be orthogonal to all linear combinations of these -- i.e. everything in the vector space and hence a null vector, but the identity matrix is non-singular so we'd have $\\mathbf x_1 = \\mathbf 0$ which is a contradiction)   \n",
    "\n",
    "thus we know in the case of diagonalizable $P$ that $\\mathbf x_1^T\\mathbf x_1 \\neq 0$  \n",
    "\n",
    "\n",
    "(ii)    \n",
    "we now address the nuisance case of eigenvalues $\\{1,-1,-1\\}$ associated with defective matrices  \n",
    "\n",
    "\n",
    "Reasoning by Jordan Forms, or instead by Cayley Hamilton and Sylvester Rank Inequality (see Artin_chp4.ipynb) we have, where we use the characteristic polynomial $f$ (since $p$ looks too much like our matrix $P$)    \n",
    "\n",
    "\n",
    "$3$  \n",
    "$= \\dim \\ker\\Big(\\big(P-  I\\big) + \\dim \\ker\\Big(\\big(P +I\\big)^2\\Big)$  \n",
    "$\\geq\\dim \\ker\\Big(\\big(P- (+1)I\\big)\\big(P - (-1)I\\big)^2\\Big)$  \n",
    "$=\\dim \\ker\\Big(f\\big(P\\big)\\Big)$  \n",
    "$=\\dim \\ker\\Big(\\mathbf 0\\Big) $  \n",
    "$=3$  \n",
    "\n",
    "and by assumption we know for $\\lambda_1=1$ that  \n",
    "$\\dim \\ker\\Big(\\big(P- I\\big)\\Big)=1$ thus  \n",
    "$\\dim \\ker\\Big(\\big(P- (-1) I\\big)^2\\Big) = \\dim \\ker\\Big(\\big(P + I\\big)^2\\Big) = 2$  \n",
    "so its kernel is spanned by 2 linearly independent vectors $\\{\\mathbf x_2,\\mathbf x_3\\}$  \n",
    "\n",
    "since  \n",
    "$\\big(P + I\\big)^2\\mathbf x_1 = \\big(P^2 +2P + I\\big)\\mathbf x_1 = 4\\mathbf x_1$  \n",
    " $\\{\\mathbf x_1, \\mathbf x_2,\\mathbf x_3\\}$  forms a basis for our 3d space  \n",
    " \n",
    "$P$ being invertible tells us  \n",
    "\n",
    "$\\Big(\\big(P + I\\big)^2\\Big)\\mathbf x_j=\\mathbf 0 \\longrightarrow \\big(P + 2I + P^T\\big)\\mathbf x_j = P^T\\Big(\\big(P + I\\big)^2\\Big)\\mathbf x_j =\\mathbf 0$  \n",
    "for $j\\in\\{2,3\\}$  re-arranging terms   \n",
    "\n",
    "$\\big(P +P^T\\big)\\mathbf x_j = -2\\mathbf x_j$  \n",
    "but this implies  \n",
    "\n",
    "$2\\mathbf x_1^T\\mathbf x_j = \\mathbf x_1^T\\big(P +P^T\\big)\\mathbf x_j =  -2\\mathbf x_1^T\\mathbf x_j \\longrightarrow \\langle \\mathbf x_1, \\mathbf x_j\\rangle =0 $  \n",
    "\n",
    "and for the same reason as in the diagonalizable case this implies  \n",
    "$\\langle \\mathbf x_1, \\mathbf x_1\\rangle \\neq 0$  \n",
    "\n",
    "note:  \n",
    "this is actually part of a very general result over any field-- if an eigenvalue is simple, then it's left and right eigenvector with respect to the dot product cannot be orthogonal. See e.g.   \n",
    "\"simple_eigs_nonorthogonal_eigenvectors_wrt_dotproduct.ipynb\"   \n",
    "for a bilinear forms inspired approach.  For a succinct proof, consider using the triangularization process from chapter 4, for some simple $\\lambda$ that exists in our field, which has a left eigenvector $\\mathbf w_1$ and a right eigenvector $\\mathbf x_1$. Then we have  \n",
    "\n",
    "$\\bigg[\\begin{array}{c|c}\n",
    "\\mathbf x_1 &\\mathbf x_2\n",
    "\\end{array}\\bigg]$  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A more succinct argument:  \n",
    "left and right eigenvectors associated with a simple eigenvalue are not orthogonal with respect to the dot product**   \n",
    "\n",
    "suppose $\\lambda_1 $ is simple.  When $P$ is an n x n matrix  \n",
    "\n",
    "\n",
    "$\\mathbf 0 = p\\big(P\\big) = \\big(I-P\\big)q\\big(P\\big)$  \n",
    "where $q$ is a polynomial that annihilates 'the rest of' $P$ but doesn't have 1 as an eigenvalue -- this follows  from Cayley Hamilton and factoring the characteristic polynomial.  \n",
    "\n",
    "now consider that $1=\\text{rank}\\Big(q\\big(P\\big)\\Big)=\\text{rank}\\Big(q\\big(P\\big)^2\\Big)$   \n",
    "for the first part, that $1=\\text{rank}\\Big(q\\big(P\\big)\\Big)$, consider e.g. triangularizing the matrix, or using  Sylvester's Rank Inequality.  \n",
    "\n",
    "For the second part, i.e. rank stability:  it must be so otherwise a polynomial without eigenvalue $\\lambda_1$ would annihilate $P$ but $q\\big(P\\big)^2\\mathbf x_1 = q\\big(\\lambda_1\\big)^2\\mathbf x_1 \\neq \\mathbf 0$ because $\\lambda_1$ is not a root of $q\\big(z\\big)^2$) and per chp 4 notes, over than the zero vector, there is no intersection between the image and kernel of $q\\big(P\\big)$ -- i.e. our vector space is a direct sum of image and kernel of $q\\big(P\\big)$. \n",
    "\n",
    "\n",
    "Now create invertible matrix $B$ that has the n-1 vectors in the kernel of $q\\big(P\\big)$ and also $\\mathbf x_1$ which is in its image, with eigenvalue \n",
    "$\\lambda_1' =  q\\big(\\lambda_1\\big)\\neq 0$  \n",
    "and now consider the left eigenvector given by $\\mathbf w_1^T$, so  \n",
    "\n",
    "$B = \\bigg[\\begin{array}{c|c|c|c}\n",
    "\\mathbf x_1 &\\mathbf b_2& ... &\\mathbf b_n\n",
    "\\end{array}\\bigg]$  \n",
    "\n",
    "$\\lambda_1' \\mathbf w_1^T B =\\Big(\\mathbf w_1^T q\\big(P\\big)\\Big) B =\\mathbf w_1^T q\\big(P\\big) B =\\mathbf w_1^T \\Big(q\\big(P\\big) B\\Big)$  \n",
    "\n",
    "where  \n",
    "\n",
    "$\\Big(q\\big(P\\big) B\\Big) = \\bigg[\\begin{array}{c|c|c|c}\n",
    "\\lambda_1' \\mathbf x_1 &\\mathbf 0& ... &\\mathbf 0\n",
    "\\end{array}\\bigg]\\longrightarrow \\lambda_1' \\mathbf x_1^T B =  \\mathbf x_1^T \\Big(q\\big(P\\big) B\\Big) = \\bigg[\\begin{array}{c|c|c|c}\n",
    "\\lambda_1' \\mathbf x_1^T\\mathbf x_1 & 0 & ... & 0\n",
    "\\end{array}\\bigg]$  \n",
    "\n",
    "interpreting $\\mathbf w_1$ as an $n \\times 1$ matrix, and since $B^{-1}$ exists and $\\lambda_1'\\neq 0$, we have  \n",
    "$\\text{rank}\\Big(\\lambda_1' \\mathbf w_1^T B \\Big) = \\text{rank}\\Big( \\mathbf w_1^T B\\Big)= \\text{rank}\\Big( \\mathbf w_1^T \\Big)=1$  \n",
    "thus $\\mathbf x_1^T \\mathbf x_1 \\neq 0$  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8.4.4**  \n",
    "\n",
    "$G=SO_3\\big(\\mathbb C\\big)$  \n",
    "\n",
    "*(a)* Prove that left multiplication by $G$ is a transitive operation on the set of vectors $\\mathbf x$ such that $\\mathbf x^T\\mathbf x =1$   \n",
    "\n",
    "note $g\\in G$ given by matrix $P$, satisfies $P^TP = PP^T = I$ so each row and column has 'length' one and is orthogonal with respect to the bilinear form given by the dot product.  Thus  \n",
    "\n",
    "Let $P$ have $\\mathbf x$ as its first row, then  \n",
    "\n",
    "$P^T \\mathbf x = \\mathbf e_1$  \n",
    "\n",
    "and for some $\\mathbf y$ such that $\\mathbf y^T\\mathbf y = 1$, let  \n",
    "\n",
    "$P'$ have $\\mathbf y$ as its first column.  \n",
    "\n",
    "then \n",
    "\n",
    "$P'P^T\\mathbf x = \\mathbf y$  \n",
    "\n",
    "so the operation is transitive \n",
    "\n",
    "*remark: how can we be sure that such a P and P' exist?*  \n",
    "revisiting chapter 7 notes as needed, \n",
    "we may construct such matrices by assigning the first column to be $\\mathbf x$, and $\\mathbf y$ respectively. Then extend to a basis in each case -- we focus on the first case for concreteness.  Now run Gram Schmidt with respect to the second linearly independent vector and $\\mathbf x$. (Gram Schmidt preserves linear independence -- see chapter 7 notes.)  The result is $\\mathbf x_2$ such that $\\mathbf x^T\\mathbf x_2 = 0$ but necessarily  $\\mathbf x_2^T\\mathbf x_2 \\neq 0$ (and we rescale so that it equals one) --- we will see why momentarily.  Now select the final linearly independent vector and run Gram Schmidt to get $\\mathbf x_3$  (then rescale at the end to be 1) \n",
    "\n",
    "$P= \\bigg[\\begin{array}{c|c|c|c|c}\n",
    "\\mathbf x_1 & \\mathbf x_2 &\\mathbf x_3\n",
    "\\end{array}\\bigg]$   \n",
    "\n",
    "and $\\text{rank}\\big(P\\big) = \\text{rank}\\big(P^TP\\big) = 3$   \n",
    "\n",
    "but even prior to normalization\n",
    "$P^TP = D$  \n",
    "for some diagonal matrix $D$, so each diagonal element must be non zero, i.e. $\\langle \\mathbf x_k, \\mathbf x_k\\rangle\\neq 0$   \n",
    "\n",
    "**it may be better to take this as a corollary to part c, where the first standard basis vector is reachable from application of two Householder matrices -- the first one getting to either $\\mathbf e_1$ or $-\\mathbf e_1$ then the second one being a 'dummy' reflection in the former case (no impact) or an upper reflection (mapping to e_1 proper) in the second case.  2 Householders then give a matrix that is complex orthogonal with determinant 1 so in the group.**    \n",
    "\n",
    "*(b)*  Describe the stabilizer for $\\mathbf e_1$ by left multiplication of G  \n",
    "The stabilizer is all matrices $P$ where the first column of $P$ is the first standard basis vector.  As noted in the below writeup, this implies a block upper triangular structure which has a block upper triangular inverse (ref 'Artin_chp2_misc_items.ipynb') but the inverse is given by the transpose, so the transpose is block upper triangular as well, i.e. the first row of the stabilizer is necessarily $\\mathbf e_1^T$.  \n",
    "\n",
    "*(c)*   \n",
    "Prove that $G$ is path-connected  \n",
    "\n",
    "in particular while complex, select $\\mathbf x$ such that $\\mathbf x^T\\mathbf x = 1$ and  \n",
    "$\\mathbf P =  I - 2\\mathbf {xx}^T$  \n",
    "i.e. select as a generator a $P$ that is Householder-esq   \n",
    "then $P$ is diagonalizable with trace of $n-2$ as before, and $n-1$ eigs of +1 and 1 of of -1, so determinant is -1 as before and  \n",
    "\n",
    "$\\mathbf P^2 = I$ but $\\mathbf P=\\mathbf P^T$ so $\\mathbf P^T\\mathbf P=I$   \n",
    "so we can proceed to mimic \n",
    "*ex 7.3.8*    \n",
    "(a) is above,  \n",
    "(b) is let $\\mathbf w$ be orthogonal with respect to our bilinear form called the dot product   \n",
    "(c) proceeds as before... the one tricky item becomes 'normalization' with respect to our bilinear form.  Can we ensure  \n",
    "$\\langle \\mathbf x - \\mathbf y, \\mathbf x - \\mathbf y\\rangle\\neq 0$  ?  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the idea as before is to show that we may solve for  \n",
    "\n",
    "$\\mathbf y = P^{(1)} \\mathbf x $  \n",
    "where   \n",
    "$ 1= \\mathbf y^T\\mathbf y = \\mathbf x^T\\mathbf P^T \\mathbf P \\mathbf x=\\mathbf x^T\\mathbf x = 1$  \n",
    "\n",
    "However since are only using a bilinear form, there is a pernicious case we need to address.  \n",
    "\n",
    "It is possible that while $\\mathbf x$ and $\\mathbf y$ are linearly independent with 'length' 1 (with respect to our symmetric bilinear form given by the dot product) that $\\mathbf x -\\mathbf y$ is self orthogonal, with 'length' 0.  \n",
    "\n",
    "To see this, consider  \n",
    "$\\langle \\mathbf x-\\mathbf y,\\mathbf x-\\mathbf y\\rangle = \\langle \\mathbf x,\\mathbf x-\\mathbf y\\rangle - \\langle \\mathbf y,\\mathbf x-\\mathbf y\\rangle= \\langle \\mathbf x,\\mathbf x\\rangle - \\langle \\mathbf x,\\mathbf y\\rangle - \\langle \\mathbf y,\\mathbf x\\rangle +\\langle \\mathbf y,\\mathbf y\\rangle= \\langle \\mathbf x,\\mathbf x\\rangle +  \\langle \\mathbf y,\\mathbf y\\rangle - 2\\langle \\mathbf x,\\mathbf y\\rangle = 2-2 \\langle \\mathbf x,\\mathbf y\\rangle$  \n",
    "with equality *iff*  $\\langle \\mathbf x,\\mathbf y\\rangle = 1$   \n",
    "\n",
    "- - - -  \n",
    "for a very explicit and relevant example of this consider  \n",
    "$\\mathbf y := \\mathbf e_1$  \n",
    "\n",
    "$\\mathbf v := \\mathbf x - \\mathbf y= \\mathbf x - \\mathbf e_1$   then   \n",
    "$\\mathbf v^T \\mathbf v $  \n",
    "$= v_1^2 + \\sum_{k=2}^n v_k^2$  \n",
    "$= v_1^2 + \\sum_{k=2}^n x_k^2$  \n",
    "$= \\big(x_1-1)^2 + \\sum_{k=2}^n x_k^2$  \n",
    "$=  - 2x_1 + 1 + \\big(x_1^2 + \\sum_{k=2}^n x_k^2\\big)$   \n",
    "$=  - 2x_1 + 1 + \\mathbf x^T\\mathbf x$   \n",
    "$=  - 2x_1 + 1 + 1$   \n",
    "$=  - 2x_1 + 2$   \n",
    "\n",
    "thus  \n",
    "$\\mathbf v^T \\mathbf v = 0 $  \n",
    "*iff* $x_1 = 1$  \n",
    "- - - -   \n",
    "\n",
    "thus we need to relax the approach to show that for any $\\mathbf x^T\\mathbf x=1$ and $\\mathbf y^T\\mathbf y= 1$ we may find a Householder type matrix in $O_n\\big(\\mathbb C\\big)$ such that  \n",
    "\n",
    "for some $\\alpha \\in\\{-1,1\\}$  \n",
    "$\\alpha\\mathbf y = P^{(1)} \\mathbf x $ or equivalently $\\mathbf y = P^{(1)} \\big(\\alpha\\mathbf x\\big) $  \n",
    "\n",
    "\n",
    "this gives the net result that  we can alway 'normalize' \n",
    "$\\mathbf w \\propto \\big(\\mathbf x - \\alpha\\mathbf y\\big)$   \n",
    "i.e. try  \n",
    "$\\mathbf w \\propto \\big(\\mathbf x - \\mathbf y\\big)$   \n",
    "and if that is self orthogonal then we know  \n",
    "$\\mathbf w \\propto \\big(\\mathbf x - (-\\mathbf y)\\big)$   \n",
    "will work, so    \n",
    "$\\alpha\\mathbf y = P^{(1)} \\mathbf x $ \n",
    "for some  $\\alpha \\in\\{-1,1\\}$  \n",
    "\n",
    "to be explicit: we know they both cannot be self orthogonal because:   \n",
    "\n",
    "$\\langle \\mathbf x+\\mathbf y, \\mathbf x+\\mathbf y\\rangle = \\mathbf x^T\\mathbf x + \\mathbf y^T\\mathbf y + 2 \\mathbf x^T\\mathbf y $  \n",
    "and  \n",
    "$\\langle \\mathbf x -\\mathbf y, \\mathbf x -\\mathbf y\\rangle = \\mathbf x^T\\mathbf x + \\mathbf y^T\\mathbf y + -2  \\mathbf x^T\\mathbf y $  \n",
    "\n",
    "the former is zero *iff*   $\\mathbf x^T\\mathbf y  = -1$ and the latter is zero *iff*  $\\mathbf x^T\\mathbf y =1$.  These are mutually exclusive possibilities.  \n",
    "\n",
    "\n",
    "The below writeup applies for any natural number $n\\geq 3$  \n",
    "(path connectivity is not clear by these means for $n=2$ though we can deal with this case directly by examining the class equations, which we do after proving the $n\\geq 3$ case)    \n",
    "\n",
    "Applying these Householder type matrices allows us to reduce our matrix in $SO_n\\big(\\mathbb C\\big) \\to D \\in SO_n\\big(\\mathbb R\\big)$, for some diagonal matrix $D$.   \n",
    "\n",
    "This gives a different view to the proof.  $SO_n\\big(\\mathbb C\\big)$ is difficult but through application of at most $n-1$ Householder matrices if n is odd, and at most $n-2$ Householder matrices if $n$ is even, we can reduce our matrix $ Q \\in SO_n\\big(\\mathbb C\\big)$ to a nicer matrix in the subgroup $ \\in SO_n\\big(\\mathbb R\\big)$ which we know is path connected (in \"Artin_chp8.ipynb\" see either ex 7.3.9 corollary or the additional remark under ex 7.9.6).  Formally the Householder type matrices are in  $O_n\\big(\\mathbb C\\big)$   and not  $SO_n\\big(\\mathbb C\\big)$ so it may be prudent to start by considering the group  $O_n\\big(\\mathbb C\\big)$ and then use associativity to focus later on the subgroup \n",
    "\n",
    "mimicking the approach in ex 7.3.9 we see  \n",
    "\n",
    "$\\mathbf P^{(1)}Q= \\begin{bmatrix} \\alpha_1 & * \\\\ \\mathbf 0 &  Q^{(2)} \\end{bmatrix}$  \n",
    "where $\\alpha_i \\in\\{-1,1\\}$   \n",
    "as mentioned in part b of this exercise\n",
    "\n",
    "the LHS $\\in O_n\\big(\\mathbb C\\big)$, so the RHS is as well  \n",
    "$\\begin{bmatrix} \\alpha_1 & * \\\\ \\mathbf 0 &  Q^{(2)} \\end{bmatrix}^T = \\begin{bmatrix} \\alpha_1 & * \\\\ \\mathbf 0 &  Q^{(2)} \\end{bmatrix}^{-1}\\longrightarrow \\begin{bmatrix} \\alpha_1 & * \\\\ \\mathbf 0 &  Q^{(2)} \\end{bmatrix} = \\begin{bmatrix} \\alpha_1 & \\mathbf 0^T \\\\ \\mathbf 0 &  Q^{(2)} \\end{bmatrix}$  \n",
    "because the inverse of a block upper triangular matrix is block upper triangular (see e.g. Artin_chp2_misc_items.ipynb) and a block upper triangular matrix's transpose is block upper triangular *iff* said matrix is block diagonal.  Up to rescaling by $\\alpha$ we see the form of the stabilizer for matrices in this complex special orthogonal group.  \n",
    "\n",
    "from here we mimic the argument as before and apply iteration or induction, though we stop 'one step short'   \n",
    "\n",
    "mimicking the earlier argument with blocked multiplication and induction/iteration  \n",
    "\n",
    "if $n$ is odd   \n",
    "$\\mathbf P^{(k)}...\\mathbf P^{(2)}\\mathbf P^{(1)} Q =  D$  \n",
    "where $k\\leq n-1$  and $D$ is diagonal with $d_{i,i}=\\alpha_i$, i.e $D^2 =I$  \n",
    "and we may assume WLOG that $k$ is even -- if it is not, mutliply each side by  \n",
    "$\\mathbf P^{(r)} =\\begin{bmatrix} \\mathbf I_{n-1} & \\mathbf 0_{n-1} \\\\  \\mathbf 0_{n-1}^T & -1 \\end{bmatrix}$  \n",
    "\n",
    "and we now have $k$ Householder matrices where $k\\leq n-1$ along with diagonal involutive $D$, and since $k$ is even we know the LHS has determinant 1, thus $\\det\\big(D\\big) =1$ i.e. $D\\in SO_n\\big(\\mathbb R\\big)$   \n",
    "\n",
    "Thus  \n",
    "$D\\mathbf P^{(k)}...\\mathbf P^{(2)}\\mathbf P^{(1)} =  Q^T$  \n",
    "or   \n",
    "$\\big(D\\mathbf P^{(k)}...\\mathbf P^{(2)}\\mathbf P^{(1)}\\big)^T =  Q$  \n",
    "\n",
    "The first expression is cleaner so we work with that for $Q^T$, and of course showing an arbitrary $Q^T \\in SO_n\\big(\\mathbb C\\big)$ is path connected (to the identity) shows any matrix in the group is path connected to the identity.  \n",
    "\n",
    "now $D \\in SO_n\\big(\\mathbb R\\big)$ is path connected to the identity by our prior work in chapter 7. And since $k$ is even we merely need to consider 2 Householder matrices at a time and show they are path connected to the identity.    \n",
    "\n",
    "as in the chp 7 notes we consider  \n",
    "some odd $i$ and $j=i+1$ (i.e. consecutive Householder matrices)  \n",
    "\n",
    "first note that if  \n",
    "$\\mathbf w_i \\propto \\mathbf w_j$  \n",
    "then this implies $\\mathbf w_i =\\alpha \\mathbf w_j$, $\\alpha \\in\\{-1,1\\}$  again by preserving the dot product  \n",
    "and then $\\mathbf P^{(i)}\\mathbf P^{(i+1)} =I$ an involution.  So there is nothing to do in this case.  Now suppose we the vectors are linearly independent, i.e. consider  \n",
    "\n",
    "$W = \\bigg[\\begin{array}{c|c}\n",
    "\\mathbf w_i & \\mathbf w_{i+1}\n",
    "\\end{array}\\bigg]$  \n",
    "\n",
    "and  \n",
    "$W^TW =  \\begin{bmatrix} 1 & c \\\\ c &  1 \\end{bmatrix}$  \n",
    "\n",
    "$\\text{rank}\\big(W^TW \\big) =2$  unless $\\mathbf w_i^T\\mathbf w_{i+1} = c \\in \\{-1,1\\}$  \n",
    "\n",
    "*we proceed by assuming* $c \\not \\in \\{-1,1\\}$ and **we deal with this nuisance case later**  \n",
    "\n",
    "so $\\text{rank}\\big(WW^T\\big)=\\text{rank}\\big(W^T\\big) =\\text{rank}\\big(W^TW\\big) =2$  and by rank-nullity there is some is some non-zero $\\mathbf z$ in the kernel of $WW^T$  \n",
    "\n",
    "we know that $\\text{rank}\\big(WW^T\\big) = \\text{rank}\\big(W^T\\big)=2 = \\text{rank}\\big(I_2\\big)$ because $W$ has a left inverse and $W^T$ has a right inverse , ref ex 4.Misc.11 and ex 4.Misc.12. This is the equality case of the chapter 4 notes' \"guiding inequality\" $\\text{rank}\\big(WW^T\\big)\\leq \\text{rank}\\big(W\\big)$ thus we know $\\text{span}\\big(WW^T\\big)= \\text{span}\\big(W\\big)$.  Since $W$ is in injective $WW^T\\mathbf x = \\mathbf 0$ *iff* $W^T \\mathbf x = 0$.  So we have a goal of showing a partition in the image and kernel of $WW^T$ -- the image being generated by the columns of $W$ and the kernel are necessarily orthogonal (with respect to the dot product) to the columns of $W$ (rows of $W^T$). \n",
    "\n",
    "Put differently, in order to ensure the existence of a satisfactory $\\mathbf z$ where  \n",
    "$\\langle \\mathbf w_1,\\mathbf z\\rangle = 0 = \\langle \\mathbf w_2,\\mathbf z\\rangle \\neq \\langle \\mathbf z,\\mathbf z\\rangle$ we want to (essentially) partition the image and kernel of $WW^T$, i.e. we desire    \n",
    "\n",
    "$\\ker\\big(WW^T\\big)\\cap \\text{image}\\big(WW^T\\big) = \\big\\{\\mathbf 0\\big\\}$   \n",
    "\n",
    "notice  $\\big(WW^T\\big)^2 = W\\big(W^TW\\big)W^T$  \n",
    "$\\longrightarrow \\text{rank}\\Big(\\big(WW^T\\big)^2\\Big) = \\text{rank}\\Big(W\\big(W^TW\\big)W^T\\Big)= \\text{rank}\\Big(W^TW\\Big) = 2 = \\text{rank}\\Big(W\\Big) =\\text{rank}\\Big(\\big(WW^T\\big)\\Big)$  \n",
    "where again the left and right inverses allow us to make the middle equality  \n",
    "\n",
    "revisiting the equality conditions of the guiding inequality of chapter 4 notes and ex 4.Misc.8, this tells us    \n",
    " $\\text{rank}\\Big(\\big(WW^T\\big)^k\\Big) =  \\text{rank}\\Big(\\big(WW^T\\big)\\Big)$ for all natural numbers $k$ and thus  \n",
    "$\\ker\\big(WW^T\\big)\\cap \\text{image}\\big(WW^T\\big) = \\big\\{\\mathbf 0\\big\\}$  \n",
    "as desired. Rank-nullity tells us that $\\dim \\ker\\big(WW^T\\big) + \\dim\\text{image}\\big(WW^T\\big) = n$ and since there is no intersection between these spaces (other than the zero vector) this means our vector space is a direct sum of these two subspaces.  \n",
    "\n",
    "This means we may select candidates for $\\mathbf z$ from $\\ker\\big(WW^T\\big)$.  For the case of $n=3$, we have $\\mathbf z:= \\mathbf z_1 \\in \\ker\\big(WW^T\\big)$. And since we require $\\mathbf z_1\\neq \\mathbf 0$, and being in the kernel, we know $\\mathbf W^T\\mathbf z_1 = \\mathbf 0$, but this bilinear form is non-degenerate, so $\\mathbf z$ cannot be orthogonal to all vectors in this space--and since all vectors may be written as linear combinations of $\\mathbf w_1, \\mathbf w_2, \\mathbf z_1$ (recall direct sum) this means $\\langle \\mathbf z_1,\\mathbf z_1\\rangle \\neq 0$ as desired.  \n",
    "\n",
    "For $n\\geq 4$, select $n-2$ linearly independent vectors from $\\ker\\big(WW^T\\big)$, i.e. $\\{\\mathbf z_3, ..., \\mathbf z_n\\}$.  Again by direct sum argument  \n",
    "$\\{\\mathbf w_1,\\mathbf w_2,\\mathbf z_3, ..., \\mathbf z_n\\}$  \n",
    "generates the entire vector space.  Not being zero, and being a nondegenerate form,  \n",
    "\n",
    "$\\langle \\mathbf z_3, \\mathbf v\\rangle \\neq 0$ for some $\\mathbf v$ in our vector space. Being in the kernel we know $\\langle \\mathbf z_3, \\mathbf w_1\\rangle = 0 = \\langle \\mathbf z_3, \\mathbf w_2\\rangle$  \n",
    "so we can assume WLOG that $\\mathbf v$ is generated solely by vectors in the kernel, thus there is some $\\mathbf v\\in \\ker\\big(WW^T\\big)$ which is not orthogonal to $\\mathbf z_3$. Echoing the technique on page 243 from chapter 7, we observe one of the following satisfactory outcomes, either \n",
    "\n",
    "$\\langle \\mathbf z_3, \\mathbf z_3\\rangle \\neq 0$ --if true $\\mathbf z:= \\mathbf z_3$ else  \n",
    "$\\langle \\mathbf v, \\mathbf v\\rangle \\neq 0$ if true $\\mathbf z:= \\mathbf v$ else    \n",
    "\n",
    "$\\mathbf z:= \\mathbf z_3+\\mathbf v, \\mathbf z_3+ \\mathbf v$  because  \n",
    "$\\langle \\mathbf z, \\mathbf z\\rangle = \\langle \\mathbf z_3+\\mathbf v, \\mathbf z_3+ \\mathbf v\\rangle = \\langle \\mathbf z_3, \\mathbf z_3\\rangle + \\langle \\mathbf v, \\mathbf v\\rangle + 2 \\langle \\mathbf z_3, \\mathbf v\\rangle = 0+0+2 \\langle \\mathbf z_3, \\mathbf v\\rangle \\neq 0$  \n",
    "\n",
    "(we are of course not in a field of characteristic 2, and in fact in $\\mathbb C$)  \n",
    "\n",
    "\n",
    "the finish comes, as before by looking at  \n",
    "\n",
    "$\\mathbf P^{(i)}(\\tau) = \\mathbf I - 2\\mathbf v_i(\\tau)\\mathbf v_i(\\tau)^T$  \n",
    "where for $\\tau \\in [0,1]$  \n",
    "$\\mathbf v_i(\\tau) = \\sqrt{1-\\tau}\\cdot\\mathbf w_i + \\sqrt{\\tau}\\cdot \\mathbf z$  \n",
    "\n",
    "so  \n",
    "$\\mathbf P^{(i)}(1)\\mathbf P^{(i+1)}(1) = I$  \n",
    "\n",
    "which completes the proof of path connectivity  \n",
    "\n",
    "- - - -  \n",
    "for an alternative approach using Gram Schmidt (ref chapter 7 notes esp QR factorization), consider o \n",
    "$W^TW =  \\begin{bmatrix} 1 & c \\\\ c &  1 \\end{bmatrix}$  \n",
    "for $c \\not \\in \\{-1,1\\}$ this means \n",
    "\n",
    "$\\mathbf q_1 := \\mathbf w_1$  and  \n",
    "$\\mathbf q_2' = \\mathbf w_2 - \\langle \\mathbf q_1, \\mathbf w_2\\rangle \\mathbf q_1 = \\mathbf w_2 - c \\mathbf w_1$  \n",
    "its immediate that $\\langle \\mathbf q_1, \\mathbf q_2'\\rangle = \\langle \\mathbf w_1, \\mathbf q_2'\\rangle =0$ but we want to confirm that $\\langle \\mathbf q_2', \\mathbf q_2'\\rangle \\neq 0$ i.e. that we can 'normalize it'  (equivalently, that $\\mathbf q_2'$ isn't self orthogonal)    \n",
    "\n",
    "$\\langle \\mathbf q_2', \\mathbf q_2'\\rangle =\\langle \\mathbf q_2', \\mathbf w_2 - c \\mathbf w_1\\rangle =\\langle \\mathbf q_2', \\mathbf w_2 \\rangle  - c\\langle \\mathbf q_2',  \\mathbf w_1\\rangle = \\langle \\mathbf q_2', \\mathbf w_2 \\rangle= \\langle \\mathbf w_2 - c \\mathbf w_1, \\mathbf w_2 \\rangle = \\langle \\mathbf w_2, \\mathbf w_2\\rangle -c \\langle \\mathbf w_1, \\mathbf w_2 \\rangle = 1-c^2 \\neq 0$  \n",
    "\n",
    "given our assumption of $c\\not \\in\\{-1,1\\}$.  \n",
    "\n",
    "Notice that if $c \\in\\{-1,1\\}$ Gram Schmidt, to ultimately find an 'orthonormal' $\\mathbf z$, breaks at this stage, irrespective of whether $n =3$ or $n=4$ or $n=5$, etc  \n",
    "\n",
    "Since $c\\not \\in\\{-1,1\\}$, $\\mathbf q_2'$  may be normalized.  Then extend to a basis and get some linearly independent vector $\\mathbf z$, from which we may run Gram Schmidt  $\\mathbf z \\mapsto \\mathbf q_3'$ where $\\mathbf q_3$ is orthogonal (with respect to our bilinear form) to $\\mathbf q_1$ and $\\mathbf q_2$. At least in the case of $n=3$ we then know that \n",
    "$Q  =\\bigg[\\begin{array}{c|c|c} \n",
    "\\mathbf q_1  & \\mathbf q_2 & \\mathbf q_3'\n",
    "\\end{array}\\bigg]$ \n",
    "\n",
    "is invertible which implies $\\langle \\mathbf q_3', \\mathbf q_3'\\rangle \\neq 0$  \n",
    "(i.e. by rank $\\text{rank}\\big(Q^TQ\\big) =3 $) hence it too may be 'normalized'   \n",
    "\n",
    "using Gram Schmidt, for $n\\geq 4$ the argument is a variant of the above.  We have normalized $\\{\\mathbf q_1,\\mathbf q_2\\}$ --- call the subspace associated with linear combinations of these two vectors $W$.  Now extend to a basis for $n-2$ more vectors, and while Gram Schmidt could fail in trying to normalize e.g.  $\\mathbf q_3'$, we can run the 'for loop' in a flipped order' so we run Gram Schmidt against these $n-2$ vectors only against the 2 generators of $W$, and temporarily ignore normalization.  We now have a these n-2 vectors as generators of $W^\\perp$. And select some non-zero $\\mathbf v \\in W^\\perp$, it cannot be orthogonal to every vector in our vector space since it isn't a null vector, apply the argument on p. 243 to find a vector that is either $\\mathbf u$ or $\\mathbf v$ or $\\mathbf u + \\mathbf v$ that is not self-orthogonal, but we know $\\mathbf v \\in \\in W^\\perp$ and we may choose $\\mathbf u \\in W^\\perp$ since the defining criterion is $\\langle \\mathbf u, \\mathbf v\\rangle \\neq 0$ but for $w\\in W$ we know $\\langle \\mathbf w, \\mathbf v\\rangle = 0$, we if\n",
    "\n",
    "$\\mathbf u = \\mathbf w + \\mathbf u'$, with  $\\mathbf u' \\in W^\\perp$ then setting $\\mathbf u:= \\mathbf u'$ will give a satisfactory result (i.e. any $\\mathbf w$ is in the kernel and may be ignored)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**nuisance case of** $c \\in \\{-1,1\\}$   \n",
    "\n",
    "unfortunately the nuisance case of some $c \\in \\{-1,1\\}$ prevents us from running Gram Schmidt (normalization of the second vector breaks) and it implies \n",
    "$\\ker\\big(WW^T\\big)\\cap \\text{image}\\big(W^T\\big) \\neq \\big\\{\\mathbf 0\\big\\}$  \n",
    "\n",
    "preventing us from getting the nice direct sum of our vector space as being the kernel and image of $W$.  \n",
    "\n",
    "One way forward is to notice the blocked structure of the Householder matrices and find a way to exploit this with particularly simple Householder matrices involving standard basis vectors -- and as always, taking advantage of involutions.  \n",
    "\n",
    "$\\mathbf P^{'}:= I - 2\\mathbf e_1\\mathbf e_1^T$  \n",
    "\n",
    "recalling that $k$ is even, so $I= \\big(\\prod_{r=1}^{k} \\mathbf P^{'}\\big) $   \n",
    "\n",
    "we need to show path connectivity of    \n",
    "$\\mathbf P^{(k)}...\\mathbf P^{(2)}\\mathbf P^{(1)} = \\mathbf P^{(k)}...\\mathbf P^{(2)}\\big(\\prod_{r=1}^{k} \\mathbf P^{'}\\big)\\mathbf P^{(1)} = \\big(\\mathbf P^{(k)}...\\mathbf P^{(2)}\\prod_{r=1}^{k-1} \\mathbf P^{'}\\big)\\big(\\mathbf P^{'}\\mathbf P^{(1)}\\big) $  \n",
    "\n",
    "$\\big(\\mathbf P^{(k)}...\\mathbf P^{(2)}\\prod_{r=1}^{k-1} \\mathbf P^{'}\\big)$  \n",
    "is easily shown to be path connected to the identity since, by focusing on   \n",
    "$\\mathbf P^{(j)}\\mathbf P^{'}$  \n",
    "for $j\\in\\{2,3,...,k\\}$  \n",
    "\n",
    "noting that for each of the above $\\mathbf P^{(j)}$ $j\\geq 2$ has $\\mathbf e_1^T \\mathbf w_j =0 = c^{(j)}$  i.e. the first component of each $\\mathbf w_j$ is zero given the blocked structure in the proof.  By our earlier work, in each case   \n",
    "$V := \\bigg[\\begin{array}{c|c}\\mathbf w_j & \\mathbf e_1\\end{array}\\bigg]$  \n",
    "then   \n",
    "$\\det\\big(V^TV\\big) \\neq 0$    \n",
    "because $-1\\neq c \\neq 1$  \n",
    "\n",
    "\n",
    "and $V^T$  has some non-self orthogonal vector $\\mathbf z$ in its kernel  (i.e. $\\mathbf z$ is chosen to be 'orthonormal' with respective to our bilinear form), hence for each pair we have   \n",
    "\n",
    "$\\mathbf P^{(j)}(\\tau) = \\mathbf I - 2\\mathbf v_j(\\tau)\\mathbf v_j(\\tau)^T$  \n",
    "$\\mathbf v_j(\\tau) = \\sqrt{1-\\tau}\\cdot\\mathbf w_j + \\sqrt{\\tau}\\cdot \\mathbf z$  \n",
    "$\\mathbf P^{'}(\\tau) = \\mathbf I - 2\\mathbf v'(\\tau)\\mathbf v'(\\tau)^T$  \n",
    "$\\mathbf v'(\\tau) = \\sqrt{1-\\tau}\\cdot\\mathbf e_1 + \\sqrt{\\tau}\\cdot \\mathbf z$  \n",
    "for $\\tau \\in [0,1]$  \n",
    "This allows us to demonstrate the desired path connectivity in each case.  \n",
    "- - - - -  \n",
    "*note:*  \n",
    "since the composition of two paths, is sufficient to show path connecitivty to the identity, if so desired we could take  \n",
    "\n",
    "$\\mathbf P^{(k)}...\\mathbf P^{(2)}\\mathbf P^{(1)} = \\big(\\mathbf P^{(k)}...\\mathbf P^{(2)}\\big)\\mathbf P^{(1)}$  \n",
    "and for the same reasons above, compose a path for the $k-1$ matrices in   \n",
    "$\\big(\\mathbf P^{(k)}...\\mathbf P^{(2)}\\big)$  \n",
    "$\\mathbf P^{'}$  \n",
    "being involutary and $k-1$ being odd means   \n",
    "$\\big(\\mathbf P^{'}\\big)^{k-1}=\\mathbf P^{'}$  \n",
    "so we end up with the same problem in the end of needing to composes a path for  \n",
    "$\\mathbf P^{'}\\mathbf P^{(1)}$  \n",
    "\n",
    "- - - - -   \n",
    "\n",
    "It remains to find a common Householder matrix for  \n",
    "$\\mathbf P^{'}\\mathbf P^{(1)}$  \n",
    "\n",
    "there is some $r\\in\\{1,2,...,n\\}$ such that    \n",
    "$ w_r^{(1)} \\not \\in \\{-1,1\\}$   \n",
    "because \n",
    "$1 = \\mathbf w_1^T \\mathbf  w_1 = \\sum_{=1}^n (w_r^{(1)})^2$  \n",
    "that is not all terms in the summation may be one -- i.e. not all have a pre-image under squaring of -1 or 1     \n",
    "\n",
    "\n",
    "if $r=1$ then by our earlier work, we can find an 'orthonormal' vector $\\mathbf z$ that we can map to via a path for  \n",
    "$\\mathbf P^{(1)}$ and $\\mathbf P^{'}$.  \n",
    "\n",
    "if $r\\neq 1$ we may suppose e.g. that $r=2$, then consider  \n",
    "$\\mathbf P^{''}= I - 2\\mathbf e_2\\mathbf e_2^T$  \n",
    "\n",
    "\n",
    "$\\mathbf P^{(1)}\\mathbf P^{'} = \\mathbf P^{(1)}I \\mathbf P^{'}= \\mathbf P^{(1)}\\big(\\mathbf P^{''}\\mathbf P^{''}\\big)\\mathbf P^{'} = \\big(\\mathbf P^{(1)}\\mathbf P^{''}\\big)\\big(\\mathbf P^{''}\\mathbf P^{'}\\big)$  \n",
    "and by our earlier work \n",
    "$\\big(\\mathbf P^{(1)}\\mathbf P^{''}\\big)$ has some satisfactory 'orthonormal' vector $\\mathbf z$ which gives us a path in each case to $\\big(I-2\\mathbf {zz}^T\\big)$ and of course  \n",
    "\n",
    "$\\mathbf P^{''}\\mathbf P^{'}$  \n",
    "may each be joined by a path to   \n",
    "$\\mathbf P^{'''} = I -\\mathbf e_3\\mathbf e_3^T$  \n",
    "\n",
    "so at the end of the paths we have  \n",
    "$\\big(I-2\\mathbf {zz}^T\\big)^2\\big(\\mathbf P^{'''} \\big)^2 = \\big(I\\big)\\big(I\\big) = I$  \n",
    "\n",
    "which completes the proof.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "210.75"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " 0.67*225 + 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\left[\\begin{matrix}- \\frac{- c + i d}{a + i b}\\\\1\\end{matrix}\\right]\n",
      " \n",
      "\\left[\\begin{matrix}\\left(a + i b\\right)^{2} + \\left(- c + i d\\right)^{2} & 0\\\\0 & 1 + \\frac{\\left(- c + i d\\right)^{2}}{\\left(a + i b\\right)^{2}}\\end{matrix}\\right]\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle a + i b + \\frac{\\left(- c + i d\\right)^{2}}{a + i b}$"
      ],
      "text/plain": [
       "a + I*b + (-c + I*d)**2/(a + I*b)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# detour computation for SO_2(C) \n",
    "z = sp.Matrix([[a+b*sp.I],[-c+ d*sp.I]])\n",
    "v = (z.T).nullspace()\n",
    "v= v[0]\n",
    "print(sp.latex(v))\n",
    "Q = sp.zeros(2,2)\n",
    "Q[:,0] = z\n",
    "Q[:,1] = v\n",
    "M = Q.T@Q\n",
    "print(\" \")\n",
    "print(sp.latex(M)) \n",
    "Q.det() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Detour: directly proving path connectivity of** $SO_2(\\mathbb C)$  **via class equations**    \n",
    "\n",
    "while the above Householder generator proof works for proving path connectivity of $SO_n(\\mathbb C)$ when  $n\\geq 3$, it is silent in the case of $n=2$.  (For $n=1$ there matrix would have to be involutary so either +1 or -1 and then the determinant of 1 implies it must be the scalar +1). \n",
    "\n",
    "However we may directly prove path connectivity of $SO_2(\\mathbb C)$ by examining it's components / the class equation  \n",
    "$Q\\in SO_2\\big(\\mathbb C\\big)$  \n",
    "$Q = \\bigg[\\begin{array}{c|c}\\mathbf z & *\\end{array}\\bigg] = \\left[\\begin{matrix}a + i b & *\\\\- c + i d & *\\end{matrix}\\right] =\\left[\\begin{matrix}a + i b & - \\frac{- c + i d}{a + i b}\\\\- c + i d & 1\\end{matrix}\\right]\n",
    "$  \n",
    "where the right hand side is, up to scaling, the unique vector in the right nullspace of the vector (interpretted as a 1 x 2 matrix) $\\mathbf z^T$  \n",
    "and computation tells us  \n",
    "$Q^T Q =\\left[\\begin{matrix}\\left(a + i b\\right)^{2} + \\left(- c + i d\\right)^{2} & 0\\\\0 & 1 + \\frac{\\left(- c + i d\\right)^{2}}{\\left(a + i b\\right)^{2}}\\end{matrix}\\right] := \\left[\\begin{matrix}1 & 0\\\\0 & 1\\end{matrix}\\right]$  \n",
    "so top left cell gives  \n",
    "$\\left(a + i b\\right)^{2} + \\left(- c + i d\\right)^{2}= 1$  \n",
    "- - - -  \n",
    "*technical note:*  the above case tacitly assumes $a+ib \\neq 0$  \n",
    "in the event $a+ib = 0$ we'd have $c^2 -d^2 -icd =  (-c+id)^2=1 \\longrightarrow d =0\\longrightarrow c\\in\\{-1,1\\}$ which further implies that $q_{4,4}=0$ and $q_{3,4} \\in\\{-1,1\\}$ (where the determinant specifies the value)   \n",
    "i.e. $Q$ is a particularly simple matrix and $Q\\in SO_2\\big(\\mathbb R\\big)\\lt SO_2\\big(\\mathbb C\\big)$ (i.e. Q is in a particularly nice subgroup that is easy to work with).  The rest of the writeup assumes $a+ib \\neq 0$    \n",
    "- - - -    \n",
    "\n",
    "the determinant gives  \n",
    "$\\det\\big(Q\\big) = \\displaystyle a + i b + \\frac{\\left(- c + i d\\right)^{2}}{a + i b}:=1$  \n",
    "multiplying each side by $(a + i b)$ gives   \n",
    "$a + i b = (a + i b)^2 +\\left(- c + i d\\right)^{2} = 1$\n",
    "but $a, b \\in \\mathbb R \\longrightarrow b = 0$  so  \n",
    "\n",
    "$Q=\\left[\\begin{matrix}a  & - \\frac{- c + i d}{a }\\\\- c + i d & 1\\end{matrix}\\right]$  \n",
    "and since  \n",
    "$\\left(a \\right)^{2} + \\left(- c + i d\\right)^{2}=\\left(a + i b\\right)^{2} + \\left(- c + i d\\right)^{2}= 1$  \n",
    "this implies at least one of $c=0$ or $d=0$  \n",
    "\n",
    "i.e. either  \n",
    "$Q=\\left[\\begin{matrix}a  & \\frac{c }{a }\\\\- c  & 1\\end{matrix}\\right]$  \n",
    "**note:** the above matrix is real and has orthogonal columns... however because we never fixed the lengths of the columns, it is not true that each column has \"norm\" (with repsect to bilinear form given by dot product) of 1.  \n",
    "**thus the class equations approach here, already bogged down in computations is not quite right -- more care is technically needed in normalizing the lengths of the second column -- something we have not yet done. The approach taken in here is approximately correct though ultimately not quite right.  It is debatable whether it is worth taking more time to fix this as the Householder and Cayley Transform approaches are more general and much more useful**  \n",
    "\n",
    "or  \n",
    "$Q=\\left[\\begin{matrix}a  & - \\frac{  id}{a }\\\\i d & 1\\end{matrix}\\right]$  \n",
    "with $a^2 + (id)^2 = a^2 - d^2 =1$  or  $d^2 = a^2 -1$  (this implies $\\vert a \\vert \\geq 1$)  \n",
    "in this final case $Q$ is complex but has a particularly simple form and e.g. if we wanted to prove path connectivity we'd have (with a slight overload of notation)  \n",
    "for $\\tau \\in [0,1]$  \n",
    "$a(\\tau) = \\text{sign}(a)\\cdot \\big((1-\\tau)a + \\tau\\big)$  \n",
    "and  \n",
    " $d(\\tau) = \\text{sign}(d)\\cdot  \\sqrt{a(\\tau)^2 - 1}$  \n",
    "hence we have established a path from $Q$ to either $I$ or $-I$, depending on the sign of $a$.  We require a path to $I$ and it suffices to show a path from $-I$ to $I$ but since $-I \\in SO_2\\big(\\mathbb R\\big)\\lt SO_2\\big(\\mathbb C\\big)$--and from chapter 7 notes we know this subgroup is path connected to the identity-- then we are done.  Finally  in all other cases we have $Q \\in SO_2\\big(\\mathbb R\\big)$ by identical reasoning is path connected to the identity.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**corollary**  \n",
    "The above *incomplete* direct proof for path connectivity of $SO_2\\big(\\mathbb C\\big)$ would seem to imply yet another proof of path connectivity for $SO_3\\big(\\mathbb C\\big)$, and perhaps the one that was sought by Artin.  \n",
    "\n",
    "I.e. suppose $P \\in SO_3\\big(\\mathbb C\\big)$. Now further suppose we find some $X \\in SO_3\\big(\\mathbb C\\big)$, where $P \\mathbf x_1 = \\mathbf x_1$  (more on this in moment).  Then  \n",
    "\n",
    "$P = XBX^T = XBX^{-1}$  and $B \\in SO_3\\big(\\mathbb C\\big)$ and is the stabilizer of $\\mathbf e_1$ for left multiplication by $G$ (i.e., this is ex 8.4.4b) and in particular we have  \n",
    "$B = \\left[\\begin{matrix} 1  & \\mathbf 0_2\\\\ \\mathbf 0_2^T  & Q\\end{matrix}\\right]$  \n",
    "and our direct class equations work tells us that $Q$ is path connected to the identity and hence $B$ is path connected to the identity as well, thus at $\\tau =1$ we'd have  \n",
    "\n",
    "$P(\\tau =1) =XIX^{-1} = I$   \n",
    "so $P$ is path connected to the identity.  And we implicitly believe that $X$ must exist by the result from ex 8.4.3c -- i.e. said exercise tells us there is some non-self orthogonal eigenvector with eigenvalue 1 associated with $P$.  We normalize so that $\\mathbf x_1^T\\mathbf x_1 =1$.  From there we follow the approach e.g. in chapter 7. where using this non-degenerate symmetric bilinear form (given by the dot product over $\\mathbb C$) and look at the subspace orthogonal to $\\mathbf x_1$ find the non-self orthogonal vector $\\mathbf x_2$ and 'normalize' it to be 1. Finally we recurse on the smaller subproblem to find non-self orthogonal $\\mathbf x_3$ (we can also 'find' it in the kernel of a matrix with $\\mathbf x_1$ and $\\mathbf x_2$ as it's two rows, which is rank stable, hence it is orthogonal to those 2 vectors and linearly independent of them thus appending $\\mathbf x_3$ to get $X$, an invertible matrix, means that $X^TX$ has all zeros on its bottom row, except for the diagonal -- otherwise we'd violate rank/invertibility considerations.)  \n",
    "\n",
    "As noted in ex 8.4.3c \n",
    "$P \\in SO_3\\big(\\mathbb C\\big)$ implies  \n",
    "algebraic multiplicity of $\\lambda =1$ is either 1 or 3.  The instance of algebraic multiplicity 3 introduces significant nuisances; thankfully it it suffices the consider only the case when the algebraic multiplicity is 1-- and in  ex 8.4.3c i,ii we proved this means $\\mathbf x_1^T\\mathbf x_1 \\neq 0$ (and hence it may be normalized / WLOG we may assume $\\mathbf x_1^T\\mathbf x_1 =1$).  The reason we only must consider the algebraic multiplicity of 1 case, comes from mimicking a technique we use, shortly, below to deal with a 'mirror' nuisance case when working with the Cayley Transform.  \n",
    "\n",
    "in short, suppose $P$ has $\\lambda = 1$ with algebraic multiplicity of 3.  Now consider the 3 different diagonal matrices $D$ that have two -1's on the diagonal and one +1 on the diagonal. Note that in each case $D \\in SO_3\\big(\\mathbb R\\big)\\lt  SO_3\\big(\\mathbb C\\big)$  \n",
    "\n",
    "so not only is $P'=(DP) \\in  SO_3\\big(\\mathbb C\\big)$ but (involutive) $D$ is in the nice subgroup which we know from chapter 7 is path connected to the identity.  So showing $P'$ is path connected to the identity and in turn implies $P=DP'$ is path connected to the identity.  Now suppose for a contradiction that $DP$ has $\\lambda =1$ with algebraic multiplicity of 1 for all 3 choices of $D$ -- equivalently suppose for a contradiction that $\\text{trace}\\big(DP\\big) = 3$ for all three distinct choices of $D$, then  \n",
    "\n",
    "$\\mathbf p:= \\big(P\\circ I\\big) \\mathbf 1$   \n",
    "(where $\\circ$ denotes Hadamard product)    \n",
    "\n",
    "$A\\mathbf p= \\left[\\begin{matrix}1 & -1 & -1\\\\-1 & 1 & -1\\\\-1 & -1 & 1\\end{matrix}\\right]\\left[\\begin{matrix}p_{1,1}\\\\p_{2,2}\\\\p_{3,3}\\end{matrix}\\right]  = \\left[\\begin{matrix}3\\\\3 \\\\3\\end{matrix}\\right] = 3\\cdot \\mathbf 1$  \n",
    "\n",
    "note: $\\det\\big(A\\big) = -4$ so this is uniquely solvable   \n",
    "\n",
    "$A\\big(-3\\cdot \\mathbf 1\\big) = \\big(2I-\\mathbf {11}^T\\big) \\big(-3\\cdot \\mathbf 1\\big) = 3\\big(\\mathbf {11}^T-2I\\big) \\mathbf 1 = 3\\mathbf 1$  \n",
    "so $\\mathbf p = -3\\cdot \\mathbf 1$  \n",
    "\n",
    "but this implies  \n",
    "$\\text{trace}\\big(P\\big) = p_{1,1}+p_{2,2}+p_{3,3} = \\mathbf 1^T \\mathbf p= \\mathbf 1^T \\big(-3\\mathbf 1\\big) = -9 \\neq 3$  \n",
    "which is the contradiction we sought    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**alternative approach to proving path connectivity for**  $SO_3\\big(\\mathbb C\\big)$ -- and we get path connectivity of $SO_2\\big(\\mathbb C\\big)$ as a corollary.  With a lot cleverness we even get the desired result for $SO_n\\big(\\mathbb C\\big)$ in general.     \n",
    "\n",
    "using some mixture of results from ex 7.8.5 and ex 8.Misc.3, we can get to the desired result via the Cayley Transform.  \n",
    "\n",
    "consider for $P\\in SO_3\\big(\\mathbb C\\big)$   \n",
    "$\\mathbf S = \\big(I-P\\big)\\big(I+P\\big)^{-1}$  \n",
    "$P = \\big(I-\\mathbf S\\big)\\big(I+\\mathbf S\\big)^{-1}$  \n",
    "\n",
    "*note* the first formula assumes $P$ doesn't have an eigenvalue of -1.  The workaround for the nuisance case of $P$ having an eigenvalue of -1 will be dealt with at the end.    \n",
    "\n",
    "\n",
    "- - -  - - \n",
    "*remark:*  \n",
    "(this result is referenced in *ex 8.Misc.3b*.  And conversely *ex 8.Misc.3b* may be referenced for a better understanding of fine points of the Cayley Transform.)  \n",
    "\n",
    "we confirm that $\\mathbf S$ in fact skew symmetric:  \n",
    "$\\mathbf S = \\big(I-P\\big)\\big(I+P\\big)^{-1} =\\big(I+P\\big)^{-1} - P\\big(I+P\\big)^{-1}$  \n",
    "\n",
    "$\\mathbf S^T $  \n",
    "$= \\big(I+P^T\\big)^{-1} - \\big(I+P^T\\big)^{-1}P^T$  \n",
    "$= \\big(I+P^{-1}\\big)^{-1} - \\big(I+P^{-1}\\big)^{-1}P^{-1}$  \n",
    "$= P\\big(P+I\\big)^{-1} - \\big(P+I\\big)^{-1}PP^{-1}$  \n",
    "$= P\\big(P+I\\big)^{-1} - \\big(P+I\\big)^{-1}$  \n",
    "$=  -\\Big( I\\big(P+I\\big)^{-1} -P\\big(P+I\\big)^{-1}\\Big)$  \n",
    "$=-\\mathbf S$  \n",
    "\n",
    "for the key step consider invertible matrix  \n",
    "$B := P\\big(P+I\\big)^{-1}$  \n",
    "and do the simplfication on the inverse problem  \n",
    "$B^{-1} =\\big(P+I\\big)P^{-1} = \\big(I+P^{-1}\\big)\\longrightarrow  B= \\big(I+P^{-1}\\big)^{-1}= P\\big(P+I\\big)^{-1}$  \n",
    "- - -  - - \n",
    "\n",
    "since $\\mathbf S$ is skew symmetric, we know that if we construct a valid path containing only skew symmetric matrices from $\\mathbf S \\to \\mathbf 0$ we'll have proven path connectivity of $P$.  And since   \n",
    "$P(\\tau)^T P(\\tau) = I \\longrightarrow \\det\\big(P(\\tau)\\big) \\in\\{-1,1\\} \\longrightarrow  \\det\\big(P(\\tau)\\big) = 1$   \n",
    "since the determinant is continuous and is equal to one at either 'end' of the path, hence is 1 throughout, i.e.    $P(\\tau) \\in SO_3\\big(\\mathbb C\\big) $ for any $\\tau \\in [0,1]$  \n",
    "\n",
    "a matrix is skew symmetric over $\\mathbb C$ is defined by  \n",
    "$-\\mathbf S =\\mathbf S^T$  \n",
    "this is invariant to rescaling by any $\\alpha$  \n",
    "\n",
    "we'd like to do the simple path  \n",
    "$\\mathbf S(\\tau):= (1-\\tau)\\cdot \\mathbf S + \\tau\\cdot  \\mathbf 0= (1-\\tau)\\cdot \\mathbf S$  \n",
    "\n",
    "but we cannot be sure \n",
    "    $\\big(I+\\mathbf S(\\tau)\\big)^{-1}$  exists over this path since $\\mathbf S$ could have non-zero real eigenvalues (and in particular an eigenvalue equal to -1).  Now, from ex 8.Misc.3 we know it does exist when $\\tau = 0$.  The simple finish is to note that $\\mathbf S$ has at most $n$ distinct eigenvalues.  So we can instead compose two paths, one by $\\alpha(\\tau_1)$  so that we rotate $\\mathbf S$ so that none of the resulting eigenvalues are on the punctured real line (geometrically speaking: draw an Argand diagram and plot all n eigenvalues, then rotate the x-y axis slightly so no eigenvalues are real -- ignoring the value zero. From an analytic view: let $\\theta$ be the angle of a non-real eigenvalue closest to the x axis (i.e. the closer of $\\pi$ and $0$) -- there are finitely many eigenvalues so the min exists.  Now select e.g. $\\frac{\\theta}{2}$ as the rotation angle as the end point for our first path.)  Let $\\mathbf S'$ be the rotated matrix  and the second path is  \n",
    "    \n",
    "$\\mathbf S'(\\tau):= (1-\\tau)\\cdot \\mathbf S' + \\tau \\mathbf 0$  \n",
    "which completes the proof  \n",
    "\n",
    "- - - - \n",
    "*nuisance case*  \n",
    "when $P$ has an eigenvalue of one,  $\\mathbf S = \\big(I-P\\big)\\big(I+P\\big)^{-1}$ cannot exist.  \n",
    "the remedy is to instead focus on  \n",
    "$P' := PD$  \n",
    "where $D$ is some diagonal matrix with two -1s and one +1 on its diagonal (hence D has determinant 1 and is involutive and real orthogonal)  \n",
    "\n",
    "as we'll show, a proper choice of $D$ ensures $P'$ does not have any eigenvalues of -1, hence the above argument shows that $P'$ is path connected to the identity  \n",
    "\n",
    "multiplying each side by $D$ gives  \n",
    "$P = P'D$  \n",
    "and $P$ is path connected to the identity because $P'$ is and $D\\in SO_3\\big(\\mathbb R\\big)$ i.e. a nice subgroup of $SO_3\\big(\\mathbb C\\big)$ which we know from chp 7 exercises is path connected.  \n",
    "\n",
    "There are $\\binom{3}{2}=3$ possibilities for $D$ and we show that at least one satisfactory $D$ must exist.   \n",
    "\n",
    "From ex 8.4.3 we know that $P$ has an eigenvalue of 1 and this in combination with its determinant means if it has an eigenvalue of -1, it has 2 copies of -1, and a single copy of +1.  I.e. if $P$ has an eigenvalue of -1, then it is necessary for its trace to be -1.  Thus we can reduce the nuisance case to:  \n",
    "\n",
    "suppose for a contradiction that  \n",
    "(i) $\\text{trace}\\big(P'\\big)= -1$ for all choices of $D$  \n",
    "(ii) $\\text{trace}\\big(P\\big) =-1$ and    \n",
    "    \n",
    "Looking at (i), we have the below linear system  \n",
    "\n",
    "$\\mathbf p:= \\big(P\\circ I\\big) \\mathbf 1$   \n",
    "(where $\\circ$ denotes Hadamard product)    \n",
    "\n",
    "$A\\mathbf p= \\left[\\begin{matrix}1 & -1 & -1\\\\-1 & 1 & -1\\\\-1 & -1 & 1\\end{matrix}\\right]\\left[\\begin{matrix}p_{1,1}\\\\p_{2,2}\\\\p_{3,3}\\end{matrix}\\right]  = \\left[\\begin{matrix}-1\\\\-1 \\\\-1\\end{matrix}\\right] = -\\mathbf 1$  \n",
    "\n",
    "note that $\\det\\big(A\\big) = -4$ so we have  \n",
    "$\\mathbf p = -A^{-1}\\mathbf 1 = \\mathbf 1$  \n",
    "\n",
    "but this implies  \n",
    "$\\text{trace}\\big(P\\big) = p_{1,1}+p_{2,2}+p_{3,3} = \\mathbf 1^T \\mathbf p= \\mathbf 1^T \\mathbf 1 = 3 \\neq -1$  \n",
    "which is the contradiction we sought.  \n",
    "\n",
    "put differently, what we've shown  is  \n",
    "$\\left[\\begin{matrix}1 & -1 & -1\\\\-1 & 1 & -1\\\\-1 & -1 & 1\\\\1 & 1 & 1\\end{matrix}\\right]\\left[\\begin{matrix}p_{1,1}\\\\p_{2,2}\\\\p_{3,3}\\end{matrix}\\right]  = \\left[\\begin{matrix}-1\\\\-1 \\\\-1\\\\-1\\end{matrix}\\right]$  \n",
    "is an inconsistent linear system / the vector on the RHS is *not* in the column space of the LHS.  This contradiction tells us that *some* satisfactory must $D$ exist.       \n",
    "\n",
    "**extension**  \n",
    "We can apply elements of the above argument to get an easy proof of path connectivity of $SO_2\\big(\\mathbb C\\big)$ as shown below   \n",
    "\n",
    "given the determinant of 1, the nuisance case of $P$ having an eigenvalue of -1 means two copies of the eigenvalue -1 and we can mimic the structure of the above argument e.g. consider  \n",
    "$\\text{trace}\\big(P\\big) = -2$  \n",
    "and here $D := -I$   \n",
    "$\\text{trace}\\big(P'\\big) = \\text{trace}\\big(PD\\big) = \\text{trace}\\big(P(-I)\\big) = -\\text{trace}\\big(P\\big)= 2$  \n",
    "noting $\\det\\big(-I\\big) = (-1)^2 = 1$  \n",
    "Thus $P'$ does not have eigenvalues of $-1$ and by an identical argument we see that $SO_2\\big(\\mathbb C\\big)$ is path connected.  \n",
    "\n",
    "**extension:**  \n",
    "It turns out that using the form  \n",
    "$P' := PD$  \n",
    "also may be used to prove path connectivity for $n\\geq 3$.  If we reference Gallier's short note \"Remarks on the Cayley Representation of Orthogonal Matrices and on Perturbing the Diagonal of a Matrix to Make it Invertible\" (2014), we see that for *any* matrix $P$ -- we specialize of course to $P\\in SO_n\\big(\\mathbb C\\big)$-- there must be some diagonal matrix $D$ which only has $+1$ or $-1$ on it's diagonal, such that \n",
    "\n",
    "$\\det\\big(I+P'\\big)=\\det\\big(I+PD\\big) \\neq 0$   \n",
    "taking advantage of involutions, this is equivalent to noting the existence of a $D$ in this form such that  \n",
    "$\\det\\big(D+P\\big) \\neq 0$  \n",
    "\n",
    "now, since such a $D$ exists, it is necessarily in $O_n\\big(\\mathbb R\\big)$ though it is not immediately clear that it is in $SO_n\\big(\\mathbb R\\big)$ which is what we want.  So we confirm that if a satisfying $D$ exists, then \n",
    "\n",
    "$P'=\\big(DP\\big) \\in O_n\\big(\\mathbb C\\big)$  \n",
    "(because it involves multiplication of two matrices in said group)  \n",
    "\n",
    "now being in this group we have  $P^T = P^{-1}$ so  \n",
    "\n",
    "$\\text{char poly}\\big(P'\\big)=\\text{char poly}\\big((P')^T\\big)=\\text{char poly}\\big((P')^{-1}\\big)$  \n",
    "\n",
    "which means for any non-involutive eigenvalue $\\lambda$, i.e. $\\lambda \\not \\in\\{-1,1\\}$  \n",
    "then if $\\lambda^{-1}$ is an eigenvalue with the same algebraic multiplicity as $\\lambda$ (the argument can be seen by factoring the characteristic polynomials or effecting a similarity transform so that $P$ is triangular).  \n",
    "\n",
    "recalling that the determinant of a matrix is the product of all eigenvalues (with algebraic multiplicties), we can see that the product of all the non-involutive eigenvalues of $P'$ is necessarily 1.  (If all eigenvalues are involutive then skip to the next step.) When we consider further multiplying by all involutive eigenvalues -- multiplication by $\\lambda =1$ still gives a product of 1, and by construction $P'$ has no eigenvalues of $-1$, thus we see $\\det\\big(P'\\big) = 1$  \n",
    "\n",
    "so  \n",
    "$1=\\det\\big(P'\\big) = \\det\\big(PD\\big)=\\det\\big(P\\big)\\cdot \\det\\big(D\\big) = 1\\cdot \\det\\big(D\\big)$  \n",
    "thus $D\\in SO_n\\big(\\mathbb R\\big)$ as desired  \n",
    "\n",
    "the rest of the argument follows identically as in the $n=3$ case for proving path connectivity  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**maybe a way to do partitioning with conjugacy classes...**  \n",
    "**or maybe have a rational function in $\\tau$ and winding number is finite, and we know now poles.... so finitely many roots?**     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**remark:**  \n",
    "It is conceivable that there is a way to streamline 8.4.4 and most of 8.4.3 using a one parameter subgroups structure.  E.g. is clear that for *complex skew symmetric matrix* $\\mathbf S$ (not skew hermitian but skew symmetric, which necessarily is traceless)  \n",
    "\n",
    "$\\exp\\big(\\mathbf S\\big) \\in  SO_n\\big(\\mathbb C\\big)$  \n",
    "\n",
    "and this includes the zero matrix.  \n",
    "\n",
    "And convex combinations of any complex skew symmetric matrix and the zero matrix gives a skew symmetric matrix, so an easy path connectivity argument is  \n",
    "$\\exp\\big(\\tau \\mathbf 0 + (1-\\tau)\\mathbf S \\big) \\in SO_n\\big(\\mathbb C\\big)$  \n",
    "which is the identity matrix when $\\tau =1$.  Hence every matrix of this form is path connected to the identity.    \n",
    "And when $n$ is odd, esp n =3, $\\mathbf S$ is singular (ref chp 7) so it has an eigenvalue of 0, which becomes the eigenvalue of 1 under the exponential map.  \n",
    "\n",
    "unfortunately, it is not clear to your author that *all* complex orthogonal matrices are in the image of the exponential map applied to skew symmetric matrices -- i.e. surjectivity is not clear.  \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8.5.4**  \n",
    "\n",
    "$\\phi: \\mathbb R^{+} \\longrightarrow GL_n(\\mathbb R)$  \n",
    "i.e. a homomorphism from scalar addition on the real line to $GL_n$ with real valued matrices  \n",
    "\n",
    "Prove that either $\\ker \\phi$ is trivial, the whole group, or else it is infinite cyclic   \n",
    "*The proof that follows works when* $\\phi$ *is a continuous map-- though not stated as such it seems that this is a primary condition that Artin would require.*   (The main proof uses the fact that there must be some minimum distance between values in a nontrivial kernel -- if that point are arbitrarily close together then the entire group is in the kernel -- this is implicitly an approximation / density argument, i.e. if for every $\\delta \\gt 0$ there exists some $t^* \\in (t-\\delta, t+\\delta)$ such that $\\big \\Vert \\phi(t^*) - I \\big \\Vert_F =0 \\longrightarrow \\phi(t) \\in \\ker$ because  \n",
    "$\\phi(t) = I^{-1}\\phi(t)= \\phi(t^*)^{-1}\\phi(t) = \\phi(t-t^*) = \\phi(d)$ for $d\\in (-\\delta,\\delta)$, where $d$ may be made arbitrarily close to zero, hence we can make $\\big\\Vert \\phi(t) - \\phi(0)\\big\\Vert_F = \\big\\Vert \\phi(d) - \\phi(0)\\big\\Vert_F \\lt \\epsilon$ for any $\\epsilon \\gt 0$.  It isn't clear to your author that the minimum distance property holds for non-continuous homomorphisms.   \n",
    "\n",
    "\n",
    "a) trivial $\\ker \\phi$ is easy: just map $\\phi: t \\in \\mathbb R\\mapsto e^{t}I_n$  \n",
    "-- this obeys the homomorphism \"splitting property\" and is an injective mapping since the exponential is injective over reals    \n",
    "b) entire group $\\ker \\phi$ is easy: consider $\\phi: t \\in \\mathbb R\\mapsto I_n$  \n",
    "\n",
    "c) if it is neither of these, then the homomorphism must create an infinite cyclic group   \n",
    "an easy example c would be $\\phi: t \\in \\mathbb R\\mapsto e^{i\\theta t}I_n$  \n",
    "though technically we want our image to be real valued, so instead use a 2x2 rotation matrix to represention points on the unit circle, e.g.  \n",
    "\n",
    " $\\phi: t \\in \\mathbb R\\mapsto \\begin{bmatrix} \\cos(t\\cdot \\theta) & -\\sin(t\\cdot \\theta) & \\mathbf 0^T\\\\ \\sin(t\\cdot \\theta) & \\cos(t\\cdot \\theta) &\\mathbf 0^T \\\\ \\mathbf 0 & \\mathbf 0& I_{n-2}\\end{bmatrix}$  \n",
    "\n",
    "proof: \n",
    "suppose the homomorphism is neither (a) nor (b).  It then has as nontrivial kernel and image.  \n",
    "note: we focus on the real non-negative part of the real line -- by symmetry / the fact that subtraction in our group of addition on the real line corresponds to the inverse in the homomorphism (matrix multiplication) any t mapped to the identity matrix implies -t is mapped there as well.  \n",
    "\n",
    "note if neither a nor b is true, there can be no connected set in the kernel -- if there was some $(t',t'')$ in the kernel then we could pick two arbitrarily close points in said interval each of which is mapped to the identity, hence there difference is mapped to the identity as well (use homomorphism splitting property and invertibility of the group that is the image) and we then find that the image of the homomorphism is trivial.  By essentially the same reasoning, there must be some minimum distance between each pairs of points in the kernel (if there is only one point in the kernel then we have b), call this $m$. Thus $t=0$ and $t=m$ is in the kernel and $m$ is the minimum positive value in the kernel.  The claim is that every t in the kernel (again by symmetry we focus WLOG on non-negative values of t) may be written as $t = k\\cdot m$ for integer $k$.  Suppose for a contradiction that we find some $t^* = k\\cdot m + \\alpha$ for some $\\alpha \\in (0,m)$ in what amounts to modding-out / quotienting out $m$, this implies $t^{**}= k\\cdot m - t^* = \\alpha$ is in the kernel because    \n",
    "$\\phi\\big(\\alpha \\big)=\\phi\\big(t^{**}\\big) = \\phi\\big(k\\cdot m + (-t^*)\\big) = \\phi\\big(k\\cdot m\\big) \\phi\\big(- t^*\\big) = \\phi\\big(m\\big)^k \\phi\\big( t^*\\big)^{-1} = I^k I^{-1} = I$  \n",
    "\n",
    "hence $\\alpha \\in \\ker \\phi$ but $0\\lt \\alpha \\lt m$ which contradicts $m$ being the minimum positive value in the kernel.  \n",
    "\n",
    "Note: that this is conceptually this is very similar to page 166-169, discussing discrete groups of motion  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8.5.9**  \n",
    "$\\frac{d}{dt}X = AX$  \n",
    "implies  \n",
    "$X = e^{tA}B$  \n",
    "so where $B$ and $A$ are 'plain' matrices that do not depend on $t$.  \n",
    "\n",
    "*remark:* \n",
    "Artin does not explicitly say that $B$ is invertible, though it seems like it should be as it is a coset associated with some group (presumably $GL_n$) acting on some subgroup. For avoidance of doubt, we assume the invertibility of $B$.  \n",
    "\n",
    "By inspection  \n",
    "\n",
    "$\\frac{d}{dt}X = \\frac{d}{dt} e^{tA}B =  Ae^{tA}B = AX$   \n",
    "\n",
    "$W:= X B^{-1}$  \n",
    "\n",
    "and  \n",
    "\n",
    "$\\frac{d}{dt} X =\\frac{d}{dt}\\big(WB\\big)= \\big(\\frac{d}{dt}W\\big)B = \\big(e^{tA}\\big)B =  \\big(Ae^{tA}\\big)B = AX$   \n",
    "where the first equality is e.g. implied by linearity or by the product rule (where $B(t) = B$ for all t so $\\frac{d}{dt}B(t) = \\mathbf 0$) see page 141 and ex 4.8.11.  \n",
    "\n",
    "*note:*    \n",
    "there is an interesting interpretation in terms of $X$ being a coset to this one parameter subgroup, and 'translation' of the Lie Group from the identity to $B$.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8.6.1**  \n",
    "Compute $\\big(A+B\\epsilon\\big)^{-1}$ assuming that $A$ is invertible  \n",
    "\n",
    "*recall:* $\\epsilon$ is an algebraic element that formally represents infinitesimals by the rule $\\epsilon^2=0$  (as noted in the chapter we are no longer operating in a field, but algebriacly this *is* a ring)  \n",
    "\n",
    "\n",
    "$C:=-A^{-1}B$  \n",
    "\n",
    "$\\big(A+B\\epsilon\\big)^{-1}$  \n",
    "$=\\Big(A\\big(I+A^{-1}B\\epsilon\\big)\\Big)^{-1}$  \n",
    "$=\\big(I+A^{-1}B\\epsilon\\big)^{-1}A^{-1}$  \n",
    "$=\\big(I-C\\epsilon\\big)^{-1}A^{-1}$  \n",
    "$= \\big(I +C \\epsilon + \\sum_{k=2}^\\infty C^{k}\\epsilon^k\\big)A^{-1} $   \n",
    "$= \\big(I +C \\epsilon\\big)A^{-1}$  \n",
    "$= \\big(I - A^{-1}B \\epsilon\\big)A^{-1}$  \n",
    "$= A^{-1}-A^{-1}BA^{-1}\\epsilon$  \n",
    "\n",
    "note: we can verify this holds because  \n",
    "\n",
    "$\\big(A+B\\epsilon\\big)\\big(A^{-1}-A^{-1}BA^{-1}\\epsilon\\big) = I + \\big(- BA^{-1}\\epsilon + BA^{-1}\\epsilon\\big) -\\big(A^{-1}BA^{-1}\\epsilon^2\\big)= I + 0 + 0 = I$  \n",
    "\n",
    "\n",
    "*remark:*  \n",
    "since we have  \n",
    "$\\big(I +C \\epsilon\\big) = \\big(I -C \\epsilon\\big)^{-1}$ or  $\\big(I +C \\epsilon\\big)^{-1} = \\big(I -C \\epsilon\\big)$  \n",
    "it seems like there may be a way to relate this to the Cayley Transform, i.e.  \n",
    "$C\\epsilon \\mapsto \\big(I -C \\epsilon\\big)\\big(I +C \\epsilon\\big)^{-1} = \\big(I -C \\epsilon\\big)^2 = I -2 C\\epsilon + C^2\\epsilon^2 = I -2 C\\epsilon = C'$  \n",
    "and applying the Cayley Transform once more we get    \n",
    "$C'\\mapsto \\big(I -(I -2 C\\epsilon)\\big)\\big(I +(I -2 C\\epsilon)\\big)^{-1} = \\big(2 C\\epsilon\\big)\\big(2I -2 C\\epsilon\\big)^{-1}= \\big(C\\epsilon\\big)\\big(I - C\\epsilon\\big)^{-1}= \\big(C\\epsilon\\big)\\big(I + C\\epsilon\\big)=C\\epsilon$  \n",
    "\n",
    "which is still an involution  \n",
    "since $\\epsilon$ breaks our field properties, it was not immediately clear to your author that the involution would be preserved  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here is the Cayley Transform of $\\big(I-C\\epsilon\\big)$  \n",
    "$\\big(I-C\\epsilon\\big) \\mapsto \\big(I -(I-C \\epsilon)\\big)\\big(I+ (I-C \\epsilon)\\big)^{-1}= C\\epsilon \\cdot \\big(2I -C \\epsilon\\big)^{-1}= 2\\epsilon \\cdot \\big(I -\\frac{1}{2}C \\epsilon\\big)^{-1}= 2C\\epsilon \\cdot \\big(I +\\frac{1}{2}C \\epsilon\\big)=2C\\epsilon = C'$  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8.6.7**  \n",
    "prove the formula:  \n",
    "$\\det\\big(I+A\\epsilon\\big) = 1 + \\text{trace}\\big(A\\epsilon\\big)$  \n",
    "(where, aside from $\\epsilon$, it is assumed that the scalars are in $\\mathbb C$ or $\\mathbb R$)   \n",
    "\n",
    "*proof 1:*  \n",
    "suppose $A$ is upper triangular, then the determinant gives the product of the diagonals:  \n",
    "where we collect the eigenvalues of $\\mathbf A\\epsilon$ in a vector $\\mathbf x$  \n",
    "$\\det\\big(I+A\\epsilon\\big) = (1+\\epsilon \\lambda_1)(1+\\epsilon \\lambda_2)...(1+\\epsilon \\lambda_n)= e_0\\big(\\mathbf x\\big) + e_1\\big(\\mathbf x\\big) + \\Big(e_2\\big(\\mathbf x\\big) + ... + e_n\\big(\\mathbf x\\big)\\Big)=1 + e_1\\big(\\mathbf x\\big) = 1 + \\text{trace}\\big(A\\epsilon\\big)$  \n",
    "\n",
    "in general  \n",
    "$e_k\\big(\\mathbf x\\big) = \\epsilon^k \\cdot e_k\\big(\\mathbf y\\big)$  where $\\mathbf y$ has the eigenvalues of $\\mathbf A$ \n",
    "(and $e_0$ is simply a formality that is defined as one), thus for $k \\geq 2$, $e_k\\big(\\mathbf x\\big)=0$ \n",
    "\n",
    "And since determinants are invariant to conjugation and over $\\mathbb C$ every matrix is similar to an upper triangular matrix, then the above gives the result.  (The issue of $\\epsilon$ being an element of a ring and not a field is not a concern since we triangularize $A$ and then scale by $\\epsilon$ only at the end.)   \n",
    "\n",
    "*proof 2:*  \n",
    "conceptually simple but a bit tedious, with $z=-1$  \n",
    "$\\det\\big(I+A\\epsilon\\big) = (-1)^n\\det\\big(-I-A\\epsilon\\big)= (-1)^n\\det\\big(zI-A\\epsilon\\big) = p(z)$  \n",
    "where in particular we want $z=1$  and $p$ is the characteristic polynomial of the matrix $A\\epsilon$  \n",
    "\n",
    "now using ex 4.4.11(b) \n",
    "(again conceptually simple but rather tedious -- Meyer's *Matrix Analysis* chapter on determinants has a relatively succinct derivation), we see  \n",
    "\n",
    "$p(z) = z^n - s_1\\big(A\\epsilon\\big)z^{n-1} +  s_2\\big(A\\epsilon\\big)  z^{n-2}+...+  (-1)^{n-1}s_{n-1}\\big(A\\epsilon\\big)  z^{1}+  (-1)^{n}s_{n}\\big(A\\epsilon\\big) =  z^n - c_1\\big(A\\epsilon\\big)z^{n-1}=  z^n - \\text{trace}\\big(A\\epsilon\\big)z^{n-1}$  \n",
    "where $s_{k}\\big(A\\epsilon\\big)$  denotes the sum over all sized k principal minors (i.e. determinants of $k$ x $k$ principal submatrices)    \n",
    "and in general  \n",
    "$s_{k}\\big(A\\epsilon\\big) = \\epsilon^k \\cdot s_{k}\\big(A\\big)\\longrightarrow s_{k}\\big(A\\epsilon\\big)=0 \\text{ for } k\\geq 2$  \n",
    "\n",
    "now for even $n$   \n",
    "$p(-1) = (-1)^n\\big((-1)^n - \\text{trace}\\big(A\\epsilon\\big)(-1)^{n-1}\\big)= 1 + \\text{trace}\\big(A\\epsilon\\big)$ \n",
    "\n",
    "and for odd $n$  \n",
    "$p(-1) = (-1)^n\\big((-1)^n - \\text{trace}\\big(A\\epsilon\\big)(-1)^{n-1}\\big)= (-1)\\big(-1  -\\text{trace}\\big(A\\epsilon\\big)= 1 + \\text{trace}\\big(A\\epsilon\\big)$  \n",
    "\n",
    "part b  \n",
    "$\\det\\big(A+B\\epsilon)= \\det\\big(A(I+A^{-1}B\\epsilon)\\Big)= \\det\\big(A\\big)\\det\\big(I+A^{-1}B\\epsilon\\Big) = \\det\\big(A\\big)\\cdot \\big(1 + \\text{trace}(A^{-1}B\\epsilon)\\big)$  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8.6.13**  \n",
    "The *adjoint representation* of a linear group $G$ is the representation by conjugation on its Lie algebra $G x L \\longrightarrow L$ is defined to be $P,A \\mapsto PAP^{-1}$. \n",
    "\n",
    "The symmetric bilinear form  \n",
    "$\\langle A,A'\\rangle = \\text{trace}\\big(AA'\\big)$ \n",
    "is called the *Killing form*  \n",
    "\n",
    "verify for each of the groups $SO_n(\\mathbb R), SU_n, SO_n(\\mathbb C), O_{3,1}, SP_{2n}(\\mathbb R)$  \n",
    "that if $P\\in G$ and $A\\in L$, then $PAP^{-1} \\in L$ and prove that the Killing form is symmetric and bilinear and that  \n",
    "$\\langle A,A'\\rangle = \\langle PAP^{-1}, PA'P^{-1}\\rangle $  \n",
    "\n",
    "*solution:*    \n",
    "working backwards gives an easy result  \n",
    "$ \\langle PAP^{-1}, PA'P^{-1}\\rangle = \\text{trace}\\big(PAP^{-1}PA'P^{-1}\\big) =  \\text{trace}\\big(PAA'P^{-1}\\big)= \\text{trace}\\big(AA'P^{-1}P\\big)=  \\text{trace}\\big(AA'\\big) = \\langle A,A'\\rangle $   \n",
    "by cyclic property of trace \n",
    "\n",
    "the form is symmetric, also because of cyclic property of trace  \n",
    "$\\langle A',A\\rangle = \\text{trace}\\big(AA'\\big)= \\text{trace}\\big(AA'\\big)=\\langle A,A'\\rangle$  \n",
    "\n",
    "and it is linear with respect to scaling one argument due to linearity of the trace  \n",
    "$\\gamma \\langle  A',A\\rangle= \\gamma \\cdot \\text{trace}\\big( AA'\\big)= \\text{trace}\\big((\\gamma A)A'\\big) = \\langle \\gamma A,A'\\rangle = \\text{trace}\\big(A(\\gamma A')\\big)=  \\langle A,\\gamma A'\\rangle$   \n",
    "\n",
    "hence the Killing form is a symmetric bilinear form and compatible with conjugation  \n",
    "\n",
    "- - - -   \n",
    "we now proceed to prove that for each of these classical linear groups,  \n",
    "$P\\in G$ and $A\\in L$, $\\longrightarrow PAP^{-1} \\in L$   \n",
    "\n",
    "we can streamline the proof by observing, as noted in the discussion at the end of part d of in ex 8.Misc.3 in context of the Cayley Transform, we have   \n",
    "$P^T B P = B\\longrightarrow (P')^TB =-BP'$  and  \n",
    "$(P')^TB =-BP' \\longrightarrow P^T B P = B$  \n",
    "for the classical linear groups  \n",
    "a quick look through sections 5 and 6 of the chapter tell us this is the defining relation for the Lie Group to the Lie Algebra (and vice versa)  \n",
    "\n",
    "and in our particular problem $A = P'$ which unfortunately overloads notation, so we compromise by using $A' \\in L$  \n",
    "\n",
    "so we have \n",
    "$(A')^TB =-BA'$  \n",
    "for our Lie algebra.  Now consider  \n",
    "\n",
    "$(PA'P^{-1})^TB =  P^{-T} (A')^T (P^{T}BP)P^{-1} =  P^{-T} (A')^TB P^{-1} = -  P^{-T} BA' P^{-1}= -  (P^{-T} BP^{-1})PA' P^{-1} = B PA' P^{-1}$     \n",
    "hence $PA' P^{-1} \\in L$  \n",
    "as required.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8.6.14**  \n",
    "Prove that the Killing form is negative definite on the Lie Algebra of $SU_n$ and $SO_n(\\mathbb R)$.  \n",
    "\n",
    "*remark:*  \n",
    "we explicitly prove the result for $SU_n$ as the proof for $SO_n$ follows uses a subset (real skew basis \"matrix-vectors\") of the result developed for $SO_n$  \n",
    "\n",
    "$SU_n$ has trace-zero skew-Hermitian matrices as its Lie Algebra.  \n",
    "\n",
    "In some sense negative definiteness is obvious, since for any skew Hermitian matrix $A$, using a quadratic form argument we have  \n",
    "$\\langle A, A\\rangle = \\text{trace}\\big(AA\\big) = \\text{trace}\\big((-A^*)A\\big) = -\\text{trace}\\big(A^*A\\big)= - \\big\\Vert A \\big \\Vert_F^2\\leq 0$   \n",
    "with equality *iff* $A=\\mathbf 0$   \n",
    "\n",
    "However, we elect take the longer route of using tools from chapter 7 and explicitly form an 'orthonormal' basis for these skew hermitian matrices.  This result will be quite useful in interpreting ex 8.6.16.  \n",
    "\n",
    "mimicking page 278, and recalling a that a skew-Hermitian matrix is the same as a Hermitian matrix, except multiplied by $i$, so trace-zero skew-Hermitian matrices, treated as a real vector space, have dimension \n",
    "\n",
    "$n^2-1 = n-1 + \\binom{n}{2}+\\binom{n}{2} =  \\text{number of traceless diagonal matrices} + \\text{real skew matrices} + \\text{symmetric matrices } \\cdot i$  \n",
    "\n",
    "where each traceless diagonal matrix has 2 entries -- an $i$ in component k,k  for $k\\in{1,2,...,n-1}$ and $-i$ in component $n,n$, each real skew matrix has two entries -- plus one in component $k,n-k+1$ such that $n-k+1 \\gt k$ (i.e. upper triangular portion) and a minus on component $n-k+1,k$, and the imaginary symmetric matrices are in essence the same as real skew matrices except they have a plus one in each case (no minus one) and are then rescaled by $i$. For concreteness we give an explicit basis for the 3x3 case, below  \n",
    "\n",
    "  $\\left\\{\\begin{bmatrix}  i &0&0\\\\ 0 &0& 0 \\\\ 0 & 0 & -i \\end{bmatrix}, \\begin{bmatrix}  0 &0&0\\\\ 0 &i& 0 \\\\ 0 & 0 & -i \\end{bmatrix}  , \\begin{bmatrix}  0 &1&0\\\\ -1 &0& 0 \\\\ 0 & 0 & 0 \\end{bmatrix},  \\begin{bmatrix}  0 &0&1\\\\ 0 &0& 0 \\\\ -1\n",
    " & 0 & 0 \\end{bmatrix}, \\begin{bmatrix}  0 &0&0\\\\ 0 &0& 1 \\\\ 0 & -1 & 0 \\end{bmatrix},\\begin{bmatrix}  0 &i&0\\\\ i &0& 0 \\\\ 0 & 0 & 0 \\end{bmatrix},  \\begin{bmatrix}  0 &0&i\\\\ 0 &0& 0 \\\\ i\n",
    " & 0 & 0 \\end{bmatrix}, \\begin{bmatrix}  0 &0&0\\\\ 0 &0& i \\\\ 0 & i & 0 \\end{bmatrix} \n",
    "  \\right\\}$  \n",
    "  \n",
    "we can partition this into generators of 3 real subspaces $\\{\\mathbf D, \\mathbf S, \\mathbf C\\}$ where $\\mathbf D$ has the diagonal matrices, $\\mathbf S$ has real skew matrices, and $\\mathbf C$ has the imaginary symmetric matrices.  (We can also characterize each generator in $\\mathbf S$ as being an upper triangular elementary type 1 matrix, with a one above the diagonal minus its transpose).  \n",
    "\n",
    "note that for $S \\in \\mathbf S$ and $X \\in \\mathbf D$ or $\\mathbf X \\in C$, $X=X^T$ but $S^T=-S$ so  \n",
    "\n",
    "$\\text{trace}\\big(SX\\big)=\\text{trace}\\big(SX\\big)^T=\\text{trace}\\big((SX)^T\\big)=\\text{trace}\\big(X^TS^T\\big) =\\text{trace}\\big(S^TX\\big)=-1\\cdot \\text{trace}\\big(SX\\big)\\longrightarrow \\text{trace}\\big(SX\\big)=0$  \n",
    "thus we see $S$ and $X$ are orthogonal with respect to our bilinear form.  \n",
    "\n",
    "\n",
    "now for each generator $S^{(j)}, S^{(k)}\\in \\mathbf S$  \n",
    "if $j\\neq k$ \n",
    "$\\text{trace}\\big(S^{(j)}S^{(k)}\\big)=0$  \n",
    "by inspection of the underlying elementary matrices  \n",
    "(note: the actual details are to verify orthogonality a bit granular to verify -- however since each vector may be 'normalized' -- see below-- we may run Gram Schmidt, see e.g. ex 7.4.16 in chapter 7 notes, by adapting it to a 'norm' of minus one or just considering the complementary bilinear form given by $-\\text{trace}(AA')$ and dealing with the negative sign at the end)  \n",
    "\n",
    "and  \n",
    "$\\text{trace}\\big(S^{(j)}S^{(j)}\\big)=-2$  \n",
    "we can then 'normalize'-- rescale each generator by $\\frac{1}{\\sqrt{2}}$ so that subsequent to rescaling     \n",
    "$\\text{trace}\\big(S^{(j)}S^{(j)}\\big)=-1$  \n",
    "\n",
    "similarly for each generator $C^{(j)}, C^{(k)}\\in \\mathbf C$, we have    \n",
    "$\\text{trace}\\big(C^{(j)}C^{(k)}\\big)=0$  \n",
    "for identical reasons as above  and  \n",
    "$\\text{trace}\\big(C^{(j)}C^{(j)}\\big) = i\\cdot i \\cdot \\text{trace}\\big((-i\\cdot C^{(j)})(-i\\cdot C^{(j)})\\big)=-1 \\cdot 2$   \n",
    "where we can see $(-i\\cdot C^{(j)})$  is a real symmetric matrix with two ones and all else zero, so the trace of its square is its squared Frobenius norm of 2.  \n",
    "\n",
    "So again we may normalize such that subsequent to normalization  \n",
    "$\\text{trace}\\big(C^{(j)}C^{(j)}\\big) = -1$  \n",
    "\n",
    "thus $\\mathbf S$ and $\\mathbf C$ have mutually ortho-'normal' generators  \n",
    "\n",
    "finally, for $\\mathbf D$  we can see by inspection that $D\\in \\mathbf D$ and $C \\in \\mathbf C$  \n",
    "$\\text{trace}\\big(DC\\big)=0$   \n",
    "because the resulting matrix has all zeros on its diagonal.  This with our earlier work tells us that $D \\in \\mathbf D$ is orthogonal to the subspace $\\mathbf C \\oplus \\mathbf S$.  It remains to consider create an orthonormal set of generators in $\\mathbf D$   \n",
    "\n",
    "now for \n",
    "$\\text{trace}\\big(D^{(j)}D^{(k)}\\big)=(-i)^2 = -1$  \n",
    "and  \n",
    "$\\text{trace}\\big(D^{(j)}D^{(j)}\\big) = 2(-i)^2 = -2$    \n",
    "\n",
    "so at this point we've confirmed at all generators have trace under this bilinear form, and hence all real linear combinations under this form have a real trace. However the structure of the generators of $\\mathbf D$, as is, is not particularly nice.  A better approach to assign  \n",
    "\n",
    "$\\mathbf w_k =  -i\\cdot (D^{(k)}\\circ I)\\mathbf 1$  \n",
    "(i.e. grabbing diagonal components of the matrix, putting them in a vector and rescaling by -i)  \n",
    "and recognize that this gives us a set of real coordinate vectors with n components -- and we have  \n",
    "$\\mathbf w_1, \\mathbf w_2, ..., \\mathbf w_{n-1}$  \n",
    "where this is the subspace such that  \n",
    "\n",
    "$\\mathbf 1^T \\mathbf w_k = 0$, and observing that  \n",
    "$\\mathbf w_j^T \\mathbf w_k = -1\\cdot \\text{trace}\\big((D^{(j)})^T D^{(k)}\\big) = \\text{trace}\\big(D^{(j)}D^{(k)}\\big)$  \n",
    "\n",
    "so after running Gram Schmidt in the typical case, for using the standard inner product (real dot product) on $\\mathbf w_1, \\mathbf w_2, ..., \\mathbf w_{n-1}\\mapsto \\mathbf q_1, \\mathbf q_2, ..., \\mathbf q_{n-1}$, we will then have an orthonormal set in the standard sense, so that we can have ortho-'normal'  \n",
    "\n",
    "$D^{(k)}$ such that  \n",
    "$D^{(k)}: = i\\cdot \\text{diag}\\big(\\mathbf q_k)$\n",
    "\n",
    "after this process of Gram Schmidt terminates we have an ortho-'normal' set of generators for our $n^2-1$ real vector space.  Let the resulting ordered set be called $U$.  \n",
    "\n",
    "Treated abstractly for $\\mathbf v, \\mathbf v' in V$  we have  \n",
    "$\\mathbf v = \\sum_{k=1}^{n^2-1} \\alpha_k U^{(k)}$  \n",
    "and  \n",
    "$\\mathbf v' = \\sum_{k=1}^{n^2-1} \\alpha_k' U^{(k)}$  \n",
    "so  \n",
    "$\\langle \\mathbf v, \\mathbf v'\\rangle = \\text{trace}\\Big(\\big(\\sum_{k=1}^{n^2-1} \\alpha_k U^{(k)}\\big)\\big(\\sum_{j=1}^{n^2-1} \\alpha_j' U^{(j)}\\big)\\Big) = \\sum_{k=1}^{n^2-1} \\alpha_k\\alpha_k' \\cdot \\text{trace}\\Big(U^{(k)}U^{(k)}\\Big) = \\sum_{k=1}^{n^2-1} -\\alpha_k\\alpha_k'$  \n",
    "because $\\text{trace}\\Big(U^{(k)}U^{(j)}\\Big) = -1$ if $j= k$ and is zero otherwise  \n",
    "\n",
    "*another chapter 7 note:* \n",
    "since the Gram Schmidt process is given by the standard real dot-product, we could alternatively create the matrix $\\bigg[\\begin{array}{c|c|c|c} \n",
    "\\mathbf 1 & \\mathbf w_1 & \\mathbf w_2 & \\cdots & \\mathbf w_{n-1}\n",
    "\\end{array}\\bigg]$  \n",
    "and run the standard QR factorization algorithm via the use of Householder reflection matrices -- this is developed as an extension to ex 7.3.9-- and then ignore column 0 in the result matrix, and assign column j to be $\\mathbf q_j$)     \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8.6.15**  \n",
    "$SL_n(\\mathbb R)$ has a Lie Algebra given by real n x n trace zero matrices.  Any $A\\in L$ is given by  \n",
    "\n",
    "$A = \\frac{1}{2}\\big(A+A^T\\big) + \\frac{1}{2}\\big(A-A^T\\big)$  \n",
    "i.e. it is a direct sum of traceless symmetric matrices and skew symmetric matrices (which are naturally traceless).   \n",
    "\n",
    "The former subspace of real symmetric matrices has dimension of $n^2 - \\binom{n}{2}$  and applying the Killing form we have  \n",
    "\n",
    "$\\langle A,A'\\rangle_\\text{Killing form} = \\text{trace}\\big(AA'\\big)= \\text{trace}\\big(A^TA'\\big) = \\langle A,A'\\rangle_\\text{standard Frobenius Inner Product}$  \n",
    "\n",
    "hence the subspace of real symmetric matrices is positive definite.  As a corollary of our work on ex 8.6.14 we know that real skew symmetric matrices are negative definite with respect to the Killing form.  \n",
    "\n",
    "Since the space of Lie Algebras is a direct sum of these two subspaces, we see the signature of the Killing form on the Lie Algebra of $SL_n$ is given by  \n",
    "\n",
    "$\\big(n^2-1 -\\binom{n}{2},\\binom{n}{2}, 0\\big)$   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8.6.16**  \n",
    "(a)  use the adjoint representation of $SU_n$ to define a homomorphism $\\phi: SU_n \\longrightarrow SO_m(\\mathbb R)$ where $m=n^2-1$  \n",
    "\n",
    "*remark:*  \n",
    "the key ideas are on pages 278, 279, top of 280, and  282  \n",
    "\n",
    "*proof:*  \n",
    "We make use of the orthonormal basis for our skew-Hermitian forms developed in ex 8.6.14 Lie Algebra for $SU_n$.   \n",
    "\n",
    "For $P\\in SU_n$ and $A, A' \\in L$, it's Lie Algebra   \n",
    "We have from ex 8.6.14 that $L$ is a real vector space with signature $(0, n^2-1,0)$ and we selected an ortho-'normal' basis for it.   \n",
    "\n",
    "Then mimicking page 278, 279  \n",
    "we know that conjugation by $P$ is a linear operator on a real $n^2-1$ dimensional vector space and thus the map  \n",
    "\n",
    "$P \\mapsto \\phi(P)$  \n",
    "defines a homomorphism $SU_n\\longrightarrow GL_{n^2-1}(\\mathbb R)$  \n",
    "where we confirm invertibility of this map (i.e. so it is in the general linear group) by  \n",
    "$\\phi\\big(P^{-1}\\big)\\phi\\big(P\\big) = \\phi\\big(I_n\\big) = I_{n^2-1}$  \n",
    "\n",
    "further, we consider the **negative**  of the Killing Form, given by  \n",
    "\n",
    "$\\langle A, A'\\rangle = -1\\cdot \\text{trace}\\big(AA'\\big) = \\sum_{k=1}^{n^2-1} \\alpha_k\\alpha_k'$  \n",
    "\n",
    "$A = \\mathbf v = \\sum_{k=1}^{n^2-1} \\alpha_k U^{(k)}$  \n",
    "$A' = \\mathbf v' = \\sum_{k=1}^{n^2-1} \\alpha_k' U^{(k)}$  \n",
    "(see end of ex 8.8.14)  \n",
    "\n",
    "which is a positive definite form given by the standard real dot product of coordinate vectors containing $\\alpha_k$ and $\\alpha_k'$.  And \n",
    "$\\langle \\phi(A), \\phi(A')\\rangle = -1\\cdot \\text{trace}\\big(PAP^{-1}PA'P^{-1}\\big)= -1\\cdot \\text{trace}\\big(AA'P^{-1}P\\big)= -1\\cdot \\text{trace}\\big(AA'\\big) = \\sum_{k=1}^{n^2-1} \\alpha_k\\alpha_k'$  \n",
    "hence $\\phi\\big(P\\big)\\in O_n(\\mathbb R)$   \n",
    "\n",
    "Since $SU_n$ is path connected to the identity, and the homomorphism is continuous and \n",
    "\n",
    "$\\det\\Big(\\phi\\big(I_n\\big)\\Big) = \\det\\Big(I_{n^2-1}\\Big)=1$, then the image of the homorphism is path connected to the identity which implies that the determinant of the homomorphism is 1 (i.e. we have a composition of continuous functions taking on values $\\in \\{-1,1\\}$ so the value is constant and must be equal to one since it we may connect it by a path to $I_{n^2-1}$-- which in effect serves as a boundary condition for us)  \n",
    "\n",
    "**open item:**  \n",
    "how do we know that $\\phi$ is continuous?  \n",
    "suppose we select arbitrary $P \\in SU_n$ and sufficiently close $P'\\in SU_n$  \n",
    "\n",
    "we may confirm point-wise continuity:  for any $A\\in L$  \n",
    "\n",
    "$\\Big\\Vert PAP^{-1} - P'A (P')^{-1}\\Big\\Vert_F = \\Big\\Vert\\big(P - P'\\big)A\\big(P^{-1} -(P')^{-1}\\big)\\Big\\Vert_F \\leq \\Big\\Vert P - P'  \\Big\\Vert_F \\Big\\Vert A  \\Big\\Vert_F  \\Big\\Vert P^{-1} -(P')^{-1} \\Big\\Vert_F \\lt \\epsilon$  \n",
    "\n",
    "where $\\Big\\Vert P - P'  \\Big\\Vert_F$  may be made arbitrarily small by selecting small enough $\\delta$ (it *is* in some sense the delta neighbor hood for our metric) and $\\Big\\Vert P^{-1} -(P')^{-1} \\Big\\Vert_F$ too becomes arbitrarily small, since the inverse is given by the conjugate transpose since each are in $SU_n$.  (An alternative approach would e.g. be to use the vec operator and Kronecker products.)    \n",
    "\n",
    "\n",
    "(b) show that when $n=2$ this representation is equivalent to the orthogonal representation defined in Section 3   omitted.  In essence our argument mirrors the abstract one given on page 278-279, so should generate the same result.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8.7.1**  \n",
    "compute the dimensions of the following groups   \n",
    "\n",
    "(a) $SU_n$   \n",
    "as discussed in ex 7.6.14, $SU_n$ has traceless skew Hermitian matrices as its Lie Algebra and hence has a real vector space dimension of  \n",
    "$n-1 +\\binom{n}{2}$  \n",
    "\n",
    "(b) $SO_n(\\mathbb C) $     \n",
    "has traceless complex skew-Hermitian matrices. That is $(A')^T = -A'$ which implies the diagonal is all zero, and hence these are naturally traceless.  This is a complex vector space that has the same dimension as real skew symmetric matrices --- $\\binom{n}{2}$  \n",
    "\n",
    "(c) $SP_{m}= SP_{2n}$  \n",
    "borrowing from the Cayley Transform in 8.Misc.3.d    \n",
    "we have   \n",
    "$(A')^TJ = -JA'$  \n",
    "or  \n",
    "$(A')^T = -JA'J^T$   \n",
    "(so e.g. we see that $A'$ is traceless and that is 'handled' )  \n",
    "and a matrix in our symplectic group is symplectic if and only if  \n",
    "$\\big((A')^TJ\\big) = Z$ is a real symmetric matrix   \n",
    "\n",
    "so, up to isomorphism, $A'$ -- which is the Lie Algebra-- is real symmetric.  And real symmetric vector spaces have dimension $m^2 - \\binom{m}{2}$   \n",
    "\n",
    "(d) $O_{3,1}$  \n",
    "revisiting the relation  \n",
    "$A^T D A = D\\longrightarrow (A')^TD =-DA'$  \n",
    "\n",
    "where  \n",
    "$D = \\left[\\begin{matrix} \\mathbf I_{3}  & 0\\\\  0  & -1\\end{matrix}\\right]$    \n",
    "\n",
    "or we have  \n",
    "$A'^T = -DA'D $  \n",
    "\n",
    "The top left 3x3 principal submatrix is skew symmetric and thus has dimension $\\binom{3}{2} = 3$.  The bottom row  underneath this block is equal to the right column next to this block, which means implies a vector space dimension of 3.  Finally,   $a_{n,n}' =0$.  Thus the we have dimension of $3+3 = 6$.   \n",
    "\n",
    "a better way to get to this result: the defining equations is  \n",
    "$S = (A')^TD =-DA'$ and $S^T = \\big((A')^TD\\big)^T = D A' = -S$  \n",
    "so $S$ is real skew symmetric and treated as a vector space, it has dimension $\\binom{4}{2}=6$.   \n",
    "of course since $D$ is invertible $A'$ has dimension of $6$ as well   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8.7.2**  \n",
    "$P^2 = I$   \n",
    "$I$ satisfies this... and in a neighborhood of the identity, this is unique (check eigenvalues) or we have  \n",
    "\n",
    "using (6.15) on page 290:  \n",
    "$I = \\big(I+C\\epsilon\\big)\\big(I+C\\epsilon\\big) = I + 2\\epsilon C + 0\\longrightarrow C = 0$  \n",
    "so the solution is unique.  \n",
    "\n",
    "note: the official problem asks for explicit use of the exponential function, though mimicking the argument in for (6.15) $P^TP =I \\longrightarrow PP=I$ (this problem) seems preferable.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8.7.5**  \n",
    "Polar Decomposition  \n",
    "$A$ is an invertible matrix.  And $B$ is Hermitian Positive Definite given by $B=\\big(AA^*\\big)^\\frac{1}{2}$.  \n",
    "\n",
    "**for legacy/prior work reasons, we do a slight change of variable, focusing on**  \n",
    "$B=\\big(A^*A\\big)^\\frac{1}{2}$\n",
    "\n",
    "*(a)* (after change of variables)  \n",
    "show that $U=AB^{-1}$ is unitary    \n",
    "\n",
    "for any $\\mathbf v \\in \\mathbb C^n$   \n",
    "$\\langle U\\mathbf v,U\\mathbf v \\rangle$  \n",
    "$=\\langle AB^{-1}\\mathbf v,AB^{-1}\\mathbf v \\rangle$  \n",
    "$=\\langle B^{-1}\\mathbf v,(A^*A)B^{-1}\\mathbf v \\rangle$  \n",
    "$=\\langle B^{-1}\\mathbf v,(A^*A)^\\frac{1}{2} (A^*A)^\\frac{1}{2} B^{-1}\\mathbf v \\rangle$   \n",
    "$=\\langle (A^*A)^\\frac{1}{2} B^{-1}\\mathbf v,(A^*A)^\\frac{1}{2} B^{-1}\\mathbf v \\rangle$  \n",
    "$=\\langle \\mathbf v,\\mathbf v \\rangle$  \n",
    "where $\\langle, \\rangle$ denotes the standard Frobenius inner product, hence $U$ is unitary  \n",
    "\n",
    "*(b) existence*  \n",
    "show that every invertible $A$ may be written as $A=PU$ for \n",
    "HPD $P$ and unitary $U$  \n",
    "(prior to the change of variables, the original problem says $A=PU$ but it should be $A^*=PU$ or do the change of variables as we have done)  \n",
    "\n",
    "It suffices to prove  \n",
    "$AU^* = P$  where $P \\succ 0$  \n",
    "\n",
    "$AU^* = A(AB^{-1})^* = AB^{-*}A^* = AB^{-1}A^* = P$   \n",
    "where $P$ is congruent to $B^{-1}\\succ \\mathbf 0 \\longrightarrow P\\succ \\mathbf 0$  \n",
    "\n",
    "\n",
    "\n",
    "*(c) uniqueness*  \n",
    "this may be done directly via algebraic manipulations, but we may instead consider using Cauchy-Schwarz, see \"the above approach can be easily used to prove that for invertible\" in  \"Fun_with_Trace_and_Quadratic_Forms_and_CauchySchwarz.ipynb\"   \n",
    "\n",
    "*(d) what does this say about the operation of left multiplication by the unitary group $U_n$ on the group $GL_n$*   \n",
    "**TBD**   \n",
    "\n",
    "\n",
    "**remark**  \n",
    "the Polar Decomposition implies SVD (unitarily diagonalize $P$) and SVD implies Polar Decomposition.  For invertible matrices, Polar decomposition is unique and hence may be preferable.  The Polar Decomposition also has an interesting geometric interpretation when we compute the trace of a square matrix. Further the Polar Decomposition may be generalized to deal with arbitrary rectangular matrices (over $\\mathbb R$ or $\\mathbb C$), though this problem is concerned with \"non-singular\" matrices (original wording in the book), which is a bit ambiguous though your author interpreted this to mean square non-singular i.e. invertible.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "quaternion_list = [sp.Matrix([[1,0],[0,1]]), sp.Matrix([[sp.I,0],[0,-sp.I]]), sp.Matrix([[0,1],[-1,0]]), sp.Matrix([[0,sp.I],[sp.I,0]])]\n",
    "scalar_list = [a,b,c,d]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}a + i b & c + i d\\\\- c + i d & a - i b\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "Matrix([\n",
       "[ a + I*b, c + I*d],\n",
       "[-c + I*d, a - I*b]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = sp.zeros(2,2)\n",
    "for idx in range(len(quaternion_list)):\n",
    "    A += scalar_list[idx]*quaternion_list[idx]  \n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle a^{2} + 2 i a b - b^{2} + c^{2} - 2 i c d - d^{2}$"
      ],
      "text/plain": [
       "a**2 + 2*I*a*b - b**2 + c**2 - 2*I*c*d - d**2"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M[0,0].expand()\n",
    "# so cd = ab and \n",
    "# (a^2 - b^2) - (c^2 -d^2) = 1\n",
    "# which is a bit of a mess  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}0\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "Matrix([[0]])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.T@v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_star = A.conjugate().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}a^{2} + b^{2} + c^{2} + d^{2} & 0\\\\0 & a^{2} + b^{2} + c^{2} + d^{2}\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "Matrix([\n",
       "[a**2 + b**2 + c**2 + d**2,                         0],\n",
       "[                        0, a**2 + b**2 + c**2 + d**2]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B = A_star @ A\n",
    "B.simplify()\n",
    "B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle a^{2} + b^{2} + c^{2} + d^{2}$"
      ],
      "text/plain": [
       "a**2 + b**2 + c**2 + d**2"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.det().expand().simplify()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}0 & 0\\\\0 & 0\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "Matrix([\n",
       "[0, 0],\n",
       "[0, 0]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_star-( a*quaternion_list[0]-b*quaternion_list[1]-c*quaternion_list[2]-d*quaternion_list[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle a^{2} + b^{2} + c^{2} + d^{2}$"
      ],
      "text/plain": [
       "a**2 + b**2 + c**2 + d**2"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result= A.det()\n",
    "result.simplify()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\left[\\begin{matrix}a + i b & c + i d\\\\- c + i d & a - i b\\end{matrix}\\right]\n"
     ]
    }
   ],
   "source": [
    "print(sp.latex(A))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8.Misc.1**   \n",
    "\n",
    "recall from page 48, the basis for quaternions:\n",
    "(with below ordered sets)  \n",
    "$\\Big\\{\\mathbf 1, \\mathbf i, \\mathbf j, \\mathbf k\\Big\\}$  \n",
    "or  \n",
    "$\\Big\\{\\begin{bmatrix} 1 &  0 \\\\ 0 & 1 \\end{bmatrix}, \\begin{bmatrix} i &  0 \\\\ 0 & -i \\end{bmatrix} , \\begin{bmatrix} 0 &  1 \\\\ -1 & 0 \\end{bmatrix}, \\begin{bmatrix} 0 &  i \\\\ i & 0 \\end{bmatrix}\\Big\\}$  \n",
    "\n",
    "so using this matrix representation a quaternion is given by  \n",
    "$\\alpha = A = a\\mathbf 1 + b\\mathbf  i + c \\mathbf j + d\\mathbf k$  \n",
    "$A=\\left[\\begin{matrix}a + i b & c + i d\\\\- c + i d & a - i b\\end{matrix}\\right]$  \n",
    "\n",
    "\n",
    "and  \n",
    "$\\bar{\\alpha} = A^* = a\\mathbf 1 - b\\mathbf  i - c \\mathbf j - d\\mathbf k$  \n",
    "where $a,b,c,d \\in \\mathbb R$  \n",
    "\n",
    "*claim:*    \n",
    "every $ A \\neq \\mathbf 0$ is invertible  \n",
    "\n",
    "*proof:*    \n",
    "$ A$ is the zero matrix *iff* $a=b=c=d=0$ \n",
    "and  \n",
    "$\\det\\big(A\\big) = a^2+b^2+c^2+d^2$  \n",
    "so $ A$ is singular *iff* its sum of real squares is zero, i.e. each coefficient is zero.  \n",
    "\n",
    "Alternative proof:  since  \n",
    "while it is clear that if  $a=b=c=d=0$ then $ A$ has rank zero, since  \n",
    "$\\text{rank}\\big( A^* A\\big) \\leq \\text{rank}\\big( A\\big)$  \n",
    "(working over $\\mathbb C$ this is actually met with equality, but a lower bound suffices) then we see   \n",
    "\n",
    "\n",
    "$A^*A = \\displaystyle \\left[\\begin{matrix}a^{2} + b^{2} + c^{2} + d^{2} & 0\\\\0 & a^{2} + b^{2} + c^{2} + d^{2}\\end{matrix}\\right]$  \n",
    "\n",
    "supposing *not* all of a,b,c,d are zero we have   \n",
    "$2=\\text{rank}\\big( A^* A\\big) \\leq \\text{rank}\\big( A\\big)\\leq 2$  \n",
    "i.e. $ \\text{rank}\\big( A\\big)=2$, so $A$ is invertible, as desired   \n",
    "\n",
    "*claim*  \n",
    "the set of quaternions where each element satisfies $a^2+b^2+c^2+d^2=1$ forms a group under multiplication which is isomorphic to $SU_2$  \n",
    "\n",
    "*proof:*  \n",
    "using the matrix representation (/ we can view this as homomorphism from quaternion space to matrix space) and our work, this is almost immediate, i.e. we see $A^*A= I_2$ and $\\det\\big(A\\big)=1$.  The representation/homorphism is necessarily injective --i.e. 2 matrices agree *iff* they matches for all 4 of a, b,c and d.  As for surjectivity, consider an argument similar to that used in chapter 7:  suppose we want to construct some arbitrary element $Q$ of $SU_2$ 1 column at a time-- we show this may be done by selection of $a$, $b$,$c$,$d$  \n",
    "by via inspection:  \n",
    "by selection of choice of $a,b,c,d$ such that $a^2+b^2+c^2+d^2=1$ we may select *any* 2x1 column vector (interpreted as a matrix) $\\mathbf z$ that has norm of one.  \n",
    "\n",
    "Then for finding orthogonal vectors, consider \n",
    "$\\mathbf z^*$ has rank 1 and by rank-nullity $\\dim\\ker\\big(\\mathbf z^*\\big) = 1$, so we know that up to rescaling there is exactly one vector $\\mathbf v$ that is orthogonal to $\\mathbf z$, and fixing the length of $\\mathbf v$ to be 1, that means we have $\\gamma \\cdot \\mathbf v$ for some $\\gamma$ on the unit circle..  \n",
    "\n",
    "$Q = \\left[\\begin{matrix}\\mathbf z & \\gamma\\mathbf v\\end{matrix}\\right]$  \n",
    "and $\\det\\big(Q\\big) =1$ which, using multilinearity of the determinant uniquely specifies $\\gamma$ -- which implies $Q=A$.  Since the selection of $Q$ was arbitrary, we see the mapping/(representation) is surjective as well as injective and hence we have an isomorphism.  \n",
    "\n",
    "*remark:*  \n",
    "the above result is interesting to consider in connection with the double covering / choice of spin homomorphism from $SU_2\\big(\\mathbb C\\big) \\longrightarrow SO_3\\big(\\mathbb R\\big)$, especially since quaternions are commonly used in creating / modeling 3d graphics    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8.Misc.2**   \n",
    "(this is closely related to ex 5.2.14 on page 189)  \n",
    "\n",
    "For the n-dimensional affine group,  using matrix representation / subgroup of the form  \n",
    "$P = \\left[\\begin{matrix} A  & \\mathbf t\\\\  0  & 1\\end{matrix}\\right]$  \n",
    "and  \n",
    "$P' = \\left[\\begin{matrix} A'  & \\mathbf t'\\\\  0  & 1\\end{matrix}\\right]$  \n",
    "$PP' =\\left[\\begin{matrix} AA'  & *\\\\  0  & 1\\end{matrix}\\right]$  \n",
    "(ref ex 2.4.12 in the chp2 notebook for more details on block triangular matrix subgroups)  \n",
    "$A, A' \\in GL_n\\big(\\mathbb R\\big)$\n",
    "\n",
    "and $P,P'$ act on  \n",
    "$\\mathbf x = \\left[\\begin{matrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\x_n  \\\\  1 \\end{matrix}\\right]$   \n",
    "using standard matrix multiplication given by $P\\mathbf x$  \n",
    "\n",
    "so the subgroup $T_n$ is given by  \n",
    "$P = \\left[\\begin{matrix} I_n  & \\mathbf t\\\\  0  & 1\\end{matrix}\\right]$  \n",
    "\n",
    "In particular construct the homomorphism that maps from our affine group to (something isomorphic to) the $GL_n$ (i.e. $GL_n$  embedded as an n+1 x n+1 matrix subgroup)  \n",
    "\n",
    "$\\phi\\big(P\\big) =P = \\left[\\begin{matrix} A  & \\mathbf 0\\\\  0  & 1\\end{matrix}\\right]$  \n",
    "  \n",
    "where the homomorphism $\\phi$ quotients out $T_n$ and  \n",
    "\n",
    "the kernel of this homomorphism is  \n",
    "$\\left[\\begin{matrix} I_n & A^{-1}\\mathbf t\\\\  0  & 1\\end{matrix}\\right]= \\left[\\begin{matrix} I_n & \\mathbf t''\\\\  0  & 1\\end{matrix}\\right]$   \n",
    "and (referencing page 52), we have that the kernel of a homomorphism is a normal subgroup.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8.Misc.3  Cayley Transform**    \n",
    "\n",
    "let $U$ be the set of matrices that do not have minus one as an eigenvalue, i.e.  we define $U$ as  \n",
    "$A\\in U\\longrightarrow \\big(A+I\\big)^{-1} \\text{ exists}$   \n",
    "\n",
    "**note:** *the results are focused on* $\\mathbb R$ and $\\mathbb C$ *in this chapter, though the results for this problem really hold over any field with characteristic* $\\neq 2$.  \n",
    "*Also see ex 7.8.5 and the extension that follows in the chapter 7 notebook, which relates the Cayley Transform to a special case of the stereographic projection*         \n",
    "\n",
    "$A':=\\big(I-A\\big)\\big(I+A\\big)^{-1} = \\big(I+A\\big)^{-1}-A\\big(I+A\\big)^{-1}$  \n",
    "\n",
    "**a)** we show $\\big(I+A'\\big)^{-1} \\text{ exists}$  and that $A'' = A$ i.e. we have an involutary operation   \n",
    "\n",
    "first  \n",
    "$I+A'= I  + \\big(I-A\\big)\\big(I+A\\big)^{-1}= \\big(I+A\\big)\\big(I+A\\big)^{-1} +\\big(I-A\\big)\\big(I+A\\big)^{-1}= \\big(I+A + I - A\\big)\\big(I+A\\big)^{-1} = 2\\big(I+A\\big)^{-1}$  \n",
    "$\\det\\Big(I+A'\\Big)= \\det\\Big(2\\big(I+A\\big)^{-1}\\Big)\\neq 0 \\longrightarrow \\big(I+A'\\big)^{-1}=\\frac{1}{2}\\big(I+A\\big)$  \n",
    "\n",
    "\n",
    "second  \n",
    "$A''$  \n",
    "$= \\big(I-A'\\big)\\big(I+A'\\big)^{-1}$  \n",
    "$= \\big(I-\\big(I-A\\big)\\big(I+A\\big)^{-1}\\big)\\big(I+\\big(I-A\\big)\\big(I+A\\big)^{-1}\\big)^{-1}$    \n",
    "$= \\Big\\{\\Big(\\big(I+A\\big)\\big(I+A\\big)^{-1}-\\big(I-A\\big)\\big(I+A\\big)^{-1}\\Big)\\Big\\}\\Big(\\big(I+A\\big)\\big(I+A\\big)^{-1}+\\big(I-A\\big)\\big(I+A\\big)^{-1}\\Big)^{-1}$  \n",
    "$= \\Big\\{2A\\big(I+A\\big)^{-1}\\Big\\}\\Big(\\big(I+A\\big)\\big(I+A\\big)^{-1}+\\big(I-A\\big)\\big(I+A\\big)^{-1}\\Big)^{-1}$   \n",
    "$=   2A\\big(I+A\\big)^{-1}\\Big(2\\big(I+A\\big)^{-1}\\Big)^{-1}$  \n",
    "$=   A\\big(I+A\\big)^{-1}\\big(I+A\\big)$  \n",
    "$=A$  \n",
    "as desired  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b)** note its is immediate that if we have real skew symmetric matrices without eigenvalues (e.g. see ex 7.8.4) then the Cayley Transform maps them to the special orthogonal group in a way that is continuous because it involves matrix multiplication.  Similarly if we have real special orthogonal matrices they are mapped to real skew symmetric matrices see above section \"alternative approach to proving path connectivity for  $SO_3\\big(\\mathbb C\\big)$\" -- the computation follows identically for any n though we are interested in the nicer subgroup of real special orthogonal matrices.  This inverse operation is necessarily continuous as well, and hence we have a homeomorphism, in particular for real skew symmetric matrices in the neighborhood of zero to real special orthogonal matrices in the neighborhood of the identity.  \n",
    "\n",
    "**c)** the argument is virtually identical as in (b) to show that skew hermitian matrices in the neighborhood of zero are homeomorphic to the special unitary group in the neighborhood of the identity.     \n",
    "\n",
    "**d)** Letting $J_{2n}$ or simply $J$ denote the standard symplectic matrix, whose stabilizer defines the symplectic group  \n",
    "i.e.$A$ is symplectic *iff* $A^TJ A = J$ for *real* $A$  \n",
    "\n",
    "prove $A\\in U$ is symplectic *iff*  \n",
    "$(A')^TJ = -JA'$  \n",
    "\n",
    "*remark:*  \n",
    "if we compute the transpose  \n",
    "$\\big((A')^TJ\\big)^T = J^T A' = -JA'$  \n",
    "so $A$ is symplectic if and only if    \n",
    "$\\big((A')^TJ\\big)$ is (real) symmetric  \n",
    "\n",
    "**remark:**  \n",
    "this uniquely characterizes all real symplectic matrices -- except as always, it is silent in the case of matrices with an eigenvalue of -1.  \n",
    "\n",
    "**leg one:**    \n",
    "$A$ is symplectic $\\longrightarrow$  $(A')^TJ = -JA'$   \n",
    "  \n",
    "$J=A^TJA = \\big(I-(A')^T\\big)\\big(I+(A')^T\\big)^{-1}J\\big(I-A'\\big)\\big(I+A')^{-1}$   \n",
    "taking advantage of commutativity and clearing the inverses      \n",
    "$LHS = \\big(I+(A')^T\\big)J\\big(I+A')= \\big(I-(A')^T\\big)J\\big(I-A'\\big) = RHS$ \n",
    "$LHS = \\big(J+(A')^TJ \\big)\\big(I+A'\\big) = J+JA'+(A')^TJ + (A')^TJA'$  \n",
    "$RHS = \\big(J-(A')^TJ\\big)\\big(I-A'\\big) = J -JA'-(A')^TJ + (A')^TJA'$  \n",
    "so  \n",
    "$0 = LHS - RHS = 2JA'+ 2(A')^TJ \\longrightarrow (A')^TJ =-JA'$   \n",
    "\n",
    "**leg two:**  \n",
    "$(A')^TJ = -JA'$ $\\longrightarrow$ $A$ is symplectic   \n",
    "\n",
    "note $J^{-1} =J^T = -J$  \n",
    "$A^TJA$  \n",
    "$=\\big(I-(A')^T\\big)\\big(I+(A')^T\\big)^{-1}J\\big(I-A'\\big)\\big(I+A'\\big)$  \n",
    "$=-1\\cdot\\big(I-(A')^T\\big)\\big(I+(A')^T\\big)^{-1}J^{-1}\\big(I-A'\\big)\\big(I+A'\\big)$  \n",
    "$=-1\\cdot\\big(I-(A')^T\\big)\\big(J+(A')^TJ\\big)^{-1}\\big(I-A'\\big)\\big(I+A'\\big)$  \n",
    "$=-1\\cdot\\big(I-(A')^T\\big)\\big(J-JA'\\big)^{-1}\\big(I-A'\\big)\\big(I+A'\\big)$  \n",
    "$=-1\\cdot\\big(I-(A')^T\\big)J^{-1}\\big(I-A'\\big)^{-1}\\big(I-A'\\big)\\big(I+A'\\big)$  \n",
    "$=\\big(I-(A')^T\\big)\\big(-1\\cdot J^{-1}\\big)\\big(I-A'\\big)^{-1}\\big(I-A'\\big)\\big(I+A'\\big)$  \n",
    "$=\\big(I-(A')^T\\big)\\big(J\\big)\\big(I-A'\\big)^{-1}\\big(I-A'\\big)\\big(I+A'\\big)$  \n",
    "$=\\big(J-(A')^TJ\\big)\\big(I-A'\\big)^{-1}\\big(I-A'\\big)\\big(I+A'\\big)$  \n",
    "$=\\big(J+JA'\\big)\\big(I-A'\\big)^{-1}\\big(I-A'\\big)\\big(I+A'\\big)$  \n",
    "$=J\\big(I+A'\\big)\\big(I-A'\\big)^{-1}\\big(I-A'\\big)\\big(I+A'\\big)$  \n",
    "$=J$  \n",
    "\n",
    "*remark*  \n",
    "Leg two explicitly uses the fact that $J^{-1} = -J$.  On the other hand leg one does not use any explicit properties of $J$, so (again ignoring the field of characteristic 2 case) we can consider the general stabilizer equation for $A\\in U$ and see that  \n",
    "\n",
    "$A^T B A = B\\longrightarrow (A')^TB =-BA'$   \n",
    "\n",
    "*additional remark*  \n",
    "while leg one works for any form (ignoring field of characteristic 2), leg two used \n",
    "$J^{-1} = -J$, where $J$ has period 4 i.e. $J^4=I$  \n",
    "\n",
    "a careful re-examination of the proof tells us that period 2 for $J$ (or $B$) would work as well.  i.e. if $J$ (or $B$) is involutive with $J^2=I$. In fact any case where $J^{-1} = \\alpha \\cdot J$ would work (with the constraint of determinant being one, and using m x m matrices, this would imply $\\alpha$ is an mth root of unity).  The involutive case however is the main other special case of interest as it covers the special orthogonal group, and  generalizations thereof like the Lorenz Form, as well as the special unitary group (where transposition is changed to become conjugate transpose).  \n",
    "\n",
    "*remark:*   \n",
    "compare this with the methodology on page 290.  The *iff* and only *iff* criterion for the classical groups seems to be the same -- i.e. the Cayley Transform from a Lie Group seems to give us the elements of the Lie Algebra (and one more application gives us the elements of the Lie Group again)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**application of Cayley Transform:**  \n",
    "generating symplectic matrices (without eigenvalue of -1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}1 & 0 & 0 & 0\\\\0 & 1 & 0 & 0\\\\0 & 0 & 1 & 0\\\\0 & 0 & 0 & 1\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "Matrix([\n",
       "[1, 0, 0, 0],\n",
       "[0, 1, 0, 0],\n",
       "[0, 0, 1, 0],\n",
       "[0, 0, 0, 1]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp.eye(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Cayley_Transform(M):\n",
    "    \"\"\"\"Cayley Transform to be applied to numpy arrays or sympy matrices\"\"\"      \n",
    "    try: \n",
    "        # for numpy arrays\n",
    "        I = np.identity(M.shape[0])\n",
    "        inverse_part = np.linalg.inv(I+M)        \n",
    "    except TypeError: \n",
    "        # triggered when working with a sympy matrix  \n",
    "        I = sp.eye(M.shape[0])\n",
    "        inverse_part = (I+M).inv()\n",
    "        \n",
    "    return (I-M)@ inverse_part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 4\n",
    "assert (m % 2 == 0)\n",
    "\n",
    "## creation of symplectic J\n",
    "J = np.zeros((m,m))\n",
    "j = -1\n",
    "for i in reversed(range(m)):\n",
    "    j +=1\n",
    "    if i>(m-1)/2:\n",
    "        J[i,j]= 1\n",
    "    else:\n",
    "        J[i,j]= -1\n",
    "\n",
    "J_sp = sp.zeros(m,m)\n",
    "j = -1\n",
    "for i in reversed(range(m)):\n",
    "    j +=1\n",
    "    if i>(m-1)/2:\n",
    "        J_sp[i,j]= 1\n",
    "    else:\n",
    "        J_sp[i,j]= -1\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# B = np.random.random((m,m))\n",
    "# B = np.random.normal(0,1,m**2)\n",
    "B = np.random.randint(-10000, 10000, m**2)\n",
    "B = np.reshape(B,(m,m))\n",
    "\n",
    "B\n",
    "\n",
    "# B = 1/2*(B+B.T) \n",
    "B = B+B.T  \n",
    "\n",
    "# -J(A') = B, symmetric B\n",
    "# -J^TB = A' \n",
    "A_prime = -J.T@B\n",
    "A = Cayley_Transform(A_prime)\n",
    "assert(np.linalg.norm(A.T@J@A - J)<0.00001)\n",
    "# Frobenius Norm -- setting aside numerical issues, should be 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.21e+42, -9.17e+41,  1.26e+42,  1.13e+38],\n",
       "       [ 4.31e+41,  6.18e+41, -4.23e+37, -1.26e+42],\n",
       "       [ 9.09e+41, -2.52e+37,  6.18e+41, -9.17e+41],\n",
       "       [ 4.51e+37, -9.09e+41,  4.31e+41,  1.21e+42]])"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C_placeholder = np.identity(m)\n",
    "C = np.zeros((m,m))\n",
    "\n",
    "for k in range(10):\n",
    "    C_placeholder = C_placeholder @ A_prime\n",
    "    C += C_placeholder\n",
    "#     print(C_placeholder)\n",
    "    \n",
    "# so this 'skewness' relative to anti-diagonal is not quite preserved under products \n",
    "# (it is preserved under sums)\n",
    "C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.+0.j, -1.-0.j, -1.+0.j, -1.+0.j])"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eigs = np.linalg.eigvals(A)\n",
    "eigs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "rounding_threshold = 2\n",
    "B_sp = sp.Matrix(np.round(B,rounding_threshold))\n",
    "A_prime_sp = -J_sp.T@B_sp\n",
    "A_prime_sp  \n",
    "A_sp = Cayley_Transform(A_prime_sp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}- \\frac{13788702071717}{13777846496662} & - \\frac{12627535433}{68889232483310} & \\frac{76833579683}{68889232483310} & \\frac{86580575143}{103333848724965}\\\\- \\frac{57075963611}{41333539489986} & - \\frac{41348164841159}{41333539489986} & \\frac{26190348581}{13777846496662} & \\frac{69170320514}{62000309234979}\\\\- \\frac{10869807379}{41333539489986} & - \\frac{41716794701}{206667697449930} & - \\frac{68864836345033}{68889232483310} & \\frac{56897054576}{310001546174895}\\\\- \\frac{11935162993}{13777846496662} & - \\frac{3620046331}{13777846496662} & \\frac{19023793813}{13777846496662} & - \\frac{20650483388609}{20666769744993}\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "Matrix([\n",
       "[-13788702071717/13777846496662,    -12627535433/68889232483310,     76833579683/68889232483310,    86580575143/103333848724965],\n",
       "[   -57075963611/41333539489986, -41348164841159/41333539489986,     26190348581/13777846496662,     69170320514/62000309234979],\n",
       "[   -10869807379/41333539489986,   -41716794701/206667697449930, -68864836345033/68889232483310,    56897054576/310001546174895],\n",
       "[   -11935162993/13777846496662,     -3620046331/13777846496662,     19023793813/13777846496662, -20650483388609/20666769744993]])"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\operatorname{PurePoly}{\\left( \\lambda^{4} + \\frac{275556899400913}{68889232483310} \\lambda^{3} + \\frac{372001800451685}{62000309234979} \\lambda^{2} + \\frac{275556899400913}{68889232483310} \\lambda + 1, \\lambda, domain=\\mathbb{Q} \\right)}$"
      ],
      "text/plain": [
       "PurePoly(lambda**4 + 275556899400913/68889232483310*lambda**3 + 372001800451685/62000309234979*lambda**2 + 275556899400913/68889232483310*lambda + 1, lambda, domain='QQ')"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_sp.charpoly()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}0 & 0 & 0 & -1\\\\0 & 0 & -1 & 0\\\\0 & 1 & 0 & 0\\\\1 & 0 & 0 & 0\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "Matrix([\n",
       "[0, 0,  0, -1],\n",
       "[0, 0, -1,  0],\n",
       "[0, 1,  0,  0],\n",
       "[1, 0,  0,  0]])"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_sp.T@J_sp@A_sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}- \\frac{13788702071717}{13777846496662} & - \\frac{12627535433}{68889232483310} & \\frac{76833579683}{68889232483310} & \\frac{86580575143}{103333848724965}\\\\- \\frac{57075963611}{41333539489986} & - \\frac{41348164841159}{41333539489986} & \\frac{26190348581}{13777846496662} & \\frac{69170320514}{62000309234979}\\\\- \\frac{10869807379}{41333539489986} & - \\frac{41716794701}{206667697449930} & - \\frac{68864836345033}{68889232483310} & \\frac{56897054576}{310001546174895}\\\\- \\frac{11935162993}{13777846496662} & - \\frac{3620046331}{13777846496662} & \\frac{19023793813}{13777846496662} & - \\frac{20650483388609}{20666769744993}\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "Matrix([\n",
       "[-13788702071717/13777846496662,    -12627535433/68889232483310,     76833579683/68889232483310,    86580575143/103333848724965],\n",
       "[   -57075963611/41333539489986, -41348164841159/41333539489986,     26190348581/13777846496662,     69170320514/62000309234979],\n",
       "[   -10869807379/41333539489986,   -41716794701/206667697449930, -68864836345033/68889232483310,    56897054576/310001546174895],\n",
       "[   -11935162993/13777846496662,     -3620046331/13777846496662,     19023793813/13777846496662, -20650483388609/20666769744993]])"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle - \\frac{275556899400913}{68889232483310}$"
      ],
      "text/plain": [
       "-275556899400913/68889232483310"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_sp.trace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_sp.rank()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}8 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "Matrix([\n",
       "[8, 0, 0, 0, 0, 0, 0, 0],\n",
       "[0, 0, 0, 0, 0, 0, 0, 0],\n",
       "[0, 0, 0, 0, 0, 0, 0, 0],\n",
       "[0, 0, 0, 0, 0, 0, 0, 0],\n",
       "[0, 0, 0, 0, 0, 0, 0, 0],\n",
       "[0, 0, 0, 0, 0, 0, 0, 0],\n",
       "[0, 0, 0, 0, 0, 0, 0, 0],\n",
       "[0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}- \\frac{13788702071717}{13777846496662} & - \\frac{12627535433}{68889232483310} & \\frac{76833579683}{68889232483310} & \\frac{86580575143}{103333848724965}\\\\- \\frac{57075963611}{41333539489986} & - \\frac{41348164841159}{41333539489986} & \\frac{26190348581}{13777846496662} & \\frac{69170320514}{62000309234979}\\\\- \\frac{10869807379}{41333539489986} & - \\frac{41716794701}{206667697449930} & - \\frac{68864836345033}{68889232483310} & \\frac{56897054576}{310001546174895}\\\\- \\frac{11935162993}{13777846496662} & - \\frac{3620046331}{13777846496662} & \\frac{19023793813}{13777846496662} & - \\frac{20650483388609}{20666769744993}\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "Matrix([\n",
       "[-13788702071717/13777846496662,    -12627535433/68889232483310,     76833579683/68889232483310,    86580575143/103333848724965],\n",
       "[   -57075963611/41333539489986, -41348164841159/41333539489986,     26190348581/13777846496662,     69170320514/62000309234979],\n",
       "[   -10869807379/41333539489986,   -41716794701/206667697449930, -68864836345033/68889232483310,    56897054576/310001546174895],\n",
       "[   -11935162993/13777846496662,     -3620046331/13777846496662,     19023793813/13777846496662, -20650483388609/20666769744993]])"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Z = J_sp + sp.eye(m)\n",
    "\n",
    "\n",
    "H = sp.zeros(m)\n",
    "C_sp = sp.eye(m)\n",
    "\n",
    "for i in range(m):\n",
    "#     print(\"\\n\")\n",
    "    for j in range(0,i+1):\n",
    "        idx = i-j\n",
    "#         print(idx,j)\n",
    "        H[idx,j] = sp.trace(C_sp)\n",
    "    C_sp = C_sp @ A_sp\n",
    "#     C_sp = C_sp @ Z\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}4 & - \\frac{275556899400913}{68889232483310} & \\frac{170846072956483526680269555121}{42711537169255803024857004900} & - \\frac{3923149439449905735617498194686017515813691}{980788337924133103732899379108291946073000}\\\\- \\frac{275556899400913}{68889232483310} & \\frac{170846072956483526680269555121}{42711537169255803024857004900} & - \\frac{3923149439449905735617498194686017515813691}{980788337924133103732899379108291946073000} & 0\\\\\\frac{170846072956483526680269555121}{42711537169255803024857004900} & - \\frac{3923149439449905735617498194686017515813691}{980788337924133103732899379108291946073000} & 0 & 0\\\\- \\frac{3923149439449905735617498194686017515813691}{980788337924133103732899379108291946073000} & 0 & 0 & 0\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "Matrix([\n",
       "[                                                                                      4,                                                         -275556899400913/68889232483310,                            170846072956483526680269555121/42711537169255803024857004900, -3923149439449905735617498194686017515813691/980788337924133103732899379108291946073000],\n",
       "[                                                        -275556899400913/68889232483310,                            170846072956483526680269555121/42711537169255803024857004900, -3923149439449905735617498194686017515813691/980788337924133103732899379108291946073000,                                                                                       0],\n",
       "[                           170846072956483526680269555121/42711537169255803024857004900, -3923149439449905735617498194686017515813691/980788337924133103732899379108291946073000,                                                                                       0,                                                                                       0],\n",
       "[-3923149439449905735617498194686017515813691/980788337924133103732899379108291946073000,                                                                                       0,                                                                                       0,                                                                                       0]])"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0]"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "determinant_list = [0 for i in range(r)]\n",
    "determinant_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not positive definite per Sylvester's Criterion because\n",
      "2 : -151441069878603668881637/42711537169255803024857004900\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[\\begin{matrix}4 & - \\frac{275556899400913}{68889232483310}\\\\- \\frac{275556899400913}{68889232483310} & \\frac{170846072956483526680269555121}{42711537169255803024857004900}\\end{matrix}\\right]$"
      ],
      "text/plain": [
       "Matrix([\n",
       "[                              4,                              -275556899400913/68889232483310],\n",
       "[-275556899400913/68889232483310, 170846072956483526680269555121/42711537169255803024857004900]])"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = H.rank()\n",
    "all_positive = True \n",
    "determinant_list = [(H[0:k,0:k]).det() for k in range(1,r+1)]\n",
    "\n",
    "for i in range(r):\n",
    "    if determinant_list[i]<=0:\n",
    "        print(\"not positive definite per Sylvester's Criterion because\")\n",
    "        print(i + 1,\":\", determinant_list[i])\n",
    "        break\n",
    "H[0:i+1,0:i+1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
